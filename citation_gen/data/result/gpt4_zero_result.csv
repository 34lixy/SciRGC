prompt,answer,responses
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a system which automatically generates shallow semantic frame structures for conversational speech in unrestricted domains. We argue that such shallow semantic representations can indeed be generated with a minimum amount of linguistic knowledge engineering and without having to explicitly construct a semantic knowledge base. The system is designed to be robust o deal with the problems of speech dysfluencies, ungrammaticalities, and imperfect speech recognition. Initial results on speech transcripts are promising in that correct mappings could be identified in 21% of the clauses of a test set (resp. 44% of this test set where ungrammatical or verb-less clauses were removed).
###Abstract of the cited paper: We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate hat a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount 1.
###Context text: Components We use the following resources to build our system: ? the SWITCHBOARD #OTHEREFR with a POS based grammar to generate the chunks 3 (phrasal constituents) . WordNet 1.5 #OTHEREFR for the extraction of subcategorization (subcat) frames for all senses of a verb (including semantic features, such as ""animacy') ? a mapper which tries to find the ""best match"" between the chunks found within a short clause and the subcat frames for the main verb in that clause The major blocks of the system architecture are depicted in Figure I. We want to stress here that except for the development of the small POS grammar and the framemapper, the other components and resources were already present or quite simple to implement.[Citation]Figure 1: Global system architecture without he important knowledge source from Word- Net, a similar system could be built for other languages as well. Also, the Euro-WordNet project #OTHEREFR is currently underway in building WordNet resources for other European languages.
###Response:","There has also been significant work on #OTHEREFR; #REFR, such that even 3More details about the chunk parser can be found in #OTHEREFR. input urerance speech recognizer hypothesis L I POS tagger , prepro e. ,ngp p II II; chun par er II chunk sequence li II frame representation ","The system also incorporates a subcategorization dictionary constructed from textual corpora, as explained by #REFR, which enhances the accuracy of the parser."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we propose a novel neural network model called RNN Encoder. Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder?Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.
###Abstract of the cited paper: We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets.
###Context text: This is closely related to both the proposed RNN Encoder?Decoder and the model proposed in #OTHEREFR, except that their input representation of a phrase is a bag-of-words. A similar approach of using bag-of-words representations was proposed in #OTHEREFR as well. Earlier, a similar encoder?decoder model using two recursive neural networks was proposed in #OTHEREFR, but their model was restricted to a monolingual setting, i.e. the model reconstructs an input sentence.[Citation]One important difference between the proposed RNN Encoder?Decoder and the approaches in #OTHEREFR is that the order of the words in source and target phrases is taken into account. The RNN Encoder?Decoder naturally distinguishes between sequences that have the same words but in a different order, whereas the aforementioned approaches effectively ignore order information. The closest approach related to the proposed RNN Encoder?Decoder is the Recurrent Continuous Translation Model #OTHEREFR.
###Response:","More recently, another encoder?decoder model using an RNN was proposed in #REFR, where the decoder is conditioned on a representation of either a source sentence or a source context.","The joint language and translation model proposed by #REFR, which recognizes both source and target words' unbounded history, has enhanced the translation accuracy."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Spelling-checkers have become an integral part of most text processing software. From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words. This approach is sufficient for languages with little inflection such as English, but fails for highly inflective languages such as Czech, Russian, Slovak or other Slavonic languages. We have developed a special method for describing inflection for the purpose of building spelling-checkers for such languages. The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy, whereas the number of recognized word forms exceeds 6 million (for Czech). Further, a special method has been developed for easy word classification.
###Abstract of the cited paper: The SRI Core Language Engine (CLE) is a general-purpose natural anguage front end for interactive systems. It translates English expressions into representations of their literal meanings. This paper , presents the lexical acquisition component of the CLE, which allows the creation of lexicon entries by users with knowledge of the application domain but not of linguistics or of the detailed workings of the system. It is argued that the need to cater for a wide range of types of back end leads naturally to an approach based on eliciting grammaticality judgments from the user. This approach, which has been used to define a 1200-word core lexicon of En- ? glish, is described and evaluated.
###Context text: The system also facilitates the process of adding word forms to the user is own dictionary. Due to the reasons discussed above, this causes problems, as the other forms of that word cannot be included fully automatically. An algorithm exists (see below) how to accomplish this task with the user is assisstance.[Citation]The hnplementation f the algoritlma together with its user interface will be included as an off-line utility (in the first version, available in autumn '89, there was no such utility; it should be included in the second version).
###Response:","The idea is similar to Finkler and Neumann #OTHEREFR, though simplified for our purposes; #REFR in his VEX system also uses the method of giving sunple questions to the user (supposedly non-linguist) to learn about word is behaviour, but it is for English and primarily intended for assigning syntax properties rather than morphological.","Drawing inspiration from the SRI Core Language Engine that allows non-linguists to create lexicon entries, a user-assisted augmentation approach is proposed to tackle this problem [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we propose a joint model for unsupervised Chinese word segmentation (CWS). Inspired by the ?products of experts? idea, our joint model firstly combines two generative models, which are word-based hierarchical Dirichlet process model and character-based hidden Markov model, by simply multiplying their probabilities together. Gibbs sampling is used for model inference. In order to further combine the strength of goodness-based model, we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler. We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff. Test results on these two datasets show that the joint model achieves much better results than all of its component models. Statistical significance tests also show that it is significantly better than stateof-the-art systems, achieving the highest F-scores. Finally, analysis indicates that compared with nVBE and HDP, the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation.
###Abstract of the cited paper: The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation. Twenty three groups submitted 130 result sets over two tracks and four different corpora. We found that the technology has improved over the intervening two years, though the out-of-vocabulary problem is still or paramount importance. 1! Introduction Chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing applications including monoand cross-lingual information retrieval and text-to-speech systems. This word segmentation problem has been active area of research in computational linguistics for almost two decades and is a topic of active research around the world. As the very notion of ?word-hood? in Chinese is hotly debated, so the determination of the correct division of a Chinese sentence into ?words? can be very complex. In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International Chinese Word Segmentation Bakeoff (Sproat and Emerson, 2003). That competition was the first conducted outside of China and has become the benchmark with which researchers evaluate their segmentation systems. During the winter of 2004 it was decided to hold a second evaluation to determine how the latest research has affected segmentation technology. 2! Details of the Contest 2.1! The Corpora Four corpora were used in the evaluation, two each using Simplified and Traditional Chinese characters.1 The Simplified Chinese corpora were provided by Beijing University and Microsoft Research Beijing. The Traditional Chinese corpora were provided by Academia Sinica in Taiwan and the City University of Hong Kong. Each provider supplied separate training and truth data sets. Details on each corpus are provided in Table!1. With one exception, all of the corpora were provided in a single character encoding. We decided to provide all of the data in both Unicode (UTF-8 encoding) and the standard encoding used in each locale. This would allow systems that use one or the other encoding to chose appropriately while ensuring consistent transcoding across all sites. This conversion was problematic in two cases:
###Context text: [Citation]Most previous works reported their results on these two datasets, this will make it convenient to directly compare our joint model with theirs.
###Response:","In this section, we test our joint model on PKU and MSRA datesets provided by the Second Segmentation Bake-off #OTHEREFR #REFR.","""Drawing on the findings from the second international Chinese word segmentation bakeoff as described in #REFR, our joint model was tested on same datasets for accurate comparison."""
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Even though, a lot of research has already been done on Machine Translation, translating complex sentences has been a stumbling block in the process. To improve the performance of machine translation on complex sentences, simplifying the sentences becomes imperative. In this paper, we present a rule based approach to address this problem by simplifying complex sentences in Hindi into multiple simple sentences. The sentence is split using clause boundaries and dependency parsing which identifies different arguments of verbs, thus changing the grammatical structure in a way that the semantic information of the original sentence stay preserved.
###Abstract of the cited paper: Lottg alld eolni)licated seltteltces prov(: to b(: a. stumbling block for current systems relying on N\[, input. These systenls stand to gaill frolil ntethods that syntacti<:aHy simplily su<:h sentences. '\]b simplify a sen= tence, we nee<t an idea of tit(."" structure of the sentence, to identify the <:omponents o be separated out. Obviously a parser couhl be used to obtain the complete structure of the sentence. \]\[owever, hill parsing is slow a+nd i)rone to fa.ilure, especially on <:omph!x sentences. In this l)aper, we consider two alternatives to fu\]l parsing which could be use<l for simplification. The tirst al)l)roach uses a Finite State Grammar (FSG) to prodn<:e noun and verb groups while the second uses a Superta.gging model to i)roduce dependency linkages. We discuss the impact of these two input representations on the simplification pro(:ess.
###Context text: In this section we try to identify the definition of sentence complexity in the context of machine translation. In general, complex sentences have more than one clause #OTHEREFR and these clauses are combined using connectives.[Citation]Soni et al. #OTHEREFR has also mentioned that the number of verb chunks increases with the length of sentence. They have also mentioned the criteria for defining complexity of a sentence and the same criteria is apt for our purpose also. We consider a sentence to be complex based on the following criteria: .
###Response:","In the context of machine translation, the performance of system generally decreases with increase in the length of the sentence #REFR.","The identification and separation of sentence components for simplification, highlighted by the research of #REFR, aligns with our approach to tackling complex sentence translation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme. We present a new morpho-syntactic factored lexicon that models systematic variations in morphology, syntax, and semantics across word classes. The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered, thereby enabling effective learning from less data. Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data, including up to 45% relative test-error reduction.
###Abstract of the cited paper: The Air Travel Information System (ATIS) domain serves as the common evaluation task for ARPA""spoken language system developers. 1 To support this task, the Multi-Site ATIS Data COllection Working group (MADCOW) coordinates data collection activities. This paper describes recent MADCOW activities. In particular, this paper describes the migration of the ATIS task to a richer elational database and development corpus (ATIS-3) and describes the ATIS-3 corpus. The expanded atabase, which includes information on 46 US and Canadian cities and 23,457 flights, was released inthe fall of 1992, and data collection for the ATIS-3 corpus began shortly thereafter. The ATIS-3 corpus now consists of a total of 8297 released training utterances and 3211 utterances reserved for testing, collected at BBN, CMU, MIT, NIST and SRI. 2906 of the training utterances have been annotated with the correct information from the database. This paper describes the ATIS-3 corpus in detail, including breakdowns of data by type (e.g. context-independent, context-dependent, and unevaluable)and variations in the data collected at different sites. This paper also includes a description of the ATIS-3 database. Finally, we discuss future data collection and evaluation plans.
###Context text: Each lexical entry has (1) a word stem, automatically constructed from Wiktionary, with part-of-speech and morphological attributes, (2) a lexeme that is learned and pairs the stem with semantic content that is invariant to syntactic usage, and (3) a lexical template that specifies the remaining syntactic and semantic content. The full set of templates is defined in terms of a small set of base templates and template transformations that model morphological variants such as passivization and nominalization of verbs. This approach allows us to efficiently encode a general grammar for semantic parsing while also eliminating large classes of incorrect analyses considered by previous work.[Citation]In both cases, our approach achieves state-of-the-art performance, including a nearly 45% relative error reduction on the ATIS test set. We also show that the gains increase with less data, including matching previous model?s performance with less than 25% of the training data. Such gains are particularly practical for semantic parsers; they can greatly reduce the amount of data that is needed for each new application domain.
###Response:",We perform experiments in two benchmark semantic parsing datasets: GeoQuery #OTHEREFR and ATIS #REFR.,"As demonstrated by the ATIS test set developed by the Multi-Site ATIS Data Collection Working Group, our semantic parsing system shows remarkable error reduction [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus. First, the sentences and phrases in the corpus are aligned across the two languages, and the pairs with high translation confidence are selected and stored in the translation memory. Then, for a given input sentences, the system searches for fitting examples based on both the monolingual similarity and the translation confidence of the pair, and the obtained results are then combined to generate the translation. Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach.
###Abstract of the cited paper: We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \[5,9,10,15,17\] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \[9\]. The major technical innovation is tire use of a ""ma~ximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.
###Context text: verb. The Japanese parser outputs the phrasal dependency structure of an input, and that is used as is.[Citation]Step 1: Estimation of basic phrasal correspondences. We started with the word-level alignment to get the basic phrasal alignment. We used translation dictionaries for this process.
###Response:",We used The Japanese parser KNP #OTHEREFR and The English nl-parser #REFR.,"Our method integrated the Japanese parser as presented by #REFR, a system known for its high accuracy in establishing phrasal dependency structures."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Word Sense Induction (WSI) aims to automatically induce meanings of a polysemous word from unlabeled corpora. In this paper, we first propose a novel Bayesian parametric model to WSI. Unlike previous work, our research introduces a layer of hidden concepts and view senses as mixtures of concepts. We believe that concepts generalize the contexts, allowing the model to measure the sense similarity at a more general level. The Zipf?s law of meaning is used as a way of pre-setting the sense number for the parametric model. We further extend the parametric model to non-parametric model which not only simplifies the problem of model selection but also brings improved performance. We test our model on the benchmark datasets released by Semeval-2010 and Semeval-2007. The test results show that our model outperforms state-of-theart systems.
###Abstract of the cited paper: Graph-based methods have gained attention in many areas of Natural Language Processing (NLP) including Word Sense Disambiguation (WSD), text summarization, keyword extraction and others. Most of the work in these areas formulate their problem in a graph-based setting and apply unsupervised graph clustering to obtain a set of clusters. Recent studies suggest that graphs often exhibit a hierarchical structure that goes beyond simple flat clustering. This paper presents an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word. The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs significantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction.
###Context text: A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies on the distributional hypothesis, which states that words occurring in similar contexts will have similar meanings.[Citation]For Bayesian models, Brody and Lapata #OTHEREFR firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model #OTHEREFR and senses as topics. They trained a separate model for each target word and included a variety of features such as words, part-of-speech and dependency information.
###Response:","Different work exploits distributional information in different forms, including context clustering models #OTHEREFR; #REFR and Bayesian Formally, the position feature is the context words with its relative position to the target word. models.","The graph-based methods presented by #REFR, focusing on inferring hierarchical groupings of polysemous word senses, offer a distinct approach to traditional distributions."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we look at the problem of cleansing noisy text using a statistical machine translation model. Noisy text is produced in informal communications such as Short Message Service (SMS), Twitter and chat. A typical Statistical Machine Translation system is trained on parallel text comprising noisy and clean sentences. In this paper we propose an unsupervised method for the translation of noisy text to clean text. Our method has two steps. For a given noisy sentence, a weighted list of possible clean tokens for each noisy token are obtained. The clean sentence is then obtained by maximizing the product of the weighted lists and the language model scores.
###Abstract of the cited paper: Electronic written texts used in computermediated interactions (e-mails, blogs, chats, etc) present major deviations from the norm of the language. This paper presents an comparative study of systems aiming at normalizing the orthography of French SMS messages: after discussing the linguistic peculiarities of these messages, and possible approaches to their automatic normalization, we present, evaluate and contrast two systems, one drawing inspiration from the Machine Translation task; the other using techniques that are commonly used in automatic speech recognition devices. Combining both approaches, our best normalization system achieves about 11% Word Error Rate on a test set of about 3000 unseen messages.
###Context text: Typical statistical machine translation systems use large parallel corpora to learn the translation probabilities #OTHEREFR. Traditionally such corpora have consisted of news articles and other well written articles. Therefore in theory P (e|h) should be constructed by examining sentence pairs of clean and noisy sentences.[Citation]However, all of these techniques require an aligned corpus of SMS and conventional language for training. Aligned parallel corpora for noisy sentence is difficult to obtain. This lack of data for a language and the domain dependence of noise makes it impractical to construct corpus from which P (e|h) can be learnt automatically.
###Response:",There exists some work to remove noise from SMS #OTHEREFR #REFR.,"The linguistic peculiarities and normalization of noisy electronic texts, particularly SMS, have been studied previously #REFR. Yet, these methods rely heavily on aligned parallel corpora, costly to obtain for every language and domain."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We study the relationship between the structure of"" discourse and a set of summarization heuristics that are employed by current systems. A tight coupling of the two enables us to learn genre-specific combinations of heuristics that can be used for disambiguation during discourse parsing. The same coupling enables us to construct discourse structures that yield summaries that contain textual units that are not only important according to a variety of position-, title-, and lexical-similarity-based heuristics, but also central to the main claims of texts. A careful analysis of our results enables us to shed some new light on issues related to summary evaluation and learning.
###Abstract of the cited paper: This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications uch as information retrieval, routing, and text summarization.
###Context text: The intuition that we capture in this way is that a discourse structure should be constructed so that it promotes as close to the root as possible the units that are similar with the title. According to the title-based mettic, we consider that a discourse structure A is ""better"" that a discourse structure B if the title-based score of .4 is higher than the title-based score of/3. The position-based metric.[Citation]Our position-based metric captures this intuition by assigning a positive score to each textual unit that belongs to the first two or last sentences of the the first three or last two paragraphs. We compute a position-based score for each discourse structure by averaging the position-based scores of the units that are promoted as salient in that discourse structure. The intuition that we capture in this way is that a discourse structure should be constructed so that it promotes as close to the root as possible the units that are located at the beginning or end of a text.
###Response:","Research in summarization #OTHEREFR; #REFR has shown that, in genres with stereotypical structure, important sentences are often located at the beginning or end of paragraphs/documents.","Expanding on this concept, the study by #REFR utilized an Optimal Position Policy to determine the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We introduce TRAAM, or Transduction RAAM, a fully bilingual generalization of Pollack?s (1990) monolingual Recursive Auto-AssociativeMemory neural networkmodel, in which each distributed vector represents a bilingual constituent?i.e., an instance of a transduction rule, which specifies a relation between two monolingual constituents and how their subconstituents should be permuted. Bilingual terminals are special cases of bilingual constituents, where a vector represents either (1) a bilingual token?a token-totoken or ?word-to-word? translation rule ?or (2) a bilingual segment?a segmentto-segment or ?phrase-to-phrase? translation rule. TRAAMs have properties that appear attractive for bilingual grammar induction and statistical machine translation applications. Training of TRAAM drives both the autoencoder weights and the vector representations to evolve, such that similar bilingual constituents tend to have more similar vectors.
###Abstract of the cited paper: We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model?s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.
###Context text: RAAM approaches, and related recursive autoencoder approaches, can be more flexible than convolutional networks. Like SRNs, they can be extended in numerous ways. The URAAM #OTHEREFR extended RAAM to demonstrate the possibility of using neural networks to perform more sophisticated operations like unification directly upon the distributed vector representations of hierarchical feature structures.[Citation]Scheible and Sch?tze #OTHEREFR automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified structures via manual evaluation, and showed that sentiment classification accuracy is not affected.
###Response:","#REFR used monolingual recursive autoencoders for sentiment prediction, with or without parse tree information; this was perhaps the first use of a RAAM style approach on a large scale NLP task, albeit monolingual.","Building on this concept, our Transduction RAAM model incorporates bilingual constituents to enable bilingual grammar induction and enhance machine translation applications #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes the 2007 Ngram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the previous years system, being highlighted and empirically compared. Mainly, these include a novel word ordering strategy based on: (1) statistically monotonizing the training source corpus and (2) a novel reordering approach based on weighted reordering graphs. In addition, this system introduces a target language model based on statistical classes, a feature for out-of-domain units and an improved optimization procedure. The paper provides details of this system participation in the ACL 2007 SECOND WORK- SHOP ON STATISTICAL MACHINE TRANSLA- TION. Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks.
###Abstract of the cited paper: In statistical natural anguage processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes uitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
###Context text: The SMR system can be seen as a SMT system which translates from an original source language (S) to a reordered source language (S?), given a target language (T).[Citation]Figure 1: SMR approach in the (A) training step (B) in the test step (the weight of each arch is in brackets).
###Response:","The SMR technique works with statistical word classes #REFR instead of words themselves (particularly, we have used 200 classes in all experiments). ","Building upon techniques used to handle sparse data in statistical natural language processing, bilingual word classes were leveraged to enhance translation [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Models of latent document semantics such as the mixture of multinomials model and Latent Dirichlet Allocation have received substantial attention for their ability to discover topical semantics in large collections of text. In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect that character-level noise can have on unsupervised topic modeling. We show the effects both with document-level topic analysis (document clustering) and with word-level topic analysis (LDA) on both synthetic and real-world OCR data. As expected, experimental results show that performance declines as word error rates increase. Common techniques for alleviating these problems, such as filtering low-frequency words, are successful in enhancing model quality, but exhibit failure trends similar to models trained on unprocessed OCR output in the case of LDA. To our knowledge, this study is the first of its kind.
###Abstract of the cited paper: We present V-measure, an external entropybased cluster evaluation measure. V- measure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the ?problem of matching?, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.
###Context text: We specified the number of clusters to be the same as the number of classes provided with the data. Clusters were evaluated using several external cluster quality metrics which compare ?gold standard? labels to those created through clustering. The metrics used were Variation of Information #OTHEREFR.[Citation]We did not cluster the Eisenhower data because of the absence of the class labels necessary for evaluation. For both the synthetic and non-synthetic data we also trained LDA topic models #OTHEREFR using Gibbs sampling. We used the implementation found in the MALLET software package #OTHEREFRa).
###Response:","Other metrics were also calculated (e.g. the V-Measure #REFR, and Average Entropy #OTHEREFR), but these results were excluded due to space constraints and the fact that their plots are similar to those shown.","One of these metrics was the V-measure as proposed in a previous study, effectively addressing issues prevalent in traditional metrics #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In machine transliteration we transcribe a name across languages while maintaining its phonetic information. In this paper, we present a novel sequence transduction algorithm for the problem of machine transliteration. Our model is discriminatively trained by the MIRA algorithm, which improves the traditional Perceptron training in three ways: (1) It allows us to consider k-best transliterations instead of the best one. (2) It is trained based on the ranking of these transliterations according to user-specified loss function (Levenshtein edit distance). (3) It enables the user to tune a built-in parameter to cope with noisy non-separable data during training. On an Arabic-English name transliteration task, our model achieves a relative error reduction of 2.2% over a perceptron-based model with similar features, and an error reduction of 7.2% over a statistical machine translation model with more complex features.
###Abstract of the cited paper: We describe a discriminatively trained sequence alignment model based on the averaged perceptron. In common with other approaches to sequence modeling using perceptrons, and in contrast with comparable generative models, this model permits and transparently exploits arbitrary features of input strings. The simplicity of perceptron training lends more versatility than comparable approaches, allowing the model to be applied to a variety of problem types for which a learned edit model might be useful. We enumerate some of these problem types, describe a training procedure for each, and evaluate the model?s performance on several problems. We show that the proposed model performs at least as well as an approach based on statistical machine translation on two problems of name transliteration, and provide evidence that the combination of the two approaches promises further improvement.
###Context text: To evaluate our algorithm, we generate 1-best (or 5- best) hypotheses using the beam search procedure and measure accuracy as the percentage of instances in which the target sequence e is one of the 1-best (or 5-best) targets. The input features are based on character m-grams for m = 1, 2, 3. Unlike previous generative transliteration models, no additional language model feature is used.[Citation]The SMT system directly models the posterior probability Pr(e|f) using a log-linear combination of several sub-models: a characterbased phrase translation model, a character-based lexicon model, a character penalty and a phrase penalty. In the PTEM model, the update rule only considers the best target sequence and modifies the parameters w?+1 = w? + ?(a, e, f) ? ?(a?, e?, f) if the score s(e?, f) ? s(e, f). Model (train+dev) 1-best 5-best SMT 0.528 0.824 PTEM 0.552 0.803 MIRA 0.562 0.841
###Response:",We compare our model against a state-of-the-art statistical machine translation #OTHEREFR and an averaged perceptron edit model (PTEM) with identical features #REFR.,"Our approach improves upon the perceptron-based model proposed by #REFR, which also privileges arbitrary features of input strings and exhibits versatility across different problem types."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The work presented here is intended as an evolutionary task-specific module for referring expression generation and aggregation to be enclosed in a generic flexible architecture. Appearances of concepts are considered as genes, each one encoding the type of reference used. Three genetic operators are used: classic crossover and mutation, plus a specific operator dealing with aggregation. Fitness functions are defined to achieve elementary coherence and stylistic validity. Experiments are described and discussed.
###Abstract of the cited paper: We simplify previous work in the development of algorithms for the generation of referring expre~ sions while at the same time taking account of psycholinguistic findings and transcript data. The result is a straightforward algorithm that is computationally tractable, sensitive to the preferences of human users, and reasonably domain-independent. We provide a specification of the resources a host system must provide in order to make use of the algorithm, and describe an implementation used in the IDAS system.
###Context text: Possible simple algorithms for deciding when to use a pronoun and when to use the full noun produce poor results. Two occurrences of the same concept in a paragraph can be far apart, and this may confuse the reader. Knowledge intensive approaches modelled on the way humans do it require a certain measure of content understanding that is resource hungry.[Citation]Therefore, it is essential to choose a reference which matches these constraints. Taking into account these features, Reiter and Dale proposed an algorithm to generate definite noun phrases to identify objects in the current focus of attention of the reader or the hearer. However, Krahmer and Theune [Krahmer and Theune, 2000] argue that due to the original motivation of the work of Reiter and Dale of making distinguishing descriptions, various other aspects of the generation of definites remained somewhat underdeveloped.
###Response:","As shown in [#REFR], a referring expression must communicate enough information to be able to uniquely identify the intended referent in the current discourse context, but avoiding the presence of redundant or otherwise unnecessary modifiers.","To address these issues, the algorithm suggested by #REFR, which reduces complexity and is cognizant of user preferences, should be considered."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we present a novel method for unsupervised coreference resolution. We introduce a precision-oriented inference method that scores a candidate entity of a mention based on the most informative mention pair relation between the given mention entity pair. We introduce an informativeness score for determining the most precise relation of a mention entity pair regarding the coreference decisions. The informativeness score is learned robustly during few iterations of the expectation maximization algorithm. The proposed unsupervised system outperforms existing unsupervised methods on all benchmark data sets.
###Abstract of the cited paper: The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.
###Context text: [Citation]The experimental results for the OntoNotes and ACE data sets are presented in Tables 1 and 2, respectively. On the OntoNotes test set, we compare our method with the three best publicly available coreference systems including the Berkeley system #OTHEREFR. The Berkeley and IMS systems are both supervised approaches with a rich set of lexical features.
###Response:","We evaluate our proposed model with the most commonly used metrics for coreference resolution: for the OntoNotes data sets MUC #OTHEREFR, CEAF #REFR and their average F1 as used in the CoNLL 2011 and 2012 shared tasks; for the ACE data sets MUC and B .","Our method's efficacy is shown by surpassing unsupervised methods in coreference resolution, scoring high on the Constrained Entity-Alignment F-Measure proposed by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes efforts underway to construct a largescale ontology to support semantic processing in the PAN- GLOSS knowledge-base machine translation system. Because we axe aiming at broad sem~tntic coverage, we are focusing on automatic and semi-automatic methods of knowledge acquisition. Here we report on algorithms for merging complementary online resources, in particular the LDOCE and WordNet dictionaries. We discuss empirical results, and how these results have been incorporated into the PANGLOSS ontology.
###Abstract of the cited paper: The atttolnatic oustmction of all IS A taxonomy of noun senses from a machine readable dictionary (MRD) has long been sought, but achieved with only limited success. The task requires the solution to two problems: 1) To define an algorithm to automatically identify the genres or hypemym of a noun definition, and 2) to define an algorithm for lexical disambiguation of the genus term. In the last ~ew years, effective methods for solving the first problem have been developed, but the problem of creating an algorithm for lexical disambiguation of the genus terms is one that has proven to be very difficult. In COL1NG 90 we described our itutial work on the automatic reation of a taxonomy of noun senses from Longman is Dictiorlary of Contemporary English (LDOCE). The algorithm for lexical disambiguation of the genus teml was accurate about 80% of the time aid made use of the semantic ategories, the subject area luarkings and the frequency of use uffonnatiou in LDOCE. In this paper we report a series of experimcuts which weight the three factors in various ways, and describe our improvements to the algorithm (to about 90% accnracy).
###Context text: The semantic code hierarchy is induced by a set of 33 semantic odes drawn up by Longman lexicographers. Each sense is marked with one of these codes, e.g., ""H"" for human ""P"" for plant, ""J"" for movable object. The other hierarchy is the genus sense hierarchy.[Citation]H ie rarchy-Match
###Response:",Researchers at New Mexico State University have built an automatic algorithm \[#REFR\] for locating and disambiguating enus terms (head nouns) in sense definitions.,"By applying the methodology developed by #REFR, we effectively established the taxonomy of noun senses, improving the semantic identification within the PANGLOSS ontology."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text. In this system we are using several techniques for tagging the words in the text and figuring out their types and their features. The major techniques that we are using are: finding phrases, analyzing the affixes of the words, and analyzing their pattems. Proper nouns are particularly difficult to identify in the Arabic language; we describe techniques for isolating them.
###Abstract of the cited paper: Automatic part of speech tagging is an area of natural anguage processing where statistical techniques have been more successful than rulebased methods. In this paper, we present asimple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements o the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.
###Context text: Zhang and Kim \[1990\] developed a system for automated learning of morphological word function rules. This system divided a string into three regions and inferred from training examples their correspondence to underlying morphological features. More advanced word-guessing methods use word features such as leading and trailing word segments to determine possible tags for unknown words.[Citation]Another sign that indicates whether a word is a noun or a verb is the pattern. In the Arabic language the patterns function as an important guide in recognizing the type of the word; some of these patterns are used just for nouns; some of them are usedjustfor verbs; and others are used for both nouns and verbs. One more sign comes from grammatical rules; several grammatical rules can be used to distinguish between ouns and verbs, some letters in the Arabic language (letters of signification are similar to prepositions in the English language) mark the nouns; others mark the verbs
###Response:","Such methods can achieve better performance, reaching a tagging accuracy of up to 85% on unknown words for English \[#REFR; Weischedel et al, 1993\].","Drawing on the insights from the rule-based part of speech tagger developed by #REFR, our system also uses affix analysis and pattern recognition for tagging Arabic text."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline.
###Abstract of the cited paper: In this paper, we focus on the challenge of automatically converting a constituency treebank (source treebank) to fit the standard of another constituency treebank (target treebank). We formalize the conversion problem as an informed decoding procedure: information from original annotations in a source treebank is incorporated into the decoding phase of a parser trained on a target treebank during the parser assigning parse trees to sentences in the source treebank. Experiments on two Chinese treebanks show significant improvements in conversion accuracy over baseline systems, especially when training data used for building the parser is small in size.
###Context text: To avoid binary distinctions used in previous approaches to automatic treebank conversion, we proposed in this paper a feature-based approach.[Citation]Acknowledgments We thank Kenji Sagae for helpful discussions on the implementation of shift-reduce parser and the three anonymous reviewers for comments. This work was supported in part by the National Science Foundation of China #OTHEREFR, the Fundamental Research Funds for the Central Universities and Natural Science Foundation of Liaoning Province of China.
###Response:",Experiments on two Chinese treebanks showed that our approach outperformed the baseline system #REFR by 1.31%.,"Our method, which encodes bracketing structures in a treebank into features to guide conversion, draws upon the informed decoding procedure formalized by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a novel abstractive summarization system for product reviews by taking advantage of their discourse structure. First, we apply a discourse parser to each review and obtain a discourse tree representation for every review. We then modify the discourse trees such that every leaf node only contains the aspect words. Second, we aggregate the aspect discourse trees and generate a graph. We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines.
###Abstract of the cited paper: We present the results of a large-scale, end-to-end human evaluation of various sentiment summarization models. The evaluation shows that users have a strong preference for summarizers that model sentiment over non-sentiment baselines, but have no broad overall preference between any of the sentiment-based models. However, an analysis of the human judgments suggests that there are identifiable situations where one summarizer is generally preferred over the others. We exploit this fact to build a new summarizer by training a ranking SVM model over the set of human preference judgments that were collected during the evaluation, which results in a 30% relative reduction in error over the previous best summarizer.
###Context text: [Citation]However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text #OTHEREFR; . The contribution of the first two authors to this paper was equal.
###Response:",Most existing works on sentiment summarization focus on predicting the overall rating on an entity #OTHEREFR; #REFR).,"Through an extensive evaluation, it was demonstrated that users preferred summarizers modeling sentiment, notably in certain identifiable situations #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a ?bag-of-words? kernel.
###Abstract of the cited paper: Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
###Context text: Our algorithm is similar to that described by Zelenko et al #OTHEREFR. Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity. Brin #OTHEREFR apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora.[Citation]Whereas Miller et al #OTHEREFR use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. Also, Roth and Yih #OTHEREFR learn a Bayesian network to tag entities and their relations simultaneously. We experiment with a more challenging set of relation types and a larger corpus.
###Response:",#REFR propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types.,"Taking inspiration from earlier statistical parsing algorithms used in information extraction #REFR, our method offers improved classification of relations by leveraging a dependency tree kernel."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering?an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.
###Abstract of the cited paper: We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.
###Context text: The language processing community actively works on the problem of automatically inducing grammatical structure from a corpus of text #OTHEREFR.[Citation]In principle, these models must discover the syntactic patterns that govern a language from the sequences of word tokens alone. These models are often evaluated relative to reference treebank annotations. Grammar induction in the context of machine translation reordering offers different properties.
###Response:","Some success in this area has been demonstrated via generative models #OTHEREFR, which often benefit from wellchosen priors #REFR or posterior constraints #OTHEREFR.","The work by #REFR develops a method for inducing parse trees, providing insights into the process of grammar induction and enhancing machine translation reordering."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.
###Abstract of the cited paper: We present a system for the semantic role labeling task. The system combines a machine learning technique with an inference procedure based on integer linear programming that supports the incorporation of linguistic and structural constraints into the decision process. The system is tested on the data provided in CoNLL- 2004 shared task on semantic role labeling and achieves very competitive results.
###Context text: [Citation]In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems.
###Response:","Much attention has recently been devoted to integer linear programming #OTHEREFR; #REFR, dependency parsing #OTHEREFR, among others.","The application of integer linear programming in semantic role labeling, which involves the integration of linguistic and structural constraints, was demonstrated effectively by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Short Messaging Service (SMS) has become a popular form of communication. While it is predominantly used for monolingual communication, it can be extremely useful for facilitating cross-lingual communication through statistical machine translation. In this work we present an application of statistical machine translation to SMS messages. We decouple the SMS translation task into normalization followed by translation so that one can exploit existing bitext resources and present a novel unsupervised normalization approach using distributed representation of words learned through neural networks. We describe several surrogate data that are good approximations to real SMS data feeds and use a hybrid translation approach using finite-state transducers. Both objective and subjective evaluation indicate that our approach is highly suitable for translating SMS messages.
###Abstract of the cited paper: In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system.
###Context text: We used a phrase-based translation framework with the phrase table represented as a finite-state transducer #OTHEREFR.[Citation]The phrase table is then represented as a finite-state transducer (FST). The FST decoder was used with minimum error rate training (MERT) to compute a set of weights for the log-linear model. It is important to note that the cost of arcs of the FST is a composite score (dot product of scores and weights) and hence requires an additional lookup during the N-best generation phase in MERT to obtain the component scores.
###Response:",Our framework proceeds by using the standard procedure of performing word alignment using GIZA++ #OTHEREFR and obtaining phrases from the word alignment using heuristics #REFR and subsequently scoring them.,Our translation system's phrase-based approach was improved through the incorporation of an efficient monotone search algorithm as suggested by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phraseinternal translation and reordering. However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We propose a new model to address this imbalance, based on a word-based Markov model of translation which generates target translations left-to-right. Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: [Citation]These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context.
###Response:","Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based #REFR and syntax-based approaches #OTHEREFR.","In contrast to traditional methods, our model, based on a word-based Markov model, conditions translation decisions on prior ones, circumventing the independence assumption [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech. We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose a new method that aims to better integrate prosodic breaks into parsing. Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce fine-grained PCFG models with latent annotations (PCFG- LA) (Matsuzaki et al, 2005; Petrov and Klein, 2007) that perform significantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries. We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the nbest parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks. Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser.
###Abstract of the cited paper: We identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques.
###Context text: We have investigated using prosodic information in the form of automatically detected ToBI break indexes for parsing spontaneous speech by comparing three prosodic enrichment methods. Although prosodic enrichment improves the basic PCFGs, that performance gain disappears when latent variables are used, partly due to the impact of misclassified (?bad?) breaks that are assigned to words that do not occur at phrase boundaries. However, we find that by simply restricting the search space of the three prosodically enriched latent variable parser models to the n-best parses from the baseline PCFG-LA parser, all of them attain significant improvements.[Citation]Our findings from the oracle experiment suggest that integrating ToBI classification with syntactic parsing should increase the accuracy of both tasks. Acknowledgments We would like to thank Izhak Shafran for providing break indexes for Fisher and Switchboard and for comments on an earlier draft of this paper. This research was supported in part by NSF IIS-0703859.
###Response:",Our analysis more fully explains the positive results achieved by #REFR from reranking with prosodic features and suggests that the hypothesis that inserted prosodic punctuation breaks n-gram dependencies only partially explains the negative results of #OTHEREFR.,"Building on prior research that integrated prosodic cues for parsing conversational speech, improving accuracy over solely lexical and syntactic features-based systems (#REFR), we made enhancements that provided significant improvement."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper investigates the usefulness of sentence-internal prosodic cues in syntactic parsing of transcribed speech. Intuitively, prosodic cues would seem to provide much the same information in speech as punctuation does in text, so we tried to incorporate them into our parser in much the same way as punctuation is. We compared the accuracy of a statistical parser on the LDC Switchboard treebank corpus of transcribed sentence-segmented speech using various combinations of punctuation and sentence-internal prosodic information (duration, pausing, and f0 cues). With no prosodic or punctuation information the parser?s accuracy (as measured by F-score) is 86.9%, and adding punctuation increases its F-score to 88.2%. However, all of the ways we have tried of adding prosodic information decrease the parser?s F-score to between 84.8% to 86.8%, depending on exactly which prosodic information is added. This suggests that for sentence-internal prosodic information to improve speech transcript parsing, either different prosodic cues will have to used or they will have be exploited in the parser in a way different to that used currently.
###Abstract of the cited paper: We present a simple architecture for parsing transcribed speech in which an edited-word detector rst removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words. The edit detector achieves a misclassi cation rate on edited words of 2.2%. (The NULL-model, which marks everything as not edited, has an error rate of 5.9%.) To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indi erent to the exact tree position of EDITED nodes. By this metric the parser achieves 85.3% precision and 86.5% recall.
###Context text: [Citation]We deleted disfluency tags and EDITED subtrees from our training and test corpora. We investigated several combinations of prosodic pseudo-punctuation symbols. For each of these we generated a training and test corpus.
###Response:","We tried to incorporate the binned prosodic information described in the previous subsection in a manner that corresponds as closely as possible to the way that punctuation is represented in this corpus, because previous experiments have shown that punctuation improves parser performance #REFR.","Following the method of removing disfluency tags and 'EDITED' subtrees before parsing as done by #REFR, we then incorporated prosodic cues into our process."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness.
###Abstract of the cited paper: Information extraction (IE) systems are costly to build because they require development texts, parsing tools, and specialized dictionaries for each application domain and each natural language that needs to be processed. We present a novel method for rapidly creating IE systems for new languages by exploiting existing IE systems via crosslanguage projection. Given an IE system for a source language (e.g., English), we can transfer its annotations to corresponding texts in a target language (e.g., French) and learn information extraction rules for the new language automatically. In this paper, we explore several ways of realizing both the transfer and learning processes using off-theshelf machine translation systems, induced word alignment, attribute projection, and transformationbased learning. We present a variety of experiments that show how an English IE system for a plane crash domain can be leveraged to automatically create a French IE system for the same domain.
###Context text: Given the accelerated growth of the number of multilingual documents on the Web and elsewhere, the need for effective multilingual and cross-lingual text processing techniques is becoming increasingly important. In this paper, we address the task of cross-lingual semantic relatedness, and introduce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages.[Citation]The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows.
###Response:","Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval #OTHEREFR, cross-language annotation and resource projections to a second language #REFR.","Our approach draws inspiration from the novel method of rapid information extraction across languages presented by #REFR, applying a similar concept in gauging semantic relatedness."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present the AAWD corpus, a collection of 365 discussions drawn from Wikipedia talk pages and annotated with labels capturing two kinds of social acts: alignment moves and authority claims. We describe these social acts and our annotation process, and analyze the resulting data set for interactions between participant status and social acts and between the social acts themselves.
###Abstract of the cited paper: In this paper we introduce the new task of social event extraction from text. We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact. We annotate part of Automatic Content Extraction (ACE) data, and perform experiments using Support Vector Machines with Kernel methods. We use a combination of structures derived from phrase structure trees and dependency trees. A characteristic of our events (which distinguishes them from ACE events) is that the participating entities can be spread far across the parse trees. We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks. We also use three data sampling approaches to solve the problem of data skewness. Sampling methods improve the F1-measure for the task of relation detection by over 20% absolute over the baseline.
###Context text: Such tasks involve recognizing information that is implicit in the linguistic signal but nonetheless part of its structure. Tasks such as named-entity recognition and word sense disambiguation are also close to the linguistic structure of the signal. Authority claims and alignment moves, on the other hand, are examples of communicative moves aimed at social positioning of a discussant within a group of participants, which may be specialized dialog acts but are referred to here as ?social acts..[Citation]The primary value of this new data set is in facilitating computational modeling of a new task type, i.e. the identification of fine-grained social acts in linguistic interaction. While there has been some prior work on detecting agreements and disagreements in multiparty discussions #OTHEREFR. Computational modeling of these phenomena and automatic detection will help with understanding effective argumentation strategies in online discussions and automatic identification of divisive or controversial discussions and online trolls.
###Response:","We distinguish social acts from ?social events? as described in #REFR: social events correspond to types of interactions among people, whereas a social act is associated with a fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of a turn.","In line with the work on extracting social events from text implemented by #REFR, our task also entails excavating implied social dynamics from linguistic interactions."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a new semantic orientation, Excitation, and its automatic acquisition method. Excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral. We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer ? develop cancer) and causality pairs (e.g., increase in crime ? heighten anxiety). Our experiments show that with automatically acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus with reasonable precision.
###Abstract of the cited paper: We present an integrated probabilistic model for Japanese syntactic and case structure analysis. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. We evaluate both syntactic structure and case structure. In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers.
###Context text: Final judgments were made by majority vote between the annotators. They were nonexperts without formal training in linguistics or semantics. See supplementary materials for our annotation manuals (translated into English).[Citation]We restricted the argument positions of templates to ha (topic), ga (nominative), wo (accusative), ni (dative), and de (instrumental). We discarded templates appearing fewer than 20 times in compound sentences (regardless of connectives) in our corpus.
###Response:",We used 600 million Japanese Web pages #OTHEREFR parsed by KNP #REFR as a corpus.,The annotation methodology was greatly inspired by the integrated probabilistic model for syntactic and case analysis of the Japanese language presented by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present an approach of expanding parallel corpora for machine translation. By utilizing Semantic role labeling (SRL) on one side of the language pair, we extract SRL substitution rules from existing parallel corpus. The rules are then used for generating new sentence pairs. An SVM classifier is built to filter the generated sentence pairs. The filtered corpus is used for training phrase-based translation models, which can be used directly in translation tasks or combined with baseline models. Experimental results on Chinese- English machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets.
###Abstract of the cited paper: We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.
###Context text: In contrast, we propose to build new parallel sentences that convey different information, yet retain correct grammatical and semantic structures. The basic idea of the proposed method is to substitute source and target phrase pairs in a sentence pair with phrase pairs from other sentences. The problem is how to identify where a substitution should happen and which phrase pairs are valid candidates for the substitution.[Citation]If we allow all the NPs to be replaced with other NPs, each sentence pair can generate huge number of new sentences. Instead, we resort to Semantic Role Labeling #OTHEREFR to provide more lexicalized and semantic constraints to select the candidates. The method only requires running SRL labeling on either side of the language pair, and that enables applications on low resource languages.
###Response:","While syntactical constraints have been proven to helpful in identifying good paraphrases #REFR, it is insufficient in our task because it cannot properly filter the candidates for the replacement.","In this context, the phrase substitution strategy adheres to the concept of maintaining syntactic types stated in the paraphrase extraction method proposed by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical cooccurrence and error-driven filtering. We present initial evaluation results and discuss future directions.
###Abstract of the cited paper: An essential problem of example-based translation is how to utilize more than one translation example for translating one source sentence. This 1)aper proposes a method to solve this problem. We introduce tile representation, called .matching e,,z:pressio~z, which tel)resents the combination of fragments of translation examples. The translation process consists of three steps: (.1) Make the source matching expression from lhe source sentence. (2) TransDr the source matching expression into the target matching expression. (3) Construct the target sentence from the target matching expression. This mechanism generates some candidates of translation. To select, the best translation out of them, we define the score of a translation.
###Context text: The approach consists of inducing transfer rules using the four major steps described in more detail below: (i) aligning the nodes of the parses; (ii) generating candidate rules from these alignments; (iii) ordering candidate rules by cooccurrence; and (iv) applying error-driven filtering to select the final set of rules. Our approach is based on lexico-structural transfer #OTHEREFR about Korean to English transfer in particular. Whereas Han et al. focus on high quality domain-specific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules.[Citation]While such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process. Our approach is similar to the recent work of #OTHEREFR where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from #OTHEREFR in several important points.
###Response:",Our approach can be considered a generalization of syntactic approaches to example-based machine translation #OTHEREFR; #REFR.,"This approach notably resembles the method proposed by #REFR, which also involves translating source sentences into target expressions before finalizing the translation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Recent studies have shown that incremental systems are perceived as more reactive, natural, and easier to use than non-incremental systems. However, previous work on incremental NLG has not employed recent advances in statistical optimisation using machine learning. This paper combines the two approaches, showing how the update, revoke and purge operations typically used in incremental approaches can be implemented as state transitions in a Markov Decision Process. We design a model of incremental NLG that generates output based on micro-turn interpretations of the user?s utterances and is able to optimise its decisions using statistical machine learning. We present a proof-of-concept study in the domain of Information Presentation (IP), where a learning agent faces the trade-off of whether to present information as soon as it is available (for high reactiveness) or else to wait until input ASR hypotheses are more reliable. Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence.
###Abstract of the cited paper: We present a general model and conceptual framework for specifying architectures for incremental processing in dialogue systems, in particular with respect to the topology of the network of modules that make up the system, the way information flows through this network, how information increments are ?packaged?, and how these increments are processed by the modules. This model enables the precise specification of incremental systems and hence facilitates detailed comparisons between systems, as well as giving guidance on designing new systems.
###Context text: [Citation]It assumes that the left buffer of a module, such as the NLG module, receives IUs from one or more other processing modules, such as the dialogue manager. These input IUs are then passed on to the processor, where they are mapped to corresponding (higher-level) IUs. For an NLG module, this could be a mapping from the dialogue act present(cuisine=Indian) to the realisation ?they serve Indian food?.
###Response:",A general abstract model of incremental processing based on buffers and a processor was developed by #REFR and is illustrated in Figure 2.,"Leveraging the conceptual framework for incremental processing in dialogue systems offered by #REFR, this model maps given instructions to corresponding user responses."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification. We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks. The method is robust and works well on both snippets and longer documents.
###Abstract of the cited paper: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.
###Context text: In the present work, we propose a simple but novel supervised method to adjust the term frequency portion in tf-idf by assigning a credibility adjusted score to each token. We find that it outperforms the traditional unsupervised tf-idf weighting scheme on multiple benchmarks. The benchmarks include both snippets and longer documents.[Citation]We additionally find that the traditional tf-idf performs competitively against other, more sophisticated methods when used with the right scaling and normalization parameters.
###Response:","We also compare our method against Wang and #REFR?s Naive-Bayes Support Vector Machine (NBSVM), which has achieved state-of-the-art results (or close to it) on many datasets, and find that it performs competitively against NBSVM.","Our proposed method extends the compositional vector representation learning approach of matrix-vector Recursive Neural Networks presented by #REFR, further improving sentiment analysis outcomes."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The development of summarization systems requires reliable similarity (evaluation) measures that compare system outputs with human references. A reliable measure should have correspondence with human judgements. However, the reliability of measures depends on the test collection in which the measure is meta-evaluated; for this reason, it has not yet been possible to reliably establish which are the best evaluation measures for automatic summarization. In this paper, we propose an unsupervised method called Heterogeneity- Based Ranking (HBR) that combines summarization evaluation measures without requiring human assessments. Our empirical results indicate that HBR achieves a similar correspondence with human assessments than the best single measure for every observed corpus. In addition, HBR results are more robust across topics than single measures.
###Abstract of the cited paper: Automatically produced texts (e.g. translations or summaries) are usually evaluated with n-gram based measures such as BLEU or ROUGE, while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes. In this paper we first present an indepth analysis of the state of the art in order to clarify this issue. After this, we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies. These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process. In addition, the greater the heterogeneity of the measures (which is measurable) the higher their combined reliability. These results support the use of heterogeneous measures in order to consolidate text evaluation results.
###Context text: [Citation]This property establishes a relationship between heterogeneity and reliability of measures. However, this work does not provide any method to evaluate and rank summaries given a set of available automatic evaluation measures. We now reformulate the heterogeneity property in order to define a method to combine measures and rank systems.
###Response:",The proposal in this paper is grounded on the heterogeneity property of evaluation measures introduced in #REFR.,"Drawing on the insights of #REFR, which verified that diverse measures enhance text evaluation reliability, we devised the Heterogeneity-Based Ranking approach for summary appraisal."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present Multi-Relational Latent Semantic Analysis (MRLSA) which generalizes Latent Semantic Analysis (LSA). MRLSA provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor. Similar to LSA, a lowrank approximation of the tensor is derived using a tensor decomposition. Each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a.
###Abstract of the cited paper: Knowing the degree of antonymy between words has widespread applications in natural language processing. Manually-created lexicons have limited coverage and do not include most semantically contrasting word pairs. We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus. The approach is evaluated on a set of closest-opposite questions, obtaining a precision of over 80%. Along the way, we discuss what humans consider antonymous and how antonymy manifests itself in utterances.
###Context text: [Citation]Each question in this test consists of a target word and five candidate words, where the goal is to pick the candidate word that has the most opposite meaning to the target word. In order to have a fair comparison, we use the same data split as in #OTHEREFR, with 162 questions used for the development set and 950 for test. Following #OTHEREFR, we report the results in precision (accuracy of the questions that the system attempts to answer), recall (percentage of the questions answered correctly over all questions) and F1 (the harmonic mean of precision and recall).
###Response:",The first task is to answer the closest-opposite questions from the GRE test provided by #REFR4.,"Incorporating the method from #REFR, we effectively determined the degree of antonymy between target and candidate words, enhancing the precision of our results."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a study on how grammar binarization empirically affects the efficiency of the CKY parsing. We argue that binarizations affect parsing efficiency primarily by affecting the number of incomplete constituents generated, and the effectiveness of binarization also depends on the nature of the input. We propose a novel binarization method utilizing rich information learnt from training corpus. Experimental results not only show that different binarizations have great impacts on parsing efficiency, but also confirm that our learnt binarization outperforms other existing methods. Furthermore we show that it is feasible to combine existing parsing speed-up techniques with our binarization to achieve even better performance.
###Abstract of the cited paper: We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.
###Context text: Therefore in Collins #OTHEREFR grammar rules are already factorized into a set of probabilities. In order to capture the dependency relationship between lexcial heads Collins #OTHEREFR breaks down the rules from head outwards, which prevents us from factorizing them in other ways. Therefore our binarization cannot apply to the lexicalized parser.[Citation]
###Response:","However, there are state-of-the-art unlexicalized parsers #REFR, to which we believe our binarization can be applied.","Our research builds upon the A* search procedure extension for tabular PCFG parsing presented by #REFR, adapting it to optimize the efficiency of CKY parsing through a unique binarization method."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We address the problem of localized error detection in Automatic Speech Recognition (ASR) output to support the generation of targeted clarifications in spoken dialogue systems. Localized error detection finds specific mis-recognized words in a user utterance. Targeted clarifications, in contrast with generic ?please repeat/rephrase? clarifications, target a specific mis-recognized word in an utterance (Stoyanchev et al 2012a) and require accurate detection of such words. We extend and modify work presented in (Stoyanchev et al., 2012b) by experimenting with a new set of features for predicting the likelihood of a local error in an ASR hypothesis on an unsifted version of the original dataset. We improve over baseline results, where only ASR- generated features are used, by constructing optimal feature sets for utterance and word mis-recognition prediction. The f-measure for identifying incorrect utterances improves by 2.2% and by 3.9% for identifiying incorrect words.
###Abstract of the cited paper: ITSPOKE is a spoken dialogue system that uses the Why2-Atlas text-based tutoring system as its ?back-end?. A student first types a natural language answer to a qualitative physics problem. ITSPOKE then engages the student in a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors.
###Context text: A reprise clarification question targets a specific misrecognized word and incorporates recognized context into a clarification question. We investigate replacing generic please repeat clarifications with more natural targeted clarifications in automatic spoken systems. Targeted clarifications allow users to provide a concise response to a clarification question which is beneficial for spoken systems accepting broad vocabulary and flexible syntax.[Citation]To enable Spoken Dialogue Systems (SDS) to generate targeted clarification questions, we must first be able to identify mis-recognized words with high accuracy. We term such mis-recognition detection localized error detection. Accurate distinction between correctly and incorrectly recognized words is essential to the creation of appropriate targeted clarification questions.
###Response:","Examples of such systems include tutoring systems, intelligent assistants, and spoken translation systems #REFR.","In the application of ITSPOKE spoken dialogue system, understanding and rectifying utterance misconceptions by engaging students in resourceful feedback dialogue is prioritized, as noted by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Metaphor is an important way of conveying the affect of people, hence understanding how people use metaphors to convey affect is important for the communication between individuals and increases cohesion if the perceived affect of the concrete example is the same for the two individuals. Therefore, building computational models that can automatically identify the affect in metaphor-rich texts like ?The team captain is a rock.?, ?Time is money.?, ?My lawyer is a shark.? is an important challenging problem, which has been of great interest to the research community. To solve this task, we have collected and manually annotated the affect of metaphor-rich texts for four languages. We present novel algorithms that integrate triggers for cognitive, affective, perceptual and social processes with stylistic and lexical information. By running evaluations on datasets in English, Spanish, Russian and Farsi, we show that the developed affect polarity and valence prediction technology of metaphor-rich texts is portable and works equally well for different languages.
###Abstract of the cited paper: We present a novel approach to automatic metaphor identification in unrestricted text. Starting from a small seed set of manually annotated metaphorical expressions, the system is capable of harvesting a large number of metaphors of similar syntactic structure from a corpus. Our method is distinguished from previous work in that it does not employ any hand-crafted knowledge, other than the initial seed set, but, in contrast, captures metaphoricity by means of verb and noun clustering. Being the first to employ unsupervised methods for metaphor identification, our system operates with the precision of 0.79.
###Context text: Metaphor is a figure of speech in which a word or phrase that ordinarily designates one thing is used to designate another, thus making an implicit comparison #OTHEREFR. For instance, in ?My lawyer is a shark? the speaker may want to communicate that his/her lawyer is strong and aggressive, and that he will attack in court and persist until the goals are achieved. By using the metaphor, the speaker actually conveys positive affect because having an aggressive lawyer is good if one is being sued.[Citation]However, in this paper we focus on an equally interesting, challenging and important problem, which concerns the automatic identification of affect carried by metaphors. Building such computational models is important to understand how people use metaphors to convey affect and how affect is expressed using metaphors. The existence of such models can be also used to improve the communication between individuals and to make sure that the speakers perceived the affect of the concrete metaphor example in the same way.
###Response:",There has been a substantial body of work on metaphor identification and interpretation #OTHEREFR; #REFR.,The methodology applied in this study for automatic metaphor identification leans heavily on the unsupervised approach proposed by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A hot task in the Computer Assisted Translation scenario is the integration of Machine Translation (MT) systems that adapt sentence after sentence to the postedits made by the translators. A main role in the MT online adaptation process is played by the information extracted from source and post-edited sentences, which in turn depends on the quality of the word alignment between them. In fact, this step is particularly crucial when the user corrects the MT output with words for which the system has no prior information. In this paper, we first discuss the application of popular state-of-the-art word aligners to this scenario and reveal their poor performance in aligning unknown words. Then, we propose a fast procedure to refine their outputs and to get more reliable and accurate alignments for unknown words. We evaluate our enhanced word-aligner on three language pairs, namely English-Italian, English- French, and English-Spanish, showing a consistent improvement in aligning unknown words up to 10% absolute F- measure.
###Abstract of the cited paper: We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.
###Context text: We consider three widely-used word aligners, namely berkeley, fast-align, and mgiza++. We analyze their performance in aligning an held-out test corpora; in particular, we compare their capability in handling the unknown words.[Citation]We trained berkeley aligner using 5 iterations of model 1 followed by 5 iterations of HMM. When applied to new sentence pairs, the system produces bi-directional symmetrized alignment. fast-align is a recently developed unsupervised word aligner that uses a log-linear reparametrization of IBM model 2 for training the word alignment models #OTHEREFR. We exploited the default configuration with 5 iterations for training.
###Response:","For a fair comparison, all aligners are trained on the same training corpora described in Section 3.2. berkeley aligner #REFR applies the co-training approach for training the IBM model 1 and HMM.","Our observations align with prior research, which showed that asymmetric models trained jointly can improve data likelihood and model agreement #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present the syntax-based string-totree statistical machine translation systems built for the WMT 2013 shared translation task. Systems were developed for four language pairs. We report on adapting parameters, targeted reduction of the tuning set, and post-evaluation experiments on rule binarization and preventing dropping of verbs.
###Abstract of the cited paper: We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particu- Jar representational strata.
###Context text: These parameters are language-dependent and should be set to values that best represent the characteristics of the target trees on which the rule extractor is trained on. Therefore the style of the treebanks used for training the syntactic parsers will also influence these numbers. The default values have been set based on experiments on the English-German language pair #OTHEREFR.[Citation]In Section 3 we present some experiments where we choose different settings of these parameters for the German-English language pair. We use those settings for all language pairs where the target language is English.
###Response:",It is worth noting that the German parse trees #REFR tend to be broader and shallower than those for English.,Our parameter settings were influenced by a stratificational notion of language as posited by #REFR for non-configurational languages.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models. Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar. Compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. Furthermore, phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance.
###Abstract of the cited paper: Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&A) collections. Previous studies address the issue by implicitly expanding queries with the help of translation models pre-constructed using statistical techniques. However, since it is possible for unimportant words (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in order to construct compact translation models for retrieval purposes. Experiments conducted on a real world Q&A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.
###Context text: To improve the efficiency of cross-lingual data selection process, we consider the translation task, the LM training corpus and the parallel corpus in our task are constructed by the key words or important words, and thus construct TM by the key words or important words, which is another key difference between our task and SMT.[Citation]Thus, the average number of words (the total word number inQ and S) in cross-lingual sentence selection model would be minimized naturally, and the efficiency of cross-lingual data selection would be improved. In this paper, we adopt a variant of TextRank algorithm #OTHEREFR, a graphbased ranking model for key word extraction which achieves state-of-the-art accuracy. It identifies and eliminates unimportant words from the corpus, and assumes that a word is unimportant if it holds a relatively low significance in the corpus.
###Response:","We identify and eliminate unimportant words, somewhat similar to Q&A retrieval #REFR.",Our methodology was partly influenced by prior research on eliminating unimportant words in constructing compact translation models for retrieval tasks #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Acquiring syntactic and semantic information about a new application domain for a natural anguage processing system is often a time-consuming task. To address this problem, various researchers have developed acquisition tools to speed the process. While such tools are very useful, they are typically tied to particular systems and so their benefits cannot be shared by other researchers. In this paper, we discuss an experiment using the Leamer--a software tool for acquiring information about a new task domain for Parlance, l an ATN-based natural language system---to configure a quite different natural language system, the BBN ACFG, a unification-based system. We have used the Learner to produce information in three major areas: syntactic and semantic information about the lexical items used in the new domain; translation rules from the parser output to the application system; and a class grammar for use in the speech recognition component of HARC, the BBN spoken language system. Initial results are encouraging: 1499 lexical items have been acquired, of which 91% were directly usable, without any manual editing; all of the translation rules are usable; and a speech vocabulary of 2170 items, with an associated class grammar with a perplexity of 89, has been acquired with a small amount of manual editing.
###Abstract of the cited paper: Developing knowledge bases for AI systems takes too long and costs too much. Even a ""portable"" system may be expensive to use because its installation takes a long time or requires the labor of scarce, highly-trained people. BBN has recently created a tool for acquisition which dramatically reduces the time and cost of installing a natural language system. During 1988, BBN used its Learner tm tool to configure the Parlance tm database interface to two different versions of a large Navy database. The configuration process was performed primarily with development versions of the Learner, which is a software tool for creating the knowledge bases, vocabulary, and mappings to the database that enable the Parlance interface to understand questions addressed to a particular database. The Learner reduced the time required to create Parlance configurations from months to weeks, and demonstrated that the Learner works effectively on databases with many hundreds of fields.
###Context text: This problem is sometimes referred to as the ""knowledge acquisition bottle neck"": how can the knowledge of an application area be combined with the knowledge of a working system and its component technologies to produce a useful system without placing heavy burdens on either the end users or the system designers. THE LEARNER Some researchers have addressed this problem by developing acquisition tools targeted for end users, to allow them to provide the syntactic and semantic information necessary for the NL system, but without requiring them to become xperts in these areas. Such tools usually take the burden of providing detailed syntactic and semantic analyses off the user through a guided acquisition proccedure #OTHEREFR.[Citation]The Learner creates a number of files that are used to configure the Parlance natural anguage processing system for a new application domain in a short time. Like the tools already mentioned, it uses an interactive, guided procedure to acquire syntactic and semantic information from the user. It also acquires information about the structure and content of the database directly from the database itself.
###Response:","BBN has developed a software package, the Learner, as a porting tool for non-expert users #REFR.","Following the same approach, BBN utilized the Learner tool to facilitate the configuration of Parlance to varying versions of a broad Navy database, considerably minimizing the time needed for creating configurations #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results.
###Abstract of the cited paper: We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al, 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al, 2007; Chiang et al, 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO?s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.
###Context text: [Citation]Training is performed by SGDwith a parameter projection #OTHEREFR using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggressive algorithm #OTHEREFR. Learning is efficiently parallelized #OTHEREFR and the locally learned parameters are mixed by an additional line search step.
###Response:",We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of #REFR.,The training process derives from the pairwise ranking optimization (PRO) method for parameter tuning in statistical machine translation #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The use of pivot languages and wordalignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases. However, inherent ambiguities in the pivot language(s) can lead to inadequate paraphrases. We propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language, i.e., the language to be paraphrased. Text in the input language is annotated with ?senses? in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment. This approach shows 62% relative improvement over previous work in generating paraphrases that are judged both more accurate and more fluent.
###Abstract of the cited paper: Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called ?low density? languages. But pivoting requires additional parallel texts. We address this problem by deriving paraphrases monolingually, using distributional semantic similarity measures, thus providing access to larger training resources, such as comparable and unrelated monolingual corpora. We present what is to our knowledge the first successful integration of a collocational approach to untranslated words with an end-to-end, state of the art SMT system demonstrating significant translation improvements in a low-resource setting.
###Context text: Paraphrase extraction is done by pivoting using word-alignment information, as before, but sentences are syntactically annotated and paraphrases are restricted to those with the same syntactic category. This addresses categorial ambiguity by preventing that words with a given category (e.g. a noun) are paraphrased by words with other categories (e.g., a verb). However, the approach does not solve the more complex issue of polysemous paraphrases: words with the same category but different meanings, such as the noun bank as financial institution and land alongside a river/lake.[Citation]The approach has the advantage of not requiring bilingual parallel data, but it suffers from issues typical of distributional similarity metrics. In particular, it produces paraphrases that share the same or similar contexts but are related in ways that do not always characterise paraphrasing, such as antonymy.
###Response:",#REFR derive paraphrases from monolingual data using distributional similarity metrics.,"The nuanced problem of polysemous paraphrases, identified as a challenge in paraphrasing techniques, is addressed in our novel methodology by discriminating word senses in the input language (#REFR), a solution not touched upon in previous research such as #OTHEREFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Open Information Extraction is a recent paradigm for machine reading from arbitrary text. In contrast to existing techniques, which have used only shallow syntactic features, we investigate the use of semantic features (semantic roles) for the task of Open IE. We compare TEXTRUNNER (Banko et al, 2007), a state of the art open extractor, with our novel extractor SRL-IE, which is based on UIUC?s SRL system (Punyakanok et al, 2008). We find that SRL-IE is robust to noisy heterogeneous Web data and outperforms TEXTRUN- NER on extraction quality. On the other hand, TEXTRUNNER performs over 2 orders of magnitude faster and achieves good precision in high locality and high redundancy extractions. These observations enable the construction of hybrid extractors that output higher quality results than TEXTRUNNER and similar quality as SRL-IE in much less time.
###Abstract of the cited paper: Almost all automatic semantic role labeling (SRL) systems rely on a preliminary parsing step that derives a syntactic structure from the sentence being analyzed. This makes the choice of syntactic representation an essential design decision. In this paper, we study the influence of syntactic representation on the performance of SRL systems. Specifically, we compare constituent-based and dependencybased representations for SRL of English in the FrameNet paradigm. Contrary to previous claims, our results demonstrate that the systems based on dependencies perform roughly as well as those based on constituents: For the argument classification task, dependencybased systems perform slightly higher on average, while the opposite holds for the argument identification task. This is remarkable because dependency parsers are still in their infancy while constituent parsing is more mature. Furthermore, the results show that dependency-based semantic role classifiers rely less on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data.
###Context text: Because not all roles feature in each verb the roles are commonly divided into meta-roles (A0-A7) and additional common classes such as location, time, etc. Each Ai can represent a different role based on the verb, though A0 and A1 most often refer to agents and patients respectively. Availability of lexical resources such as Propbank #OTHEREFR, which annotates text with meta-roles for each argument, has enabled significant progress in SRL systems over the last few years.[Citation]We use UIUC-SRL as our base SRL system #OTHEREFR. Our choice of the system is guided by the fact that its code is freely available and it is competitive with state of the art #OTHEREFR shared task). UIUC-SRL operates in four key steps: pruning, argument identification, argument classification and inference.
###Response:","Recently, there have been many advances in SRL #OTHEREFR; #REFR.","As noted in prior research, the choice of syntactic representation in SRL systems, such as UIUC-SRL, can considerably influence their performance #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classifiers on error-annotated ESL text generally outperforms training on native text alone and that adaptation of error correction models to the native language (L1) of the writer improves performance. Nevertheless, most extant models have poor precision, particularly when attempting error correction, and this limits their usefulness in practical applications requiring feedback. We experiment with various feature types, varying quantities of error-corrected data, and generic versus L1-specific adaptation to typical errors using Na??ve Bayes (NB) classifiers and develop one model which maximizes precision. We report and discuss the results for 8 models, 5 trained on the HOO data and 3 (partly) on the full error-coded Cambridge Learner Corpus, from which the HOO data is drawn.
###Abstract of the cited paper: In this paper, we consider the problem of generating candidate corrections for the task of correcting errors in text. We focus on the task of correcting errors in preposition usage made by non-native English speakers, using discriminative classifiers. The standard approach to the problem assumes that the set of candidate corrections for a preposition consists of all preposition choices participating in the task. We determine likely preposition confusions using an annotated corpus of nonnative text and use this knowledge to produce smaller sets of candidates. We propose several methods of restricting candidate sets. These methods exclude candidate prepositions that are not observed as valid corrections in the annotated corpus and take into account the likelihood of each preposition confusion in the non-native text. We find that restricting candidates to those that are observed in the non-native data improves both the precision and the recall compared to the approach that views all prepositions as possible candidates. Furthermore, the approach that takes into account the likelihood of each preposition confusion is shown to be the most effective.
###Context text: The task of detecting and correcting writing errors made by learners of English as a Second Language (ESL) has recently become a focus of research. The majority of previous papers in this area have presented machine learning methods with models being trained on well-formed native English text #OTHEREFR.[Citation]Both approaches show improvements over the models trained solely on well-formed native text. Training a model on error-tagged non-native text is expensive, as it requires large amounts of manually-annotated data, not currently publically available. In contrast, using native language-specific error statistics to adapt a model to a writer?s first or native language (L1) is less restricted by the amount of training data.
###Response:","However, some recent approaches have explored ways of using annotated non-native text either by incorporating error-tagged data into the training process #OTHEREFR, or by using native language-specific error statistics #REFR.","Following the work of #REFR, applying discriminative classifiers to correct non-native English speakers' errors and limiting potential correction options can indeed enhance the model's precision and recall."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: During the course of first language acquisition, children produce linguistic forms that do not conform to adult grammar. In this paper, we introduce a data set and approach for systematically modeling this child-adult grammar divergence. Our corpus consists of child sentences with corrected adult forms. We bridge the gap between these forms with a discriminatively reranked noisy channel model that translates child sentences into equivalent adult utterances. Our method outperforms MT and ESL baselines, reducing child error by 20%. Our model allows us to chart specific aspects of grammar development in longitudinal studies of children, and investigate the hypothesis that children share a common developmental path in language acquisition.
###Abstract of the cited paper: We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.
###Context text: Previous work in CALL error correction includes identifying word choice errors in TOEFL essays based on context #OTHEREFR. In a 2011 shared ESL correction task #OTHEREFR corrected preposition, article, punctuation and spelling errors by building classifiers for each category. This line of work is grounded in the practical application of automatic error correction as a learning tool for ESL students.[Citation]In the realm of error correction, SMT has been applied to identify and correct spelling errors in internet search queries #OTHEREFR. Within CALL, Park and Levy #OTHEREFR took an unsupervised SMT approach to ESL error correction using Weighted Finite State Transducers (FSTs). The work described in this paper is inspired by that of Park and Levy, and in Section 6 we detail differences between our approaches.
###Response:",Statistical Machine Translation #OTHEREFR and prediction of twitter responses #REFR.,Our approach to translating child sentences into adult utterances builds on the data-driven methodology for response generation as presented in #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper investigates the impact of different morphological and lexical information on data-driven dependency parsing of Persian, a morphologically rich language. We explore two state-of-the-art parsers, namely MSTParser andMaltParser, on the recently released Persian dependency treebank and establish some baselines for dependency parsing performance. Three sets of issues are addressed in our experiments: effects of using gold and automatically derived features, finding the best features for the parser, and a suitable way to alleviate the data sparsity problem. The final accuracy is 87.91% and 88.37% labeled attachment scores for Malt- Parser and MSTParser, respectively.
###Abstract of the cited paper: Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available. In this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of a beam, combining the advantages of transition-based and graph-based approaches. We also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores.
###Context text: [Citation]This approach is languageindependent and is solely dependent on the availability of annotated corpora. Using data-driven parsers for some languages requires careful selection of features and tuning of the parameters to reach maximum performance. Difficulty of dependency parsing in each language depends on having either free word order or morphological information.
###Response:",Researchers have paid a lot of attention to datadriven dependency parsing in recent years #REFR.,"Drawing on the research of #REFR, the challenges of dependency parsing vary between languages and hinge on morphological information and word order."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we apply lightly-supervised training to a hierarchical phrase-based statistical machine translation system. We employ bitexts that have been built by automatically translating large amounts of monolingual data as additional parallel training corpora. We explore different ways of using this additional data to improve our system. Our results show that integrating a second translation model with only non-hierarchical phrases extracted from the automatically generated bitexts is a reasonable approach. The translation performance matches the result we achieve with a joint extraction on all training bitexts while the system is kept smaller due to a considerably lower overall number of phrases.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: [Citation]We thus in fact conduct a cross-system and cross-paradigm variant of lightlysupervised training. Translating the monolingual Arabic data has been performed by LIUM, Le Mans, France. We thank Holger Schwenk for kindly providing the translations.
###Response:",The unsupervised data that we integrate has been created by automatic translations of parts of the Arabic LDC Gigaword corpus (mostly from the HYT collection) with a standard phrase-based system #REFR.,"The model we use for lightly-supervised training is based on the approach proposed by #REFR, which emphasizes the effectiveness of phrase-based translation systems."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper is concerned with using a semantic hierarchy to estimate the frequency with which a word sense appears as a given argument of a verb, assuming the data is not sense disambiguated. The standard approach is to split the count for any noun appearing in the data equally among the alternative senses of the noun. This can lead to inaccurate stimates. We describe a reestimation process which uses the accumulated counts of hypernyms of the alternative senses in order to redistribute the count. In order to choose a hypernym for each alternative sense, we employ a novel technique which uses a X 2 test to measure the homogeneity of sets of concepts in the hierarchy.
###Abstract of the cited paper: This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Roget is Thesaurus categories. Roget is categories serve as approximations of conceptual classes. The categories li ted for a word in Roger is index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful evel of sense disambiguatiou. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework. Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unresUicted monolingual text without human intervention. Applied to the 10 million word Grolier is Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature.
###Context text: Let freq(c, v,r), for a particular c, v and r, be the number of (n, v, r) triples in the data in which n is being used to denote c, and let freq(v, r) be the number of times verb v appears with something in position r in the data; then the relevant maximum likelihood estimates, for c E C, v E 12, r E 7~, are as follows. freq(~, v, r) freq(v, r) ~eee freq(g, v, r) freq(v, r) if(Fir) = Evevfreq(c' v'r) ~,ev freq(v, r) _ Ever ~,ev freq(v, r) Since we do not have sense disambiguated data, we cannot obtain freq(c, v, r) by simply counting senses. The standard approach is to estimate freq(c, v, r) by distributing the count for each noun n in syn(c) evenly among all senses of the noun as follows: freq(n, v, r) freq(c, v, r) = ~ I cn(n)l nEsyn(c) where freq(n, v, r) is the number times the triple (n,v,r) appears in the data, and \[ cn(n)\] is the cardinality of an(n).[Citation]To see why, consider two instances of possible triples in the data, drink wine and drink water. #OTHEREFR.) The word water is a member of seven synsets in WordNet 1.6, and wine is a member of two synsets. Thus each sense of water will be incremented by 0.14 counts, and each sense of wine will be incremented by 0.5 counts. Now although the incorrect senses of these words will receive counts, those concepts in the hierarchy which dominate more than one of the senses, such as <beverage>, will accumulate more substantial counts.
###Response:","Although this approach can give inaccurate estimates, the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise, and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words #REFR.","Our approach deviates from the standard method mentioned, applying a unique reestimation process aligned with the category-based sense disambiguation model presented by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a novel sentence clustering scheme based on projecting sentences over term clusters. The scheme incorporates external knowledge to overcome lexical variability and small corpus size, and outperforms common sentence clustering methods on two reallife industrial datasets.
###Abstract of the cited paper: We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme: (i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classification probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-theart performance using only the category names as initial seeds.
###Context text: Clusters To obtain sentence clusters, the given sentence set has to be projected in some manner over the term clusters obtained in Step 1.[Citation]
###Response:","Our projection procedure resembles unsupervised text categorization #REFR, with categories represented by term clusters that are not predefined but rather emerge from the analyzed data:","In this process, our methodology employs a generalized bootstrapping algorithm as described in #REFR, which utilizes features like Latent Semantic space and Gaussian Mixture algorithm for categorization."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar (CCG). The parsing model uses predicate-argument dependencies for training, which are derived from sequences of CCG lexical categories rather than full derivations. A simple method is used for extracting dependencies from lexical category sequences, resulting in high precision, yet incomplete and noisy data. The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. Remarkably, the accuracy of the parser trained on data derived from category sequences alone is only 1.3% worse in terms of F-score than the parser trained on complete dependency structures.
###Abstract of the cited paper: A parsing system returning analyses in the form of sets of grammatical relations can obtain high precision if it hypothesises a particular relation only when it is certain that the relation is correct. We operationalise this technique?in a statistical parser using a manually-developed wide-coverage grammar of English?by only returning relations that form part of all analyses licensed by the grammar. We observe an increase in precision from 75% to over 90% (at the cost of a reduction in recall) on a test corpus of naturally-occurring text.
###Context text: Our novel idea is that, since there is so much information in the lexical category sequence, most of the correct dependencies can be easily inferred from the categories alone. More specifically, for a given sentence and lexical category sequence, we train on those predicate-argument dependencies which occur in k% of the derivations licenced by the lexical categories. By setting the k parameter high, we can produce a set of high precision dependencies for training.[Citation]Using this procedure we are able to produce dependency data with over 99% precision and, remarkably, up to 86% recall, when compared against the complete gold-standard dependency data. The high recall figure results from the significant amount of syntactic information in the lexical categories, which reduces the ambiguity in the possible dependency structures. Since the recall is not 100%, we require a log-linear training method which works with partial data.
###Response:",A similar idea is proposed by #REFR for producing high precision data for lexical acquisition.,"As proposed by the high precision parsing system in #REFR, we only consider the relations that are part of all licensed analyses, thereby boosting our precision."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.
###Abstract of the cited paper: We present a novel approach to relation extraction that integrates information across documents, performs global inference and requires no labelled text. In particular, we tackle relation extraction and entity identification jointly. We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia). For inference we run an efficient Gibbs sampler that leads to linear time joint inference. We evaluate our approach both for an indomain (Wikipedia) and a more realistic outof-domain (New York Times Corpus) setting. For the in-domain setting, our joint model leads to 4% higher precision than an isolated local approach, but has no advantage over a pipeline. For the out-of-domain data, we benefit strongly from joint modelling, and observe improvements in precision of 13% over the pipeline, and 15% over the isolated baseline.
###Context text: The problem is that we lack the large amount of labeled examples that are typically required to apply supervised learning techniques. We describe an overview of these techniques and the methodological choices we made to implement our study. Figure 1 illustrates the overall workflow of a distant supervision system.[Citation]
###Response:","At each step of the distant supervision process, we closely follow the recent literature #OTHEREFR; #REFR.","Building on the strategy devised by #REFR, which optimizes relation extraction through global inference with no need for labelled texts, our study also utilizes distant supervision."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Text-level discourse parsing remains a challenge: most approaches employ features that fail to capture the intentional, semantic, and syntactic aspects that govern discourse coherence. In this paper, we propose a recursive model for discourse parsing that jointly models distributed representations for clauses, sentences, and entire discourses. The learned representations can to some extent learn the semantic and intentional import of words and larger discourse units automatically,. The proposed framework obtains comparable performance regarding standard discoursing parsing evaluations when compared against current state-of-art systems.
###Abstract of the cited paper: In this paper we consider the problem of analysing sentence-level discourse structure. We introduce discourse chunking (i.e., the identification of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing. Our experiments show that the proposed modelling approach yields results comparable to state-of-the-art while exploiting knowledge-lean features and small amounts of discourse annotations. We also demonstrate how discourse chunking can be successfully applied to a sentence compression task.
###Context text: In a coherent text, units (clauses, sentences, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson #OTHEREFR define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated.[Citation]Despite recent progress in automatic discourse segmentation and sentence-level parsing #OTHEREFR, document-level discourse parsing remains a significant challenge. Recent attempts #OTHEREFR) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define.
###Response:","Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization #OTHEREFR, sentence compression #REFR or questionanswering #OTHEREFR.","The proposed model here addresses this issue, integrating distributed representations for larger discourse units and comparable to state-of-the-art systems #REFR, similar to a discourse chunking approach."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Current alternatives for language modeling are statistical techniques based on large amounts of training data, and hand-crafted context-free or finite-state grammars that are difficult to build and maintain. One way to address the problems of the grammar-based approach is to compile recognition grammars from grammars written in a more expressive formalism. While theoretically straight-forward, the compilation process can exceed memory and time bounds, and might not always result in accurate and efficient speech recognition. We will describe and evaluate two approaches to this compilation problem. We will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model.
###Abstract of the cited paper: The Air Travel Information System (ATIS) domain serves as the common evaluation task for ARPA""spoken language system developers. 1 To support this task, the Multi-Site ATIS Data COllection Working group (MADCOW) coordinates data collection activities. This paper describes recent MADCOW activities. In particular, this paper describes the migration of the ATIS task to a richer elational database and development corpus (ATIS-3) and describes the ATIS-3 corpus. The expanded atabase, which includes information on 46 US and Canadian cities and 23,457 flights, was released inthe fall of 1992, and data collection for the ATIS-3 corpus began shortly thereafter. The ATIS-3 corpus now consists of a total of 8297 released training utterances and 3211 utterances reserved for testing, collected at BBN, CMU, MIT, NIST and SRI. 2906 of the training utterances have been annotated with the correct information from the database. This paper describes the ATIS-3 corpus in detail, including breakdowns of data by type (e.g. context-independent, context-dependent, and unevaluable)and variations in the data collected at different sites. This paper also includes a description of the ATIS-3 database. Finally, we discuss future data collection and evaluation plans.
###Context text: Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which there is no human-human analog interaction. The statistical approach is unworkable in such cases for both the commercial developers and for some research systems #OTHEREFR. Even in cases for which there is no impediment to collecting data, the expense and time required to collect a corpus can be prohibitive.[Citation]A major problem with grammar-based finitestate or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases. One way to address this problem is to write the grammar in a more expressive formalism and generate an approximation of this grammar in the format needed by the recognizer. This approach has been used in several systems, CommandTalk #OTHEREFR.
###Response:",The existence of the ATIS database #REFR is no doubt a factor in the popularity of the travel domain among the research community for exactly this reason.,"Notably, these challenges were mitigated in the Air Travel Information System domain, where data was systematically gathered by the MADCOW working group, forming the extensive ATIS-3 corpus #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1- best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. This framework offers two promising advantages. 1) ambiguity encoded in parse forests compromises noise in 1-best parse trees. During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.
###Abstract of the cited paper: In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
###Context text: Given an input sentence x = w w ...w n , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h,m) : 0 ? h ? n, 0 < m ? n}, where (h,m) indicates a directed arc from the head word w h to the modifier w m , and w is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages.[Citation]
###Response:","The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph #OTHEREFR; #REFR, while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree #OTHEREFR.","One of these methods, as discussed in #REFR, uses an ambiguity-aware ensemble training framework, while the other incorporates higher-order feature representations into the MST parsing framework as outlined by McDonald et al. (2005c) #OTHEREFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We consider the problem of training a statistical parser in the situation when there are multiple treebanks available, and these treebanks are annotated according to different linguistic conventions. To address this problem, we present two simple adaptation methods: the first method is based on the idea of using a shared feature representation when parsing multiple treebanks, and the second method on guided parsing where the output of one parser provides features for a second one. To evaluate and analyze the adaptation methods, we train parsers on treebank pairs in four languages: German, Swedish, Italian, and English. We see significant improvements for all eight treebanks when training on the full training sets. However, the clearest benefits are seen when we consider smaller training sets. Our experiments were carried out with unlabeled dependency parsers, but the methods can easily be generalized to other featurebased parsers.
###Abstract of the cited paper: We present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
###Context text: Multiple Treebanks We now describe the two adaptation methods to leverage multiple treebanks for parser training. For clarity of presentation, we assume that there are two treebanks, although we can easily generalize to more.[Citation]In a graph-based parser, for a given sentence x the task of finding the top-scoring parse y? is stated as an optimization problem of maximizing a linear objective function: y? = argmax y w ? f(x, y). Here w is a weight vector produced by some learning algorithm and f(x, y) a feature representation that maps the sentence x with a parse tree y to a high-dimensional vector; the adaptation methods presented in this work is implemented as modifications of the feature representation function f . Since the search space is too large to be enumerated, the maximization must be handled carefully, and how this is done determines the expressivity of the feature representation f .
###Response:",We use a common graph-based parsing technique #REFR; the approaches described here could be used in transition-based parsing as well.,"Our adaptation methodology draws on the dependency parsing model incorporating rich factors, as described in #REFR, adapting it to optimize the feature representation function in multi-treebank environments."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.
###Abstract of the cited paper: We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system?s error trends.
###Context text: The second version estimates the mixture coefficients to maximize likelihood, then expands the POS tags (?5), using the result to initialize training of the final model. We call this model ?Mixture+DG.. No Tag Dictionary For each of the above configurations, we ran purely unsupervised training without a tag dictionary, and evaluated using one-to-one mapping accuracy constraining at most one HMM state to map to a unique treebank tag in the test data, using maximum bipartite matching.[Citation]This tag dictionary was constructed only from the training section of a given language?s treebank. It is widely known that such knowledge improves the quality of the model, though it is an open debate whether such knowledge is realistic to assume. For this experiment we removed punctuation from the training and test data, enabling direct use within the dependency grammar induction experiments.
###Response:","This is a variant of the greedy one-to-one mapping scheme of #REFR.8 With a Tag Dictionary We also ran a second version of each experimental configuration, where we used a tag dictionary to restrict the possible path sequences of the HMM during both learning and inference.","The robust performance of our technique echoes the benefits noted in the prototype-driven learning model, where sparse prior information significantly improved sequence modeling outcomes (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Three versions of the Covington algorithm for non-projective dependency parsing have been tested on the ten different languages for the Multilingual track of the CoNLL- X Shared Task. The results were achieved by using only information about heads and daughters as features to guide the parser which obeys strict incrementality.
###Abstract of the cited paper: An open issue in data-driven dependency parsing is how to handle non-projective dependencies, which seem to be required by linguistically adequate representations, but which pose problems in parsing with respect to both accuracy and efficiency. Using data from five different languages, we evaluate an incremental deterministic parser that derives non-projective dependency structures in O(n2) time, supported by SVM classifiers for predicting the next parser action. The experiments show that unrestricted non-projective parsing gives a significant improvement in accuracy, compared to a strictly projective baseline, with up to 35% error reduction, leading to state-of-the-art results for the given data sets. Moreover, by restricting the class of permissible structures to limited degrees of non-projectivity, the parsing time can be reduced by up to 50% without a significant decrease in accuracy.
###Context text: In this paper we focus on two things.[Citation]Second, we test the performance of a pure grammar-based feature model in strictly incremental fashion. The grammar model relies only on the knowledge of heads and daughters of two given words, as well as the words themselves, in order to decide whether they can be linked with a certain dependency relation. In addition, none of the three parsing algorithms guarantees that the output dependency graph will be projective.
###Response:","First, we investigate the impact of using different flavours of Covington?s algorithm #OTHEREFR for nonprojective dependency parsing on the ten different languages provided for CoNLL-X Shared Task #REFR.","First, we tackle the challenge of handling non-projective dependencies, a problem noted in the data-driven dependency parsing field [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Synchronous tree substitution grammars are a translation model that is used in syntax-based machine translation. They are investigated in a formal setting and compared to a competitor that is at least as expressive. The competitor is the extended multi bottom-up tree transducer, which is the bottom-up analogue with one essential additional feature. This model has been investigated in theoretical computer science, but seems widely unknown in natural language processing. The two models are compared with respect to standard algorithms (binarization, regular restriction, composition, application). Particular attention is paid to the complexity of the algorithms.
###Abstract of the cited paper: We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result.
###Context text: The rules of Figure 2 are applied in a derivation in Figure 3. The first displayed derivation step uses the context S(NP(t1), ) and any substitution ? such that ?(x2) = t2 and ?(x3) = t3. It is argued by Chiang #OTHEREFR that STSGs (and extended tree transducers) have sufficient power for syntax-based machine translation.[Citation]Since our newly proposed device, the MBOT, should be at least as powerful as STSGs, we quickly demonstrate how each STSG can be coded as an MBOT. An STSG production and the corresponding MBOT rule are displayed in Figure 1. Since the correspondence is rather trivial, we omit a formal definition.
###Response:",#REFR presents a detailed overview that also mentions short-comings.,The use of binarization methods in restructuring phrase structures for enhanced translation has also been emphasized #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach.
###Abstract of the cited paper: In developing an Infbrmation Extraction tIE) system tbr a new class of events or relations, one of the major tasks is identifying the many ways in which these events or relations may be expressed in text. This has generally involved the manual analysis and, in some cases, the annotation of large quantities of text involving these events. This paper presents an alternative approach, based on an automatic discovery procedure, ExDIsCO, which identifies a set; of relewmt documents and a set of event patterns from un-annotated text, starting from a small set of ""seed patterns."" We evaluate ExDIScO by comparing the pertbrmance of discovered patterns against that of manually constructed systems on actual extraction tasks.
###Context text: A key choice in the development of such an algorithm is step 4, the process of ranking the candidate patterns, which effectively determines which of the candidate patterns will be learned.[Citation]This approach, which can be viewed as being document-centric, operates by associating confidence scores with patterns and relevance scores with documents. Initially seed patterns are given a maximum confidence score of 1 and all others a 0 score. Each document is given a relevance score based on the patterns which occur within it.
###Response:",#REFR chose an approach motivated by the assumption that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns.,The method of ranking candidate patterns is notably similar to ExDIScO's automatic discovery procedure for relevant patterns as outlined by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English.
###Abstract of the cited paper: We present an approach to statistical machine translation that combines ideas from phrase-based SMT and traditional grammar-based MT. Our system incorporates the concept of multi-word translation units into transfer of dependency structure snippets, and models and trains statistical components according to stateof-the-art SMT systems. Compliant with classical transfer-based MT, target dependency structure snippets are input to a grammar-based generator. An experimental evaluation shows that the incorporation of a grammar-based generator into an SMT framework provides improved grammaticality while achieving state-of-the-art quality on in-coverage examples, suggesting a possible hybrid framework.
###Context text: Unlike Hiero, their translation model uses a full range of named nonterminal symbols in the synchronous grammar. As an alternative way to relax strict parser-based constituency requirements, they explore the use of phrases spanning generalized, categorial-style constituents in the parse tree, e.g. type NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via ?chunking? noun phrases and translating them separately #OTHEREFR.[Citation]They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a ?fragment grammar? for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet|fsnippet) and its converse), side by side to?traditional? lexical weights. Riezler and Maxwell #OTHEREFR do not achieve higher BLEU scores, but do score better according to human grammaticality judgments for in-coverage cases.
###Response:","Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by #REFR.","Our approach diverges, introducing soft, constituent-level constraints post-translation modeling, paralleling the cited work's integration of a grammar-based generator to refine translation units #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Topic segmentation classically relies on one of two criteria, either finding areas with coherent vocabulary use or detecting discontinuities. In this paper, we propose a segmentation criterion combining both lexical cohesion and disruption, enabling a trade-off between the two. We provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmentation. Experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination. Gains were observed in all conditions, with segments of either regular or varying length and abrupt or smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences.
###Abstract of the cited paper: This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.
###Context text: [Citation]Choi?s data set consist of 700 documents, each created by concatenating the first z sentences of 10 articles randomly chosen from the Brown corpus, assuming each article is on a different topic. Table 1 provides Figure 1: An example of a lattice L. z = 3?11 3?5 6?8 9?11 # samples 400 100 100 100
###Response:",The artificial data set of #REFR is widely used in the literature and enables comparison of a new segmentation method with existing ones.,"The new segmentation criterion we propose builds upon the method described by Choi, who used rank in local context for linear text segmentation [#REFR][#OTHEREFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this work, we propose two extensions of standard word lexicons in statistical machine translation: A discriminative word lexicon that uses sentence-level source information to predict the target words and a trigger-based lexicon model that extends IBM model 1 with a second trigger, allowing for a more fine-grained lexical choice of target words. The models capture dependencies that go beyond the scope of conventional SMT models such as phraseand language models. We show that the models improve translation quality by 1% in BLEU over a competitive baseline on a large-scale task.
###Abstract of the cited paper: We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese- English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task? and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. ?This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants
###Context text: The models in this paper are also related to word sense disambiguation (WSD). For example, #OTHEREFR trained a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier. They showed improvements in translation quality in a hierarchical phrase-based translation system.[Citation]Our model differs from the latter in three ways. First, our approach models word selection of the target sentence based on global sentence-level features of the source sentence. Second, instead of disambiguating phrase senses as in #OTHEREFR, we model word selection independently of the phrases used in the MT models.
###Response:",Another WSD approach incorporating context-dependent phrasal translation lexicons is given in #REFR and has been evaluated on several translation tasks.,"Similar to #OTHEREFR's approach, which enhanced SMT performance by integrating a word sense disambiguation system, we further refine this strategy with our extensions of word lexicons (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why ?syntactic? constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at http://nlp.cs.nyu.edu/GenPar/ACL06
###Abstract of the cited paper: Data-Oriented Translation (DOT), based on Data- Oriented Parsing (DOP), is a language-independent MT engine which exploits parsed, aligned bitexts to produce very high quality translations. However, data acquisition constitutes a serious bottleneck as DOT requires parsed sentences aligned at both sentential and sub-structural levels. Manual substructural alignment is time-consuming, error-prone and requires considerable knowledge of both source and target languages and how they are related. Automating this process is essential in order to carry out the large-scale translation experiments necessary to assess the full potential of DOT. We present a novel algorithm which automatically induces sub-structural alignments between context-free phrase structure trees in a fast and consistent fashion requiring little or no knowledge of the language pair. We present results from a number of experiments which indicate that our method provides a serious alternative to manual alignment.
###Context text: We computed the phrase length that would be necessary to cover the words involved in each (3,1,4,2) permutation in the MTEval bitext. Figure 5 shows the cumulative percentage of these cases that would be covered by phrases up to a certain length. Only 9 of the 171 cases (5.2%) could be covered by phrases of length 10 or less.[Citation]Both of these approaches can increase coverage of the training data, but, as explained in Section 2, they risk losing generalization ability. Our study suggests that there might be some benefits to an alternative approach using discontinuous constituents, as proposed, e.g., by Melamed et al #OTHEREFR. The large differences in failure rates between the first and second columns of Table 3 are largely independent of the tightness of our lower bounds.
###Response:","Analogous techniques for tree-structured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals #REFR, or, given a constraining parse tree, to ?flatten? it #OTHEREFR.","Finding that implementation of a Data-Oriented Translation modeling, as proposed in the cited study #REFR, could potentially advance our understanding of bitext translation complexities."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Conventional approaches to Chinese word segmentation treat the problem as a characterbased tagging task. Recently, semi-Markov models have been applied to the problem, incorporating features based on complete words. In this paper, we propose an alternative, a latent variable model, which uses hybrid information based on both word sequences and character sequences. We argue that the use of latent variables can help capture long range dependencies and improve the recall on segmenting long words, e.g., named-entities. Experimental results show that this is indeed the case. With this improvement, evaluations on the data of the second SIGHAN CWS bakeoff show that our system is competitive with the best ones in the literature.
###Abstract of the cited paper: The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation. Twenty three groups submitted 130 result sets over two tracks and four different corpora. We found that the technology has improved over the intervening two years, though the out-of-vocabulary problem is still or paramount importance. 1! Introduction Chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing applications including monoand cross-lingual information retrieval and text-to-speech systems. This word segmentation problem has been active area of research in computational linguistics for almost two decades and is a topic of active research around the world. As the very notion of ?word-hood? in Chinese is hotly debated, so the determination of the correct division of a Chinese sentence into ?words? can be very complex. In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International Chinese Word Segmentation Bakeoff (Sproat and Emerson, 2003). That competition was the first conducted outside of China and has become the benchmark with which researchers evaluate their segmentation systems. During the winter of 2004 it was decided to hold a second evaluation to determine how the latest research has affected segmentation technology. 2! Details of the Contest 2.1! The Corpora Four corpora were used in the evaluation, two each using Simplified and Traditional Chinese characters.1 The Simplified Chinese corpora were provided by Beijing University and Microsoft Research Beijing. The Traditional Chinese corpora were provided by Academia Sinica in Taiwan and the City University of Hong Kong. Each provider supplied separate training and truth data sets. Details on each corpus are provided in Table!1. With one exception, all of the corpora were provided in a single character encoding. We decided to provide all of the data in both Unicode (UTF-8 encoding) and the standard encoding used in each locale. This would allow systems that use one or the other encoding to chose appropriately while ensuring consistent transcoding across all sites. This conversion was problematic in two cases:
###Context text: Conventional approaches to Chinese word segmentation treat the problem as a character-based la- 1Following previous work, in this paper, words can also refer to multi-word expressions, including proper names, long named entities, idioms, etc. beling task #OTHEREFR. Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random fields #OTHEREFR.[Citation]While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order #OTHEREFR. To address this difficulty, a choice is to relax the Markov assumption by using the semi-Markov conditional random field model #OTHEREFR.
###Response:","In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) #REFR, two of the highest scoring systems in the closed track competition were based on a CRF model #OTHEREFR.","In the second international Chinese word segmentation bakeoff, it was found that tackling the word segmentation issue remained a challenge despite the technological advancements #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The state-of-the-art methods used for relation classification are primarily based on statistical machine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language processing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings . Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the features are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.
###Abstract of the cited paper: We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.
###Context text: The task of relation classification is to predict semantic relations between pairs of nominals and can be defined as follows: given a sentence S with the annotated pairs of nominals e and e , we aim to identify the relations between e and e #OTHEREFR. There is considerable interest in automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP applications.[Citation]Supervised approaches are further divided into feature-based methods and kernel-based methods. Feature-based methods use a set of features that are selected after performing textual analysis. They convert these features into symbolic IDs, which are then transformed into a vector using a paradigm that is similar to the bag-of-words model .
###Response:",The most representative methods for relation classification use supervised paradigm; such methods have been shown to be effective and yield relatively high performance #OTHEREFR; #REFR.,Our novel method employing a convolutional DNN to extract crucial features is inspired by #REFR's successful application of the shortest path dependency kernel for relation extraction.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe an exact decoding algorithm for syntax-based statistical translation. The approach uses Lagrangian relaxation to decompose the decoding problem into tractable subproblems, thereby avoiding exhaustive dynamic programming. The method recovers exact solutions, with certificates of optimality, on over 97% of test examples; it has comparable speed to state-of-the-art decoders.
###Abstract of the cited paper: This paper presents a new approach to statistical sentence generation in which Mternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation ffers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.
###Context text: Decoding with these models is challenging, largely because of the cost of integrating an n-gram language model into the search process. Exact dynamic programming algorithms for the problem are well known #OTHEREFR, but are too expensive to be used in practice.2 Previous work on decoding for syntax-based SMT has therefore been focused primarily on approximate search methods. This paper describes an efficient algorithm for exact decoding of synchronous grammar models for translation.[Citation]2E.g., with a trigram language model they run in O#OTHEREFR by using Lagrangian relaxation to decompose the decoding problem into the following sub-problems:
###Response:","We avoid the construction of (Bar-Hillel 1This problem is also relevant to other areas of statistical NLP, for example NL generation #REFR.","Building on the new approach to statistical sentence generation using packed sets of trees presented by #REFR, our algorithm ensures optimal decoding of synchronous grammars."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Our system for the CoNLL 2008 shared task uses a set of individual parsers, a set of stand-alone semantic role labellers, and a joint system for parsing and semantic role labelling, all blended together. The system achieved a macro averaged labelled F - score of 79.79 (WSJ 80.92, Brown 70.49) for the overall task. The labelled attachment score for syntactic dependencies was 86.63 (WSJ 87.36, Brown 80.77) and the labelled F -score for semantic dependencies was 72.94 (WSJ 74.47, Brown 60.18).
###Abstract of the cited paper: We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing. The first stage consists in tuning a single-parser system for each language by optimizing parameters of the parsing algorithm, the feature model, and the learning algorithm. The second stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the optimal parameters settings for each language. When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score.
###Context text: When considering one predicate at a time, SRL becomes a regular labelling problem. Given a predicted predicate, joint learning of syntactic and semantic dependencies can be carried out by simultaneously assigning an argument label and a dependency relation. This is possible because we know a priori where to attach the argument, since there is only one predicate candidate .[Citation]A large feature model was constructed, and backward selection was carried out until no further gain could be observed. The feature model of MaltParser consists of a number of feature types, each describing a starting point, a path through the structure so far, and a column of the node arrived at. The number of feature types was reduced from 37 to 35 based on the labelled F -score.
###Response:","The MaltParser system for English described in #REFR was used as a baseline, and then optimized for this new task, focusing on feature selection.","This approach aligns with the two-stage optimization process of the MaltParser system, which involved optimizing parsing strategies and the feature model for each language(#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins. State-of-the-art methods for arc filtering use separate classifiers to make pointwise decisions about the tree; they label tokens with roles such as root, leaf, or attaches-tothe-left, and then filter arcs accordingly. Because these classifiers overlap substantially in their filtering consequences, we propose to train them jointly, so that each classifier can focus on the gaps of the others. We integrate the various pointwise decisions as latent variables in a single arc-level SVM classifier. This novel framework allows us to combine nine pointwise filters, and adjust their sensitivity using a shared threshold based on arc length. Our system filters 32% more arcs than the independently-trained classifiers, without reducing filtering speed. This leads to faster parsing with no reduction in accuracy.
###Abstract of the cited paper: We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.
###Context text: A dependency tree represents syntactic relationships between words using directed arcs #OTHEREFR. Each token in the sentence is a node in the tree, and each arc connects a head to its modifier.[Citation]In the graph-based setting, a complete search finds the highest-scoring tree under a model that decomposes over one or two arcs at a time. Much of the time for parsing is spent scoring each potential arc in the complete dependency graph #OTHEREFR, one for each ordered word-pair in the sentence. Potential arcs are scored using rich linear models that are discriminatively trained to maximize parsing accuracy #OTHEREFR.
###Response:","There are two dominant approaches to dependency parsing: graph-based and transition-based, where graphbased parsing is understood to be slower, but often more accurate #REFR.","In addressing this issue, we implemented a technique that filters out unlikely arcs proactively to improve parsing speed, a model that extends beyond the traditional approaches #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar. Several variants are presented. One of these deterministically constructs a parse structure while postponing commitment to labels. We investigate theoretical time complexities and report experiments.
###Abstract of the cited paper: We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.
###Context text: In combination with Appendix A, this brings the time complexity down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to #OTHEREFR, in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height.[Citation]For comparing different configurations, one would need to multiply the values E(?, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack ?. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing #OTHEREFR, which is considerably simpler than LR parsing.
###Response:","Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search #OTHEREFR, or A ? search #REFR in order to keep the running time low.","Our methodology aligns with the extended A* search procedure for tabular PCFG parsing proposed by #REFR, dramatically minimizing time complexity."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees. Currently our best automatically induced grammars achieve 80.97% f-score for fstructures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).
###Abstract of the cited paper: We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 EnglJish headwords. We consider the types of errors which arise in creagng such a lexicon, and how such errors can be measured and controlled.
###Context text: F-structure annotations allow us to distinguish passive and active frames. Table 3 shows the most frequent semantic forms for accept. Passive frames are marked p.[Citation]We report on the evaluation of GF-based frames for the full frames with complete prepositional and particle infomation. We use relative conditional probability thresholds #OTHEREFR provide a more detailed description of the extraction and evaluation of semantic forms. Without Prep/Part With Prep/Part Lemmas 3586 3586 Sem.
###Response:",We carried out a comprehensive evaluation of the automatically acquired verbal semantic forms against the COMLEX Resource #REFR for the 2992 active verb lemmas that both resources have in common.,"Table 3's design draws on the methodology used in constructing the Comlex Syntax lexicon, which distinguishes between semantic forms for headwords #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Many recent statistical parsers rely on a preprocessing step which uses hand-written, corpus-specific rules to augment the training data with extra information. For example, head-finding rules are used to augment node labels with lexical heads. In this paper, we provide machinery to reduce the amount of human effort needed to adapt existing models to new corpora: first, we propose a flexible notation for specifying these rules that would allow them to be shared by different models; second, we report on an experiment to see whether we can use Expectation- Maximization to automatically fine-tune a set of hand-written rules to a particular corpus.
###Abstract of the cited paper: We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \[5,9,10,15,17\] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \[9\]. The major technical innovation is tire use of a ""ma~ximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.
###Context text: All of these parsers recover the ?hidden? lexicalizations in a treebank and find the most probable lexicalized tree when parsing, only to strip out this hidden information prior to evaluation. Also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents. In fact, nearly identical head-lexicalizations were used in the dis- S(caught?VBD) NP(boy?NN) DET The NN boy ADVP(also?RB) RB also VP(caught?VBD) VBD caught NP(ball?NN) DET the NN ball[Citation]Inducing a lexicalized structure based on heads has a two-pronged effect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability model?s dependencies, but it also determines which of all possible dependencies? both syntactic and lexical?will be included in the model itself. For example, in Figure 2, the nonterminal NP(boy?NN) is dependent on VP(caught?VBD) and not the other way around.
###Response:","Figure 2: A simple lexicalized parse tree. criminative models described in #OTHEREFR, the generative model in #REFR, the lexicalized TAG extractor in #OTHEREFR.","Our finding aligns with the parser model that obtained outstanding performance on ""standard"" sections of the Wall Street Journal treebank, by guessing lexical head information before their propagation (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with ?- operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain.
###Abstract of the cited paper: This paper explores the use of statistical machine translation (SMT) methods for tactical natural language generation. We present results on using phrase-based SMT for learning to map meaning representations to natural language. Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations. Finally, we show that hybridizing these two approaches results in still more accurate generation systems. Automatic and human evaluation of generated sentences are presented across two domains and four languages.
###Context text: In our previous work #OTHEREFR, semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in various domains.[Citation]Currently, the use of learned synchronous grammars in semantic parsing and NL generation is limited to simple MRLs that are free of logical variables. This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables. This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role #OTHEREFR.
###Response:","More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL #REFR.","Using the statistical machine translation methods explored in #REFR, our work extends to applying these techniques to the learning of synchronous grammars that can generate logical forms."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Semantic relationships between words comprised by thesauri are essential features for IR, text mining and information extraction systems. This paper introduces a new approach to identification of semantic relations such as synonymy by a lexical graph. The graph is generated from a text corpus by embedding syntactically parsed sentences in the graph structure. The vertices of the graph are lexical items (words), their connection follows the syntactic structure of a sentence. The structure of the graph and distances between vertices can be utilized to define metrics for identification of semantic relations. The approach has been evaluated on a test set of 200 German synonym sets. Influence of size of the text corpus, word generality and frequency has been investigated. Conducted experiments for synonyms demonstrate that the presented methods can be extended to other semantic relations.
###Abstract of the cited paper: This work investigates the variation in a word?s distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations).
###Context text: The least independent word is regarded synonymous. Blondell et al #OTHEREFR encode a monolingual dictionary as a graph and identify synonyms by finding subgraphs that are similar to the subgraph corresponding to the queried term. The common evaluation method for similarity metrics is comparing their performance on the same test set with the same context representations with some manually created semantic source as the gold standard #OTHEREFR.[Citation]Different bias towards words with high or low frequency is recognized as one reason for the significant variance of k-nearest neighbors sets of different similarity metrics.
###Response:","Abstracting from results for concrete test sets, #REFR try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends.","The presence of variance in distributionally closest neighbors of a word, influenced by the relative frequency of neighbor words, is noted by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper is about the flow of inference between communicative intentions, discourse structure and the domain during discourse processing. We augment a theory of discourse interpretation with a theory of distinct mental attitudes and reasoning about them, in order to provide an account of how the attitudes interact with reasoning about discourse structure.
###Abstract of the cited paper: We examine the role of temporal connectives in multi-sentence discourse. In certain contexts, sentences containing temporal connectives that are equivalent in temporai structure can fail to be equivalent in terms of discourse coherence. We account for this by offering a novel, formal mechanism for accommodating the presuppositions in temporal subordinate clauses. This mechanism encompasses both accommodation by discourse aftachme,f and accommodation by temporal addition. As such, it offers a precise and systematic model of interactions between presupposed material, discourse context, and the reader is background knowledge. We show how the results of accommodation help to determine a discou~e is coherence.
###Context text: This paper provides the first steps towards a formal analysis of the interaction between intentional structure and informational structure. Our framework for discourse structure analysis is SDRT #OTHEREFR. The basic representational structures of that theory may be used to characterise cognitive states.[Citation]BUSH'S REQUIREMENTS We must represent both the intentional import and the informational import of a discourse simultaneously. So we need a theory of discourse structure where discourse relations central to intentional import and to informational import can hold simultaneously between the same constituents. A logical framework in which all those plausible relations between constituents that are consistent with each other are inferred, such as a nonmonotonic logic like that in DICE #OTHEREFRa), would achieve this.
###Response:","We will extend the logical engine used to infer rhetorical relations--DiCE #OTHEREFRb, #REFR--to model inferences about intentional structure and its interaction with informational structure.",We integrate insights from a study that explored the role of temporal connectives in discourse coherence #REFR. Their findings support our argument for a more comprehensive interpretive framework.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The first step in graph-based semi-supervised classification is to construct a graph from input data. While the k-nearest neighbor graphs have been the de facto standard method of graph construction, this paper advocates using the less well-known mutual k-nearest neighbor graphs for high-dimensional natural language data. To compare the performance of these two graph construction methods, we run semi-supervised classification methods on both graphs in word sense disambiguation and document classification tasks. The experimental results show that the mutual k-nearest neighbor graphs, if combined with maximum spanning trees, consistently outperform the knearest neighbor graphs. We attribute better performance of the mutual k-nearest neighbor graph to its being more resistive to making hub vertices. The mutual k-nearest neighbor graphs also perform equally well or even better in comparison to the state-of-the-art b-matching graph construction, despite their lower computational complexity.
###Abstract of the cited paper: Graph-based semi-supervised learning has recently emerged as a promising approach to data-sparse learning problems in natural language processing. All graph-based algorithms rely on a graph that jointly represents labeled and unlabeled data points. The problem of how to best construct this graph remains largely unsolved. In this paper we introduce a data-driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classifier. We apply this technique in the framework of label propagation and evaluate it on two different classification tasks, a multi-class lexicon acquisition task and a word sense disambiguation task. Significant improvements are demonstrated over both label propagation using conventional graph construction and state-of-the-art supervised classifiers.
###Context text: Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data. In particular, graph-based techniques for semi-supervised classification #OTHEREFR are recognized as a promising approach.[Citation]However, the focus of these studies is how to assign accurate labels to vertices in a given graph. By contrast, there has not been much work on how such a graph should be built, and graph construction remains ?more of an art than a science? #OTHEREFR. Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results.
###Response:","Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation #REFR, sentiment analysis #OTHEREFR, to name but a few.","In addressing this issue, our study adopts the mutual k-nearest neighbor graphs as a more effective method for graph construction, inspired by a data-driven technique optimized by a supervised classifier presented by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: It is important to identify complementtaking nouns in order to properly analyze the grammatical and implicative structure of the sentence. This paper examines the ways in which these nouns were identified and classified for addition to the BRIDGE natural language understanding system.
###Abstract of the cited paper: The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides aset of coindexed null elements in what can be thought of as ""underlying"" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides ome non-context free annotational mechanism to allow the structure of discontinuous constituents o be easily recovered, and allows for a clear, concise tagging system for some semantic roles.
###Context text: However, there are many complement-taking nouns that are not deverbal.[Citation]For each of these lexical items, we extracted words in the same semantic class from WordNet. Classes include words like fact, which direct attention to the clausal complement, as in (15), and nouns expressing emotion, as in (16). (15) It?s a fact that Mary came. (16) Bob?s joy that Mary had returned reduced him to tears. These semantic classes provided a starting point for discovering more of these nouns: the class of emotion nouns, for example, has more than a hundred hyponyms.
###Response:","To expand our lexicon of these nouns, we started with a seed set garnered from the Penn Treebank #REFR, which uses distinctive tree structures for complement-taking nouns, and a small list of linguistically prominent nouns.","The recent development in the Penn Treebank annotation scheme, which clears up the tagging system for semantic roles, is a vital tool in identifying these nouns [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We study subjective language in social media and create Twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams. Starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap Twitter-specific sentiment lexicons, using a small amount of labeled data to guide the process. Our experiments on English, Spanish and Russian show that the resulting lexicons are effective for sentiment classification for many underexplored languages in social media.
###Abstract of the cited paper: Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has properties that support the Polyanna Hypothesis. Using the General Inquirer as gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above.
###Context text: Mihalcea et.al #OTHEREFR classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Dictionary-based methods rely on existing lexical resources to bootstrap sentiment lexicons. Many researchers have explored using relations in WordNet #OTHEREFR for Spanish.[Citation]Other works #OTHEREFR automatically expands and evaluates German and Arabic lexicons. However, the lexical resources that dictionary-based methods need, do not yet exist for the majority of languages in social media. There is also a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language of social media.
###Response:",#REFR use a thesaurus to aid in the construction of a sentiment lexicon for English.,"In contrast, the simple approach proposed by the referenced study utilizes a Roget-like thesaurus to generate a comprehensive semantic orientation lexicon, supporting various languages #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.
###Abstract of the cited paper: Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate ?A because B? as ?B because A.. Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics.
###Context text: A perennial challenge in building Statistical Machine Translation (SMT) systems is the dearth of high-quality bitext in the domain of interest. An effective and practical solution is adaptation data selection: the idea is to use language models (LMs) trained on in-domain text to select similar sentences from large general-domain corpora. The selected sentences are then incorporated into the SMT training data.[Citation]Although previous works in data selection #OTHEREFR have shown substantial gains, we suspect that the commonly-used n-gram LMs may be sub-optimal. The small size of the in-domain text implies that a large percentage of generaldomain sentences will contain words not observed in the LM training data. In fact, as many as 60% of general-domain sentences contain at least one unknown word in our experiments.
###Response:",Analyses have shown that this augmented data can lead to better statistical estimation or word coverage #REFR.,"In light of the BLEU metric's limitations highlighted by #REFR, their relevance to evaluating the effectiveness of data selection using neural language models deserves scrutiny."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a web mining method for discovering and enhancing relationships in which a specified concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work. Our method is based on clustering patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision.
###Abstract of the cited paper: This paper presents an unsupervised method for assembling semantic knowledge from a part-ofspeech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word.
###Context text: [Citation]Although examination of any such set invariably makes it clear why these words have been grouped together into a single concept, it is important to emphasize that the method itself provides no explicit concept definition; in some sense, the implied class is in the eye of the beholder. Nevertheless, both human judgment and comparison with standard lists indicate that the generated sets correspond to concepts with high precision. We wish now to build on that result in the following way.
###Response:","In several studies (e.g., #REFR it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense.","Drawing on the unsupervised method for assembling semantic knowledge proposed by #REFR, our web mining approach enhances relationships between concept words."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Most research on semantic role labeling (SRL) has been focused on training and evaluating on the same corpus in order to develop the technology. This strategy, while appropriate for initiating research, can lead to over-training to the particular corpus. The work presented in this paper focuses on analyzing the robustness of an SRL system when trained on one genre of data and used to label a different genre. Our state-of-the-art semantic role labeling system, while performing well on WSJ test data, shows significant performance degradation when applied to data from the Brown corpus. We present a series of experiments designed to investigate the source of this lack of portability. These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown corpus data. Our results indicate that while syntactic parses and argument identification port relatively well to a new genre, argument classification does not. Our analysis of the reasons for this is presented and generally point to the nature of the more lexical/semantic features dominating the classification task and general structural features dominating the argument identification task.
###Abstract of the cited paper: The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides aset of coindexed null elements in what can be thought of as ""underlying"" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides ome non-context free annotational mechanism to allow the structure of discontinuous constituents o be easily recovered, and allows for a clear, concise tagging system for some semantic roles.
###Context text: The arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a transitive verb) ARG1 is the PROTO-PATIENT (usually its direct object), etc. In addition to these CORE ARGU- MENTS, 16 additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked. More recently the PropBanking effort has been extended to encompass multiple corpora.[Citation]The WSJ PropBank data comprise 24 sections of the WSJ, each section representing about 100 documents. PropBank release 1.0 contains about 114,000 predicates instantiating about 250,000 arguments and covering about 3,200 verb lemmas. Section 23, which is a standard test set and a test set in some of our experiments, comprises 5,400 predicates instantiating about 12,000 arguments.
###Response:",In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank #REFR and part of the Brown portion of the Penn Treebank.,"A new syntactic annotation scheme implemented in the Penn Treebank highlights predicate-argument structure, providing a clear tagging system for semantic roles [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids. These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions. We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid, which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter, Wikipedia, and Flickr. We also show that logistic regression performs feature selection effectively, assigning high weights to geocentric terms.
###Abstract of the cited paper: The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation. In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions. High-level topics such as ?sports? or ?entertainment? are rendered differently in each geographic region, revealing topic-specific regional distinctions. Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency. The model also enables prediction of an author?s geographic location from raw text, outperforming both text regression and supervised topic models.
###Context text: Images in social networks such as Flickr may be geotagged by a camera and their textual tags can be treated as documents. Likewise, tweets in Twitter are often geotagged; in this case, it is possible to view either an individual tweet or the collection of tweets for a given user as a document, respectively identifying the location as the place from which the tweet was sent or the home location of the user. Early work on document geolocation used heuristic algorithms, predicting locations based on toponyms in the text #OTHEREFR.[Citation]A number of researchers have used metadata of various sorts for document or user geolocation, including document links and social network connections. This research has sometimes been applied to Wikipedia #OTHEREFR. We tackle document geolocation using supervised methods based on the textual content of documents, ignoring their metadata.
###Response:","More recently, various researchers have used topic models for document geolocation #OTHEREFR; #REFR or other types of geographic document summarization #OTHEREFR.","Building upon the multi-level generative model presented by #REFR, we have made strides in improving geolocation accuracy by utilizing logistic regression models."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: To date, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence in the noisy domain of learner texts. We present the first systematic analysis of several methods for assessing coherence under the framework of automated assessment (AA) of learner free-text responses. We examine the predictive power of different coherence models by measuring the effect on performance when combined with an AA system that achieves competitive results, but does not use discourse coherence features, which are also strong indicators of a learner?s level of attainment. Additionally, we identify new techniques that outperform previously developed ones and improve on the best published result for AA on a publically-available dataset of English learner free-text examination scripts.
###Abstract of the cited paper: We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.
###Context text: They assess semantic relatedness using vector-based similarity between adjacent sentences. They argue that LSA may be more appropriate for comparing the relative quality of texts; for determining the overall text coherence it may be difficult to set a criterion for the coherence value since it depends on a variety of different factors, such as the size of the text units to be compared. Nevertheless, our results show that ISA, a similar distributional semantic model with dimensionality reduction, improves FCE grading accuracy.[Citation]In the HMM, states correspond to distinct topics, and transitions between states represent the probability of moving from one topic to another. This approach has the advantage of capturing the order in which different topics appear in texts; however, the HMMs are highly domain specific and would probably need retraining for each distinct essay prompt. Soricut and Marcu #OTHEREFR use a log-linear model that combines local and global models of coherence and show that it outperforms each of the individual ones on news articles and accident reports.
###Response:",#REFR implement lexicalized content models that represent global text properties on news articles and narratives using Hidden Markov Models (HMMs).,"By incorporating techniques from #REFR, which involves modeling the content structure and chronological topic shifts within a text domain, FCE grading accuracy can further improve."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a novel approach to the problem of overfitting in the training of stochastic models for selecting parses generated by attributevalued grammars. In this approach, statistical features are merged according to the frequency of linguistic elements within the features. The resulting models are more general than the original models, and contain fewer parameters. Empirical results from the task of parse selection suggest hat the improvement in performance over repeated iterations of iterative scaling is more reliable with such generalized models than with ungeneralized models.
###Abstract of the cited paper: We argue that some of the computational complexity associated with estimation of stochastic attributevalue grammars can be reduced by training upon an informative subset of the full training set. Results using the parsed Wall Street Journal corpus show that in some circumstances, it is possible to obtain better estimation results using an informative sample than when training upon all the available material. Further experimentation demonstrates that with unlexicalised models, a Gaussian prior can reduce overfitting. However, when models are lexiealised and contain overlapping features, overfitting does not seem to be a problem, and a Gmlssian prior makes minimal difference to performance. Our approach is applicable for situal;ions when there are an infeasibly large mnnber of parses in the training set, or else for when recovery of these parses fl'om a packed representation is itself comi)utationally expensive.
###Context text: The merged feature represents he union of the sets of tokens described by the unmerged feature types. All instances of the original two features would now be replaced in the data by the merged feature. specific features which occur rarely to produce more general features which occur more often, resulting in fewer total features used. Even if the events are not noisy or inaccurate in actual fact, they may still contribute to overfitting if their features occur too infrequently in the data to give accurate frequencies.[Citation]Each feature, as described above, is made up of discrete elements, which may include such objects as lexical items, POS tags, and grammatical attribute information, depending on the schema being used. The rarity of the feature in the data is largely--although not entirely-- determined by the rarity of elements within it. In the present merging scheme, a set of elements is collected whose empirical frequencies are below some predetermined cutoff point.
###Response:","The merging procedure seeks to address overfitting at the level of the features themselves and remain true to the spirit of the maximum entropy approach, which seeks to represent what is unknown about the data with uniformity of the distribution, rather than by making adjustments on the model distribution itself, such as the Gaussian prior of #REFR.","As noted by #REFR, reducing computational complexity is possible by training on an informative subset of the full training set, which can in turn mitigate overfitting."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: When speech understanding systems are used in real applications, they encounter incidental noise generated by the speaker and the environment. Such noises can cause serious problems for speech recognizers not designed to cope with them. We attempt to model these noises by training HMM ""noise words"" to match classes of noises. The noise words were incorporated into the Sphinx system and performance compared to the system without noise words. Initial results uggest that the technique does increase system performance significantly.
###Abstract of the cited paper: To study the spoken language interface in the context of a complex problem-solving task, a group of users were asked to perform a spreadsheet task, alternating voice and keyboard input. A total of 40 tasks were performed by each participant, the first thirty in a group (over several days), the remaining ones a month later. The voice spreadsheet program used in this study was extensively instrumented to provide detailed information about the components of the interaction. These data, as well as analysis of the participants is utterances and recognizer output, provide a fairly detailed picture of spoken language interaction. Although task completion by voice took longer than by keyboard, analysis hows that users would be able to perform the spreadsheet task faster by voice, if two key criteria could be met: recognition occurs in real-time, and the error rate is sufficiently low. This initial experience with a spoken language system also allows us to identify several metrics, beyond those traditionally associated with speech recognition, that can be used to characterize system performance.
###Context text: [Citation]These experiments are presented by the authors in these proceedings. Among other things, they show that non-verbal events (non-stationary noises) do create serious problems for speech recognizers. These sounds are generated both by the speaker and by the environment.
###Response:",Recent experiments performed by two groups of researchers at CMU have gathered ata on subjects using speech recognizers in office-like environments #REFR.,"The study by #REFR reveals that incidental noises, such as those produced by the speaker and the environment, pose substantial challenges to the efficiency of speech recognition systems."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain.
###Abstract of the cited paper: Applying the noisy channel model to search query spelling correction requires an error model and a language model. Typically, the error model relies on a weighted string edit distance measure. The weights can be learned from pairs of misspelled words and their corrections. This paper investigates using the Expectation Maximization algorithm to learn edit distance weights directly from search query logs, without relying on a corpus of paired words.
###Context text: As a simple example, if we want to avoid maintaining a manually-crafted dictionary to accommodate the wealth of new terms introduced every day (e.g., ipod and ipad), we may decide that atachment is a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli ? sandeep kohli (rather than, e.g., sadeep kohli ? sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell ? excel.[Citation]However, while query logs are abundant in the context of Web search, in many other search applications (e.g. email search, desktop search, and even small-enterprise search) query logs are too scarce to provide statistical information that is sufficient for effective spelling correction. Even an email provider of a massive scale (such as GMail) may need to rely on the (possibly tiny) query log of the single user at hand, due to privacy or security concerns; moreover, as noted earlier about kohli, the statistics of one user may be relevant to one user, while irrelevant to another. The focus of this paper is on spelling correction for search applications like the above, where querylog analysis is impossible or undesirable (with email search being a prominent example).
###Response:",A source of statistics widely used in prior work is the query log #OTHEREFR; #REFR.,"In line with the application of the noisy channel model for search query spelling correction by #REFR, it is significant to implement an algorithm that learns from corpus data, as opposed to relying solely on query logs."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Integration of language constraints into a large vocabulary speech recognition system often leads to prohibitive complexity. We propose to factor the constraints into two components. The first is characterized by a covering grammar which is small and easily integrated into existing speech recognizers. The recognized string is then decoded by means of an efficient language post-processor in which the full set of constraints is imposed to correct possible errors introduced by the speech recognizer.
###Abstract of the cited paper: We report on some recent improvements o an HMM- based, continuous peech recognition system which is being developed at AT&T Bell Laboratories. These advances, which include the incorporation of inter-word, context-dependent units and an improved feature analysis, lead to a recognition system which achieves better than 95% word accuracy for speaker independent recognition of the 1000-word, DARPA resource management task using the standard word-pair grammar (with a perplexity of about 60). It will he shown that the incorporation of inter-word units into training results in better acoustic models of word juncture coarticulation and gives a 20% reduction in error rate. The effect of an improved set of spectral and log energy features is to further reduce word error rate by about 30%. We also found that the spectral vectors, corresponding to the same speech unit, behave differently statistically, depending on whether they are at word boundaries or within a word. The results suggest hat intra-word and inter-word units should be modeled independently, even when they appear in the same context. Using a set of sub-word units which included variants for intra-word and inter-word, context-dependent phones, an additional decrease of about 10% in word error rate resulted.
###Context text: Given a sequence of acoustic patterns, corresponding to an uttered sentence, find the sequence of acoustic states in the decoding network that gives the highest likelihood score when aligned with the input sequence of acoustic patterns. This problem can be solved efficiently and effectively using a dynamic programming search procedure. The resulting optimal path through the network gives the optimal sequence of acoustic states, which represents a sequence of phonetic units, and eventually the recognized string of words.[Citation]The complexity of such an algorithm consists of two factors. The first is the complexity arising from the computation of the likelihood scores for all the possible pairs of acoustic state and acoustic pattern. Given an utterance of fixed length the complexity is linear with the number of distinct acoustic states.
###Response:",Details about the speech recognition system we refer to in the paper can be found in #REFR/1).,"Drawing on the work at AT&T Bell Laboratories, the incorporation of inter-word, context-dependent units into training significantly enhances our acoustic models (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process.
###Abstract of the cited paper: The quality of the part-of-speech (PoS) annotation in a corpus is crucial for the development of PoS taggers. In this paper, we experiment with three complementary methods for automatically detecting errors in the PoS annotation for the Icelandic Frequency Dictionary corpus. The first two methods are language independent and we argue that the third method can be adapted to other morphologically complex languages. Once possible errors have been detected, we examine each error candidate and hand-correct the corresponding PoS tag if necessary. Overall, based on the three methods, we handcorrect the PoS tagging of 1,334 tokens (0.23% of the tokens) in the corpus. Furthermore, we re-evaluate existing state-ofthe-art PoS taggers on Icelandic text using the corrected corpus.
###Context text: The methods presented so far have limited definitions of comparability.[Citation]Namely, we include: a) frequency information of an overall dependency rule and b) information on how likely each dependent is to be in a relation with its head, described next.
###Response:","As using complementary information has been useful in, e.g., POS error detection #REFR, we explore other simple comparable properties of a dependency grammar.","Drawing upon similar techniques as in #REFR, which used three methods to detect and rectify annotation errors in part-of-speech tagging, our approach also identifies potential inaccuracies in dependency rules."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes a novel functionality of the VERBMOBIL system, a large scale translation system designed for spontaneously spoken multilingual negotiation dialogues. The task is the on-demand generation of dialogue scripts and result summaries of dialogues. We focus on summary generation and show how the relevant data are selected from the dialogue memory and how they are packed into an appropriate abstract representation. Finally, we demonstrate how the existing generation module of VERBMOBIL was extended to produce multilingual and result summaries from these representations.
###Abstract of the cited paper: The paper discusses how compositional semantics is implemented in the Verbmobil speech-to-speech translation system using LUD, a description language for underspecified discourse representation structures. The description language and its formal interpretation in DRT are described as well as its implementation together with the architecture of the system is entire syntactic-semantic processing module. We show that a linguistically sound theory and formalism can be properly implemented in a system with (near) real-time requirements.
###Context text: Other acts can carry propositional content, like SUGGEST and INFORM_FEATURE. To obtain a good translation and enhance the robustness of the overall system the translation is based on several competing translation tracks, each based on different paradigms. The deep translation track consists of an HPSG based analysis, semantic transfer and finally a TAG-based generator (VM- GECO).[Citation]It consists of a set of semantic onditions (i.e. predicates, roles, operators and quantifiers) and allows for underspecification with respect o scope and subordination or inherent underspecification. A graphical representation of the VIT for the English sentence ""They will meet at the station"" is shown in figure 2. Besides the deep translation track several shallow tracks have been developed.
###Response:",The linguistic information within this track is encoded in a so-called VIT 2 #REFR which is a formalism following DRT.,"The implementation of compositional semantics in the VERBMOBIL translation system via LUD, as discussed in #REFR, forms a fundamental basis for these semantic conditions."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a novel two-stage method for mining opinion words and opinion targets. In the first stage, we propose a Sentiment Graph Walking algorithm, which naturally incorporates syntactic patterns in a Sentiment Graph to extract opinion word/target candidates. Then random walking is employed to estimate confidence of candidates, which improves extraction accuracy by considering confidence of patterns. In the second stage, we adopt a self-learning strategy to refine the results from the first stage, especially for filtering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous methods. The experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods.
###Abstract of the cited paper: The technology of opinion extraction allows users to retrieve and analyze people?s opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues. Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks.
###Context text: There have been extensive works on mining opinion words and opinion targets by syntactic pattern learning. Riloff and Wiebe #OTHEREFR performed pattern learning through bootstrapping while extracting subjective expressions. Zhuang et al #OTHEREFR obtained various dependency relationship templates from an annotated movie corpus and applied them to supervised opinion words/targets extraction.[Citation]Our approach is similar to #OTHEREFR, all of which apply syntactic pattern learning and adopt self-learning strategy. However, the task of #OTHEREFR was to classify sentiment orientations in sentence level, while ours needs to extract more detailed information in term level. In addition, our method extends #OTHEREFR, and we give a more complete and in-depth analysis on the aforementioned problems in the first section.
###Response:",#REFR adopted a supervised learning technique to search for useful syntactic patterns as contextual clues.,"Our sentiment graph walking algorithm and subsequent refinement process share some foundation with the technology of opinion extraction detailed by #REFR, which utilized contextual and statistical cues for evaluation extraction."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.
###Abstract of the cited paper: We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.
###Context text: Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring #OTHEREFR.[Citation]In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning #OTHEREFR and additive to its pruning power. The main intuition of our method is to find those source phrases (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints. We show that by using highly-efficient sequence labelling models learned from the bitext used for translation model training, such phrases can be effectively identified prior to MT decoding, and corresponding chart cells can be excluded for decoding without affecting translation quality.
###Response:","For more expressive, linguistically-motivated syntactic MT models #REFR, the grammar complexity has grown considerably over hierarchical phrase-based models #OTHEREFR.","Building upon the theory of word-level alignments defined over parallel corpora #REFR, we have applied this to identify source phrases unlikely to align with target constituents."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper reports on a shared task involving the assignment of ICD-9-CM codes to radiology reports. Two features distinguished this task from previous shared tasks in the biomedical domain. One is that it resulted in the first freely distributable corpus of fully anonymized clinical text. This resource is permanently available and will (we hope) facilitate future research. The other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels. The number of participants was larger than in any previous biomedical challenge task. We describe the data production process and the evaluation measures, and give a preliminary analysis of the results. Many systems performed at levels approaching the inter-coder agreement, suggesting that human-like performance on this task is within the reach of currently available technologies.
###Abstract of the cited paper: Deidentification of clinical records is a crucial step before these records can be distributed to non-hospital researchers. Most approaches to deidentification rely heavily on dictionaries and heuristic rules; these approaches fail to remove most personal health information (PHI) that cannot be found in dictionaries. They also can fail to remove PHI that is ambiguous between PHI and non-PHI. Named entity recognition (NER) technologies can be used for deidentification. Some of these technologies exploit both local and global context of a word to identify its entity type. When documents are grammatically written, global context can improve NER. In this paper, we show that we can deidentify medical discharge summaries using support vector machines that rely on a statistical representation of local context. We compare our approach with three different systems. Comparison with a rulebased approach shows that a statistical representation of local context contributes more to deidentification than dictionaries and hand-tailored heuristics. Comparison with two well-known systems, SNoW and IdentiFinder, shows that when the language of documents is fragmented, local context contributes more to deidentification than global context.
###Context text: These data are among those most commonly used, and are designed to provide enough codes to cover a substantial proportion of pediatric radiology activity. Expunging patient identity to meet HIPAA standards included three steps: disambiguation, anonymization, and data scrubbing #OTHEREFR. Ambiguity and Anonymization.[Citation]The reason is that clinical text is dense with medical jargon, abbreviations, and acronyms, many of which turn out to be ambiguous between a sense that needs anonymization and a different sense that does not. For example, in a clinical setting, FT can be an abbreviation for full-term, fort (as in Fort Bragg), feet, foot, field test, full-time or family therapy. Fort Bragg, being a place name, and a possible component of an address, could indirectly lead to identification of the patient.
###Response:","Not surprisingly, some degree of disambiguation is needed to carry out effective anonymization #REFR.","The successful deidentification process applied in our task was largely facilitated by leveraging local context, as demonstrated by previous studies #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Our field has seen significant improvements in the quality of machine translation systems over the past several years. The single biggest factor in this improvement has been the accumulation of ever larger stores of data. However, we now find ourselves the victims of our own success, in that it has become increasingly difficult to train on such large sets of data, due to limitations in memory, processing power, and ultimately, speed (i.e., data to models takes an inordinate amount of time). Some teams have dealt with this by focusing on data cleaning to arrive at smaller data sets (Denkowski et al 2012a; Rarrick et al 2011), ?domain adaptation? to arrive at data more suited to the task at hand (Moore and Lewis, 2010; Axelrod et al 2011), or by specifically focusing on data reduction by keeping only as much data as is needed for building models e.g., (Eck et al 2005). This paper focuses on techniques related to the latter efforts. We have developed a very simple n-gram counting method that reduces the size of data sets dramatically, as much as 90%, and is applicable independent of specific dev and test data. At the same time it reduces model sizes, improves training times, and, because it attempts to preserve contexts for all n-grams in a corpus, the cost in quality is minimal (as measured by BLEU ). Further, unlike other methods created specifically for data reduction that have similar effects on the data, our method scales to very large data, up to tens to hundreds of millions of parallel sentences.
###Abstract of the cited paper: It is possible to reduce the bulk of phrasetables for Statistical Machine Translation using a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score. In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrasetable smoothing is employed.
###Context text: This demonstrates the importance of developing a scalable algorithm when addressing the data selection problem. The careful reader may observe that an alternate strategy for reducing model sizes #OTHEREFRb) 2.3M Table 1: Data Sizes for Related Systems than reduce training data size.[Citation]The principal of our work versus theirs is where the data reductions occur: before or after training. The primary benefit of manipulating the training data directly is the impact on training performance. Further, given the increasing sizes of training data, it has become more difficult and more time consuming to train on large data, and in the case of very large data (say tens to hundreds of millions of sentence pairs), it may not even be possible to train models at all.
###Response:","A good example of work in this space is shown in #REFR, who describe a method for phrase table reduction, sometimes substantial (>90%), with no impact on the resulting BLEU scores.","Through focusing on phrase pair co-occurrence in the parallel corpus, we can significantly reduce the size of phrasetables for statistical machine translation #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper extends the training and tuning regime for phrase-based statistical machine translation to obtain fluent translations into morphologically complex languages (we build an English to Finnish translation system). Our methods use unsupervised morphology induction. Unlike previous work we focus on morphologically productive phrase pairs ? our decoder can combine morphemes across phrase boundaries. Morphemes in the target language may not have a corresponding morpheme or word in the source language. Therefore, we propose a novel combination of post-processing morphology prediction with morpheme-based translation. We show, using both automatic evaluation scores and linguistically motivated analyses of the output, that our methods outperform previously proposed ones and provide the best known results on the English- Finnish Europarl translation task. Our methods are mostly language independent, so they should improve translation into other target languages with complex morphology.
###Abstract of the cited paper: We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ? may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.
###Context text: The work on morphology in MT can be grouped into three categories, factored models, segmented translation, and morphology generation.[Citation]We discussed the drawbacks of factored models for our task in Section 2.1. While #OTHEREFR obtain improvements using factored models for translation into English, German, Spanish, and Czech, these models may be less useful for capturing long-distance dependencies in languages with much more complex morphological systems such as Finnish. In our experiments factored models did worse than the baseline.
###Response:","Factored models #REFR factor the phrase translation probabilities over additional information annotated to each word, allowing for text to be represented on multiple levels of analysis.","In enhancing the phrase-based statistical machine translation, we incorporated an extended version of the factored translation models presented by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: How can proteins fold so quickly into their unique native structures. We show here that there is a natural analogy between parsing and the protein folding problem, and demonstrate that CKY can find the native structures of a simplified lattice model of proteins with high accuracy.
###Abstract of the cited paper: We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.
###Context text: This may indicate a deeper, natural connection between these two processes. Given that hierarchical search strategies for protein folding have been proposed in the biological literature, our primary interest here has been the question of whether a greedy, hierarchical search as implemented in CKY is able to identify the native state of proteins in the HP model. The research presented here aims to verify these predictions with an explicit computational model.[Citation]We also plan to adapt this technique to other, more realistic, representations of proteins, and to longer sequences. For longer sequences, we will take advantage of the fact that CKY is easily parallelizable, since any operation which combines the entries of two cells chart i k and chart k 1 j is completely independent of other parts of the chart. If the routes by which proteins fold really are trees, a dynamic programming technique such as CKY is inherently suited to model this process, since it is the most efficient way to search all possible trees.
###Response:","Therefore, we were less concerned with improving efficiency, and more with the properties of this algorithm, which we consider a baseline method upon which more sophisticated techniques such as best-first parsing #OTHEREFR or A search #REFR may well be able to improve.","Utilizing the techniques presented by #REFR, where A* search method dramatically reduces time in finding best parses, would enhance progress in protein structure identification."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively.
###Abstract of the cited paper: We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.
###Context text: [Citation]Given our improved segmentation results presented in the previous section, improvements would be expected in full sentencelevel discourse parsing. To achieve this, we modified the SPADE script to accept our segmentations when building the fully hierarchical discourse tree. The results for three systems are presented in Table 3: SPADE, our ?Full finite-state? system, and our system with all features.
###Response:",It has been shown that accurate discourse segmentation within a sentence greatly improves the overall parsing accuracy to near human levels #REFR.,"Influenced by the work of #REFR, which introduces probabilistic models to identify discourse units and construct discourse parse trees, our research also shows promising results."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Named Entity Recognition (NER) is the task of identifying and classifying all proper nouns in a document as person names, organization names, location names, date & time expressions and miscellaneous. Previous work (Cucerzan and Yarowsky, 1999) was done using the complete words as features which suffers from a low recall problem. Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. Applying the same technique on Indian Languages, we experimented with Conditional Random Fields (CRFs), a discriminative model, and evaluated our system on two Indian Languages Telugu and Hindi. The character n-gram based models showed considerable improvement over the word based models. This paper describes the features used and experiments to increase the recall of Named Entity Recognition Systems which is also language independent.
###Abstract of the cited paper: Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the manyto-many mapping between ames and their referents. We analyze the types of ambiguity - - structural and semantic - - that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center.
###Context text: There are many types of features used in NER systems.[Citation]Following are examples of commonly used binary features: All-Caps #OTHEREFR etc. The features that correspond to the capitalization are not applicable to Indian languages. Also, we have not used any of the binary features in any of our models.
###Response:","Many systems use binary features i.e. the word-internal features, which indicate the presence or absence of particular property in the word. #OTHEREFR; #REFR.","Addressing the challenges with identifying proper names in texts due to structural and semantic ambiguities, we leveraged the heuristics implemented in the Nominator module #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Parser disambiguation with precision grammars generally takes place via statistical ranking of the parse yield of the grammar using a supervised parse selection model. In the standard process, the parse selection model is trained over a hand-disambiguated treebank, meaning that without a significant investment of effort to produce the treebank, parse selection is not possible. Furthermore, as treebanking is generally streamlined with parse selection models, creating the initial treebank without a model requires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these HPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources.
###Abstract of the cited paper: Statisti(:a,1 signiticance testing of (litl'erelmeS in v;~hl(`-s of metri(:s like recall, i)rccision and batau(:(~(l F-s(:()rc is a ne(:(`-ssary t)art of eml)irical ual;ural language 1)ro(:essing. Unfortunately, we lind in a set of (;Xl)erinlc\]d;s (;hal; many (:oreinertly used tesl;s ofte, n underest imate t.he s ignif icancc an(l so are less likely to detect differences that exist 1)el;ween ditl'ercnt techniques. This undel'esi;imation comes from an in(let)endcn('(~ a,-;SUlnl)tion that is often violated. \~fe l)oint out some useful l;e,%s (;hal; (lo nol; make this assuml)- lion, including computationally--intcnsive rand()mizat,ion 1;cs|;s.
###Context text: In that original work, the supertags were LTAG #OTHEREFR elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple #OTHEREFR both look at how discriminatory a tag sequence is in filtering a parse forest.[Citation]Toutanova et al #OTHEREFR present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model #OTHEREFR. In Dridan #OTHEREFR, tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high.
###Response:","This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in #REFR, with p < 0.05. work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse.","In evaluating the efficacy of these techniques, the relevance of statistical significance testing in processing natural language metrics becomes apparent #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres ? data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formulation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.
###Abstract of the cited paper: Within the area of general-purpose finegrained subjectivity analysis, opinion topic identification has, to date, received little attention due to both the difficulty of the task and the lack of appropriately annotated resources. In this paper, we provide an operational definition of opinion topic and present an algorithm for opinion topic identification that, following our new definition, treats the task as a problem in topic coreference resolution. We develop a methodology for the manual annotation of opinion topics and use it to annotate topic information for a portion of an existing general-purpose opinion corpus. In experiments using the corpus, our topic identification approach statistically significantly outperforms several non-trivial baselines according to three evaluation measures.
###Context text: Some shifts are gradual, others ? more abrupt. Topical text segmentation identifies the more noticeable topic shifts. A topical segmenter?s output is a very simple picture of the document?s structure.[Citation]That is why improved quality of text segmentation can benefit other language-processing tasks. We present Affinity Propagation for Segmentation #OTHEREFR.1 The original AP algorithm considerably improved exemplarbased clustering both in terms of speed and the quality of solutions. That is why we chose to adapt it to segmentation.
###Response:","Segmentation is a useful intermediate step in such applications as subjectivity analysis #REFR, automatic summarization #OTHEREFR and others.","Leveraging the method provided by #REFR for opinion topic identification, our approach treats the task of text segmentation as a problem in topic coreference resolution."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Hierarchical Hidden Markov Model (HHMM) parsers have been proposed as psycholinguistic models due to their broad coverage within human-like working memory limits (Schuler et al, 2008) and ability to model human reading time behavior according to various complexity metrics (Wu et al, 2010). But HHMMs have been evaluated previously only with very wide beams of several thousand parallel hypotheses, weakening claims to the model?s efficiency and psychological relevance. This paper examines the effects of varying beam width on parsing accuracy and speed in this model, showing that parsing accuracy degrades gracefully as beam width decreases dramatically (to 2% of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser.
###Abstract of the cited paper: A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times.
###Context text: Hale #OTHEREFR introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t ? 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. #OTHEREFRa) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser.[Citation]Wu et al #OTHEREFR evaluate the same Hierarchical Hidden Markov Model parser used in this work in terms of its ability to reproduce human-like results for various complexity metrics, including some of those mentioned above, and introduce a new metric called embedding difference. This metric is based on the idea of embedding depth, which is the number of elements in the memory store required to hold a given hypothesis. Using more memory elements corresponds to center embedding in phrase structure trees, and presumably correlates to some degree with complexity.
###Response:","#REFR dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and showed that these new metrics are strong predictors of reading difficulty.","In their research, the authors presented methods for deriving lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we introduce several vector space manipulation methods that are applied to trained vector space models in a post-hoc fashion, and present an application of these techniques in semantic role labeling for Finnish and English. Specifically, we show that the vectors can be circularly shifted to encode syntactic information and subsequently averaged to produce representations of predicate senses and arguments. Further, we show that it is possible to effectively learn a linear transformation between the vector representations of predicates and their arguments, within the same vector space.
###Abstract of the cited paper: We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org
###Context text: This gives us the opportunity to build two models, one for the word forms and the other for the lemmas. Both Finnish models have 300 dimensions. For English, the vector representation is induced on the union of the English Wikipedia (1.7B tokens) and the English Gigaword corpus (4B tokens), the total training data size thus being 5.7 billion tokens.[Citation]The English model has 200 dimensions.
###Response:",Sentence splitting and tokenization was carried out using the relevant modules from the BRAT package #REFR.,"We employed the BRAT annotation tool as documented in #REFR which supports text annotation, aiding us in our model construction for semantic role labeling."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost of annotation are active learning and crowdsourcing. However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.
###Abstract of the cited paper: We consider the impact Active Learning (AL) has on effective and efficient text corpus annotation, and report on reduction rates for annotation efforts ranging up until 72%. We also address the issue whether a corpus annotated by means of AL ? using a particular classifier and a particular feature set ? can be re-used to train classifiers different from the ones employed by AL, supplying alternative feature sets as well. We, finally, report on our experience with the AL paradigm under real-world conditions, i.e., the annotation of large-scale document corpora for the life sciences.
###Context text: AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling.[Citation]Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk #OTHEREFR.
###Response:","AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging #OTHEREFR, and named entity recognition (NER) #REFR.","AL can significantly minimize annotation efforts, reducing them up to 72% and providing valuable data for training different classifiers #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.
###Abstract of the cited paper: This paper explores a parsimonious approach to Data-Oriented Parsing. While allowing, in principle, all possible subtrees of trees in the treebank to be productive elements, our approach aims at finding a manageable subset of these trees that can accurately describe empirical distributions over phrase-structure trees. The proposed algorithm leads to computationally much more tracktable parsers, as well as linguistically more informative grammars. The parser is evaluated on the OVIS and WSJ corpora, and shows improvements on efficiency, parse accuracy and testset likelihood.
###Context text: Indeed, previous work has shown or suggested this complementarity. Sima?an #OTHEREFR showed modest gains from enriching structural relations with semi-lexical (prehead) information. Charniak and Johnson #OTHEREFR showed accuracy improvements from composed local tree features on top of a lexicalized base parser.[Citation]Our work reinforces this intuition by demonstrating how complementary they are in our model (?20% error reduction on adding refinement to an all-fragments grammar, as shown in the last two rows of Table 4). Table 4 shows results for a basic PCFG, and its augmentation with either basic refinement (parent annotation and one level of markovization), with all-fragments rules (as in previous sections), or both. The basic incorporation of large fragments alone does not yield particularly strong performance, nor does basic symbol refinement.
###Response:",#REFR showed a slight improvement in parsing accuracy when enough fragments were added to learn enrichments beyond manual refinements.,"Drawing from the methodology introduced by #REFR, which also focuses on data-oriented parsing, we were able to enhance parsing accuracy in our model."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.
###Abstract of the cited paper: We present UBY, a large-scale lexicalsemantic resource combining a wide range of information from expert-constructed and collaboratively constructed resources for English and German. It currently contains nine resources in two languages: English WordNet, Wiktionary, Wikipedia, FrameNet and VerbNet, German Wikipedia, Wiktionary and GermaNet, and multilingual OmegaWiki modeled according to the LMF standard. For FrameNet, VerbNet and all collaboratively constructed resources, this is done for the first time. Our LMF model captures lexical information at a fine-grained level by employing a large number of Data Categories from ISOCat and is designed to be directly extensible by new languages and resources. All resources in UBY can be accessed with an easy to use publicly available API.
###Context text: Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. Sense inventories. Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide.[Citation]The system can automatically convert between various versions of WordNet using the UPC mappings #OTHEREFR. Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require.
###Response:","There is currently support for Word- Net #OTHEREFR, and UBY #REFR, which provides access to WordNet, Wikipedia, Wiktionary, GermaNet, VerbNet, FrameNet, Omega- Wiki, and various alignments between them.","The system borrows from UBY's large-scale lexical-semantic resources for these conversions, enhancing data modeling capabilities #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Image retrieval models typically represent images as bags-of-terms, a representation that is wellsuited to matching images based on the presence or absence of terms. For some information needs, such as searching for images of people performing actions, it may be useful to retain data about how parts of an image relate to each other. If the underlying representation of an image can distinguish between images where objects only co-occur from images where people are interacting with objects, then it should be possible to improve retrieval performance. In this paper we model the spatial relationships between image regions using Visual Dependency Representations, a structured image representation that makes it possible to distinguish between object co-occurrence and interaction. In a query-by-example image retrieval experiment on data set of people performing actions, we find an 8.8% relative increase in MAP and an 8.6% relative increase in Precision@10 when images are represented using the Visual Dependency Representation compared to a bag-of-terms baseline.
###Abstract of the cited paper: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.
###Context text: [Citation]This task is framed as a supervised learning problem, where the aim is to construct a Maximum Spanning Tree from a fully-connected directed weighted graph over the labelled regions #OTHEREFR. Reducing the fully-connected graph to the Maximum Spanning Tree removes the region?region edges that are not important in defining the prominent relationships between the regions in an image. The score of the VDR y over the image regions is calculated as the sum of the scores of the directed labelled edges: score(x, y) = ? (a,b)?y w ? f(a, b) (1) where the score of an edge between image regions a and b is calculated using a vector of weighted feature functions f .
###Response:",We follow the approach of #REFR and predict the VDR y of an image over a collection of labelled region annotations x.,"Building upon the approach of identifying relationships between image regions as introduced by #REFR, this task aims to maximally define these prominent relationships."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English?French. Furthermore, we investigate post-ordering and tunable POS distortion models for English. German.
###Abstract of the cited paper: We describe an approach to automatically learn reordering rules to be applied as a preprocessing step in phrase-based machine translation. We learn rules for 8 different language pairs, showing BLEU improvements for all of them, and demonstrate that many important order transformations (SVO to SOV or VSO, headmodifier, verb movement) can be captured by this approach.
###Context text: Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT.[Citation]For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle by SMT systems with limited reordering capabilities such as phrase-based models. Preordering is often done on the entire training data as well to optimize translation models for the pre-ordered input.
###Response:",Handcrafted rules #OTHEREFR; #REFR for preordering training data and system input have been explored in numerous publications.,"This preprocessing step approach can be extensively observed in the work by #REFR, where reordering rules for phrase-based machine translation were successfully established and applied."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Early primary children?s literature poses some interesting challenges for automated readability assessment: for example, teachers often use fine-grained reading leveling systems for determining appropriate books for children to read (many current systems approach readability assessment at a coarser whole grade level). In previous work (Ma et al, 2012), we suggested that the fine-grained assessment task can be approached using a ranking methodology, and incorporating features that correspond to the visual layout of the page improves performance. However, the previous methodology for using ?found? text (e.g., scanning in a book from the library) requires human annotation of the text regions and correction of the OCR text. In this work, we ask whether the annotation process can be automated, and also experiment with richer syntactic features found in the literature that can be automatically derived from either the humancorrected or raw OCR text. We find that automated visual and text feature extraction work reasonably well and can allow for scaling to larger datasets, but that in our particular experiments the use of syntactic features adds little to the performance of the system, contrary to previous findings.
###Abstract of the cited paper: Several sets of explanatory variables ? including shallow, language modeling, POS, syntactic, and discourse features ? are compared and evaluated in terms of their impact on predicting the grade level of reading material for primary school students. We find that features based on in-domain language models have the highest predictive power. Entity-density (a discourse feature) and POS-features, in particular nouns, are individually very useful but highly correlated. Average sentence length (a shallow feature) is more useful ? and less expensive to compute ? than individual syntactic features. A judicious combination of features examined here results in a significant improvement over the state of the art.
###Context text: Since our previous work only uses surface level of text features, we are interested in investigating the contribution of high-level structural features to the current system.[Citation]We utilize the Stanford Parser #OTHEREFR: . Parsed Syntactic Features for NPs and VPs
###Response:",#REFR found several parsing-based features and part-of-speech based features to be useful.,"Drawing on research that found in-domain language models having the highest predictive power for determining grade levels of reading materials #REFR, we examined high-level structural features."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation f the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words. Introduction I There are now numerous systems for automatic assignment of parts of speech (""tagging""), employing many different machine learning methods. Among recent op performing methods are Hidden Markov Models (Brants 2000), maximum entropy approaches (Ratnaparkhi 1996), and transformation-based learning (Brill 1994). An overview of these and other approaches can be found in Manning and Schiitze (1999, ch. 10). However, all these methods use largely the same information sources for tagging, and often almost he same features as well, and as a consequence they also offer very similar levels of performance. This stands in contrast to the (manually-built) EngCG tagger, which achieves better performance by using lexical and contextual information sources and generalizations beyond those available to such statistical taggers, as Samuelsson and Voutilainen (1997) demonstrate. i We thank Dan Klein and Michael Saunders for useful discussions, and the anonymous reviewers for many helpful comments. This paper explores the notion that automatically built tagger performance can be further improved by expanding the knowledge sources available to the tagger. We pay special attention to unknown words, because the markedly lower accuracy on unknown word tagging means that this is an area where significant performance gains seem possible. We adopt a maximum entropy approach because it allows the inclusion of diverse sources of information without causing fragmentation and without necessarily assuming independence b tween the predictors. A maximum entropy approach as been applied to partof-speech tagging before (Ratnaparkhi 1996), but the approach is ability to incorporate nonlocal and non-HMM-tagger-type evidence has not been fully explored. This paper describes the models that we developed and the experiments we performed to evaluate them.
###Abstract of the cited paper: Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs ignificantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.
###Context text: In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation f the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words. Introduction I There are now numerous systems for automatic assignment of parts of speech (""tagging""), employing many different machine learning methods.[Citation]An overview of these and other approaches can be found in Manning and Schiitze #OTHEREFR, ch. 10). However, all these methods use largely the same information sources for tagging, and often almost he same features as well, and as a consequence they also offer very similar levels of performance.
###Response:","Among recent op performing methods are Hidden Markov Models #REFR, maximum entropy approaches #OTHEREFR.","Among these is the Trigrams'n'Tags (TnT), a statistical part-of-speech tagger that notably outperforms others, as demonstrated in #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.
###Abstract of the cited paper: We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.
###Context text: The parameter setting representing the % impact of the reordering component for the different versions of the LRscore metric. pus. A 4 or 5-gram language model would have led to higher scores for all objective functions, but would not have changed the findings in this paper. We used the MERT code available in the MOSES repository #OTHEREFR.[Citation]We first extracted the LRscore Kendall?s tau distance from the monotone for the Chinese-English test set and this value was 66.1%. This is far more reordering than the other language pairs shown in Table 2. We then calculated the optimal parameter setting, using the reordering amount as a power exponent.
###Response:","The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 #REFR, with the posterior probability to being 0.5.",The unsupervised approach to symmetric word alignment discussed in #REFR was instrumental in our reordering component calculations for the LRscore metric.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects. Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases. An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.
###Abstract of the cited paper: The recently introduced online confidence-weighted (CW) learning algorithm for binary classification performs well on many binary NLP tasks. However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case. We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus. Our best algorithm outperforms state-of-the-art online and batch methods on eight of the nine tasks. We also show that the confidence information maintained during learning yields useful probabilistic information at test time.
###Context text: The first row shows absolute (average) accuracy for a single classifier trained on all data, ignoring domain distinctions. The remaining cells indicate absolute improvements against the baseline. First, we note for the well-studied AMAZON dataset that our results with true domains are consistent with the previous literature.[Citation]Both MDR-L2 and MDR-KL improve upon the single classifier baseline, again as per Dredze et al#OTHEREFR. And finally, MTRL also improves upon the single classifier baseline. Although the MTRL improvement is not as dramatic as in the original paper3, the average accuracy that we achieve for MTRL (84.2%) is better than the best average accuracy in the original paper (83.65%).
###Response:",FEDA is known to not improve upon a single classifier baseline for that dataset #REFR.,This consistency with prior work echoes the findings of the multi-class CW setting in the cited work by Dredze et al#REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Consideration of when Right Association works and when it fails lead to a restatement of this parsing principle in terms of the notion of heaviness. A computational investigation of a syntactically annotated corpus provides evidence for this proposal and suggest circumstances when RA is likely to make correct attachment predictions.
###Abstract of the cited paper: The paper claims that the right attachment rules for phrases originally suggested by Frazier and Fodor are wrong, and that none of the subsequent patchings of the rules by syntactic methods have improved the situation. For each rule there are perfectly straightforward and indefinitely large classes of simple counter-examples. We then examine suggestions by Ford et M., Schubert and Hirst which are quasi-semantic in nature and which we consider ingenious but unsatisfactory. We point towards a straightforward solution within the framework of preference semantics, set out in detail elsewhere, and argue that the principal issue is not the type and nature of information required to get appropriate phrase attachments, but the issue of where to store the information and with what processes to apply it. SYNTACTIC APPROACHES Recent discussion of the issue of how and where to attach right-hand phrases (and more generally, clauses) in sentence analysis was started by the claims of Frasier and Fodor (1979). They offered two rules : (i) R ight Associat ion which is that phrases on the right should be attached as low as possible on a syntax tree, thus JOHN BOUGHT THE BOOK THAT I HAD BEEN TRYING TO OBT t~/OR SUSAN) which attaches to OBTAIN not to BOUGHT. But this rule fails for JOHN BOUGHT THE BOOK (FOR SUSAN) which requires attachment to BOUGHT not BOOK. A second principle was then added : (ii) M in ima l A t tachment which is that a phrase must be attached higher in a tree if doing that minimizes the number of nodes in the tree (and this rule is to take precedence over (i)). So, in : V / carried as part of VP / / ' - . b,. NP PP for Mary /. & grocenes for Mary JOHN CARRIED THE GROCERIES (FOR MARY) attaching FOR MARY to the top of the tree, rather than to the NP, will create a tree with one less node. Shieber (1983) has an alternative analysis of this phenomenon, based on a clear parsing model, which produces the same effect as rule (ii) by preferring longer reductions in the paining table; i.e., in the present ease, preferring VP <- VNPPPto NP <- NP PP. But there axe still problems with (i) and (ii) taken together, as is seen in : SHE WANTED THE DRESS~ THAT RACK) rather than attaching (ON THAT RACK) to WANTED, as (ii) would cause . SEMANTIC APPROACHES (i) Lexieal Preference At this point Ford et al (1981) suggested the use of lexical preference, which is conventional case information associated with individual verbs, so as to select for attachment PPs which match that case information. This is semantic information in the broad sense in which that term has traditionally been used in AI. Lexical preference allows rules (i) and (ii) above to be overridden if a verb is coding expresses a strong preference for a certain structure. The effect of that rule differs from system to system: within Shieber is parsing model (1983) that rule means in effect that a verb like WANT will prefer to have only a single NP to its right. The parser then performs the longest reduction it can with the strongest leftmost stack element. So, if POSITION, say, prefers two entities to its right, Shieber will obtain : THE WOMAN WANTED THE DRESS~ THE RACK) and THE WOMAN POSITIONED 'THE DRESS (ON THE RACK). But this iterative patching with more rules does not work, because to every example, under every rule (i, ii and lexical preference), there are clear and simple counter-examples. Thus, there is : JOE TOOK THE BOOK THAT I BOUGHT (FOR SUSAN) which comes under (i) and there is JOE BROUGHT THE BOOK THAT I LOVED (FOR SUSAN) which Shieber is parser must get wrong and not in a way that (ii) could rescue. Under (ii) itself, there is JOE LOST THE T I C ~ O PARIS) which Shieber is conflict reduction rule must get wrong. For Shieber is version of lexical preference there will be problems with : DAUGHTER) which the rules he gives for WANT must get wrong. (ii) Schuber t Schubert (1984) presents ome of the above counter-examples in an attack on syntactically based methods. He proposes a syntacticosemantic network system of what he calls preference trade-offs. He is driven to this, he says, because he rejects any system based wholly on lexically-based semantic preferences (which is part of what we here will call preference semantics, ee below, and which would subsume the simpler versions of lexicM preference). He does this on the grounds that there are clear cases where ""syntactic preferences prevail over much more coherent alternatives"" (Schubert, 1984, p.248), where by ""coherent"""" he means interpretations imposed by semantics/pragmatics. His examples are : (where full lines show the ""natural"" pragmatic interpretations, and dotted ones the interpretations that Schubert says are imposed willynilly by the syntax). Our informants disagree with Schubert : they attach as the syntax suggests to LIVE, but still insist that the leave is Mary is (i.e. so interpreting the last clause that it contains an elided (WHILE) SHE WAS (ON....). If that is so the example does not split off semantics from syntax in the way Schubert wants, because the issue is who is on leave and not when something was done. In such circumstances the example presents no special problems. JOHN M E T ~ HAIRED GIRL FROM MONTREAL THAT HE MARRIED (AT A DANCE) ivt Here our informants attach the phrase resolutely to MET as cornmonsense dictates (i.e. they ignore or are able to discount the built-in distance effect of the very long NP). A more difficult and interesting case arises if the last phrase is (AT A WEDDING) , since the example then seems to fall withing the exclusion of an ""attachment unless it yields zero information"" rule deployed within preference semantics (Wilks, 1973), which is probably, in its turn, a close relative of Grice is (1975) maxim concerned with information quantity. In the (AT A WEDDING) case, informants continue to attach to MET, seemingly discounting both the syntactic indication and the information vacuity of MARRIED AT A WEDDING. JOHN WAS NAMED (AFTER HIS TWIN SISTER) Here our informants aw genuine ambiguity and did not seem to mind much whether attachment or lexicalization of NAMED AFTER was preferred. Again, information vacuity tells against the syntactic attachment ( he example is on the model of : HE WAS NAMED AFTER HIS FATHER Wilks 1973, which was used to make a closely related point), but normal gendering of names tells against the lexicalization of the verb to NAME+AFTER. Our conclusion from Schubert is examples is the reverse of his own : these are not simple examples but very complex ones, involving distance and (in two cases) information quantity phenomena. In none of the cases do they support the straightforward primacy of syntax that his case against a generalized ""lexical preference hypothesis"" (i.e. one without rules (i) and (ii) as default cases, as in Ford et al is lexicM preference) would require. We shall therefore consider that hypothesis, under the name preference semantics, to be still under consideration. (Ul) H i~ Hirst (1984) aims to produce a conflation of the approaches of Ford et al, described above, and a principle of Crain and Steedman (1984) called The Principle of Parsimony, which is to make an attachment that corresponds to leaving the minimum number of presuppositions unsatisfied. The example usually given is that of a ""garden path"" sentence like : THE HORSE RACED PAST THE BARN FELL where the natural (initial) preference for the garden path interpretation is to he explained by the fact that, on that interpretation, only the existence of an entity corresponding to THE HORSE is to be presupposed, and that means less presuppositions to which nothing is the memory structure corresponds than is needed to opt for the existence of some THE HORSE RACED PAST THE BARN. One difficulty here is what it is for something to exist in memory: Craln and Steedman themselves note that readers do not garden path with sentences like : CARS RACED AT MONTE CARLO FETCH HIGH PRICES AS COLLECTOR'S ITEMS but that is not because readers know of any particular cars raced at Monte Carlo. Hirst accepts from (Winograd 1972) a general Principle of Referential Success (i.e. to actual existent entities), hut the general unsatisfactoriness of restricting a system to actual entities has long been known, for so much of our discourse is about possible and virtual ontologies (for a full discussion of this aspect of Winograd. see Ritchie 1978). The strength of Hirst is approach is his attempt o reduce the presuppositional metric of Craln and Steedman to criteria manipulable by basic semantie/lexieal codings, and particularly the contrast of definite and indefinite articles. But the general determination of categories like definite and indefinite is so shaky (and only indirectly related to ""the"" and ""a"" in English), and cannot possibly bear the weight that he puts on it as the solid basis of a theory of phrase attachment. So, Hirer invites counter-examples to his Principle of Referential Success (1984, p.149) adapted from Wlnograd: ""a non-generic NP presupposes that the thing it describes exists.....an indefinite NP presupposes only the plausibility of what it describes."" But this is just not so in either case : THE PERPETUAL MOTION MACHINE IS THE BANE OF LIFE IN A PATENT OFF ICE A MAN I JUST MET LENT ME FIVE POUNDS The machine is perfectly definite but the perpetual motion machine does not exist and is not presupposed by the speaker. We conclude that these notions are not yet in a state to be the basis of a theory of PP attachment. Moreover, even though beliefs about the world must play a role in attachment in certain cases, there is, as yet, no reason to believe that beliefs and presuppositions can provide the material for a basic attachment mechanism. (iv) Preference Semantics Preference Semantics has claimed that appropriate structurings can be obtained using essentially semantic information, given also a rule of preferring the most densely connected representations that can be constructed from such semantic information (Wilks 1975, Fass & Wilks 1983). Let us consider such a position initially expressed as semantic dictionary information attaching to the verb; this is essentially the position of the systems discussed above, as well as of case grammar. and the semanticsbased parsing systems (e.g. Riesbeck 1975) that have been based on it. When discussing implementation i the last section we shall argue (as in Wilks 1976) that semantic material that is to be the base of a parsing process cannot be thought of as simply attaching to a verb (rather than to nouns and all other word senses) In what follows we shall assume case predicates in the diction? ary entries of verbs, nouns etc. that express part of the meaning of the concept and determine its semantic relations. We shall write as \[OBTAIN\] the abbreviation of the semantic dictionary entry for OBTAIN, and assume that the following concepts contain at least the case entries shown (as case predicates and the types of argument fillers) : \ [OBTAIN I (recipient hum) recipient case, human. \[BUY\] (recipient hum) recipient case, human. \[POSITION\] (location *pla) location case, place. \[BRING\] (recipient human)recipient case, human. \[TICKET\] (direction *pla) direction case, place. \[WANT\] (object *physob) object case, physical object. (recipient hum) recipient case, human. The issue here is whether these are plausible preferential meaning constituents: e.g. that to obtain something is to obtain it for a recipient; to position something is to do it in association with a place; a ticket (in this sense i.e. ""billet"" rather than ""ticket"" in French) is a ticket to somewhere, and so on. They do not entail restrictions, but only preferences. Hence, ""John brought his dog a bone"" in no way violates the coding \[BRING\]. We shall refer to these case constituents within semantic representations a semantic preferences of the corresponding head concept. A F IRST TRIAL ATTACHMENT RULE The examples discussed are correctly attached by the following rule : Ru le A : moving leftwards from the right hand end of a sentence, assign the attachment of an entity X (word or phrase) to the first entity to the left of X that has a preference that X satisfies; this entails that any entity X can only satisfy the preference of one entity. Assume also a push down stack for inserting such entities as X into until they satisfy some preference. Assume also some distance limit (to be empirically determined) and a DEFAULT rule such that, if any X satisfies no preferences, it is attached locally, i.e. immediately to its left. Rule A gets right all the classes of examples discussed (with one exception, see below): e.g JOHN BROUGH BOOK THAT I LOVED (FOR M~Y) JOHN TOOK THE BOOK THAT I BOUGHT (F~R MARY) JoHN W T HE DR THE I(FOR MARY) where the last requires use of the push-down stack. The phenomenon treated here is assumed to be much more general than just phrases, as in: P~TF. DE CANARD TRUFFI~ ,~ . . __ .~ (i.e. a truflled pate of duck, not a pate of truflled ducks!) where we envisage a preference (POSS STUFF)~--- i .e. prefers to be predicated of substancesas part of \[TRUFFE\[. French gender is of no use here, since all the concepts are masculine. This rule would of course have to be modified for many special factors, e.g. pronouns, because of : \[ THE D R ~ SHE W A N T O N THE SHELF) A more substantial drawback to this substitution of a single semanticsbased rule for all the earlier syntactic complexity is that placing the preferences essentially in the verbs (as did the systems discussed earlier that used lexical preference) and having little more than semantic type information on nouns (except in cases like \[TICKET\[ that also prefers associated cases) but, most importantly, having no semantic preferences associated with prepositions that introduce phrases, we shall only succeed with rule A by means of a semantic subterfuge for a large and simple class of cases, namely: JOHN LOVED HER (FOR HER BEAUTY) or JOHN SHOT THE GIRL (IN THE PARK) Given the ""low default"" component of rule A, these can only be correctly attached if there is a very general case component in the verbs, e.g. some statement of location in all ""active types"" of verbs (to be described by the primitive type heads in their codings) like SHOOT i.e. (location *pla), which expresses the fact that acts of this type are necessarily located. (location *pla) is then the preference that (IN THE PARK) satisfies, thus preventing a low default. Again, verbs like LOVE would need a (REASON ANY) component in their coding, expressing the notion that such states (as opposed to actions, both defined i~ terms of the main semantic primitives of verbs) are dependent on some reason, which could be anything. But the clearest defect of Rule A (and, by implication, of all the verbcentered approaches discussed earlier in the paper) is that verbs in fact confront not cases, but PPs fronted by ambiguous prepositions, and it is only by taking account of their preferences that a general solution can be found. PREPOSIT ION SEMANTICS: PREPLATES In fact rule A was intentionally naive: it was designed to demonstrate (as against Shubcrt is claims in particular) the wide coverage of the data of a single semantics-based rule, even if that required additional, hard to motivate, semantic information to be given for action and states. It was stated in a verb-based lexical preference mode simply to achieve contrast with the other systems discussed. For some years, it has been a principle of preference semantics (e.g. WilLS 1973, 1975) that attachment relations of phrases, clauses etc. are to be determined by comparing the preferences emanating from all the entities involved in an attachment: they axe all, as it were, to be considered as objects seeking other preferred classes of neighbors, and the best lit, within and between each order of structures built up, is to be found by comparing the preferences and finding a best mutual fit. This point was made in (Wilks 1976) by contrasting preference semantics with the simple verb-based requests of Riesbeck is (1975) MARGIE parser. It was argued there that account had to be taken of both the preferences of verbs (and nouns), and of the preferences cued from the prepositions themselves. Those preferences were variously called paraplates (WilLS 1975), preplates (Bognraev 1979) and they were, for each preposition sense, an ordered set of predication preferences restricted by action or noun type. {WilLS 1975} contains examples of ordered paraplate stacks and their functioning, but in what follows we shall stick to the preplate notation of (Huang 1984b). We have implemented in CASSEX (see WilLS, Huang and Fass, 1985) a range of alternatives to Rule A : controlling both for ""low"" and ""high"" default; for examination of verb preferences first (or more generally those of any entity which is a candidate for the root of the attachment, as opposed to what is attached) and of what-is-attached first (i.e. prepositional phrases). We can also control for the application of a more redundant form of rule where we attach preferably on the conjunction of satisfactions of the preferences of the root and the attached (e.g. for such a rule, satisfaction would require both that the verb preferred a prepositional phrase of such a class, and that the prepositional phrase preferred a verb of such a class}. In (Wilks, Huang & Fass 1985) we describe the algorithm that best fits the data and alternates between the use of semantic information attached to verbs and nouns (i.e. the roots for attachments a in Rule A) and that of prepositions; it does this by seeking the best mutual fit between them, and without any fall back to default syntactic rules like (i) and (ii). This strategy, implemented within Huang is (1984a, 1984b) CASSEX program, correctly parses all of the example sentences in this paper. CASSEX, which is written in Prolog on the Essex GEC- 63, uses a definite clause grammar (DCG) to recognize syntactic onstituents and Preference Semantics to provide their semantic interpretation. Its content is described in detail in (WilLS, Huang & Fass 1985) and it consists in allowing the preferences of both the clause verbs and the prepositions themselves tooperate on each other and compete in a perspicuous and determinate manner, without recourse to syntactic preferences or weightings.
###Context text: Kimball #OTHEREFR proposes the parsing strategy of Right Association (RA). RA resolves modifiers attachment ambiguities by attaching at the lowest syntactically permissible position along the right frontier.[Citation]On its own, RA performs rather well, given its simplicity, but it is far from adequate: Whittemore et al evaluate RA is performance on PP attachment using a corpus derived from computer-mediated dialog. They find that RA makes correct predictions 55% of the time. Weischedel et al, using a corpus of news stories, report a 75% success rate on the general case of attachment using a strategy Closest Attachment which is essentially RA.
###Response:","Many authors (among them #REFR incorporate RA into their parsing systems, yet none rely on it solely, integrating it instead with disambiguation preferences derived from word/constituent/concept ooccurrence based.","However, the paper by #REFR contests Right Association, presenting numerous counterexamples and suggesting that modifications and subsequent improvements to RA through purely syntactic methodologies are unsatisfactory."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present several algorithms for assigning heads in phrase structure trees, based on different linguistic intuitions on the role of heads in natural language syntax. Starting point of our approach is the observation that a head-annotated treebank defines a unique lexicalized tree substitution grammar. This allows us to go back and forth between the two representations, and define objective functions for the unsupervised learning of head assignments in terms of features of the implicit lexicalized tree grammars. We evaluate algorithms based on the match with gold standard head-annotations, and the comparative parsing accuracy of the lexicalized grammars they give rise to. On the first task, we approach the accuracy of handdesigned heuristics for English and interannotation-standard agreement for German. On the second task, the implied lexicalized grammars score 4% points higher on parsing accuracy than lexicalized grammars derived by commonly used heuristics.
###Abstract of the cited paper: Many recent statistical parsers rely on a preprocessing step which uses hand-written, corpus-specific rules to augment the training data with extra information. For example, head-finding rules are used to augment node labels with lexical heads. In this paper, we provide machinery to reduce the amount of human effort needed to adapt existing models to new corpora: first, we propose a flexible notation for specifying these rules that would allow them to be shared by different models; second, we report on an experiment to see whether we can use Expectation- Maximization to automatically fine-tune a set of hand-written rules to a particular corpus.
###Context text: Given the uncertainty about the gold standards, the possibility remains that this reflects biases towards the most general headedness-rules in the annotation practice rather than a linguistically real phenomenon. Unsupervised head assignment algorithms can be used for the many applications in NLP where information on headedness is needed to convert constituency trees into dependency trees, or to extract head-lexicalised grammars from a constituency treebank. Of course, it remains to be seen which algorithm performs best in any of these specific applications.[Citation]From a linguistic point of view, our work can be seen as investigating ways in which distributional information can be used to determine headedness in phrase-structure trees. We have shown that lexicalized tree grammars provide a promising methodology for linking alternative head assignments to alternative dependency structures (needed for deeper grammatical structure, including e.g., argument structure), as well as to alternative derivations of the same sentences (i.e. the set of lexicalized elementary trees need to derive the given parse tree). In future work, we aim to extend these results by moving to more expressive grammatical formalisms (e.g., tree adjoining grammar) and by distinguishing adjuncts from arguments.
###Response:","Nevertheless, we conclude that among currently available approaches, i.e., our two algorithms and the EM-based approach of #REFR, ?familiarity maximization? is the most promising approach for automatic assignments of heads in treebanks.","In refining these algorithms, the innovation introduced by #REFR, in utilizing Expectation-Maximization to adjust hand-written rule-sets, has held noteworthy influence."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems. We present an efficient low-memory method for constructing high-order approximate n-gram frequency counts. The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint. We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine. Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods.
###Abstract of the cited paper: This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.
###Context text: [Citation]Since the use of cluster of machines is not always practical, #OTHEREFR presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in #OTHEREFR used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option.
###Response:",Using higher order LMs to improve the accuracy of SMT is not new. #REFR built 5-gram LMs over web using distributed cluster of machines and queried them via network requests.,"The cited paper proposes large-scale statistical language modeling in machine translation, providing faster decoding and training on significantly larger datasets [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Building on the use of local contexts, or frames, for human category acquisition, we explore the treatment of contexts as categories. This allows us to examine and evaluate the categorical properties that local unsupervised methods can distinguish and their relationship to corpus POS tags. From there, we use lexical information to combine contexts in a way which preserves the intended category, providing a platform for grammatical category induction.
###Abstract of the cited paper: As it serves as a basis for POS tagging, category induction, and human category acquisition, we investigate the information needed to disambiguate a word in a local context, when using corpus categories. Specifically, we increase the recall of an error detection method by abstracting the word to be disambiguated to a representation containing information about some of its inherent properties, namely the set of categories it can potentially have. This work thus provides insights into the relation of corpus categories to categories derived from local contexts.
###Context text: On their own, frames at best distinguish only very broad categorical properties.[Citation]If we want to combine contexts in a way which maps to corpus tagsets, then, we need to examine the target words. It is likely that two sets share the same tag if they contain the same words #OTHEREFR. In fact, the more a frame?s word set overlaps with another?s word set, the more likely it is unambiguous in the first place, as the other set provides corroborating evidence.
###Response:","This is perhaps unsurprising, as the finer-grained distinctions in corpora seem to be based on lexical properties more than on additional context (see, e.g., #REFR.","The information needed to disambiguate a word in a local context, as discussed in previous studies, can be instrumental in distinguishing categorical properties #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a language model consisting of a collection of costed bidirectional finite state automata ssociated with the head words of phrases. The model is suitable for incremental pplication of lexical associations in a dynamic programming search for optimal dependency tree derivations. We also present a model and algorithm for machine translation involving optimal ""tiling"" of a dependency tree with entries of a costed bilingual exicon. Experimental results are reported comparing methods for assigning cost functions to these models. We conclude with a discussion of the adequacy of annotated linguistic strings as representations formachine translation.
###Abstract of the cited paper: The Air Travel Information System (ATIS) domain serves as the common task for DARPA spoken language system research and development. The approaches and results possible in this rapidly growing area are structured by available corpora, annotations of that data, and evaluation methods. Coordination of this crucial infrastructure is the charter of the Multi-Site AT IS Data COllection Working group (MAD- COW) . We focus here on selection of training and test data, evaluation of language understanding, and the continuing search for evaluation methods that will correlate well with expected performance of the technology in applications.
###Context text: We have built an experimental translation system using the monolingual and translation models described in this paper.[Citation]The translator is in fact a subsystem of a speech translation prototype, though the experiments we describe here are for transcribed spoken utterances. (We informally refer to the transcribed utterances as sentences.) The average time taken for translation of sentences (of unrestricted length) from the ATIS corpus was around 1.7 seconds with approximately 0.4 seconds being taken by the analysis algorithm and 0.7 seconds by the transfer algorithm. English and Chinese lexicons of around 1200 and 1000 words respectively were constructed. Altogether, the entries in these lexicons made reference to around 200 structurally distinct head automata.
###Response:",The system translates entences in the ATIS domain #REFR between English and Mandarin Chinese.,"Our experimental translation system used the ATIS corpus, a data set commonly used in spoken language system research, as detailed in #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points.
###Abstract of the cited paper: Machine-produced text often lacks grammaticality and fluency. This paper studies grammaticality improvement using a syntax-based algorithm based on CCG. The goal of the search problem is to find an optimal parse tree among all that can be constructed through selection and ordering of the input words. The search problem, which is significantly harder than parsing, is solved by guided learning for best-first search. In a standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system.
###Context text: We studied the problem of word-ordering using a syntactic model and allowing permutation.[Citation]These extentions led to improvements in the BLEU evaluation. Analyzing the generated sentences suggests that, while highly fluent outputs can be produced for short sentences (? 10 words), the system fluency in general is still way below human standard. Future work remains to apply the system as a component for specific text generation tasks, for example machine translation.
###Response:","We took the model of #REFR as the baseline, and extended it with online large-margin training and an N -gram language model.","Our study was based on the successful syntax-based word ordering method with CCG by #REFR, which we extended for enhanced performance."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a graph-based semi-supervised learning for the question-answering (QA) task for ranking candidate sentences. Using textual entailment analysis, we obtain entailment scores between a natural language question posed by the user and the candidate sentences returned from search engine. The textual entailment between two sentences is assessed via features representing high-level attributes of the entailment problem such as sentence structure matching, question-type named-entity matching based on a question-classifier, etc. We implement a semi-supervised learning (SSL) approach to demonstrate that utilization of more unlabeled data points can improve the answer-ranking task of QA. We create a graph for labeled and unlabeled data using match-scores of textual entailment features as similarity weights between data points. We apply a summarization method on the graph to make the computations feasible on large datasets. With a new representation of graph-based SSL on QA datasets using only a handful of features, and under limited amounts of labeled data, we show improvement in generalization performance over state-of-the-art QA models.
###Abstract of the cited paper: Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)?s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk?s word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2% and 89.0% respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%.
###Context text: There are intensive research in this area fostered by evaluation-based conferences, such as the Text REtrieval Conference #OTHEREFR, etc. One of the focus of these research, as well as our work, is on factoid questions in English, whereby the answer is a short string that indicates a fact, usually a named entity. A typical QA system has a pipeline structure starting from extraction of candidate sentences to ranking true answers.[Citation]Our QA system has a similar pipeline structure and implements a new TE module for information extraction phase of the QA task. TE is a task of determining if the truth of a text entails the truth of another text (hypothesis). Harabagui and Hickl #OTHEREFR has shown that using TE for filtering or ranking answers can enhance the accuracy of current QA systems, where the answer of a question must be entailed by the text that supports the correctness of this answer.
###Response:","In order to improve QA systems? performance many research focus on different structures such as question processing #REFR, information retrieval #OTHEREFR for ranking, answer extraction, etc.","Drawing inspiration from the efficient feature-set proposed for question classification in the cited work #REFR, our model utilizes high-level attributes for textual entailment."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We show how punctuation can be used to improve unsupervised dependency parsing. Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank. However, approaches that naively include punctuation marks in the grammar (as if they were words) do not perform well with Klein and Manning?s Dependency Model with Valence (DMV). Instead, we split a sentence at punctuation and impose parsing restrictions over its fragments. Our grammar inducer is trained on the Wall Street Journal (WSJ) and achieves 59.5% accuracy out-of-domain (Brown sentences with 100 or fewer words), more than 6% higher than the previous best results. Further evaluation, using the 2006/7 CoNLL sets, reveals that punctuation aids grammar induction in 17 of 18 languages, for an overall average net gain of 1.3%. Some of this improvement is from training, but more than half is from parsing with induced constraints, in inference. Punctuation-aware decoding works with existing (even already-trained) parsing models and always increased accuracy in our experiments.
###Abstract of the cited paper: We identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques.
###Context text: Tightly interwoven into the fabric of writing systems, punctuation frames most unannotated plaintext. We showed that rules for converting markup into accurate parsing constraints are still optimal for inter-punctuation fragments. Punctuation marks are more ubiquitous and natural than web markup: what little punctuation-induced constraints lack in precision, they more than make up in recall ? perhaps both types of constraints would work better yet in tandem.[Citation]Our results underscore the power of simple models and algorithms, combined with common-sense constraints. They reinforce insights from joint modeling in supervised learning, where simplified, independent models, Viterbi decoding and expressive constraints excel at sequence labeling tasks #OTHEREFR. Such evidence is particularly welcome in unsupervised settings #OTHEREFR, where it is crucial that systems scale gracefully to volumes of data, on top of the usual desiderata ? ease of implementation, extension, understanding and debugging.
###Response:","For language acquisition, a natural question is whether prosody could similarly aid grammar induction from speech #REFR.","Drawing on the findings of #REFR, punctuation use, as seen in prosodic cues for parsing conversational speech, may further improve accuracy across parsing models."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents an application of PageR- ank, a random-walk model originally devised for ranking Web search results, to ranking WordNet synsets in terms of how strongly they possess a given semantic property. The semantic properties we use for exemplifying the approach are positivity and negativity, two properties of central importance in sentiment analysis. The idea derives from the observation that WordNet may be seen as a graph in which synsets are connected through the binary relation ?a term belonging to synset sk occurs in the gloss of synset si?, and on the hypothesis that this relation may be viewed as a transmitter of such semantic properties. The data for this relation can be obtained from eX- tended WordNet, a publicly available sensedisambiguated version of WordNet. We argue that this relation is structurally akin to the relation between hyperlinked Web pages, and thus lends itself to PageRank analysis. We report experimental results supporting our intuitions.
###Abstract of the cited paper: Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then collapsed into a single set of 7, 813 unique words. For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement.
###Context text: Recent years have witnessed an explosion of work on opinion mining #OTHEREFR ?Fondo Unico per la Ricerca? funding scheme. cipline that deals with the quantitative and qualitative analysis of text for the purpose of determining its opinion-related properties (ORPs). An important part of this research has been the work on the automatic determination of the ORPs of terms, as e.g., in determining whether an adjective tends to give a positive, a negative, or a neutral nature to the noun phrase it appears in.[Citation]Some authors go a step further and attach these properties not to terms but to term senses #OTHEREFR. In this paper we contribute to this latter literature with a novel method for ranking the entire set of WordNet synsets, irrespectively of POS, according to their ORPs. Two rankings are produced, one according to positivity and one according to negativity.
###Response:","While many works #OTHEREFR view the properties of positivity and negativity as categorical (i.e., a term is either positive or it is not), others #REFR view them as graded (i.e., a term may be positive to a certain degree), with the underlying interpretation varying from fuzzy to probabilistic.","Building on the previous work that extracted sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP) #REFR, we construct a novel method for ranking WordNet synsets based on their opinion-related properties."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a new general supervised word sense disambiguation method based on a relatively small syntactically parsed and semantically tagged training corpus. The method exploits a full sentential context and all the explicit semantic relations in a sentence to identify the senses of all of that sentence is content words. In spite of a very small training corpus, we report an overall accuracy of 80.3% (85.7, 63.9, 83.6 and 86.5%, for nouns, verbs, adjectives and adverbs, respectively), which exceeds the accuracy of a statistical sense-frequency based semantic tagging, the only really applicable general disambiguating technique.
###Abstract of the cited paper: A semantic oncordance is a textual corpus and a lexicon So combined that every substantive word in the text is linked to its appropriate ~nse in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic oncordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports earches of the tagged text. Some practical uses for semantic oncordances are proposed.
###Context text: #OTHEREFR pointed out that the sense of a target word is highly consistent within any given document (one sense per discourse).[Citation]The experiment indicated that the ""one sense per discourse"" hypothesis works fairly well for nouns, however, the evidence is much weaker for verbs, adverbs and adjectives. Table 3 shows the numbers of content words which appear previously in the same discourse with the same meaning (same synset), and those which appear previously with a different meaning. The experiment also confirmed our expectation that the ratio of words with the same sense to those with a different sense, depends on the distance of sentences in which the same words appear (distance I indicates that the same word appeared in the previous sentence, distance 2 that the same word was present 2 sentences before, etc.).
###Response:","Because our algorithm does not consider the context given by the preceding sentences, we have conducted the following experiment to see to what extent the discourse context could improve the performance of the wordsense disambiguation: Using the semantic concordance files #REFR, we have counted the occurrences of content words which previously appear in the same discourse file.",The training data used for our disambiguation method was constructed similarly to the semantic concordance technique in #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a discriminative, largemargin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.
###Abstract of the cited paper: In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.
###Context text: These orthographic and other features improved AER to 14.4. The running example now has the alignment in Figure 1(c), where one improvement may be attributable to the short pair feature ? it has stopped proposing the-de, partially because the short pair feature downweights the score of that pair. A clearer example of these features making a difference is shown in Figure 2, where both the exact-match and character overlap fea- 3The learned response was in fact close to a Gaussian, but harsher near zero displacement. tures are used.[Citation]The the-le error in Figure 1(c) is symptomatic of this lack. In particular, it is a slightly better pair according to the Dice value than the correct the-les. However, the latter alignment has the advantage that major-grands follows it.
###Response:","One source of constraint which our model still does not explicitly capture is the first-order dependency between alignment positions, as in the HMM model #REFR and IBM models 4+.","Drawing from the approach detailed in #REFR, the alignment probabilities adjustment based on alignment position differences significantly improved the model's performance."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present Multi-Relational Latent Semantic Analysis (MRLSA) which generalizes Latent Semantic Analysis (LSA). MRLSA provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor. Similar to LSA, a lowrank approximation of the tensor is derived using a tensor decomposition. Each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a.
###Abstract of the cited paper: Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus ? a word sense along with its synonyms and antonyms ? is treated as a ?document,? and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure.
###Context text: [Citation]Synonyms and antonyms of the same target word are grouped together as a ?document? and a document-term matrix is constructed accordingly as done in LSA. Because each word in a group belongs to either one of the two opposite relations, synonymy and antonymy, the polarity information is induced by flipping the signs of antonyms. While the absolute value of each element in the matrix is still the same TF?IDF score, the elements that correspond to the antonyms become negative.
###Response:","Analysis In order to distinguish antonyms from synonyms, the polarity inducing LSA (PILSA) model #REFR takes a thesaurus as input.","In the existing vector space model, antonyms and synonyms are positioned on opposite ends of a sphere, an improvement on typical models that fail to delineate between the two [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper reports translation results for the ?Exploiting Parallel Texts for Statistical Machine Translation? (HLT-NAACL Workshop on Parallel Texts 2006). We have studied different techniques to improve the standard Phrase-Based translation system. Mainly we introduce two reordering approaches and add morphological information.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: [Citation]During word alignment, we use 50 classes per language estimated by ?mkcls?, a freely-available tool along with GIZA++. Before aligning we work with lowercase text (which leads to an Alignment Error Rate reduction) and we recover truecase after the alignment is done. In addition, the alignment (in specific pairs of languages) was improved using two strategies: Full verb forms The morphology of the verbs usually differs in each language.
###Response:","Given a bilingual corpus, we use GIZA++ #REFR as word alignment core algorithm.","Drawing from the new phrase-based translation model proposed by #REFR, our word alignment process has been refined and has influenced our reordering approaches."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a multi-view annotation framework for Chinese treebanking, which uses dependency structures as the base view and supports conversion into phrase structures with minimal loss of information. A multi-view Chinese treebank was built under the proposed framework, and the first release (PMT 1.0) containing 14,463 sentences is be made freely available. To verify the effectiveness of the multi-view framework, we implemented an arc-standard transition-based dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0.
###Abstract of the cited paper: In this paper we present a novel phrase structure parsing approach with the help of dependency structure. Different with existing phrase parsers, in our approach the inference procedure is guided by dependency structure, which makes the parsing procedure flexibly. The experimental results show our approach is much more accurate. With the help of golden dependency trees, F1 score of our parser achieves 96.08% on Penn English Treebank and 90.61% on Penn Chinese Treebank. With the help of N-best dependency trees generated by modified MSTParser, F1 score achieves 90.54% for English and 83.93% for Chinese.
###Context text: In order to demonstrate the usefulness of our treebank in comparison with existing Chinese treebanks, we perform empirical analysis to the treebank, by the statistical dependency parsing and dependency-tostring machine translation tasks.[Citation]Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging. Since our treebank is multi-view, it is possible to combine DS features and PS features directly in the decoding process. We implemented an arc-standard transition-based dependency parser #OTHEREFR.
###Response:",Several researchers explored joint DS and PS information to enhance the quality of syntactic parsing #REFR.,Our parsing approach is similar to the novel phrase parsing which is guided by dependency structure as depicted by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Broad-coverage relation extraction either requires expensive supervised training data, or suffers from drawbacks inherent to distant supervision. We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples. We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative. In this way, we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus. Our approach gives a substantial increase of 3.9% endto-end F on the 2013 KBP Slot Filling evaluation, yielding a net F of 37.7%.
###Abstract of the cited paper: Distant supervision for relation extraction (RE) ? gathering training data by aligning a database of facts with text ? is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.
###Context text: [Citation]This allows for an evaluation where the only free variable between this and prior work is the predictions of the relation extractor. Additionally, we evaluate the relation extractors in the context of Stanford?s end-to-end KBP system #OTHEREFR English Slotfilling evaluation. In the end-to-end framework, the input to the system is a query entity and a set of articles, and the output is a set of slot fills ? each slot fill is a candidate triple in the knowledge base, the first element of which is the query entity.
###Response:",We compare against the original MIML-RE model using the same dataset and evaluation methodology as #REFR.,"Adopting the approach discussed by #REFR, we optimized the relation extraction process by addressing the learning challenges inherent in distantly supervised masses of data."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, I argue for the use of a probabilistic form of tree-adjoining grammar (TAG) in statistical natural anguage processing. I first discuss two previous statistical approaches --- one that concentrates on the probabilities of structural operations, and another that emphasizes co, occurrence relationships between words. I argue that a purely structural approach, exemplified by probabilistie context-free grammar, lacks sufficient sensitivity to lexical context, and, conversely, that lexical co-occurence analyses require a richer notion of locality that is best provided by importing some notion of structure. I then propose probabilistie TAG as a framework for statistical language modelling, arguing that it provides an advantageous combination of structure, locality, and lexical sensitivity. Issues in the acquisition of probabilistie TAG and parameter stimation are briefly considered.
###Abstract of the cited paper: The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexieally sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic ontext-free grammars). Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outsidelike iterative algorithm for estimating the parameters of a SLTAG given a training corpus. Finally, we should how SLTAG enables to define a lexicalized version of stochastic ontext-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic ontext-free grammars.
###Context text: There are a number of ways to express lexicalized tree-adjoining rammar as a probabilistic grammar formalism.[Citation]Definitions: Let I denote the set of initial trees in the grammar, and A the set of auxiliary trees. Each tree in a lexlcalized TAG has a (possibly empty) subset of its frontier nodes marked as nodes at which substitution may take place. Given a tree c~, let that subset be denoted by s(a).
###Response:",4 Here I propose what appears to me to be the most direct probabilistic generalization of lexiealized TAG; a different treatment can be found in \[#REFR\].,"As formally defined, stochastic lexicalized tree-adjoining grammar is the probabilistic version of tree-adjoining grammar and is sensitive, hierarchical and novel in its characteristics (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Combining word alignments trained in two translation directions has mostly relied on heuristics that are not directly motivated by intended applications. We propose a novel method that performs combination as an optimization process. Our algorithm explicitly maximizes the effectiveness function with greedy search for phrase table training or synchronized grammar extraction. Experimental results show that the proposed method leads to significantly better translation quality than existing methods. Analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: Word alignment is the process of identifying word-to-word links between parallel sentences.[Citation]Most word alignment models distinguish translation direction in deriving word alignment matrix. Given a parallel sentence, word alignments in two directions are established first, and then they are combined as knowledge source for phrase training or rule extraction. This process is also called symmetrization.
###Response:","It is a fundamental and often a necessary step before linguistic knowledge acquisitions, such as training a phrase translation table in phrasal machine translation (MT) system #REFR, or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework.","This combination often relies on heuristics, an approach that, although efficient, lacks direct motivation from the intended applications (#REFR). However, a novel phrase-based translation model proposed by #OTHEREFR offers a robust alternative."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion. We use both local contexts and global world knowledge to expand query language models. We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query. Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance. Compared with the official results from KBP 2010 participants, our system shows competitive performance.
###Abstract of the cited paper: The integration of facts derived from information extraction systems into existing knowledge bases requires a system to disambiguate entity mentions in the text. This is challenging due to issues such as non-uniform variations in entity names, mention ambiguity, and entities absent from a knowledge base. We present a state of the art system for entity disambiguation that not only addresses these challenges but also scales to knowledge bases with several million entries using very little resources. Further, our approach achieves performance of up to 95% on entities mentioned from newswire and 80% on a public test set that was designed to include challenging queries.
###Context text: For entity diambiguation they used the contextual comparisons between the Wikipedia article and the KB article. However, their work ignores the possibilities of acronyms in the entities. Also, the ambiguous geo-political names are not handled in their work.[Citation]Their algorithm for learning NIL has shown very good results. But their proposal for handling the alias name or stage name via multiple lists is not scalable. Unlike their approach, we use the global knowledge to handle the stage names and thus this gives an optimized solution to handle alias names.
###Response:","#REFR took the approach that large number of entities will be unlinkable, as there is a probability that the relevant KB entry is unavailable.","Addressing the challenges of entity disambiguation, the study by #REFR successfully applies a system that ensures scalability even with large knowledge bases."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress.
###Abstract of the cited paper: We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \[5,9,10,15,17\] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \[9\]. The major technical innovation is tire use of a ""ma~ximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.
###Context text: [Citation]A common? and valid?criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains #OTHEREFR.
###Response:",The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank #OTHEREFR; #REFR.,"In response to this criticism, previous work developed a parser that demonstrated improved precision/recall rates for varied sentence lengths in the Wall Street Journal treebank [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.
###Abstract of the cited paper: This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).
###Context text: [Citation]Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple ?S,Q,F ?, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished.
###Response:","Greedy local search #OTHEREFR; #REFR has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation #OTHEREFR.","Drawing on the method used by the deterministic parser developed in #REFR, we opted for a linear time complexity approach, implementing beam-search in our parser."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We introduce an interactive visualization component for the JoBimText project. JoBim- Text is an open source platform for large-scale distributional semantics based on graph representations. First we describe the underlying technology for computing a distributional thesaurus on words using bipartite graphs of words and context features, and contextualizing the list of semantically similar words towards a given sentential context using graphbased ranking. Then we demonstrate the capabilities of this contextualized text expansion technology in an interactive visualization. The visualization can be used as a semantic parser providing contextualized expansions of words in text as well as disambiguation to word senses induced by graph clustering, and is provided as an open source tool.
###Abstract of the cited paper: We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
###Context text: To avoid the intrinsic quadratic complexity of the similarity computation, we have developed an optimized process based on MapReduce #OTHEREFR that takes advantage of the sparsity of contexts, which allows scaling the process through parallelization. The result of this computation is a graph connecting the most discriminative contexts to terms and explicitly linking the most similar terms. This graph represents local models of semantic relations per term rather than a model with fixed dimensions.[Citation]The main contribution of this paper is to describe how we operationalize semantic similarity in a graph-based framework and explore this semantic graph using an interactive visualization. We describe a scalable and flexible computation of a distributional thesaurus (DT), and the contextualization of distributional similarity for specific occurrences of language elements (i.e. terms). For related works on the computation of distributional similarity, see e.g. #OTHEREFR.
###Response:","This representation departs from the vector space metaphor #OTHEREFR; #REFR, commonly employed in other frameworks for distributional semantics such as LSA #OTHEREFR.","Our approach adopts components of the adjective-noun composition model proposed by #REFR, specifically the concept of representing words as vectors in a graph-based semantic framework."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we present a two-stage approach to acquire Japanese unknown morphemes from text with full POS tags assigned to them. We first acquire unknown morphemes only making a morphologylevel distinction, and then apply semantic classification to acquired nouns. One advantage of this approach is that, at the second stage, we can exploit syntactic clues in addition to morphological ones because as a result of the first stage acquisition, we can rely on automatic parsing. Japanese semantic classification poses an interesting challenge: proper nouns need to be distinguished from common nouns. It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them. We explore lexico-syntactic clues that are extracted from automatically parsed text and investigate their effects.
###Abstract of the cited paper: Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering. A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking. However, there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting. To cope with the unit problem, we propose a character-based chunking method. Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers. Then, each character is annotated with its character types and its possible POS tags of the top n-best answers. Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs. This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result. We apply our method to IREX NE extraction task. The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method.
###Context text: They report an improvement in tagging proper nouns in Japanese. A related task is named entity recognition (NER). It can handle a named entity longer than a single morpheme and is usually formalized as a chunking problem.[Citation]In either case, NER models encode the output of morphological analysis and therefore are affected by its errors. In fact, Saito et al #OTHEREFR report that a majority of unknown named entities (those never appear in a training corpus) contain unknown morphemes as their constituents and that NER models perform poorly on them. A straightforward solution to this problem would be to acquire unknown morphemes and to assign semantic labels to them.
###Response:","Since Japanese does not delimit words by white-space, the unit of chunking can be a character #REFR or a morpheme #OTHEREFR.","Addressing unit problem, a character-based chunking method has been proposed, which has shown effectiveness in extracting named entities from Japanese texts #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Query segmentation is the process of taking a user?s search-engine query and dividing the tokens into individual phrases or semantic units. Identification of these query segments can potentially improve both document-retrieval precision, by first returning pages which contain the exact query segments, and document-retrieval recall, by allowing query expansion or substitution via the segmented units. We train and evaluate a machine-learned query segmentation system that achieves 86% segmentationdecision accuracy on a gold standard set of segmented noun phrase queries, well above recently published approaches. Key enablers of this high performance are features derived from previous natural language processing work in noun compound bracketing. For example, token association features beyond simple N-gram counts provide powerful indicators of segmentation.
###Abstract of the cited paper: Information Retrieval (IR) is an important application area of Natural Language Processing (NLP) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text. While much effort has been made to apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes. Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text. This paper proposes a new probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing. The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection. The experiment is results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance.
###Context text: Otherwise, a segmentation break is made. Query segmentation is related to the task of noun compound (NC) bracketing. NC bracketing determines the syntactic structure of an NC as expressed by a binary tree, or, equivalently, a binary bracketing #OTHEREFRa).[Citation]In principle, as N increases, the number of binary trees for an N - token compound is much greater than the 2N?1 possible segmentations. In practice, empirical NC research has focused on three-word compounds. The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure #OTHEREFR.
###Response:","#REFR first identified the importance of syntactic query/corpus parsing for information retrieval, but did not consider query segmentation itself.","Our system's high performance relies on techniques derived from NLP work in noun compound bracketing, echoing the findings of #REFR, which highlights parsing and indexing improvements."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a novel approach for extracting a minimal synchronous context-free grammar (SCFG) for Hiero-style statistical machine translation using a non-parametric Bayesian framework. Our approach is designed to extract rules that are licensed by the word alignments and heuristically extracted phrase pairs. Our Bayesian model limits the number of SCFG rules extracted, by sampling from the space of all possible hierarchical rules; additionally our informed prior based on the lexical alignment probabilities biases the grammar to extract high quality rules leading to improved generalization and the automatic identification of commonly re-used rules. We show that our Bayesian model is able to extract minimal set of hierarchical phrase rules without impacting the translation quality as measured by the BLEU score.
###Abstract of the cited paper: We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-to- English evaluation task.
###Context text: The original approach extracts a larger number of rules when compared to a phrase-based system on the same data leading to practical issues in terms of memory requirements and decoding speed. Extremely large Hiero phrase tables may also lead to statistical issues, where the probability mass has to be shared by more rules: the probability p(e|f) has to be shared by all the rules having the same source side string f , leading to fragmentation and resulting in many rules having very poor probability. Approaches to improve the inference (the induction of the SCFG rules from the bitext) typically follows two streams.[Citation]This paper falls under the latter category and we use a non-parametric Bayesian approach for rule extraction for Hiero-style systems. Our objective in this paper is to provide a principled rule extraction method using a Bayesian framework that can extract the minimal SCFG rules without reducing the BLEU score.
###Response:","One focusses on filtering the extracted hierarchical rules either by removing redundancy #OTHEREFR or by filtering rules based on certain patterns #REFR, while the other stream is concerned about alternative approaches for learning the synchronous grammar #OTHEREFR.","Building upon improvements suggested in previous work for reductions in rule sets and memory usage in translation procedures [#REFR], we develop a non-parametric Bayesian rule extraction method."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A reading difficulty measure can be described as a function or model that maps a text to a numerical value corresponding to a difficulty or grade level. We describe a measure of readability that uses a combination of lexical features and grammatical features that are derived from subtrees of syntactic parses. We also tested statistical models for nominal, ordinal, and interval scales of measurement. The results indicate that a model for ordinal regression, such as the proportional odds model, using a combination of grammatical and lexical features is most effective at predicting reading difficulty.
###Abstract of the cited paper: We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \[5,9,10,15,17\] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \[9\]. The major technical innovation is tire use of a ""ma~ximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.
###Context text: The prediction is the level of the model with the highest likelihood of generating the text. There are no grammatical features. Natural language processing techniques enable more sophisticated grammatical analysis for reading difficulty measures.[Citation]A measure by Schwarm and Ostendorf #OTHEREFR incorporates syntactic analyses, among a variety of other types of features. It includes four grammatical features derived from syntactic parses of text: the mean parse tree height, the mean number of noun phrases, mean number of verb phrases, and mean number of ?SBARs.? ?SBARs? are non-terminal nodes that are associated with subordinate clauses. Their system led to better predictions than the Flesch-Kincaid and Lexile measures, but the predictive value of the grammatical features is not entirely clear.
###Response:","Rather than using sentence length as a proxy, measures can employ tools for automatic analysis of the syntactic structure of texts (e.g., #REFR).","Our readability measure makes extensive use of grammatical features, inspired by the parser described by #REFR, which provides high-precision parse trees up to sentence length 100."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task.
###Abstract of the cited paper: We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We define a headdriven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work.
###Context text: [Citation]We leave other rewrite operations, such as paraphrasing and reordering, for future work. Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREE- BASED COMPRESSION.
###Response:",Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure #OTHEREFR; #REFR.,Our TREE-BASED COMPRESSION method draws heavily from the SCFG-based sentence compression system proposed by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe an algorithm for Japanese analysis that does both base phrase chunking and dependency parsing simultaneously in linear-time with a single scan of a sentence. In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus. Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy.
###Abstract of the cited paper: We present a novel algorithm for Japanese dependency analysis. The algorithm allows us to analyze dependency structures of a sentence in linear-time while keeping a state-of-the-art accuracy. In this paper, we show a formal description of the algorithm and discuss it theoretically with respect to time complexity. In addition, we evaluate its efficiency and performance empirically against the Kyoto University Corpus. The proposed algorithm with improved models for dependency yields the best accuracy in the previously published results on the Kyoto University Corpus.
###Context text: We used a polynomial kernel and set its degree to 3 because cubic kernels proved to be effective empirically for Japanese parsing #OTHEREFR. The number of epoch T of VP was selected using the development test set. For multiclass prediction, we used the pairwise method #OTHEREFR.[Citation]We use the following features for each morpheme: 1. major POS, minor POS, conjugation type, conjugation form, surface form (lexicalized form)
###Response:",Features We have designed rather simple features based on the common feature set #OTHEREFR; #REFR for bunsetsu-based parsers.,"Building upon the novel linear-time algorithm for Japanese dependency analysis presented by #REFR, we applied and modified this approach for efficient Japanese parsing."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a statistical, treeto-tree model for producing translations. Two main contributions are as follows: (1) a method for the extraction of syntactic structures with alignment information from a parallel corpus of translations, and (2) use of a discriminative, featurebased model for prediction of these targetlanguage syntactic structures?which we call aligned extended projections, or AEPs. An evaluation of the method on translation from German to English shows similar performance to the phrase-based model of Koehn et al (2003).
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: Appendix B: Reranking Modifier Translations The n-best reranking model for the translation of modifiers considers a list of candidate translations. We hand-labeled 800 examples, marking the element in each list that would lead to the best translation. The features of the n-best reranking algorithm are combinations of the basic features in Tables 3 and 4.[Citation]The lists also contained a supplementary candidate ?DELETED?, signifying that the modifier should be deleted from the English translation. In addition, each candidate derived from the phrase-based system contributed one new candidate to the list signifying that the first word of the candidate should be deleted. These additional candidates were motivated by our observation that the optimal candidate in the n-best list produced by the phrase-based system often included an unwanted preposition at the beginning of the string.
###Response:",Each list contained the n-best translations produced by the phrase-based system of #REFR.,"In accordance with the phrase-based model proposed by #REFR, the system predicts candidate translations based on established lexical weighting and phrase translations."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present two measures for comparing corpora based on infbrmation theory statistics uch as gain ratio as well as simple term-class ~equency counts. We tested the predictions made by these measures about corpus difficulty in two domains - - news and molecular biology - - using the result of two well-used paradigms for NE, decision trees and HMMs and found that gain ratio was the more reliable predictor. made by these measures against actual system performance. Recently IE systems based on supervised learning paradigms uch as hidden Markov models (Bikel et al, 1997), maximum entropy (Borthwick et al, 1998) and decision trees (Sekine et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains:
###Abstract of the cited paper: \~e report the results of a study into the use of a linear interpolating hidden Marker model (HMM) for the task of extra.('ting lxw\]mi(:al |;erminology fl:om MEDLINE al)stra('ts and texl;s in the molecular-bioh)gy domain. Tiffs is the first stage isl a. system that will exl;ra('l; evenl; information for automatically ut)da.ting 1)ioh)gy databases. We trained the HMM entirely with 1)igrams based (m lexical and character features in a relatively small corpus of 100 MED- LINE abstract;s that were ma.rked-ul) l)y (lomain experts wil;h term (:lasses u(:h as t)rol;eins and DNA. I.Jsing cross-validation methods we a(:\]fieved a,n \].e-score of 0.73 and we (',xmnine the ('ontrilmtion made by each 1)art of the interl)olation model to overconfing (la.ta Sl)arsen('.ss.
###Context text: reco~. i ser : NEHMM HMMs are a widely u~d class of learning algorithms and can be considered to be stochastic finite state machines.[Citation]The word features themselves are discussed below. As is common practice, we need to calculate the probabilities for a word sequence for the first word is name class and every other word differently since we have no initial name-class to make a transition from. Accordingly we use the following equation to calculate the initial name class probability, Pr(NC~\[ < Wf~,t , Flli,,~ >)= aof(NC$,,s,\[ < Wf,,,,,Ffi,,t >)+ o~f(gcs~,,,I < -,Ff~,,, >) + a~f(NCfi,.,,) (i) and for all other words and their name classes as follows: Fr(NCT~ I < Wt,Ft >,< W~-,,Ft-, >,NC~-i) = Aof(NGtl < W~,F~ >,< Wt-,,Ft-1 >,NG~-,) + Alf(NCtI < .,F~ >,< W~-I,F~-i >,NC~- i )+ A2f(NC~I < W,,F~ >, < ..
###Response:","In the following model, summarized here from the full description given in #REFR, we consider words to be ordered pairs consisting of a surface word, W, and a word feature, F , given as < W, F >.",The utility and efficiency of HMMs in extracting terminological information from MEDLINE abstracts in the molecular biology domain is well-documented in the study by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Automatic metrics are widely used in machine translation as a substitute for human assessment. With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality. This is often measured by correlation with human judgment. Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance, however. In this paper, we introduce a significance test for comparing correlations of two metrics, along with an open-source implementation of the test. When applied to a range of metrics across seven language pairs, tests show that for a high proportion of metrics, there is insufficient evidence to conclude significant improvement over BLEU.
###Abstract of the cited paper: We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.
###Context text: Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as BLEU #OTHEREFR.[Citation]Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. In this paper, we motivate and introduce a novel significance test to assess the statistical significance of differences in correlation with human judgment for pairs of automatic metrics. We apply tests to the WMT-12 shared metrics task to compare each of the participating methods, and find that for a high proportion of metrics, there is not enough evidence to conclude that they significantly outperform BLEU.
###Response:","However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests #OTHEREFR; #REFR, this has not been the case in papers proposing new metrics.","Integrating principles from the Minimum Bayes-Risk decoding methodology for machine translation can potentially align with human evaluation, as argued by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe a novel approach to error detection in adjective?noun combinations. We present and release a new dataset of annotated errors where the examples are extracted from learner texts and annotated with error types. We show how compositional distributional semantic approaches can be applied to discriminate between correct and incorrect word combinations from learner data. Finally, we show how the output of the compositional distributional semantic models can be used as features in a classifier yielding good precision and accuracy.
###Abstract of the cited paper: This paper proposes a method for evaluating grammatical error detection methods to maximize the learning effect obtained by grammatical error detection. To achieve this, this paper sets out the following two hypotheses ? imperfect, rather than perfect, error detection maximizes learning effect; and precisionoriented error detection is better than a recall-oriented one in terms of learning effect. Experiments reveal that (i) precisionoriented error detection has a learning effect comparable to that of feedback by a human tutor, although the first hypothesis is not supported; (ii) precision-oriented error detection is better than recall-oriented in terms of learning effect; (iii) -measure is not always the best way of evaluating error detection methods.
###Context text: In the previous section, we showed that a classifier that uses output of the semantic models as features outperforms the comparison-based baseline system and shows good accuracy. In this section, we analyse the classifier?s performance in more detail. We note that, from an educational point of view, it is important for an EDC system to have high precision.[Citation]This suggests that learners might be misled and confused if they are frequently notified by a system that something is an error when it is not. Since precision is measured as the proportion of true positives (TP) to the sum of true positives and false positives (FP): P = TP TP + FP (4) an EDC system that achieves precision less than 0.5 is, in fact, misleading for language learners: for example, precision of less than 0.5 on the class of errors means that the system misidentifies correct use as an error more frequently than it correctly detects an error. Our classifier achieves good precision values with respect to both out-of-context and in-context annotations, on correct and incorrect examples.
###Response:","For example, it has been shown that grammatical error detection systems with high precision maximize learning effect, and that systems with high precision but lower recall are more useful in language learning than systems with high recall and lower precision #REFR.","Our emphasis on precision-oriented error detection aligns with the findings by #REFR, which underscores the importance of maximized learning effect through higher precision."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Domain adaptation, the problem of adapting a natural language processing system trained in one domain to perform well in a different domain, has received significant attention. This paper addresses an important problem for deployed systems that has received little attention ? detecting when such adaptation is needed by a system operating in the wild, i.e., performing classification over a stream of unlabeled examples. Our method uses A- distance, a metric for detecting shifts in data streams, combined with classification margins to detect domain shifts. We empirically show effective domain shift detection on a variety of data sets and shift conditions.
###Abstract of the cited paper: Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents. Despite the successes of these systems, accuracy will always be imperfect. For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field. The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model. We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records.
###Context text: Early NLP work in the unsupervised setting monitored classification confidence values, setting a confidence threshold based on a break-even heuristic, monitoring the rate of #OTHEREFR. Confidence estimation has been used for specific NLP components such as information extraction.[Citation]EM-based confidence estimation has been used to estimate the confidence of patterns derived from partially supervised relation extraction #OTHEREFR. Confidence estimation has also been used to improve the overall effectiveness of NLP systems. Confidence estimates obtained via neural networks have shown gains for speech recognition, spoken language understanding, and machine translation #OTHEREFR.
###Response:",The correctness of fields extracted via a conditional random field extractor has been shown to correlate well to an estimate obtained by a constrained forward-backward technique #REFR.,Leveraging probabilistic models such as linear-chain conditional random fields has demonstrated its ability in estimating the confidence accurately in information extraction tasks #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters. Initial results indicate that verb senses can be pruned for highly polysemous verbs by up to 74% by the first method and by up to 85% by the second method.
###Abstract of the cited paper: Previous work \[Gale, Church and Yarowsky, 1992\] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test his empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99% accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92% using combined models of very local context.
###Context text: It also prunes senses without loss of correctness. By intersecting the resulting sense sets with the output of our clusterbased method, verb senses can be pruned further. In using the clustering method is output, we make two further assumptions.[Citation]By extrapolation, we will assume that words appear in only one sense within a homogeneous corpus, 4 except for certain high frequency verbs or for semantically empty support verbs. We will assign this predominant sense to all non-disambignated occurrences of a verb. While this provides a reasonable default, the resulting semantic tag has to be considered provisional, and validated independently.
###Response:","Previous work has shown that within a given discourse #OTHEREFR, or with respect to a given collocation #REFR, a word appears in only one sense.","Further leveraging the observation by Gale, Church and Yarowsky (1992) that a polysemous word tends to exhibit one sense per discourse, we enhance the precision of our approach [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank. Perhaps the best way to think about this problem is to contrast our work with that of Klein and Manning (2003). That research used treetransformations to create various grammars with different contextual annotations on the non-terminals. These grammars were then used in conjunction with a CKY parser. The authors explored the space of different annotation combinations by hand. Here we try to automate the process ? to learn the ?right? combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7).
###Abstract of the cited paper: We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.
###Context text: It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank #OTHEREFR.[Citation]One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation #OTHEREFR. Rather than imagining the parser roaming around the tree for picking up the information it needs, we rather relabel the nodes to directly encode this information. Thus rather than have the parser ?look? to find out that, say, the parent of some NP is an S, we simply relabel the NP as an NP [S].
###Response:","Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase #OTHEREFR; #REFR (and others).","Drawing from a neural network method for inducing parse histories, consistent success can be achieved through structurally determined soft biases in inducing the representation of the parse history #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This project note describes a systern that receives, parses, indexes, and routes news reports. The core of this ,'mtomatic indexer is a parser based on Govermnent-Binding Theory which derives thematic and binding relationships of arguments of the sentences of slories. These syntactic structures are interpreted by a semantic processor which is linked to conceptual representations of terms from a controlled indexing vocabulary. As a result, the system is capable of indexing news with respect o a large set of let+ms that denote Ihe content of the articles. I',ACKGROUNI) With the rapidly increasing \olume of text being generated. transmitted, processed, and stored, it becomes critical that informalion retrieval and routing be highly efficient, both in time of processing and accuracy. To this end, indexing techniques have become the prinmry focus of much research, a~d 3'el dlese methods have relied on automatic keyword identification from texts. This is not to say that natural hmguagc techniques have not been examined with respect to their relevance for indexing and retrieval (cf./Sparck Jones and Kay 1973/, /Walker, Karlgren, and Kay 1977/, and more recently,/Sahon mid Smith 1989/). It is that most systems rely on the presence or absence of keywords with additional mechanisms such as proximity constraints, tatistical weighting, word-stem truncation, and boolean relrieval expressions. However, these methods do ~ot take into account the syntactic and semantic structure inherent in tile text being indexed. That is, they make virtually no use of the fact that it is natural anguage and not a collection of arbitrary strings of characters that is being processed. Natural anguage processing (NLP) can make its most valuable cont,'ibutions to those aspects of indexing where the keyword approaches fail, viz., the assignment of terms ~.o text based on their semantic or conceptual content. This involves deriving abstract relationships among conceptual units. For example, consider a story stating: ( 1 ) China bought 6,000 tonncs of wheat from the United States. \[)he plausible categorization of (1) is thai it is about foreign trade. However, the phrase ""foreign trade"" does not appear in (1), and it is invariably absent fl'om foreign trade stories in general. Furthermore, it is extremely unlikely that such foreign trade stories cottld be retrieved in an efficient manner, i.e., with a tew simple queries. The central issue is that although the particulars (e.g., country names and types o fcommodilies) wuy, the basic lneanings of foreign trade slodes are equivalent at some level, and that this level is wduable for indexing purposes. This suggests that systems that could operate at a conceptual level would be capable of indexing in ways that cotdd permit highly effective retrieval. It is with the assumption that NLP technology can provide tile means of categorizing text that guide several recent efforts. In particular, each of/Hayes et al 1988/,/Kuhns 1988/, and/Rau and Jitcobs 1988/ describes ystems that characterize news reports with results that could llOt be obtained by keyword methods alone. Since a news analysis system (NAS) was first reported in/Kuhns 1988/, a number of major enhancements to its design and underlying functionality have been incorporated. It is the purpose of this paper to report on tbe current state of NAS and ils COlllpolleI\]ts. SCOPE AND OB,IECTIVES A primary design goal behind NAS is to develop a system employing NLP technology that wouh:t be capable of either routing news t'rom electronic news feeds in real-time or indexing news with respect o very large sets of indexing terms (authority files). ~ This vocabulary is broad and ranges over diverse domains, with lerms representing proper names, concrete objects, or abslract relationships. The last type is dependent on the content of the stories and is particularly suited for the syntactic/ semantic techniques of NAS. The form of the indexes that NAS produces is a set of pairs of headings and subheadings or descriptors. The headings are frequently proper names while the descriptors add detailed information to the index by denoting various relationships of entities mentioned in the news reports. ARCHITECTURAL OVERVIEW Tile architecture of NAS is modular consisting of several main subsystems, viz., a set of preprocessors and template filters, a parser, a lexicon, a semantic interpreter, a set of concept bases, and an indexer (Figure 1). The system is transportable in that it can be interfaced to different news streams and indexing vocabularies. A preprocessor receives a news stream which can be flom a satellite dish link, a direct line, or a text file, and identifies the beginning and ending of stories in addition to their titles, sentences, and words. Since the format of each news feed, e.g., Reuters or Kyodo, is distinct from the others, a single preprocessor will accept only one news feed. For rigidly-formatted articles that are numerical or non-texttlal inform, a template filter, which is an indexing component of low-level routines, categorizes them from lhe title while deriving specifics l'rom the body of tile story. 1 397 Lexicon I Template Filter Words ~1 ndexes Textual Stor ies \[Parser i I I %c~Ttl to.co, i Structures ~ I ~ Terms An Architectural Overview of NAS  Figure 1  Figure 2 is an actual example (from Reuters) that has been indexed by the ternplate filter. The system outputs the company name with a descriptor ""3rd Quarter Earnings,"" as well as the current and cumulative arnings or losses. @R101903647 /&ACTMEDIA INC <ACTM.0> 3RD QTR LOSS WESTHAMPTON BEACH, N.Y., Oct 19, Reuter - Shr loss 14 cts vs profit 10 cts Net loss 1,674,000 vs profit 1.207,000 Revs 26.7 mln vs 19.1mln Nine Months Shr loss 19 cts vs profit 34 cts Net loss 2,280,000 vs profit 4,080,000 Revs 71.6 mln vs 59.8 rain Reuter (2) Agent (COMPANY) Predicate (INTRODUCE) Theme (PRODUCT). This structure denotes that a product introduction is one where a company introduces (or, synonymously, releases) a product. In short, it is a list of typed nodes. A report will be characterized as a product introduction story if it contains a sentence some of whose grammatical components (e.g., agent, predicate, theme) can be associated tothe corresponding odes of{ 2), Suppose, l:or example, a news item reports {3) Alpha Corp said it plans to release a new workstation in Japan. The parser, in accordance with GB principles, produces: indexes: Company Name - ACTMEDIA INC Descriptor - 3RD Quarter Earnings Subject - Net loss 1,674,000 Descriptor - Current Earnings Subject - Net loss 2,280,000 Descriptor - Cumulative Earnings (4) Agent (Alpha Corp) Predicate (said) Proposition Agent (Alpha Corp) Predicate (plans) Proposition Agent (Alpha Corp) Predicate (release) Theme (a new workstation) In (Japan) A Numerical Story and Its Index  Figure 2 In contrast, textual stories require grammatical processing and these are sent to the parser and semantic interpreter. The parser which relies on the principles of Government-Binding (GB) Theory (/Chomsky 1981/3, outputs predicate-argument structure of each sentence of a sto W.a In doing so, the parser identi ties empty categories, viz., PROs, traces, and variables, and thematic relations, and resolves antecedent and anaphor and pronominal bindings. It should be noted that the parser is interfaced to a lexicon of over 17,000 items that was developed by analyzing strings (words) from a newswim. The size of the lexicon is sufficient for news processing. The semantic interpreter maps the grammatical structures onto conceptual representations or filters stored in a concept base. For instance, a representation for ""Product Introduction"" is The parser binds the pronominal it, the agent of plans, to A~)ha Cor~tx, the subject or agent of the matrix clause. The parser also detects an empty category, viz., PRO, in the embedded sentence (proposition) with release as the verb and binds the pronominal to it. Since bound arguments share the same semantic features, the semantic interpreter determines that the agent of release in (4) is of type COMPANY. In other words, PRO inherits the property of COMPANY from the agent of the matrix sentence via the intermediate pronominal k. It also determines that the predicate release is synonymous with introduce and the theme workstation is a product. With the arguments typed and membership of the predicate within a synonym class known, the semantic processor can match the corresponding nodes of the most deeply embedded clause of (4) with (2), and thus determines that the sentence is about a product introduction. Associated with each conceptual filter is a set of indexing procedures that are invoked 398 2 by the indexing mechanism when a conceptual fi ter is satisfied. These thnctions are integrated with databases containing the indexing vocabulary and they identify specific information about a story including company, personal, and product names, and descriptors indicating specific relationships, Figure 3 illustrates the corporate and personal name identification capabilities and the level of ""understanding"" as reflected by the subheadings. @ R080100252 /&ALCO HEALTH<AAIIS.0> CHIEF EXECUTIVE RETIRES VALLEY FORGE, Pa., Aug 1, Renter - Alco Health Services Corp said Ray B. Mundt has been named acting chairman and chief executive officer, succeeding John H. Kennedy, who is retiring. indexes: Company Name - Descriptor - Alco Health Services Corp Officials and Employees Personal Name - Kennedy, John H. Descriptor - Retirement Personal Name - Descriptor - Mundt, Ray B. Select ion"",Appointme nt A Textual Story and its Index  Figure 3 BENCHMARKS Formal benchnmrks have been established based on news from Reuters. On a Symbolics 3640, NAS can process entire days of news (500-600 storiesMay) in 35-40 minutes and can assign indexes to approximately 75% of the stories. (The phumed goal of at least 85% coverage iscertainly achievable.) Accuracy was .judged exlremely high by a group of independent indexers and editors. Quality cotdd not be judged quantitatively clue to the complexity artd subjectivity of the indexing terms and procedures. FUTURE DIRECTIONS In addition to continual extensions tothe various colnponents of the system, a design for an interface of NAS to deductive databases has begun. The development of this extension would enable databases to be generated automatically with indexes being stored as logical relations, thereby, permitting retrieval or alerting capabilities based on explicit as well as implicit c,r inferred information. A NOTE ON THE IMPLEMENTATION NAS was developed in ZetaLisp on Symbolics workstations. It has been converted to Common Lisp and runs on Maclvory and Macintosh computers. CONCLUSION The results of NAS demonstrate hat it is possible to employ natural language processing and, in particular, linguistic-based parsers, to extract conceptual information from texts (news). This coupling of theoretical (GB) results with a large-scale application provides insights into the possibilities and limitations of computational aswell as formal inguistics. NOTES tThe natural language component, however, is not restricted to news processing. With another preprocessor and links to an expert system, the system has processed text found on insurance applications. 2The use of a GB-based parser within this application results flom the ongoing research and development on this type of parser. (/Kuhns 1986/describes an earlier implementation f GB Theory.) From a research perspective, a parser based on linguistic theory and applied to ""real-world"" text helps identify the boundaries or interface conditions between core and peripheral aspects of the theory. In other words, since GB is a model of core grammar and language contains marginal or marked conslructions, a GB-based parser nmst co-routine principles of the theory with hmguage-specific rules in order to have wide coverage. (/Fomita 1988/makes a similar observation.) Thus, it is this combination ofa psychologically-real theory and an application using fl'ee text that may provide insight into the human sentence processing mechanism.
###Abstract of the cited paper: This paper describes a pilot version of a commercial application of natural language processing techniques to the problem of categorizing news stories into broad topic categories. The system does not perform a complete semantic or syntactic analyses of the input stories. Its categorizations are dependent on fragmentary ecognition using patternmatching techniques. The fragments it looks for are determined by a set of knowledge-based rules. The accuracy of the system is only slightly lower than that of human categorizers.
###Context text: The central issue is that although the particulars (e.g., country names and types o fcommodilies) wuy, the basic lneanings of foreign trade slodes are equivalent at some level, and that this level is wduable for indexing purposes. This suggests that systems that could operate at a conceptual level would be capable of indexing in ways that cotdd permit highly effective retrieval. It is with the assumption that NLP technology can provide tile means of categorizing text that guide several recent efforts.[Citation]Since a news analysis system #OTHEREFR/, a number of major enhancements to its design and underlying functionality have been incorporated. It is the purpose of this paper to report on tbe current state of NAS and ils COlllpolleI\]ts. SCOPE AND OB,IECTIVES A primary design goal behind NAS is to develop a system employing NLP technology that wouh:t be capable of either routing news t'rom electronic news feeds in real-time or indexing news with respect o very large sets of indexing terms (authority files). ~ This vocabulary is broad and ranges over diverse domains, with lerms representing proper names, concrete objects, or abslract relationships.
###Response:","In particular, each of/#REFR/,/Kuhns 1988/, and/Rau and Jitcobs 1988/ describes ystems that characterize news reports with results that could llOt be obtained by keyword methods alone.","In particular, the work of #REFR shows that even without complete semantic or syntactic analysis, news stories can be categorized effectively using fragmentary pattern-matching techniques."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Patent claims are the subject of legal protection. They must be formulated according to a set of precise syntactic, lexical and stylistic guidelines. Composing patent claims is a complex task, even for experts. In this paper we report about an tmplemented system for supporting authoring claims for patents describing apparatuses. The system generates claim texts from the input specified partly by the stored conceptual text schemata and partly by the input from the user. The result of the interactive content acquisition stage is a shaUow-level representation which can be considered a draft to be automatically revised into the final text of the claim. Subject Keywords: interactive, automatic, generation, conceptual schema, template, patent claim
###Abstract of the cited paper: This paper describes a system (RAREAS) which synthesizes marine weather forecasts directly from formatted weather data. Such synthesis appem~ feasible in certain natural sublanguages with stereotyped text structure. RAREAS draws on several kinds of linguistic and non-linguistic knowledge and mirrors a forecaster is apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events. The approach can easily be adapted to synthesize bilingual or multMingual texts.
###Context text: Most of text planning and realization is carried out automatically. Superficially, the architecture of our system conforms to the standard emerged in natural anguage generation #OTHEREFR in that it includes the stages of content specification, text planning and surface generation (realization). However, there are some important differences.[Citation]Lexical selection and some other text planning tasks are interleaved with the process of content specification. The latter results in the production of a ""draft"" claim. This draft, while not yet an English text, is a list of proposition-level structures (""templates"") specifying the proposition head and case role values filled by POS-tagged word strings.
###Response:","Unlike the typical content specification modules #OTHEREFR; #REFR, our system relies on an authoring workstation environment equipped with a knowledge elicitation scenario for joint humancomputer content specification #OTHEREFR, for the details of the knowledge elicitation scenario).","Drawing from the method used by RAREAS in synthesizing marine forecasts, we implemented a similar process in generating patent claims #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Prior work has shown the utility of syntactic tree fragments as features in judging the grammaticality of text. To date such fragments have been extracted from derivations of Bayesianinduced Tree Substitution Grammars (TSGs). Evaluating on discriminative coarse and fine grammaticality classification tasks, we show that a simple, deterministic, count-based approach to fragment identification performs on par with the more complicated grammars of Post (2011). This represents a significant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain.
###Abstract of the cited paper: This paper explores a parsimonious approach to Data-Oriented Parsing. While allowing, in principle, all possible subtrees of trees in the treebank to be productive elements, our approach aims at finding a manageable subset of these trees that can accurately describe empirical distributions over phrase-structure trees. The proposed algorithm leads to computationally much more tracktable parsers, as well as linguistically more informative grammars. The parser is evaluated on the OVIS and WSJ corpora, and shows improvements on efficiency, parse accuracy and testset likelihood.
###Context text: Though CFGs and TSGs are weakly equivalent, TSGs permit nonterminals to rewrite as tree fragments of arbitrary size, whereas CFG rewrites are limited to depth-one productions. Figure 1 depicts an example TSG fragment and equivalent CFG rules; note that the entire internal structure of 1a is described within a single rewrite. Unfortunately, learning probabilistic TSGs is not straight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist.[Citation]More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models #OTHEREFR. In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1The n-gram analog would be something like storing all 30- grams seen in a corpus. all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be complex, difficult to code, and slow to converge.
###Response:",Approaches to this problem began by taking all fragments Fall in a treebank #OTHEREFR; #REFR.,"While this is true, a simpler, deterministic, count-based approach to fragment identification performs adequately, in agreement with the findings of the cited study #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Previous research has shown that the plausibility of an adjective-noun combination is correlated with its corpus co-occurrence frequency. In this paper, we estimate the co-occurrence frequencies of adjective-noun pairs that fail to occur in a 100 million word corpus using smoothing techniques and compare them to human plausibility ratings. Both class-based smoothing and distance-weighted averaging yield frequency estimates that are significant predictors of rated plausibility, which provides independent evidence for the validity of these smoothing techniques.
###Abstract of the cited paper: Frequency information on co-occurrence patterns can be atttomatically collected from a syntactically analyzed corpus; this information can then serve as the basis for selectional constraints when analyzing new text; from the same domain. Tiffs information, however, is necessarily incomplete. We report on measurements of the degree of selectional coverage obtained with ditt\~rent sizes of corpora. We then describe a technique for using the corpus to identify selectionally similar terms, and for using tiffs similarity to broaden the seleetional coverage for a tixed corpus size.
###Context text: Smoothing techniques have been used in a variety of statistical natural language processing applications as a means to address data sparseness, an inherent problem for statistical methods which rely on the relative frequencies of word combinations. The problem arises when the probability of word combinations that do not occur in the training data needs to be estimated.[Citation]Discounting methods decrease the probability of previously seen events so that the total probability of observed word co-occurrences is less than one, leaving some probability mass to be redistributed among unseen co-occurrences. Class-based smoothing and distance-weighted averaging both rely on an intuitively simple idea: inter-word dependencies are modelled by relying on the corpus evidence available for words that are similar to the words of interest. The two approaches differ in the way they measure word similarity.
###Response:","The smoothing methods proposed in the literature #OTHEREFR, and distance-weighted averaging #REFR.","As stated by #REFR, frequency information based on co-occurrence patterns obtained from syntactically analyzed corpora, albeit incomplete, can provide the basis for selectional constraints."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical Pitman- Yor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the Metropolis- Hastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by using the induced semantic representation for the question answering task in the biomedical domain.
###Abstract of the cited paper: Supervised semantic role labeling (SRL) systems are generally claimed to have accuracies in the range of 80% and higher (Erk and Pado?, 2006). These numbers, though, are the result of highly-restricted evaluations, i.e., typically evaluating on hand-picked lemmas for which training data is available. In this paper we consider performance of such systems when we evaluate at the document level rather than on the lemma level. While it is wellknown that coverage gaps exist in the resources available for training supervised SRL systems, what we have been lacking until now is an understanding of the precise nature of this coverage problem and its impact on the performance of SRL systems. We present a typology of five different types of coverage gaps in FrameNet. We then analyze the impact of the coverage gaps on performance of a supervised semantic role labeling system on full texts, showing an average oracle upper bound of 46.8%.
###Context text: Statistical approaches to semantic parsing have recently received considerable attention. While some methods focus on predicting a complete formal representation of meaning #OTHEREFR. However, most of this research has concentrated on supervised methods requiring large amounts of labeled data.[Citation]Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods #OTHEREFR). These approaches cluster semantically equivalent verbalizations of relations, often relying on syntactic fragments as features for relation extraction and clustering #OTHEREFR. The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument relations.
###Response:","Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage #REFR, motivating the need for unsupervised or semi-supervised techniques.","Despite the claimed accuracy of supervised semantic role labeling (SRL) systems, evaluation at the document level exposes substantial coverage gaps, affecting performance #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: As the number of learners of English is constantly growing, automatic error correction of ESL learners? writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction.
###Abstract of the cited paper: This paper proposes a method for evaluating grammatical error detection methods to maximize the learning effect obtained by grammatical error detection. To achieve this, this paper sets out the following two hypotheses ? imperfect, rather than perfect, error detection maximizes learning effect; and precisionoriented error detection is better than a recall-oriented one in terms of learning effect. Experiments reveal that (i) precisionoriented error detection has a learning effect comparable to that of feedback by a human tutor, although the first hypothesis is not supported; (ii) precision-oriented error detection is better than recall-oriented in terms of learning effect; (iii) -measure is not always the best way of evaluating error detection methods.
###Context text: Because of the growing number of learners of English, there is an increasing demand to help learners of English.[Citation]However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors #OTHEREFR, that commonly occur in essays by ESL learners.
###Response:",It is highly effective for learners to receive feedback on their essays from a human tutor #REFR.,"To assist English learners more effectively, improvements to tense/aspect error correction were considered, inspired by the effectiveness of precision-oriented error detection #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Natural language processing (NLP) is critical for improvement of the healthcare process because it has the potential to encode the vast amount of clinical data in textual patient reports. Many clinical applications require coded data to function appropriately, such as decision support and quality assurance applications. However, in order to be applicable in the clinical domain, performance of the NLP systems must be adequate. A valuable clinical application is the detection of infectious diseases, such as surveillance of healthcare-associated pneumonia in newborns (e.g. neonates) because it produces significant rates of morbidity and mortality, and manual surveillance of respiratory infection in these patients is a challenge. Studies have already demonstrated that automated surveillance using NLP tools is a useful adjunct to manual clinical management, and is an effective tool for infection control practitioners. This paper presents a study aimed at evaluating the feasibility of an NLP-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neonates. We estimated sensitivity, specificity, and positive predictive value by comparing the detection with clinicians? judgments and our results demonstrated that the automated method was indeed feasible. Sensitivity (recall) was 87.5%, and specificity (true negative rates) was 94.1%.
###Abstract of the cited paper: Thesaurus Instantiation. M. Sc. thesis, University of Kansas, Lawrence, KS. Pendergraft, Eugene. 1967. Translating languages. In Harold Borko, editor, Automated Language Processing. John Wiley & Sons, New York. Pereira, Fernando and Naftali Tishby. 1992. Distributional similarity, phase transitions and hierarchical clustering. Working Notes of the AAAI Symposium on Probabilistic Approaches toNatural Language, pages 108-112, Cambridge, MA, October. Pereira, Fernando, Naftali Tishby, and Lilian Lee. 1993. Distributional clustering of Computational Linguistics Volume 24, Number 1 English. In Proceeedings of the 31st Annual Meeting, pages 183-190, Ohio State University, Colombus, OH, June. Association for Computational Linguistics. Pimsleur, P. 1957. Semantic frequency counts. Mechanical Translation, 4(1-2):11-13. Pustejovsky, James. 1995. The Generative Lexicon. MIT Press, Cambridge, MA. Pustejovsky, James, Branimir Boguraev, and Michael Johnston. 1995. A core lexical engine: The contextual determination of word sense. Technical Report, Department of Computer Science, Brandeis University. Quillian, M. Ross. 1961. A design for an understanding machine. Presented at the Semantic Problems in Natural Language colloquium, King is College, Cambridge University, Cambridge, UK, September. Quillian, M. Ross. 1962a. A revised design for an understanding machine. Mechanical Translation, 7(1):17-29. Quillian, M. Ross. 1962b. A semantic oding technique for mechanical English paraphrasing. Internal memorandum of the Mechanical Translation Group, Research Laboratory of Electronics, MIT, August. Quillian, M. Ross. 1967. Word concepts: A theory and simulation of some basic semantic apabilities. Behavioral Science, 12:410-430. Quillian, M. Ross. 1968. Semantic memory. In M. Minsky, editor, Semantic Information Processing. MIT Press, Cambridge, MA, pages 227-270. Quillian, M. Ross. 1969. The teachable language comprehender: A simulation program and theory of language. Communications of the ACM, 12(8):459-476. Quine, Willard V. 1960. Word and Object. MIT Press, Cambridge, MA. Quirk, Randolph. 1960. Towards a description of English usage. Transactions of the Philological Society, pages 40-61. Rayner, Keith and R. K. Morris. 1991. Comprehension processes in reading ambiguous entences: Reflections from eye movements. In G. Simpson, editor, Understanding Word and Sentence. North-Holland, Amsterdam. pages 175-198. Reifler, Erwin. 1955. The mechanical determination of meaning. In William N. Locke and A. Donald Booth, editors, Machine Translation of Languages. John Wiley & Sons, New York, pages 136-164. Resnik, Philip. 1992. WordNet and distributional nalysis: A class-based approach to statistical discovery. In Proceedings ofthe AAAI Workshop on Statistically-based Natural Language Processing Techniques, pages 48-56. San Jose, CA. Resnik, Philip. 1993a. Selection and Information: A Class-based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania. Also University of Pennsylvania Technical Report 93-42. Resnik, Philip. 1993b. Semantic lasses and syntactic ambiguity. In Proceedings ofthe ARPA Workshop on Human Language Technology, pages 278-283. Resnik, Philip. 1995a. Disambiguating noun groupings with respect o WordNet senses. In Proceedings ofthe Third Workshop on Very Large Corpora, pages 54-68, Cambridge, MA. Resnik, Philip. 1995b. Using information content o evaluate semantic similarity in a taxonomy. In Proceedings ofthe 14th International Joint Conference on Artificial Intelligence, IJCAI'95, pages 448-453, Montreal, Canada. . Resnik, Philip and David Yarowsky. 1997a. Evaluating automatic semantic taggers. In Proceedings ofthe ACL-SIGLEX Workshop ""Tagging Text with Lexical Semantics: Why, What, and How?', page 91, Washington, DC, April. Resnik, Philip and David Yarowsky. 1997b. A perspective on word sense disambiguation methods and their evaluation. In Proceedings ofthe ACL-SIGLEX Workshop ""Tagging Text with Lexical Semantics: Why, What, and How?"", pages 79-86, Washington, DC, April. Richards, I. A. 1953. Towards a theory of translation. In Studies in Chinese Thought, University of Chicago Press, Chicago. Richardson, Ray and Alan F. Smeaton. 1994. Automatic word sense disambiguation i a KBIR application. Working paper CA-0595, School of Computer Applications, Dublin City University, Dublin, Ireland. Richens, Richard H. 1958. Interlingual machine translation. Computer Journal, 1(3):144-147. Richmond, Korin, Andrew Smith, and Einat Amitay. 1997. Detecting Subject Boundaries Within Text: A Language Independent S atistical Approach. In Proceedings ofthe Second Conference on Empirical Methods in Natural Language Processing, EMNLP-2, pages 47-54, Brown University, Providence, RI, August. Roberts, D. D. 1973. The Existential Graphs of Charles S. Pierce, Mouton, The Hague. Robins, R. H. 1987. Polysemy and the Ide and V~ronis Introduction lexicographer. In R. W. Burchfield, editor, Studies in Lexicography. Oxford University Press, Oxford, UK, pages 52-75. Rosenblatt, Frank. 1958. The perceptron: A probabilistic model for information storage and organization i the brain. Psychological Review, 65:386-408. Salton, Gerard. 1968. Automatic Information Organization and Retrieval. McGraw-Hill, New York. Salton, Gerard and M. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York. Salton, Gerard, A. Wong, and C. S. Yang.
###Context text: In chest radiographs of neonates, BPD generally denotes broncopulmonary dysplasia, a condition that predisposes the patient to respiratory infection. In other types of radiology reports, for instance abdominal echography, BPD generally means biparietal diameter, a measure of the gestation age. Word sense disambiguation is a difficult problem, which is widely discussed in the computational linguistics literature.[Citation]In the clinical setting, an important part of the solution will involve identifying the particular domain and use of special purpose domain-specific disambiguators that tag ambiguous abbreviations and specify their appropriate sense prior to parsing, based on the domain and other contextual information. Defining the appropriate domain granularity will be important, but may be a difficult task because the granularity may vary with the abbreviation. For example, in the case of radiographic reports, possibly the domain should involve all chest x-rays or only chest x-rays of neonates, or the specific type of reports.
###Response:",A review of methods for word sense disambiguation is presented by Ide and colleagues #REFR.,"The difficulties in word sense disambiguation have attracted considerable attention, particularly in computational linguistics where ambiguous terms and their clarifying factors are taken into account (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.
###Abstract of the cited paper: Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
###Context text: Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox #OTHEREFR shows that translation between English and French satisfies cohesion in the majority cases.[Citation]But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two models.
###Response:",Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs #OTHEREFR; #REFR.,The approach towards tree-based modeling proposed by #REFR provides a great understanding of syntactic cohesion while considering tree-to-string translation.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students? summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.
###Abstract of the cited paper: We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
###Context text: More recently, LSA has been incorporated with a suite of NLP metrics to assess students? strategies for reading comprehension using think-aloud protocols #OTHEREFR. The resulting tool, and similar assesment tools such as Coh-Metrix, assess aspects of readability of texts, such as coherence, but do not assess students? comprehension through their writing #OTHEREFR. E-rater is an automated essay scorer for standardized tests such as GMAT that also relies on a suite of NLP techniques #OTHEREFR.[Citation]An alternate line of research attempts to replicate human reading comprehension. An automated tool to read and answer questions relies on abductive reasoning over logical forms extracted from text #OTHEREFR. One of the performance issues is resolving meanings of words: removal of WordNet features degraded performance.
###Response:","The pyramid method #REFR, was inspired in part by work in reading comprehension that scores content using human annotation #OTHEREFR.","Our research, focused on content evaluation, echoes the approach suggested by #REFR, which advocates for a method assessing relative importance of conveyed facts in summarization."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Building on work detecting errors in dependency annotation, we set out to correct local dependency errors. To do this, we outline the properties of annotation errors that make the task challenging and their existence problematic for learning. For the task, we define a feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally.
###Abstract of the cited paper: Faced with the problem of annotation errors in part-of-speech (POS) annotated corpora, we develop a method for automatically correcting such errors. Building on top of a successful error detection method, we first try correcting a corpus using two off-the-shelf POS taggers, based on the idea that they enforce consistency; with this, we find some improvement. After some discussion of the tagging process, we alter the tagging model to better account for problematic tagging distinctions. This modification results in significantly improved performance, reducing the error rate of the corpus.
###Context text: [Citation]Namely, ambiguity class information (e.g., IN/RB/RP) is added to each corpus position for training, creating complex ambiguity tags, such as <IN/RB/RP,IN>. While this results in successful correction, it is not clear how it applies to annotation which is not positional and uses NIL labels. However, ambiguity class information is relevant when there is a choice between labels; we return to this in section 5.
###Response:",Correcting POS annotation errors can be done by applying a POS tagger and altering the input POS tags #REFR.,"Drawing from strategies used to reduce annotation errors in POS tagged corpora, successful correction was achieved by including extra tag information in the model [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Automatic summarization can help users extract the most important pieces of information from the vast amount of text digitized into electronic form everyday. Central to automatic summarization is the notion of similarity between sentences in text. In this paper we propose the use of continuous vector representations for semantically aware representations of sentences as a basis for measuring similarity. We evaluate different compositions for sentence representation on a standard dataset using the ROUGE evaluation measures. Our experiments show that the evaluated methods improve the performance of a state-of-the-art summarization framework and strongly indicate the benefits of continuous word vector representations for automatic summarization.
###Abstract of the cited paper: We present a novel graph-based summarization framework (Opinosis) that generates concise abstractive summaries of highly redundant opinions. Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. The summaries are readable, reasonably well-formed and are informative enough to convey the major opinions.
###Context text: [Citation]Each of these topics contains between 50 and 575 sentences and are a collection of user reviews made by different authors about a certain characteristic of a hotel, car or a product (e.g. ?Location of Holiday Inn, London? and ?Fonts, Amazon Kindle?). The dataset is well suited for multidocument summarization (each sentence is considered its own document), and includes between 4 and 5 gold-standard summaries (not sentences chosen from the documents) created by human authors for each topic. Each summary is evaluated with ROUGE, that works by counting word overlaps between generated summaries and gold standard summaries.
###Response:",The Opinosis dataset #REFR consists of short user reviews in 51 different topics.,"Leveraging the principles of the Opinosis framework, which has demonstrated effectiveness in summarizing redundant opinions, can optimize our automatic summarization method[#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper investigates the use of promptbased content features for the automated assessment of spontaneous speech in a spoken language proficiency assessment. The results show that single highest performing promptbased content feature measures the number of unique lexical types that overlap with the listening materials and are not contained in either the reading materials or a sample response, with a correlation of r = 0.450 with holistic proficiency scores provided by humans. Furthermore, linear regression scoring models that combine the proposed promptbased content features with additional spoken language proficiency features are shown to achieve competitive performance with scoring models using content features based on prescored responses.
###Abstract of the cited paper: In this paper, we explore unsupervised techniques for the task of automatic short answer grading. We compare a number of knowledge-based and corpus-based measures of text similarity, evaluate the effect of domain and size on the corpus-based measures, and also introduce a novel technique to improve the performance of the system by integrating automatic feedback from the student answers. Overall, our system significantly and consistently outperforms other unsupervised methods for short answer grading that have been proposed in the past.
###Context text: Therefore, it is likely that many of the important lexical items from the sample response are also contained in the listening materials. Thus, the LR?S feature provided less information than the LR?S? feature. The features used in this study are all based on simple lexical overlap statistics, and are thus trivial to implement.[Citation]Furthermore, future research will address the validity of the proposed features by ensuring that there are ways to filter out responses that are too similar to the stimulus materials, and thus indicate that the test taker simply repeated the source verbatim.
###Response:","Future research will investigate more sophisticated methods of text-to-text similarity for prompt-based content scoring, such as those used in #REFR.",This is aligned with the unsupervised techniques for automatic short answer grading explored by #REFR which enhance system performance through integrating feedback from student answers.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of microblogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.
###Abstract of the cited paper: We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \[5,9,10,15,17\] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \[9\]. The major technical innovation is tire use of a ""ma~ximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.
###Context text: 3 The performance of the different models is computed using the Area Under the ROC curve (AROC) applied to the classification score returned by the SVM. The ROC curve allows us to study the behavior of the classifier in detail, and also provides a powerful way to compare among systems when the dataset is unbalanced (as in our case). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in #OTHEREFR.[Citation]We used the Charniak syntactic parser without any specific adaptation to the Twitter language. Model AROC BOW 0.592 WBOW 0.578 LEX + BOW 0.725 . LEX + WBOW 0.728 .
###Response:","We pre-process the dataset with the following tools: the Charniak Parser #REFR for parsing sentences, the WordNet similarity package #OTHEREFR for creating the SYNT and the FOR feature spaces.","The parser's performance, inspired by a ""maximum-entropy"" model, was demonstrated to greatly reduce error rates in previous studies [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a novel two-stage method for mining opinion words and opinion targets. In the first stage, we propose a Sentiment Graph Walking algorithm, which naturally incorporates syntactic patterns in a Sentiment Graph to extract opinion word/target candidates. Then random walking is employed to estimate confidence of candidates, which improves extraction accuracy by considering confidence of patterns. In the second stage, we adopt a self-learning strategy to refine the results from the first stage, especially for filtering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous methods. The experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods.
###Abstract of the cited paper: Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE?s novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.
###Context text: We call such relations that match opinion patterns but express no opinion false opinion relations. Previous pattern learning algorithms #OTHEREFR often extract opinion patterns by frequency. However, some high-frequency syntactic patterns can have very poor precision #OTHEREFR.[Citation]As can be seen, this phrase does express a positive opinion but unfortunately ?time? is not a valid opinion target for most domains such as MP3. Thus, false opinion targets are extracted. Due to the lack of ground-truth knowledge for opinion targets, non-target terms introduced in this way can be hardly filtered out.
###Response:","False Opinion Targets: In another case, the phrase ?wonderful time? can be matched by an opinion pattern ?Adj-{mod}-Noun?, which is widely used in previous works #REFR.",These false opinion targets much resemble the issues encountered when mining for important product features as presented by OPINE #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document?s raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.
###Abstract of the cited paper: The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation. In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions. High-level topics such as ?sports? or ?entertainment? are rendered differently in each geographic region, revealing topic-specific regional distinctions. Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency. The model also enables prediction of an author?s geographic location from raw text, outperforming both text regression and supervised topic models.
###Context text: Perhaps the most obvious and far-reaching application is geographic information retrieval #OTHEREFR; these allow users to browse and search for content through a geo-centric interface. The Perseus project performs automatic toponym resolution on historical texts in order to display a map with each text showing the locations that are mentioned #OTHEREFR; Google Books also does this for some books, though the toponyms are identified and resolved quite crudely. Hao et al#OTHEREFR use a location-based topic model to summarize travelogues, enrich them with automatically chosen images, and provide travel recommendations.[Citation]An intuitive and effective strategy for summarizing geographically-based data is identification of the location?a specific latitude and longitude?that forms the primary focus of each document. Determining a single location of a document is only a well-posed problem for certain documents, generally of fairly small size, but there are a number of natural situations in which such collections arise. For example, a great number of articles in Wikipedia have been manually geotagged; this allows those articles to appear in their geographic locations while geobrowsing in an application like Google Earth.
###Response:",#REFR investigate questions of dialectal differences and variation in regional interests in Twitter users using a collection of geotagged tweets.,"Drawing on research that utilized geotagged social media to explore geographic linguistic variations, this technique also offers improved accuracy in predicting an author's geographic location from raw text #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art Penn Treebank (PTB) parser. An accuracy comparison is performed by converting the CCG derivations into PTB trees. We show that the conversion is extremely difficult to perform, but are able to fairly compare the parsers on a representative subset of the PTB test section, obtaining results for the CCG parser that are statistically no different to those for the Berkeley parser.
###Abstract of the cited paper: This paper presents a methodology for the comparative performance analysis of the parsers developed for different grammar frameworks. For such a comparison, we need a common representation format of the parsing results since the representation of the parsing results depends on the grammar frameworks; hence they are not directly comparable to each other. We first convert the parsing result to a shallow CFG analysis by using an automatic tree converter based on synchronous grammars. The use of such a shallow representation as a common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations. We compared an HPSG parser with several CFG parsers in our experiment and found that meaningful differences among the parsers? performance can still be observed by such a shallow representation.
###Context text: One question that is often asked of the CCG parsing work is ?Why not convert back into the PTB representation and perform a Parseval evaluation?. By showing how difficult the conversion is, we believe that we have finally answered this question, as well as demonstrating comparable performance with the Berkeley parser.[Citation]Even the smallest loss due to mapping across representations is significant when a few tenths of a percentage point matter. Whether PTB parsers could be competitive on alternative parser evaluations, such as those using GR schemes, for which the CCG parser performs very well, is an open question. Acknowledgements James Curran was funded under Australian Research Council Discovery grant DP0665973.
###Response:","In addition, we have thrown further doubt on the possible use of the PTB for cross-framework parser evaluation, as recently suggested by #REFR.","Our assertion aligns with the methodology presented in #REFR, showcasing the challenges of comparative analysis between different parsing frameworks."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Acquisition of prosody, in addition to vocabulary and grammar, is essential for language learners. However, it has received less attention in instruction. To enable automatic identification and feedback on learners? prosodic errors, we investigate automatic pitch accent labeling for nonnative speech. We demonstrate that an acoustic-based context model can achieve accuracies over 79% on binary pitch accent recognition when trained on withingroup data. Furthermore, we demonstrate that good accuracies are achieved in crossgroup training, where native and nearnative training data result in no significant loss of accuracy on non-native test speech. These findings illustrate the potential for automatic feedback in computer-assisted prosody learning.
###Abstract of the cited paper: Many acoustic approaches to prosodic labeling in English have employed only local classifiers, although text-based classification has employed some sequential models. In this paper we employ linear chain and factorial conditional random fields (CRFs) in conjunction with rich, contextually-based prosodic features, to exploit sequential dependencies and to facilitate integration with lexical features. Integration of lexical and prosodic features improves pitch accent prediction over either feature set alne, and for lower accuracy feature sets, factorial CRF models can improve over linear chain based prediction of pitch accent.
###Context text: Recent research has highlighted the importance of context for both tone and intonation. The role of context can be seen in the characterization of pitch accents such as down-stepped high and in phenomena such as downdrift across a phrase. Further, local coarticulation with neighboring tones has been shown to have a significant impact on the realization of prosodic elements, due to articulatory constraints #OTHEREFR.[Citation]Thus, in this work, we employ a rich contextual feature set, based on that in #OTHEREFR. We build on the pitch target approximation model, taking the syllable as the domain of tone prediction with a pitch height and contour target approached exponentially over the course of the syllable, consistent with #OTHEREFR. We employ an acoustic model at the syllable level, employing pitch, intensity and duration measures.
###Response:","The use of prosodic and coarticulatory context has improved the effectiveness of tone and pitch accent recognition in a range of languages #OTHEREFR, and CRFs #REFR).","Drawing on the utilization of linear chain and factorial conditional random fields for prosodic labeling in English by #REFR, our study similarly values context-based features."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present Positive Diversity Tuning, a newmethod for tuningmachine translation models specifically for improved performance during system combination. System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other. We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems. The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines. We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores. When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems.
###Abstract of the cited paper: This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination. We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneficial for system combination. Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the bestranked machine translation engines in the
###Context text: Xiao et al#OTHEREFR explores using boosting to train an ensemble of machine translation systems. Following the standard Adaboost algorithm, each system was trained in sequence on an error-driven reweighting of the tuning set that focuses learning on the material that is the most problematic for the current ensemble. They found that using a single system to tune a large number of decoding models to different Adaboost guided weightings of the tuning data results in significant gains during system combination.[Citation]They programmatically generated variations of systems using different build and decoder hyperparameters such as choice of wordalignment algorithm, distortion limit, variations of model feature function weights, and the set of language models used. Then, in a process similar to forward feature selection, they constructed a combined system by iteratively adding the individual automatically generated system that produced the largest increase in quality when used in conjunction with the systems already selected for the combined system. They also explored producing variation by using different samplings of the the training data.
###Response:",#REFR investigated system combination using automatic generation of diverse individual systems.,"In line with this research, our study finds merit in reducing cross-system similarity for system combination, yielding significant gains despite using identical systems#REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Pure statistical parsing systems achieves high in-domain accuracy but performs poorly out-domain. In this paper, we propose two different approaches to produce syntactic dependency structures using a large-scale hand-crafted HPSG grammar. The dependency backbone of an HPSG analysis is used to provide general linguistic insights which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.?
###Abstract of the cited paper: We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.
###Context text: Since we focus on the domain adaptation issue, we incorporate a less domain dependent language resource (i.e. the HPSG parsing outputs using ERG) into the features models of statistical parsers. As mordern grammar-based parsers has achieved high runtime efficency (with our HPSG parser parsing at an average speed of?3 sentences per second), this adds up to an acceptable overhead. 3.3.1 Feature Model with MSTParser As mentioned before, MSTParser is a graphbased statistical dependency parser, whose learning procedure can be viewed as the assignment of different weights to all kinds of dependency arcs.[Citation]It is emphasized by the authors that the last two categories contribute a large improvement to the performance and bring the parser to the state-of-the-art accuracy. Therefore, we extend this feature set by adding four more feature categories, which are similar to the original ones, but the dependency relation was replaced by the dependency backbone of the HPSG outputs. The extended feature set is shown in Table 1.
###Response:","Therefore, the feature model focuses on each kind of head-child pair in the dependency tree, and mainly contains four categories of features #REFRa): basic uni-gram features, basic bi-gram features, in-between POS features, and surrounding POS features.","The MSTParser harnesses the maximum spanning trees technique in directed graphs for weighted dependency parsing, enhancing efficiency and accuracy, according to #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Learning to group words into phrases without supervision is a hard task for NLP systems, but infants routinely accomplish it. We hypothesize that infants use acoustic cues to prosody, which NLP systems typically ignore. To evaluate the utility of prosodic information for phrase discovery, we present an HMM- based unsupervised chunker that learns from only transcribed words and raw acoustic correlates to prosody. Unlike previous work on unsupervised parsing and chunking, we use neither gold standard part-of-speech tags nor punctuation in the input. Evaluated on the Switchboard corpus, our model outperforms several baselines that exploit either lexical or prosodic information alone, and, despite producing a flat structure, performs competitively with a state-of-the-art unsupervised lexicalized parser, with a substantial advantage in precision. Our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for languagelearning infants.
###Abstract of the cited paper: We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available. In addition to presenting a fully Bayesian model for the PCFG, we also develop an efficient variational inference procedure. On synthetic data, we recover the correct grammar without having to specify its complexity in advance. We also show that our techniques can be applied to full-scale parsing applications by demonstrating its effectiveness in learning state-split grammars.
###Context text: Experimental work has provided strong support for this hypothesis, for example by showing that infants begin learning basic rhythmic properties of their language prenatally #OTHEREFR. However, as far as we know, there has so far been no direct computational evaluation of the prosodic bootstrapping hypothesis. In this paper, we provide the first such evaluation by exploring the utility of acoustic cues for unsupervised syntactic chunking, i.e., grouping words into non-hierarchical syntactic phrases.[Citation]Ponvert et al describe a simple method for chunking that uses only bigram counts and punctuation; when the chunks are combined using a rightbranching structure, the resulting trees achieve unlabeled bracketing precision and recall that is competitive with other unsupervised parsers. The system?s dependence on punctuation renders it inappropriate for addressing the questions we are interested in here, but its good performance reccommends syntactic chunking as a profitable approach to the problem of grammar induction, especially since chunks can be learned using much simpler models than are needed for hierarchical structure. The models used in this paper are all variants of HMMs.
###Response:",Nearly all previous work on unsupervised grammar induction has focused on learning hierarchical phrase structure #OTHEREFR; #REFR or dependency structure #OTHEREFR.,"Leveraging the approach of a nonparametric Bayesian model of tree structures based on the HDP-PCFG model as reported by #REFR, we introduced useful adaptations for our study on unsupervised syntactic chunking."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages. However, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly. To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. Experiments showed that our methods improve the performances greatly on projections between English and Chinese.
###Abstract of the cited paper: This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection. Keywords multilingual, text analysis, part-of-speech tagging, noun phrase bracketing, named entity, morphology, lemmatization, parallel corpora
###Context text: We call it direct projection, which becomes very popular recently. The main limitation of these methods is that target language has to be similar to source language. Otherwise the performance will degrade especially when the orders of phrases between source and target languages differ a lot.[Citation]We refer them as projection based on word alignments in this paper. Compared to other types of projection methods, this type of methods is more robust to syntactic differences between languages since it trained models on the target side thus following the topology of the target language. This paper aims to build an accurate projection method with strong generality to various pairs of languages, even when the languages are from different families and are typologically divergent.
###Response:",Another common type of projection methods map labels from resource-rich language sentences to resource-scarce ones in a parallel corpus using word alignment information #REFR.,"Overcoming the noise and incompleteness of initial projections observed in previous work [#REFR], our approach utilizes target-language word representations and a novel noise-removing technique."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a new hierarchical Bayesian n-gram model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.
###Abstract of the cited paper: Maximum entropy models are a common modeling technique, but prone to overfitting. We show that using an exponential distribution as a prior leads to bounded absolute discounting by a constant. We show that this prior is better motivated by the data than previous techniques such as a Gaussian prior, and often produces lower error rates. Exponential priors also lead to a simpler learning algorithm and to easier to understand behavior. Furthermore, exponential priors help explain the success of some previous smoothing techniques, and suggest simple variations that work better.
###Context text: Pitman-Yor processes produce power-law distributions that more closely resemble those seen in natural languages, and it has been argued that as a result they are more suited to applications in natural language processing #OTHEREFR. We show experimentally that our hierarchical Pitman-Yor language model does indeed produce results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney, two of the currently best performing smoothing methods #OTHEREFR. In fact we show a stronger result?that interpolated Kneser-Ney can be interpreted as a particular approximate inference scheme in the hierarchical Pitman-Yor language model.[Citation]Thus the contributions of this paper are threefold: in proposing a langauge model with excellent performance and the accompanying advantages of Bayesian probabilistic models, in proposing a novel and efficient inference scheme for the model, and in establishing the direct correspondence between interpolated Kneser-Ney and the Bayesian approach. We describe the Pitman-Yor process in Section 2, and propose the hierarchical Pitman-Yor language model in Section 3. In Sections 4 and 5 we give a high level description of our sampling based inference scheme, leaving the details to a technical report #OTHEREFR.
###Response:","Our interpretation is more useful than past interpretations involving marginal constraints #OTHEREFR or maximum-entropy models #REFR as it can recover the exact formulation of interpolated Kneser-Ney, and actually produces superior results. #OTHEREFR has independently noted the correspondence between the hierarchical Pitman-Yor language model and interpolated Kneser-Ney, and conjectured improved performance in the hierarchical Pitman-Yor language model, which we verify here.","The use of maximum entropy models with exponential priors, as evidenced by #REFR, further substantiates our method by explaining the success of certain smoothing techniques."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees. Currently our best automatically induced grammars achieve 80.97% f-score for fstructures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).
###Abstract of the cited paper: This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a loglinear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed.1
###Context text: [Citation]The parenttransformed grammars perform best in both architectures. In all cases, there is a marked improvement (2.07-6.36%) in the f-structures after LDD resolution. We achieve between 73.78% and 80.97% preds-only and 83.79% to 87.04% all GFs f-score, depending on gold-standard.
###Response:",PARC 700 Dependency Bank following #REFR The results are given in Table 7.,"As reported in the comparative study, the deep-parsing system outperformed the Collins parser in accuracy, yet with almost equivalent speed #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Multi-word terms are traditionally identified using statistical techniques or, more recently, using hybrid techniques combining statistics with shallow linguistic information. Al)proaches to word sense disambiguation and machine translation have taken advantage of contextual information in a more meaningflfl way, but terminology has rarely followed suit. We present an approach to term recognition which identifies salient parts of the context and measures their strength of association to relevant candidate terms. The resulting list of ranked terms is shown to improve on that produced by traditional methods, in terms of precision and distribution, while the information acquired in the process can also be used for a variety of other applications, such as disambiguation, lexical tuning and term clustering.
###Abstract of the cited paper: In this paper, we make use of linguistic knowledge to identify certain noun phrases, both in English and French, which are likely to be terms. We then test and cmnl)are (lifl'e.rent statistical scores to select the ""good"" ones among tile candidate terms, and finally propose a statistical method to build correspondences of multi-words units across languages. Acknowledgement Most of this work was carried out under project EUII.OTP~A ET-10/63, co-sponsored by the European Economic Conmmnity.
###Context text: [Citation]One of the main reasons for the current lack of diversity in approaches to term recognition lies in the difficulty of extracting suitable semantic information from speeialised corpora, particularly in view of the lack of appropriate linguistic resources. The increasing development of electronic lexieal resources, coupled with new methods for automatically creating and fine-tuning them from corpora, has begun to pave the way for a more dominant appearance of natural language processing techniques in the field of terminology. The TRUCKS approach to term recognition (Term Recognition Using Combined Knowledge Sources) focuses on identifying relevant contextual information from a variety of sources, in order to enhance traditional statistical techniques of term recognition.
###Response:","Although statistical approaches to automatic term recognition, e.g. #OTHEREFR; #REFR, have achieved relative success over the years, the addition of suitable linguistic information has the potential to enhance results still further, particularly in the case of small corpora or very specialised omains, where statistical information may not be so accurate.","This methodology for identifying salient parts of the context was built upon the linguistic knowledge used to identify certain noun phrases in term recognition, as elaborated by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has properties that support the Polyanna Hypothesis. Using the General Inquirer as gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above.
###Abstract of the cited paper: Knowing the degree of antonymy between words has widespread applications in natural language processing. Manually-created lexicons have limited coverage and do not include most semantically contrasting word pairs. We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus. The approach is evaluated on a set of closest-opposite questions, obtaining a precision of over 80%. Along the way, we discuss what humans consider antonymous and how antonymy manifests itself in utterances.
###Context text: Sentiment analysis involves determining the opinions and private states #OTHEREFR.[Citation]A crucial sub-problem is to determine whether positive or negative sentiment is expressed. Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation #OTHEREFR. A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion.
###Response:",It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems #OTHEREFR and opinions #REFRa).,"Building on the automatic measure of antonymy presented by #REFR, our proposed lexicon incorporates a thesaurus-based approach, offering high coverage of semantically contrasting word pairs."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes the embedding of a statistical translation system within a text editor to produce TRANSTYPE, a system that watches over the user as he or she types a translation and repeatedly suggests completions for the text already entered. This innovative Embedded Machine Translation system is thus a specialized means of helping produce high quality translations.
###Abstract of the cited paper: This work is in the context of TRANSTYPE, a system that observes its user as he or she types a translation and repeatedly suggests completions for the text already entered. The user may either accept, modify, or ignore these suggestions. We describe the design, implementation, and performance of a prototype which suggests completions of units of texts that are longer than one word.
###Context text: We also asked them if they thought hat TRANSTYPE improved their typing speed and the majority of them said so; unfortunately the figures showed that none of them did so ... The typing rates are nevertheless quite good, given that the users were new to this environment and this style of looking at suggestions while translating. But interestingly this practical ew~luation confirmed our theoretical evaluation that atranslation can be produced with TRANSTYPE by typing less than 40% of the characters of a translation.[Citation]This experiment made us realize that this concept of real-time suggestions depends very much on the usability of the prototype; we had first developed a much simpler editor but its limitations were such that the translators found it unusable. So we are convinced that the userinterface aspects of this prototype should be thoroughly studied. But the TRANSTYPE approach would be much more useful if it was combined with other text editing tasks related to translation: for example TRANSTYPE could format the translation in the same way as the source text, this would be especially useful for titles and tables; it would also be possible to localize automatically specific entities such as dates, numbers and amounts of money.
###Response:",Results of this evaluation and comparisons with our theoretical figures are further described in #REFR.,"In fact, the prototype of TRANSTYPE presented by #REFR, which recommends the completion of text units longer than a word, affirmed this approach's potential."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering?an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.
###Abstract of the cited paper: In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.
###Context text: We compare to using an integrated lexicalized reordering model #OTHEREFR applied to the phrase-based baseline. We evaluate the impact of the proposed approach on translation quality as measured by the BLEU score on the token level #OTHEREFR. The translation model is trained on 700 million tokens of parallel text, primarily extracted from the web using automated parallel document identification #OTHEREFR.[Citation]Our dev and test data sets consist of 3100 and 1000 English sentences, respectively, that were randomly sampled from the web and translated into Japanese. The eval set is a larger, heterogenous set containing 12,784 sentences. In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training #OTHEREFR.
###Response:",Alignments were learned using two iterations of Model 1 and two iterations of the HMM alignment model #REFR.,"In aligning our translation model, we adapted the first-order Hidden Markov model for word alignment in statistical translation as suggested by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The translation of sentiment information is a task from which sentiment analysis systems can benefit. We present a novel, graph-based approach using Sim- Rank, a well-established vertex similarity algorithm to transfer sentiment information between a source language and a target language graph. We evaluate this method in comparison with SO-PMI.
###Abstract of the cited paper: Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language. In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages. Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages. Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.
###Context text: The second method is based on parallel corpora. The source language in the corpus is annotated with sentiment information, and the information is then projected to the target language. Problems arise due to mistranslations, e.g., because irony is not recognized.[Citation]Given a corpus annotated with sentiment information in one language, machine translation is used to produce an annotated corpus in the target language, by preserving the annotations. The original annotations can be produced either manually or automatically. Wan #OTHEREFR constructs a multilingual classifier using co-training.
###Response:",#REFR use machine translation for multilingual sentiment analysis.,"In their research, the authors suggest leveraging English resources to generate subjectivity resources in other languages via machine translation, suggesting its viability for new target languages #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We explore a novel computational approach to identifying ?constructions? or ?multi-word expressions? (MWEs) in an annotated corpus. In this approach, MWEs have no special status, but emerge in a general procedure for finding the best statistical grammar to describe the training corpus. The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing. We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank. We report quantitative results on the ATIS corpus of phrase-structure annotated sentences, and give examples of the MWEs extracted from this corpus.
###Abstract of the cited paper: Speech research has made tremendous progress in the past using the following paradigm: ? define the research problem, ? collect a corpus to objectively measure progress, and ? solve the research problem. Natural language research, on the other hand, has typically progressed without the benefit of any corpus of data with which to test research hypotheses. We describe the Air Travel Information System (ATIS) pilot corpus, a corpus designed to measure progress in Spoken Language Systems that include both a speech and natural anguage component. This pilot marks the first full-scale attempt o collect such a corpus and provides guidelines for future efforts.
###Context text: We have implemented the algorithms for calculating the expected frequency, and the push-n-pull algorithm for estimation. We have evaluated the algorithms on a number of simple example STSGs and found that the expected frequency algorithm correctly predicts observed frequencies. We have further found that ? unlike existing estimation methods ? the push-n-pull algorithm converges to STSGs that closely model the observed frequencies (i.e. that maximize the likelihood of the data) without putting all probability mass in the largest elementary trees (i.e. whilst retaining generalizations about the data).[Citation]Before processing, all trees (train and test set) were converted to a format that our current implementation requires (all non-terminal labels are unique, all internal nodes have two daughters, all preterminal nodes have a single lexical daughter; all unary productions and all traces were removed). The set of trees was randomly split in a train set of 462 trees, and a test set 2An important topic for future research is to clarify the relation between push-n-pull and Expectation Maximization. of 116 trees. The push-n-pull algorithm was then run in 10 passes over the train set, with d = 3, B = 0 and ? = 0.1.
###Response:",Here we report first quantitative results on the ATIS3 corpus #REFR.,"Our study utilized the Air Travel Information System (ATIS) corpus of phrase-structure annotated sentences, a significant corpus in natural language research, for testing the aforementioned algorithms #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Bibliometric measures are commonly used to estimate the popularity and the impact of published research. Existing bibliometric measures provide ?quantitative? indicators of how good a published paper is. This does not necessarily reflect the ?quality? of the work presented in the paper. For example, when hindex is computed for a researcher, all incoming citations are treated equally, ignoring the fact that some of these citations might be negative. In this paper, we propose using NLP to add a ?qualitative? aspect to biblometrics. We analyze the text that accompanies citations in scientific articles (which we term citation context). We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation.
###Abstract of the cited paper: Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others? viewpoint of the target article?s contributions and the study of its citation summary network using a clustering approach.
###Context text: This analysis of citation purpose and polarity can be useful for many applications. For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers? work more accurately. It can also be used as a preprocessing step in systems that process scholarly data.[Citation]In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. We use the term citation context to refer to this text.
###Response:","For example, citation-based summarization systems #REFR and survey generation systems #OTHEREFR can benefit from citation purpose and polarity analysis to improve paper and content selection.","This approach aligns with the model suggested by #REFR, which employs text analysis and clustering techniques to summarize viewpoints on a target article."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a novel approach for exploiting the global context for the task of word sense disambiguation (WSD). This is done by using topic features constructed using the latent dirichlet alocation (LDA) algorithm on unlabeled data. The features are incorporated into a modified na??ve Bayes network alongside other features such as part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic patterns. In both the English all-words task and the English lexical sample task, the method achieved significant improvement over the simple na??ve Bayes classifier and higher accuracy than the best official scores on Senseval-3 for both task.
###Abstract of the cited paper: We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \[5,9,10,15,17\] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \[9\]. The major technical innovation is tire use of a ""ma~ximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.
###Context text: We describe here the experimental setup on the English lexical sample task and all-words task.[Citation]All default parameters are used unless mentioned otherwise. For all standard 1http://svmlight.joachims.org 2http://www.cs.princeton.edu/?blei/lda-c/ baseline features, we use Laplace smoothing but for the soft tag (equation [2]), we use a smoothing parameter value of 2.
###Response:","We use MXPOST tagger #OTHEREFR for POS tagging, Charniak parser #REFR for extracting syntactic relations, SVMlight1 for SVM classifier and David Blei?s version of LDA2 for LDA training and inference.","The lexical analysis was significantly enhanced using a parser with a ""maximum-entropy-inspired"" model as detailed by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Parallel Ireatment of syntactic considerations in generation promises quality and speed. Parallelism should be used not only for simultaneous processing of several sub-parts of the output, but even within single parts. If beth types of parallelism are used with incremental generation it becomes unnecessary tobuild up and manipulate representations of entence structure-- the syntactic form of the output can be emergent. FIG is a structured conneetionist generator bulk in this way. Constructions and their constituents are represented in the same network which encodes world knowledge and lexical knowledge. Grammatical output results from synergy among many construetions simultaneously active at run-time. FIG incorporates new ways of handling constituency, word order and optional constituents; and simple ways to avoid the problems of instantiatien and binding. Syntactic knowledge isexpressed in a simple, readable form; this representation straightforwardly efines parts of the network.
###Abstract of the cited paper: This paper discusses word choice for natural language generation. It examines 11 issues, the solutions that have been proposed for them, and their implications for design. The issues are: o How are appropriate words chosen~ o How is conciseness ensured7 o When does choice stop? o How are patterns of lexicalization respected? o How are interactions among choices handled? o How are the correct parts of speech chosen? o How are words chosen to satisfy constituency? o What ensures that a word stands in the correct relation to its neighbors? o How is word order determined? o Are all words chosen in the same way? o In what order are the factors considered. This paper also discusses FIG, a generator which incorporates novel solutions to many of these issues. FIG violates common assumptions about the roles of modularity and grammar in generator design. Analysis of FIG leads to 4 Principles for generator design, as follows: o Have an explicit representation f the status of the generation process at each point in time. o Use a single, unified representation. o Do not rely on the details of the structure of the input. o Treat most choices as emergent.
###Context text: Consider the problem of specifying where a given concept should appear and what syntactic form it should take. In FIG this is handled by simultaneously activating a concept node and a syntactic construction orcategory node. For example, the third constituent ofgo-p specifies that 'the direction of the going' be expressed as a haverbal particle.' Activation will thus flow to an appropriate word node, such as downw, both via the concept filling the directionr slot and via the syntactic ategory vparticle.[Citation]Government, for example, the way that some verbs govern case markers, is handled in the same way.
###Response:",Thanks to this sort of activation flow FIG tends to select and emit an appropriate word in an appropriate form #REFR.,"The parallel treatment in FIG supports word choice, order, and appropriateness, reflecting the findings of the natural language generation study #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing to increase deep parsing accuracy, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing.
###Abstract of the cited paper: This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).
###Context text: Because we use dependency parsing as a step in deep parsing, it is important that we choose a parsing approach that is not only accurate, but also efficient.[Citation]Our approach is based on Nivre and Scholz?s approach, using support vector machines for classification of shift/reduce actions.
###Response:",The deterministic shift/reduce classifier-based dependency parsing approach #REFR has been shown to offer state-of-the-art accuracy #OTHEREFR with high efficiency due to a greedy search strategy.,"The deterministic dependency parser introduced by #REFR, with its high accuracy and linear-time efficiency, serves as an ideal foundation for our process."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We improve the quality of statistical machine translation (SMT) by applying models that predict word forms from their stems using extensive morphological and syntactic information from both the source and target languages. Our inflection generation models are trained independently of the SMT system. We investigate different ways of combining the inflection prediction component with the SMT system by training the base MT system on fully inflected forms or on word stems. We applied our inflection generation models in translating English into two morphologically complex languages, Russian and Arabic, and show that our model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judgements.
###Abstract of the cited paper: We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs.
###Context text: Equation (2) shows that from these n fully inflected hypotheses, the model then selects the one which has the best score, combined with the base MT score wi for Si. We should note that this method does not represent standard n-best reranking because the input from the base MT system contains sequences of stems, and the model is generating fully inflected translations from them. Thus the chosen translation may not be in the provided nbest list.[Citation]The interpolation weights ? in Equations (1) and (2) as well as the optimal number of translations n from the base MT system to consider, given a maximum of m=100 hypotheses, are trained using a separate dataset. We performed a grid search on the values of ? and n, to maximize the BLEU score of the final system on a development set #OTHEREFR sentences (Table 2). The three methods of integration differ in the way the base MT engine is applied.
###Response:","This method is more similar to the one used in #REFR, with the difference that they use only 1-best input from a base MT system.","Our approach differs from the probabilistic bilingual capitalization model by #REFR, as we utilize extensive morphological and syntactic information during the translation process."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Paraphrases are useful for statistical machine translation (SMT) and natural language processing tasks. Distributional paraphrase generation is independent of parallel texts and syntactic parses, and hence is suitable also for resource-poor languages, but tends to erroneously rank antonyms, trend-contrasting, and polarity-dissimilar candidates as good paraphrases. We present here a novel method for improving distributional paraphrasing by filtering out such candidates. We evaluate it in simulated low and mid-resourced SMT tasks, translating from English to two quite different languages. We show statistically significant gains in English-to-Chinese translation quality, up to 1 BLEU from nonfiltered paraphrase-augmented models (1.6 BLEU from baseline). We also show that yielding gains in translation to Arabic, a morphologically rich language, is not straightforward.
###Abstract of the cited paper: Knowing the degree of antonymy between words has widespread applications in natural language processing. Manually-created lexicons have limited coverage and do not include most semantically contrasting word pairs. We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus. The approach is evaluated on a set of closest-opposite questions, obtaining a precision of over 80%. Along the way, we discuss what humans consider antonymous and how antonymy manifests itself in utterances.
###Context text: [Citation]The thesaurus divides the vocabulary into about a thousand coarse categories. Each category has, on average, about a hundred closely related words. (A word with more than one sense, is listed in more than one category.) Mohammad et al first determine pairs of thesaurus categories that are contrasting in meaning. A category pair is said to be contrasting if it has a seed antonym pair.
###Response:","#REFR used a Roget-like thesaurus, co-occurrence statistics, and a seed set of antonyms to identify the degree of antonymy between two words, and generate a list of antonymous words.","Leveraging the structure of a thesaurus and corpus statistics, Mohammad et al developed an empirical measure of antonymy, which can be instrumental in identifying contrasting category pairs [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora. The motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms. Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term?s context vector does not always reliably represent a terms meaning due to the context vector?s sparsity. Our proposed method uses a weighted average of the synonyms? context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution. We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet. The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.
###Abstract of the cited paper: String transformation, which maps a source string s into its desirable form t?, is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1-regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision boundary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations.
###Context text: As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT)3 and the USA National Highway Traffic Safety Administration (NHTSA)4, respectively. The Japanese corpus contains 24090 sentences that were POS tagged using MeCab #OTHEREFR.[Citation]For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio #OTHEREFR. It was shown in #OTHEREFR that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMI5 and LLR6. For comparing two context vectors we use the cosine similarity.
###Response:","The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger #OTHEREFR, and use the Lemmatizer #REFR to extract and stem content words (nouns, verbs, adjectives, adverbs).","In generating our translation candidates, we adapted the discriminative approach for string transformation presented in #REFR, applying substring substitution rules to our data."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences. In this paper, we explore implementations of a framework based on Combinatory Categorial Grammar (CCG), in which words with certain grammatical types have meanings represented by multilinear maps (i.e. multi-dimensional arrays, or tensors). An obstacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework.
###Abstract of the cited paper: Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.
###Context text: The output is a single scalar, which is then transformed with the sigmoid function. Values over 0.5 are considered plausible. DMat The final method produces a scalar as in KKMat, but is distributional and based on corpus counts rather than regression-based.[Citation]The intuition is that, for example, the matrix for eat may have a high value for the contextual topic pair describing animate subjects and edible objects. To determine the plausibility of a new subject-object pair for a particular verb, we calculate the Kronecker product of the subject and object noun vectors for this pair, and compare the resulting matrix with the average verb matrix using cosine similarity. For label prediction, we calculate the similarity between each of the training data pairs and the learned average matrix.
###Response:",#REFR introduced a corpus-based approach for generating a K ?K matrix for each verb from an average of Kronecker products of the subject and object vectors from the positively labelled subset of the training data.,"DMat's approach aligns with the methods used in the cited study, which relied on employing matrices for relational words in an unsupervised manner #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The part-whole relation is of special importance in biomedicine: structure and process are organised along partitive axes. Anatomy, for example, is rich in partwhole relations. This paper reports preliminary experiments on part-whole extraction from a corpus of anatomy definitions, using a fully automatic iterative algorithm to learn simple lexico-syntactic patterns from multiword terms. The experiments show that meronyms can be extracted using these patterns. A failure analysis points out factors that could contribute to improvements in both precision and recall, including pattern generalisation, pattern pruning, and term matching. The analysis gives insights into the relationship between domain terminology and lexical relations, and into evaluation strategies for relation learning.
###Abstract of the cited paper: The discovery of semantic relations from text becomes increasingly important for applications such as Question Answering, Information Extraction, Text Summarization, Text Understanding, and others. The semantic relations are detected by checking selectional constraints. This paper presents a method and its results for learning semantic constraints to detect part-whole relations. Twenty constraints were found. Their validity was tested on a 10,000 sentence corpus, and the targeted partwhole relations were detected with an accuracy of 83%.
###Context text: Berland and Charniak #OTHEREFR report what they believed to be the first work finding part-whole relations from unlabelled corpora. The method used is similar to that of Hearst, but includes metrics for ranking proposed part-whole relations. They report 55% accuracy for the top 50 ranked relations, using only the two best extraction patterns.[Citation]Girju contends that the ambiguity of part-whole patterns means that more information is needed to distinguish meronymic from non-meronymic contexts. She developed an algorithm to learn semantic constraints for this differentiation, achieving 83% precision and 98% recall with a small set of manually selected patterns. Others have looked specifically at meronymy in anaphora resolution (e.g.
###Response:",Gi#REFR reports a relation discovery algorithm based on Hearst.,"Building on this, our study uses an iterative algorithm to identify lexico-syntactic patterns, which show promising results in part-whole extraction from anatomy definitions, complementing the approach of Girju #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This work explores methods of automatically detecting corrections of individual mistakes in sentence revisions for ESL students. We have trained a classifier that specializes in determining whether consecutive basic-edits (word insertions, deletions, substitutions) address the same mistake. Experimental result shows that the proposed system achieves an F -score of 81% on correction detection and 66% for the overall system, out-performing the baseline by a large margin.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: Therefore, to effectively reduce the algorithm?s mis-detection errors, we propose to Swanson and Yamangil reported an overall system with 58% F-score. build a classifier to merge with better accuracies. Other previous tasks also involve comparing two sentences. Unlike evaluating grammar error correction systems #OTHEREFR, correction detection cannot refer to a gold standard.[Citation]They are fundamentally different in that the granularity of the extracted phrase pairs is a major concern in our work ? we need to guarantee each detected phrase pair to address exactly one writing problem. In comparison, phrase extraction systems aim to improve the end-to-end MT or paraphrasing systems. A bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases.
###Response:","Our error analysis above also highlights our task?s difference with previous work that identify corresponding phrases between two sentences, including phrase extraction #REFR and paraphrase extraction #OTHEREFR.","In the similar line of thinking, we adapted the phrase-based translation models from #REFR for our corrections detection task."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we present a first attempt to characterize the semantic deviance of composite expressions in distributional semantics. Specifically, we look for properties of adjective-noun combinations within a vectorbased semantic space that might cue their lack of meaning. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). For each model, we generate composite vectors for a set of AN combinations unattested in the source corpus and which have been deemed either acceptable or semantically deviant. We then compute measures that might cue semantic anomaly, and compare each model?s results for the two classes of ANs. Our study shows that simple, unsupervised cues can indeed significantly tell unattested but acceptable ANs apart from impossible, or deviant, ANs, and that the simple additive and multiplicative models are the most effective in this task.
###Abstract of the cited paper: We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
###Context text: Statistical approaches to describe, represent and understand natural language have been criticized as failing to account for linguistic ?creativity?, a property which has been accredited to the compositional nature of natural language. Specifically, criticisms against statistical methods were based on the argument that a corpus cannot significantly sample a natural language because natural language is infinite #OTHEREFR. This cricticism also applies to distributional semantic models that build semantic representations of words or phrases in terms of vectors recording their distributional co-occurrence patterns in a corpus #OTHEREFR, but have no obvious way to generalize to word combinations that have not been observed in the corpus.[Citation]The ability to work with unattested data leads to the question of why a linguistic expression might not be attested in even an extremely large and wellbalanced corpus. Its absence might be motivated by a number of factors: pure chance, the fact that the expression is ungrammatical, uses a rare structure, describes false facts, or, finally, is nonsensical. One criticism from generative linguists is precisely that statistical methods could not distinguish between these various possibilities.
###Response:","To address this problem, there have been several recent attempts to incorporate into distributional semantic models a component that generates vectors for unseen linguistic structures by compositional operations in the vector space #REFR.","The matrix-based model suggested by #REFR is proficient at managing unattested adjective-noun pairs and distinguishing linguistic anomalies, adding another layer of semantic comprehension."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes a new Word Sense Disambiguation (WSD) algorithm which extends two well-known variations of the Lesk WSD method. Given a word and its context, Lesk algorithm exploits the idea of maximum number of shared words (maximum overlaps) between the context of a word and each definition of its senses (gloss) in order to select the proper meaning. The main contribution of our approach relies on the use of a word similarity function defined on a distributional semantic space to compute the gloss-context overlap. As sense inventory we adopt Babel- Net, a large multilingual semantic network built exploiting both WordNet and Wikipedia. Besides linguistic knowledge, BabelNet also represents encyclopedic concepts coming from Wikipedia. The evaluation performed on SemEval-2013 Multilingual Word Sense Disambiguation shows that our algorithm goes beyond the most frequent sense baseline and the simplified version of the Lesk algorithm. Moreover, when compared with the other participants in SemEval-2013 task, our approach is able to outperform the best system for English.
###Abstract of the cited paper: The resolution of lexical ambiguity is important for most natural language processing tasks, and a range of computational techniques have been proposed for its solution. None of these has yet proven effective on a large scale. In this paper, we describe a method for lexical disambiguation of text using the definitions in a machine-readable dictionm~j together with the technique of simulated annealing. The method operates on complete sentences and attempts to select the optimal combinations of word senses for all the words in the sentence simultaneously. The words in the sentences may be any of the 28,000 headwords in Longman is Dictionary of Contemporary English (LDOCE) and are disambiguated relative to the senses given in LDOCE. Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand-coding of lexical entries, or hand-tagging of text. L Introduction The problem of word-sense disambiguation is central to text processing. Recently, promising computational methods have been suggested \[Lesk, 1986; McDonald * Present address: Mathematics DepaJtment. University of Texas at El Paso, El Paso, Tx et al, 1990; Veronis and Ide, 1990; Wilks et al., 1990; Zernik and ,lacobs, 1990; Guthrie et al, 1991; Hearst, 1991\] which attempt o use the local context of the word to be disambiguated together with information about each of its word senses to solve this problem. Lesk \[1986\] described a technique which measured the amount of overlap between a dictionary sense definition and the definitions of the words in the local context of the word to be disambiguated. He illustrated his method by successfully disambiguating the word ""cone"" in the phrases ""pine cone"" and ""ice cream cone"". Later researchers have extended this basic idea in various ways. Wilks et al, \[1990\] identified neighborhoods of the 2,187 control vocabulary words in Longman is Dictionary of Contemporaay English (LDOCE) \[Procter, 1978\] based on the co-occurrence of words in LDOCE dictionary definitions. These neighborhoods were then used to expand the word sense definitions of the word to be disambiguated, and the overlap between the expanded definitions and the local context was used to select the correct sense of a word. A similar method reported by Guthrie et al, \[1991\] who defined subject specific neighborhoods of words, using the subject area markings in the machine readable version of LDOCE. Hearst \[1991\] suggests using syntactic information and partof-speech tagging to aid in the disambiguatton. She gathers co-occurrence information Aclxs DE COLING-92. NANTES, 23-28 AOt\]T 1992 3 5 9 PROC, OV COLING-92, NAN'rES, AUG. 23-28, 1992 from manually senseutagged text. Zemik and Jacobs \[1990\] also derive their neighborhoods from a training text which has I~en sense-tagged by hand. Their method incorporates other clues as to the sense of the word in question found in the morphology or by first tagging the text as to part of speech. Although each of these techniques look somewhat promising for disambiguation, the techniques have only been applied to several words, and the results have been based on experiments which repeatedly disambiguate a single word (or in \[Zernik and Jacobs. 1990\], one of three words) in a large number of sentences. In the cases where a success rate for the technique is reported, the results vary from 35% to 80%, depending on whether the correct sense is desired, or some coarser grained distraction is considered acceptable. Since only one sense is computed at a time, the question arises as to whether and how to incolporate the fact that a sense has been chosen for a word when attempting to disambiguate the next. Should this first choice be changed in light of how other word senses are selected. Although these problems were pointed out in Lesk is original paper, they have not yet been addressed. A method of word sense disambiguation which is designed to operate on a large scale and simultaneously for several words was suggested by Veronis and Ide \[1990\]. The basis of this method is the construction of large neural networks which have words and word senses (chosen from the machine readable version of the Collins Dictionary) as nodes. Links are established from a word to each of its senses, and from each sense to every word in its definition. Inhibiting links are constructed between senses of the same word. In order to disambiguate a sentence, the words in the sentence are activated in the network, and this activation is allowed to spread with feedback. This cycle is repeated a presetected number of times, e.g.,
###Context text: Ambiguity resolution has been pursued as a way to improve retrieval systems, and generally to get better information access. Despite its ancient roots and perceived importance, this task is still far from being resolved. Our WSD method relies on both the Lesk algorithm and its two variants: simplified #OTHEREFR.[Citation]Basile et al. #OTHEREFR adopted the simplified Lesk algorithm to disambiguate adjectives and adverbs, combining it with other two methods for nouns and verbs: the combination of different approaches for each part-of-speech resulted in better performance with respect to the use of a single strategy. More recently, Schwab et al. #OTHEREFR proposed GETALP, another unsupervised WSD algorithm inspired by Lesk. Their approach computes a local similarity using the classical Lesk measure (overlap between glosses), and then the local similarity is propagated to the whole text (global similarity) using an algorithm inspired by the Ant Colony.
###Response:","Several approaches have modified the Lesk algorithm to reduce is exponential complexity, like the one based on Simulated Annealing #REFR.","Building on the work of previous research that addresses lexical disambiguation through simulated annealing and definitions usage #REFR, our algorithm extends and refines these strategies."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper shows that incorporating linguistically motivated features to ensure correct animacy and number agreement in an averaged perceptron ranking model for CCG realization helps improve a state-ofthe-art baseline even further. Traditionally, these features have been modelled using hard constraints in the grammar. However, given the graded nature of grammaticality judgements in the case of animacy we argue a case for the use of a statistical model to rank competing preferences. Though subject-verb agreement is generally viewed to be syntactic in nature, a perusal of relevant examples discussed in the theoretical linguistics literature (Kathol, 1999; Pollard and Sag, 1994) points toward the heterogeneous nature of English agreement. Compared to writing grammar rules, our method is more robust and allows incorporating information from diverse sources in realization. We also show that the perceptron model can reduce balanced punctuation errors that would otherwise require a post-filter. The full model yields significant improvements in BLEU scores on Section 23 of the CCGbank and makes many fewer agreement errors.
###Abstract of the cited paper: We present a simple history-based model for sentence generation from LFG f-structures, which improves on the accuracy of previous models by breaking down PCFG independence assumptions so that more f-structure conditioning context is used in the prediction of grammar rule expansions. In addition, we present work on experiments with named entities and other multi-word units, showing a statistically significant improvement of generation accuracy. Tested on section 23 of the Penn Wall Street Journal Treebank, the techniques described in this paper improve BLEU scores from 66.52 to 68.82, and coverage from 98.18% to 99.96%.
###Context text: [Citation]To our knowledge, however, none of these models have included features specifically designed to handle grammatical agreement, an important task in surface realization. In this paper, we show that incorporating linguistically motivated features to ensure correct animacy and verbal agreement in an averaged perceptron ranking model for CCG realization helps improve a state-of-the-art baseline even further. We also demonstrate the utility of such an approach in ensuring the correct presentation of balanced punctuation marks.
###Response:","In recent years a variety of statistical models for realization ranking that take syntax into account have been proposed, including generative models #OTHEREFR; #REFR, maximum entropy models #OTHEREFR.","Building upon previous models for sentence generation from LFG f-structures, which showed an improvement in generation accuracy and BLEU scores [#REFR], we have incorporated linguistic features designed for grammatical agreement."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A machine translation model has been proposed where an input is translated through both source-language and target-language paraphrasing processes. We have implemented our prototype model for the Japanese-Chinese language pair. This paper describes our core idea of translation, where a source language paraphraser and a language transfer cooperates in translation by exchanging information about the source input.
###Abstract of the cited paper: One of the key issues in spoken language translation is how to deal with unrestricted expressions in spontaneous utterances. This research is centered on the development of a Chinese paraphraser that automatically paraphrases utterances prior to transfer in Chinese-Japanese spoken language translation. In this paper, a pattern-based approach to paraphrasing is proposed for which only morphological analysis is required. In addition, a pattern construction method is described through which paraphrasing patterns can be efficiently learned from a paraphrase corpus and human experience. Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated.
###Context text: Another advantage is task portability, since all of the paraphrasing knowledge, except for lexical paraphrasing knowledge, is independent of the task, so we do not need to fit most of the paraphrasing knowledge to the required task. It is also significant that this model?s paraphraser can be employed not only for MT but also for most natural language processing (NLP) applications. This is possible because both the input and output of a paraphraser is the same natural language.[Citation]We have already constructed a prototype for Japanese- Chinese. In this paper, we report the core concepts of this prototype and discuss issues of both our principle and our implementation.
###Response:","We have been building the Sandglass MT system for the Japanese-Chinese, Chinese- Japanese language pairs #OTHEREFR; #REFR.","Drawing from prior research, our method utilizes a Chinese paraphraser developed for spontaneous utterances in spoken language translation #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we present the results of the combination of stochastic and rule-based disambiguation methods applied to Basque languagel. The methods we have used in disambiguation are Constraint Grammar formalism and an HMM based tagger developed within the MULTEXT project. As Basque is an agglutinative language, a morphological nalyser is needed to attach all possible readings to each word. Then, CG rules are applied using all the morphological features and this process decreases morphological ambiguity of texts. Finally, we use the MULTEXT project tools to select just one from the possible remaining tags. Using only the stochastic method the error rate is about 14%, but the accuracy may be increased by about 2% enriching the lexicon with the unknown words. When both methods are combined, the error rate of the whole process is 3.5%. Considering that the training corpus is quite small, that the HMM model is a first order one and that Constraint Grammar of Basque language is still in progress, we think that this combined method can achieve good results, and it would be appropriate for other agglutinative languages.
###Abstract of the cited paper: We present an implementation f a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.
###Context text: [Citation]So, we have selected one of each group: Constraint Grammar formalism #OTHEREFR, which has been designed to be applied it to the output of a morphological analyser and the tagset can be switched easily without changing the input text. ? second \[\] third 70 M M* M+CG M*+CG Figure 1-Initial ambiguity3. We have used the second and third levels tagsets for the experiments and a small corpus -28,300 wordsdivided in a training corpus of 27,000 words and a text of 1,300 words for testing. ? second \[\] third M M* M+CG M*+CG
###Response:","There are two kinds of methods for morphological disambiguation: onone hand, statistical methods need little effort and obtain very good results #OTHEREFR; #REFR, at least when applied to English, but when we try to apply them to Basque we encounter additional problems; on the other hand, some rule-based systems #OTHEREFR are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages.","We also incorporated a hidden Markov model-based part-of-speech tagger #REFR, proven to provide high accuracy and efficient performance in similar applications."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe three PCFG-based models for Chinese sentence realisation from Lexical- Functional Grammar (LFG) f-structures. Both the lexicalised model and the history-based model improve on the accuracy of a simple wide-coverage PCFG model by adding lexical and contextual information to weaken inappropriate independence assumptions implicit in the PCFG models. In addition, we provide techniques for lexical smoothing and rule smoothing to increase the generation coverage. Trained on 15,663 automatically LFG fstructure annotated sentences of the Penn Chinese treebank and tested on 500 sentences randomly selected from the treebank test set, the lexicalised model achieves a BLEU score of 0.7265 at 100% coverage, while the historybased model achieves a BLEU score of 0.7245 also at 100% coverage.
###Abstract of the cited paper: We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \[5,9,10,15,17\] ""standard"" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \[9\]. The major technical innovation is tire use of a ""ma~ximum-entropy-inspired"" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.
###Context text: The basic generation model presented in #OTHEREFR used simple probabilistic context-free grammars. However, the independence assumptions implicit in PCFG models may not be appropriate to best capture natural language phenomena.[Citation]In this section we study the effect of such methods in LFG-based generation for Chinese.
###Response:","Methodologies such as lexicalisation #OTHEREFR; #REFR and tree transformations #OTHEREFR, weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs.","In an effort to mitigate these limitations, we enhanced the PCFG models as informed by the findings of #REFR, which demonstrated notable parsing precision with a maximum-entropy-inspired model."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we show to what degree the countability of English nouns is predictable from their semantics. We found that at 78% of nouns? countability could be predicted using an ontology of 2,710 nodes. We also show how this predictability can be used to aid non-native speakers to determine the countability of English nouns when building a bilingual machine translation lexicon.
###Abstract of the cited paper: This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English.noun phrases. Incorporating this method into the machine translation system ALTJ/E, helped tO raise the percentage ofnoun phrases generated with correct use of articles and number from 65% to 73%.
###Context text: Words that undergo conversion between different values of form can be linked with lexical rules, such as the grinding rule that links a countable animal with its uncountable interpretation as meat. These are not, however directly linked to a full ontology. Therefore there is no direct connection between being an animal and being countable.[Citation]Nouns which rarely undergo conversion are marked as either fully countable, uncountable or plural only. Nouns that are non-specified are marked as either strongly countable (for count nouns that can be converted to mass, such as cake) or weakly countable (for mass nouns that are readily convertible to count, such as beer). Conversion is triggered by surrounding context.
###Response:","#REFR suggested a division of countability into five major types, based on Allan #OTHEREFR?s noun countability preferences (NCPs).","In line with our findings, previous research has also used linguistic and semantic knowledge to predict the countability of English noun phrases in machine translation systems [#REFR, #OTHEREFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a method for the computation of prefix probabilities for synchronous contextfree grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.
###Abstract of the cited paper: This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation. These models can be viewed as pairs of probabilistic contextfree grammars working in a ?synchronous? way. Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature.
###Context text: [Citation]Let N and ? be sets of nonterminal and terminal symbols, respectively. In what follows we need to represent bijections between the occurrences of nonterminals in two strings over N ??. This is realized by annotating nonterminals with indices from an infinite set.
###Response:",In this section we introduce basic definitions related to synchronous context-free grammars and their probabilistic extension; our notation follows #REFR.,"Drawing on investigations into the computational problems of probabilistic translation models, these models work synchronously and are viewed as pairs of probabilistic context-free grammars [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a new approach to summary evaluation which combines two novel aspects, namely (a) content comparison between gold standard summary and system summary via factoids, a pseudo-semantic representation based on atomic information units which can be robustly marked in text, and (b) use of a gold standard consensus summary, in our case based on 50 individual summaries of one text. Even though future work on more than one source text is imperative, our experiments indicate that (1) ranking with regard to a single gold standard summary is insufficient as rankings based on any two randomly chosen summaries are very dissimilar (correlations average ? = 0.20), (2) a stable consensus summary can only be expected if a larger number of summaries are collected (in the range of at least 30-40 summaries), and (3) similarity measurement using unigrams shows a similarly low ranking correlation when compared with factoid-based ranking.
###Abstract of the cited paper: This paper describes a system for generating text abstracts which relies on a general, purely statistical principle, i.e., on the notion of ""relevance"", as it is defined in terms of the combination of tf*idf weights of words in a sentence. The system generates abstracts from newspaper articles by selecting the ""most relevant"" sentences and combining them in text order. Since neither domain knowledge nor text-sort-specific heuristics are involved, this system provides maximal generality and flexibility. Also, it is fast and can be efficiently ilnplemented for both on-line and off-line purposes. An experiment shows that recall and precision for the extracted sentences (taking the sentences extracted by human subjects as a baseline) is within the same range as recall/precision when the human subjects are coinpared amongst each other: this means in fact that tile performance of the system is indistinguishable from the performance of a human abstractor. Finally, the system yields significantly better results than a default ""lead"" algorithm does which chooses just some initial sentences from the text.
###Context text: The second aspect we examine is the similarity measure to be used for gold standard comparison.[Citation]The rationale for using information overlap judgement as the main evaluation metric for DUC is the wish to measure the meaning of sentences rather than use surfacebased similarity such as co-selection (which does not even take identical information expressed in different sentences into account) and string-based measures. In the DUC competitions, assessors judge the informational overlap between ?model units? ( elementary discourse units (EDUs), i.e. clause-like units, taken from the gold standard summary) and ?peer units? (sentences taken from the participating summaries) on the basis of the question: ?How much of the information in a model unit is contained in a peer unit: all of it, most, some, any, or none.. This overlap judgement is done for each system-produced summary, and weighted recall measures report how much gold standard information is present in the summaries.
###Response:","In principle, the comparison can be done via coselection of extracted sentences #OTHEREFR; #REFR, by string-based surface measures #OTHEREFR.","Building upon the work by #REFR, which establishes the efficiency of a statistical-based system in generating abstracts, we propose a more advanced comparison measure using factoids for summary evaluation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Modern Chinese Mandarin has gone through near a hundred years, it is very important to store its representative sample in digital form permanently. In this paper, we propose a Chinese Mandarin Digital Multi-modal Corpus (CMDMC), which is a digital speech museum with diachronic, opened, crossmedia and sharable features. It has over 3460 hours video and audio files with metadata tagging. The materials, which were generated by the authoritative speakers (e.g. announcers at TV or radio station) with normality, are required samples if we can get them. Based on this resource, we also intend to analyze the syntactic correlations of prosodic phrase in broadcasting news speech, and compare the phonetic and prosodic features in movie dialogues among several same-name movies in different historical eras.
###Abstract of the cited paper: It is natural to expect phrase structure to be important in predicting prosodic phrasing. Yet there appears to be a concensus that syntactic phrases do not correspond well to prosodic phrasing, and independent s ructures have been proposed to account for prosody. I propose that the problem with phrase structure lies with the particular measures of boundary strength applied to syntactic structures, and with the fact that phrase structure is viewed as an immediate constituency tree exclusively. I propose viewing phrase structure as a composite of immediate constituency and dependency relations, and present an alternative measure of boundary strength. I show that boundary strength according to this measure corresponds much more closely to empirical prosodic (and psycholinguistic) boundary strength than does syntactic boundary strength according to a standard measure.
###Context text: In English, there is a strong correlation between prosodic phrase boundaries and syntactic phrase boundaries #OTHEREFR. That is to say, prosodic phrase boundaries can play an important role in understanding utterance as punctuation marks do in written language.[Citation]In Greek, some study indicated that prosodic phrasing has a 95% identification rate, and a major effect on final tonal boundaries #OTHEREFR. In Chinese, some researchers also proposed a statistical model to predict prosodic words from lexical words. In their model, both length of the word and the tagging from POS are two essential features to predict prosodic words, and the results showed approximately 90% of prediction for prosodic words (Chen at el.
###Response:","An investigation propose that boundary strength according to the measure, which the boundary strength is applied to syntactic structures and the phrase structure is viewed as an immediate constituency tree exclusively, corresponds much more closely to empirical prosodic boundary strength than does syntactic boundary strength according to a standard measure #REFR.","This is consistent with the findings of #REFR, who argued that phrasing structure predictions significantly improve when we examine phrase structure from both immediate constituency and dependency perspectives."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a method to identify zero pronouns within a ~\]apansse entence and their antecedent equivalents within the corresponding English sentence from aligned sentence pairs. The method focuses on the characteristics of Japanese and English, in two languages from cHfBerent f~rngles and in which distribution of zero pronouns i very d.uTerent. In this method, the Japanese sentence and English translation within the Japanese and English aligned sentence pairs are analyzed. Then, the pairs of Japanese word/phrase and their English equivalent word/phrase are identified from each aligned sentence pair. Next, zero pronouns within a Japanese sentence are identified by using the syntactic and semantic structure of the Japanese sentence and their antecedents within the English sentence are identified by using the characteristics of anaphoric and deictic expressions in English. This method was implemented using the Japanese-to-English machine translation system, ALT-3/E for the analysis of Japanese sentences and Brill is tagger for the analysis of the English sentences. According to my evaluation, for 554 zero p~onouns in a sentence set for the evaluation of 3apanese-to-Engllsh machine translation systems, 91.5% of the pairs of zero pronouns in the Japanese sentences and their antecedents in the English translations were automatically identified correctly. 1 !n/ ; roduct ion
###Abstract of the cited paper: Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries.
###Context text: This is particularly so with a bilingual corpus of Japanese and Engl/sh whose language farn~lles are so different and in which the distribution of zero pronouns is also very different. This combination is more useful than the bilingual corpora of similar languages. The technique for acquiring various kinds of rules such as translation rules, grammar rules, dictionary entries and so on from bilingual corpora needs to include several kinds of sub-techniques; identification of aligned sentence pairs which consist of pairs of one language sentence and translation equivalents of the sentence (sentence alignment); identification of equivalent words/phrases pairs from aligned sentence pairs (word alignment); and extraction of rules such as translation rules, grammar rules, dictionary entries and so on from identified aligned sentence pairs and equivalent word/phrase pairs.[Citation]From the point of view of the extraction of resolution rules of zero pronouns, a technique to identify zero pronouns in a sentence in one language and their antecedents in a translation from aligned sentence pairs is needed. But there is currently no method to identify zero pronouns and their antecedents automatically within bilingual corpora.
###Response:",Several methods have been proposed with regard to aligning sentences #OTHEREFR; #REFR.,"The K-vec strategy, which begins by estimating the lexicon, as discussed in the work of #REFR, informs our efforts in addressing this deficiency."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The resolution of lexical ambiguity is important for most natural language processing tasks, and a range of computational techniques have been proposed for its solution. None of these has yet proven effective on a large scale. In this paper, we describe a method for lexical disambiguation of text using the definitions in a machine-readable dictionm~j together with the technique of simulated annealing. The method operates on complete sentences and attempts to select the optimal combinations of word senses for all the words in the sentence simultaneously. The words in the sentences may be any of the 28,000 headwords in Longman is Dictionary of Contemporary English (LDOCE) and are disambiguated relative to the senses given in LDOCE. Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand-coding of lexical entries, or hand-tagging of text. L Introduction The problem of word-sense disambiguation is central to text processing. Recently, promising computational methods have been suggested \[Lesk, 1986; McDonald * Present address: Mathematics DepaJtment. University of Texas at El Paso, El Paso, Tx et al, 1990; Veronis and Ide, 1990; Wilks et al., 1990; Zernik and ,lacobs, 1990; Guthrie et al, 1991; Hearst, 1991\] which attempt o use the local context of the word to be disambiguated together with information about each of its word senses to solve this problem. Lesk \[1986\] described a technique which measured the amount of overlap between a dictionary sense definition and the definitions of the words in the local context of the word to be disambiguated. He illustrated his method by successfully disambiguating the word ""cone"" in the phrases ""pine cone"" and ""ice cream cone"". Later researchers have extended this basic idea in various ways. Wilks et al, \[1990\] identified neighborhoods of the 2,187 control vocabulary words in Longman is Dictionary of Contemporaay English (LDOCE) \[Procter, 1978\] based on the co-occurrence of words in LDOCE dictionary definitions. These neighborhoods were then used to expand the word sense definitions of the word to be disambiguated, and the overlap between the expanded definitions and the local context was used to select the correct sense of a word. A similar method reported by Guthrie et al, \[1991\] who defined subject specific neighborhoods of words, using the subject area markings in the machine readable version of LDOCE. Hearst \[1991\] suggests using syntactic information and partof-speech tagging to aid in the disambiguatton. She gathers co-occurrence information Aclxs DE COLING-92. NANTES, 23-28 AOt\]T 1992 3 5 9 PROC, OV COLING-92, NAN'rES, AUG. 23-28, 1992 from manually senseutagged text. Zemik and Jacobs \[1990\] also derive their neighborhoods from a training text which has I~en sense-tagged by hand. Their method incorporates other clues as to the sense of the word in question found in the morphology or by first tagging the text as to part of speech. Although each of these techniques look somewhat promising for disambiguation, the techniques have only been applied to several words, and the results have been based on experiments which repeatedly disambiguate a single word (or in \[Zernik and Jacobs. 1990\], one of three words) in a large number of sentences. In the cases where a success rate for the technique is reported, the results vary from 35% to 80%, depending on whether the correct sense is desired, or some coarser grained distraction is considered acceptable. Since only one sense is computed at a time, the question arises as to whether and how to incolporate the fact that a sense has been chosen for a word when attempting to disambiguate the next. Should this first choice be changed in light of how other word senses are selected. Although these problems were pointed out in Lesk is original paper, they have not yet been addressed. A method of word sense disambiguation which is designed to operate on a large scale and simultaneously for several words was suggested by Veronis and Ide \[1990\]. The basis of this method is the construction of large neural networks which have words and word senses (chosen from the machine readable version of the Collins Dictionary) as nodes. Links are established from a word to each of its senses, and from each sense to every word in its definition. Inhibiting links are constructed between senses of the same word. In order to disambiguate a sentence, the words in the sentence are activated in the network, and this activation is allowed to spread with feedback. This cycle is repeated a presetected number of times, e.g.,
###Abstract of the cited paper: In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use of these networks for word sense disambiguation. Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models. The automatic construction of VLNNs enables real-size xperiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements. 1. \]Introduction Automated language understanding requires the determination f the concept which a given use of a word represents, a process referred to as word sense disambiguation (WSD). WSD is typically effected in natural llanguage processing systems by utilizing semantic teature lists for each word in the system is lexicon, together with restriction mechanisms such as case role selection. However, it is often impractical to manually encode such information, especially for generalized text where the variety and meaning of words is potentially unrestricted. Furthermore, restriction mechanisms usually operate within a single sentence~ and thus the broader context cannot assist in the disambiguation process. in this paper, we describe a means tor automatically building Very Large Neural Networks (VLNNs) from definition texts in machine-readable dictionaries, and denmnstrate he use of these networks for WSD. Our method brings together two earlier, independent approaches to WSD: the use of machine-readable dictionaries and spreading and activation models. The automatic onstruction of VLNNs enables real-size experiments with neural networks, which in turn The authors would like to acknowledge the contributions of St~phanc tlari6 and Gavin Huntlcy to the work presented in this paper. provides insight into their behavior and design and can lead to possible improvements.
###Context text: In those cases where the configuration selected is not the correct disambiguation of the sentence, the correct disambiguation ever had a lower value of E than the configuration selected. Experiments in which we varied the beginning temperature and the rate of cooling did not change tile configuration ultimately selected and seemed to show that those parameters are not very delicate. Direct comparisons of these success rates with those of other methods is difficult.[Citation]None of the other methods was used to disambiguate very ambiguous word in a sentence. They were applied to one, or at most a few, highly ambiguous words. It appears that in some cases the fact that our success rates include not only highly ambiguous words, but some words w~th only a few senses is offset by the fact that other researchers have used a broader definition of word sense.
###Response:","#REFR\] propose a large scale method, but results are reported for only one sentence, and no success rate is given.","Our approach aligns with the method described by Veronis and Ide [#REFR], incorporating large neural networks and using machine-readable dictionaries for automatic word sense disambiguation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.
###Abstract of the cited paper: We study correlation of rankings of text summarization systems using evaluation methods with and without human models. We apply our comparison framework to various well-established contentbased evaluation measures in text summarization such as coverage, Responsiveness, Pyramids and ROUGE studying their associations in various text summarization tasks including generic and focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions.
###Context text: In this paper, we use guided summarization data as an opportunity to reopen the investigation into the effect of domain, because multiple document clusters from the same domain are available. Summarization evaluation is typically done by comparing system output to human-written model summaries, and are validated by their correlation with user responsiveness judgments. The comparison can be done at the word level, as in ROUGE #OTHEREFR.[Citation]Several studies complement this paper by examining the best possible extractive system using current evaluation measures, such as ROUGE #OTHEREFR. They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. Genest et al #OTHEREFR ask humans to create extractive summaries, and find that they score in between current automatic systems and human-written abstracts on responsiveness, linguistic quality, and Pyramid score.
###Response:","There are also automatic measures which do not require model summaries, but compare against the source text instead #OTHEREFR; #REFR.","In their evaluation of text summarization systems, the authors applied diverse measures including coverage, responsiveness, and ROUGE, using the FRESA framework to assess correlation between rankings #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: French- English and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.
###Abstract of the cited paper: Previous attempts at identifying translational equivalents in comparable corpora have dealt with very large ?general language? corpora and words. We address this task in a specialized domain, medicine, starting from smaller non-parallel, comparable corpora and an initial bilingual medical lexicon. We compare the distributional contexts of source and target words, testing several weighting factors and similarity measures. On a test set of frequently occurring words, for the best combination (the Jaccard similarity measure with or without tf:idf weighting), the correct translation is ranked first for 20% of our test words, and is found in the top 10 candidates for 50% of them. An additional reverse-translation filtering step improves the precision of the top candidate translation up to 74%, with a 33% recall.
###Context text: Most of the improvements of the standard approach are based on the observation that the more representative the context vectors of a candidate word are, the better the bilingual lexicon extraction is.[Citation]The ambiguities that appear in the seed bilingual dictionary were taken into account more recently. #OTHEREFR modify the standard approach by weighting the different translations according to their frequency in the target corpus. In #OTHEREFR, we proposed a method that adds a word sense disambiguation process relying on semantic similarity measurement from WordNet to the standard approach. Given a context vector in the source language, the most probable translation of polysemous words is identified and used for building the corresponding vector in the target language.
###Response:","At first, additional linguistic resources, such as specialized dictionaries #REFR or transliterated words #OTHEREFR, were combined with the seed dictionary to translate context vectors.","The success of our approach aligns with research which also conducted the task in a specialized domain, achieving notable precision in translation outcomes #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Previous results have shown disappointing performance when porting a parser trained on one domain to another domain where only a small amount of data is available. We propose the use of data-defined kernels as a way to exploit statistics from a source domain while still specializing a parser to a target domain. A probabilistic model trained on the source domain (and possibly also the target domain) is used to define a kernel, which is then used in a large margin classifier trained only on the target domain. With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone.
###Abstract of the cited paper: We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.
###Context text: For a PCFG, each such feature is a function of one rule?s counts, where the counts from different candidates are weighted using the probability estimates from the model. With latent variables, the meaning of the variable (not just its value) is learned from the data, and the associated features of the data-defined kernel capture this induced meaning. There has been much recent work on latent variable models #OTHEREFR).[Citation]This parsing model is also a good candidate for our experiments because it achieves state-of-the-art results on the standard Wall Street Journal #OTHEREFR.
###Response:","We choose to use an earlier neural network based probabilistic model of parsing #REFR, whose hidden units can be viewed as approximations to latent variables.",The parsing model used in our approach draws significantly from the neural network methods for inducing representations of parse histories as presented by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A serious bottleneck of comparative parser evaluation is the fact that different parsers subscribe to different formal frameworks and theoretical assumptions. Converting outputs from one framework to another is less than optimal as it easily introduces noise into the process. Here we present a principled protocol for evaluating parsing results across frameworks based on function trees, tree generalization and edit distance metrics. This extends a previously proposed framework for cross-theory evaluation and allows us to compare a wider class of parsers. We demonstrate the usefulness and language independence of our procedure by evaluating constituency and dependency parsers on English and Swedish.
###Abstract of the cited paper: Methods for evaluating dependency parsing using attachment scores are highly sensitive to representational variation between dependency treebanks, making cross-experimental evaluation opaque. This paper develops a robust procedure for cross-experimental evaluation, based on deterministic unificationbased operations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground.
###Context text: In the constituency-to-dependency case, some constituency-based structures (e.g., coordination and ellipsis) do not comply with the single head assumption of dependency treebanks. Secondly, these scripts may be labor intensive to create, and are available mostly for English. So the evaluation protocol becomes language-dependent.[Citation]The protocol consists of three phases: converting all structures into function trees, for each sentence, generalizing the different gold standard function trees to get their common denominator, and employing an evaluation measure based on tree edit distance (TED) which discards edit operations that recover theory-specific structures. Although the protocol is potentially applicable to a wide class of syntactic representation types, formal restrictions in the procedures effectively limit its applicability only to representations that are isomorphic to dependency trees. The present paper breaks new ground in the ability to soundly compare the accuracy of different parsers relative to one another given that they employ different formal representation types and obey different theoretical assumptions.
###Response:",In #REFR we proposed a general protocol for handling annotation discrepancies when comparing parses across different dependency theories.,"Expanding upon previous work for cross-experimental evaluation of parsing results #REFR, the protocol neutralizes theory-specific structures in preference of a common standard."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a novel unsupervised approach for distinguishing literal and non-literal use of idiomatic expressions. Our model combines an unsupervised and a supervised classifier. The former bases its decision on the cohesive structure of the context and labels training data for the latter, which can then take a larger feature space into account. We show that a combination of both classifiers leads to significant improvements over using the unsupervised classifier alone.
###Abstract of the cited paper: This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions (MWEs) for robust grammar engineering. First we investigate the hypothesis that MWEs can be detected by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: mutual information (MI), ?2 and permutation entropy (PE). Our overall conclusion is that at least two measures, MI and PE, seem to differentiate MWEs from non-MWEs. We then investigate the influence of the size and quality of different corpora, using the BNC and the Web search engines Google and Yahoo. We conclude that, in terms of language usage, web generated corpora are fairly similar to more carefully built corpora, like the BNC, indicating that the lack of control and balance of these corpora are probably compensated by their size. Finally, we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources. We argue that such a process improves qualitatively, if a more compositional approach to grammar/lexicon automated extension is adopted.
###Context text: Some idioms, for instance, do not allow internal modifiers (*shoot the long breeze) or passivisation (*the bucket was kicked). They also typically only allow very limited lexical variation (*kick the vessel, *strike the bucket). Many approaches for identifying idioms focus on one of these two aspects.[Citation]Other approaches use Latent Semantic Analysis #OTHEREFR. Low similarity is supposed to indicate low compositionality. Bannard #OTHEREFR looks at the syntactic fixedness of idiomatic expressions, i.e., how likely they are to take modifiers or be passivised, and compares this to what would be expected based on the observed behaviour of the component words.
###Response:","For instance, measures that compute the association strength between the elements of an expression have been employed to determine its degree of compositionality #OTHEREFR (see also #REFR for an overview and a comparison of different measures).","The distinguishing statistical properties of idioms' component words assist in detecting them, as has been highlighted in a comprehensive evaluation of automatic acquisition methods for Multiword Expressions #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that exploits from characterto sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: the Stanford Sentiment Treebank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7% accuracy, and fine-grained classification, with 48.3% accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4%.
###Abstract of the cited paper: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.
###Context text: There are a few works on neural network architectures for sentiment analysis. In #OTHEREFR, the authors proposed a semi-supervised approach based on recursive autoencoders for predicting sentiment distributions. The method learns vector space representation for multi-word phrases and exploits the recursive nature of sentences.[Citation]The vector captures the inherent meaning of the constituent, while the matrix captures how the meaning of neighboring words and phrases are changed. In #OTHEREFRb) the authors propose the Recursive Neural Tensor Network (RNTN) architecture, which represents a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor-based composition function. Our approach differ from these previous works because it uses a feed-forward neural network instead of a recursive one.
###Response:","In #REFR, it is proposed a matrix-vector recursive neural network model for semantic compositionality, which has the ability to learn compositional vector representations for phrases and sentences of arbitrary length.","Drawing from #REFR, a Recursive Neural Network was introduced to grasp the compositional meaning of longer phrases, advancing language comprehension."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We address the grounding of natural language to concrete spatial constraints, and inference of implicit pragmatics in 3D environments. We apply our approach to the task of text-to-3D scene generation. We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data. In text-to- 3D scene generation, a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene. The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene. We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes.
###Abstract of the cited paper: Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.
###Context text: However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spatial knowledge representations from data, and for connecting them to language. Representing unstated facts is a challenging problem unaddressed by prior work and the focus of our contribution.[Citation]We present a representation for this form of spatial knowledge that we learn from 3D scene data and connect to natural language. We will show how this representation is useful for grounding language and for inferring unstated facts, i.e., the pragmatics of language describing physical environments. We demonstrate the use of this representation in the task of text-to-3D scene genera- Room
###Response:","This problem is a counterpart to the image description problem #OTHEREFR; #REFR, which has so far remained largely unexplored by the community.","Our work extends upon the concepts of the main event representation from #REFR, introducing a methodology for capturing spatial relationships from 3D data."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.
###Abstract of the cited paper: We use web-scale N-grams in a base NP parser that correctly analyzes 95.4% of the base NPs in natural text. Web-scale data improves performance. That is, there is no data like more data. Performance scales log-linearly with the number of parameters in the model (the number of unique N-grams). The web-scale N-grams are particularly helpful in harder cases, such as NPs that contain conjunctions.
###Context text: However, lexical statistics based on the training set only are typically sparse and have only a small effect on overall parsing performance #OTHEREFR. Unlabeled data can help ameliorate this sparsity problem. Backing off to cluster membership features #OTHEREFR, have both improved parsing.[Citation]However, it has so far been less effective within full parsing ? while first-order web-scale counts noticeably improved overall parsing in Bansal and Klein #OTHEREFR, the accuracy on conjunctions actually decreased when the web-scale features were added (Table 4 in that paper). In this paper we show that unlabeled data can help prepositions and conjunctions, provided that the dependency representation is compatible with how the parsing problem is decomposed for learning and inference. By incorporating unlabeled data into factorizations which capture the relevant dependencies for prepositions and conjunctions, we produce a parser for English which has an unlabeled attachment accuracy of 93.5%, over an 18% reduction in error over the best previously published parser #OTHEREFR on the current standard for dependency parsing.
###Response:",Unlabeled data has been shown to improve the accuracy of conjunctions within complex noun phrases #REFR.,"Taking inspiration from the prevalence of N-grams in web-scale data, which significantly enhances base NP parsing accuracy #REFR, we incorporated unlabeled data into our parser."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature. Also Natural Language Processing (NLP) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification. We introduce a new clustering method called Hierarchical Graph Factorization Clustering (HGFC) and extend it so that it is optimal for the task. Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set. We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.
###Abstract of the cited paper: This plq)er (lescril)es a (hit i~-(triven nlet, hod for hiera, rchicM chlstering of words ill whicii a, la, rge vo(:aJ)ul~ry of I,;ii. glis\]'l words is (:histered botl;oln--uf) >with resl)e(:t 1,o (:orpor;~ ranghig in size fi'otn 5 to 50 nlillion wor(ts, using a greedy al gorithm that I;ries I,o nliniluize i~veri~ge lOS8 Of liCllltllal iriforuu:l,l, ion of a, djax:ent classes. The resulting hierar('.hi('al (:illS- tiers of woMs are then tumirMly 1,ransrorlned to a bit-string representld, ion of (i.e. word bits for) all the words ill the vocabulary, Introducing wor(l bits hito i.he ATI{ I)ecision-Tree DOS Tagger is shown to signific~mt,ly reduce l, he ti~gging error rld;e. PortM)ility of word t)il.s h:om Olle (tonlMn to i~Hotilel: iS ~tlSO diss(:ussed.
###Context text: Levin?s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification. In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method ? agglomerative clustering #OTHEREFR.[Citation]We introduce then a new method called Hierarchical Graph Factorization Clustering #OTHEREFR. This graph-based, probabilistic clustering algorithm has some clear advantages over AGG #OTHEREFR. The method has been applied to the identification of social network communities #OTHEREFR, but has not been used (to the best of our knowledge) in NLP before.
###Response:",The method has also been popular in the related task of noun clustering #REFR.,"Drawing inspiration from the bit-driven method for hierarchical clustering described in #REFR, we reinforce our Hierarchical Graph Factorization Clustering algorithm."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning. These methods together give an order of magnitude increase in speed, and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work. Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes ignificant performance degradation i the context of a real application.
###Abstract of the cited paper: A simple and general method is described that can combine different knowledge sources to reorder N-best lists of hypotheses produced by a speech recognizer. The method is automatically trainable, acquiring information from both positive and negative xamples. In experiments, the method was tested on a 1000-utterance sample of unseen ATIS data.
###Context text: This section describes a number of experiments carried out to test the utility of the theoretical ideas presented above. The basic corpus used was a set of 16,000 utterances from the Air Travel Planning #OTHEREFR) domain. All of these utterances were available in text form; 15,000 of them were used for training, with 1,000 held out for test purposes.[Citation]The 16,000 sentence corpus was analysed by the SRI Core Language Engine #OTHEREFR. All possible grammatical nalyses of each utterance were recorded, and an interactive tool was used to allow a human judge to identify the correct and incorrect readings of each utterance. The judge was a first-year undergraduate student with a good knowledge of linguistics but no prior experience with the system; the process of judging the corpus took about wo and a half person-months.
###Response:","Care was taken to ensure not just that the utterances themselves, but also the speakers of the utterances were disjoint between test and training data; as pointed out in #REFRa), failure to observe these precautions can result in substantial spurious improvements in test data results.","The remaining 1,000 utterances were employed for testing the method described in #REFR, which uses both positive and negative examples for automatic training."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes our ongoing research project on text simplification for congenitally deaf people. Text simplification we are aiming at is the task of offering a deaf reader a syntactic and lexical paraphrase of a given text for assisting her/him to understand what it means. In this paper, we discuss the issues we should address to realize text simplification and report on the present results in three different aspects of this task: readability assessment, paraphrase representation and post-transfer error detection.
###Abstract of the cited paper: In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications. The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels (""uniform lexico-structural processing""). We describe how this framework has been used in practical NLG and MT applications, and report he lessons learned.
###Context text: [Citation]Adopting such a tree-to-tree style of representation, however, one has to address the issue of the trade-off between expressibility and comprehensibility. One may want a formalism of structural rule editing translation compilation simplified MDS transfer rule N shika V- nai -> V no wa N dake da. (someone does not V to nothing but N) (it is only to N that someone does V) MDS transfer rule sp_rule(108, negation, RefNode) :- match(RefNode, X4=[pos:postp,lex: shika]), depend(X3=[pos:verb], empty, X4), depend(X1=[pos:aux_verb,lex: nai], X2=[pos:aux_verb*], X3), depend(X4, empty, X5=[pos:noun]), replace(X1, X6=[pos:aux_verb,lex: da]), substitute(X5, X12=[pos:noun]), move_dtrs(X5, X12), substitute(X3, X10=[pos:verb]), : pos: postp lex: shika (except) pos: aux_verb lex: da (copula) pos: postp lex: wa (TOP) X6 X11 X12pos: nounlex: no (thing) pos: postp lex: dake (only) pos: noun pos: noun aux_verb* pos: aux_verb lex: nai (not) pos: verbX3 X4 X1 X5 X2 X7 X8 X10 pos: verb X9 vws MDS processing operators (=X5) (=X2) (=X3) Figure 2: Three-layered rule representation transformation patterns that is powerful enough to represent a sufficiently broad range of paraphrase patterns.
###Response:",Previous work on transfer-based MT systems #REFR and alignment-based transfer knowledge acquisition #OTHEREFR have proven that transfer knowledge can be best represented by declarative structure mapping (transforming) rules each of which typically consists of a pair of source and target partial structures as in the middle of Figure 2.,"The ""uniform lexico-structural processing"" method described in #REFR is crucial in creating a balance between expressibility and comprehensibility in text simplification."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.
###Abstract of the cited paper: We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.
###Context text: Large context-free grammars extracted from treebanks achieve high coverage and accuracy, but they are difficult to parse with because of their massive ambiguity. The application of standard chart-parsing techniques often fails due to excessive memory and runtime requirements. Treebank grammars are mostly used as probabilistic grammars and users are usually only interested in the best analysis, the Viterbi parse.[Citation]These methods reduce the number of generated edges, but increase the amount of time needed for each edge. The parser described in this paper follows a contrary approach: instead of reducing the number of edges, it minimises the costs of building edges in terms of memory and runtime. The new parser, called BitPar, is based on a bitvector implementation #OTHEREFR.
###Response:","To speed up Viterbi parsing, sophisticated search strategies have been developed which find the most probable analysis without examining the whole set of possible analyses #OTHEREFR; #REFRa).","Drawing inspiration from an extended A* search procedure tailored to tabular PCFG parsing, which aims for optimal parsing efficiency #REFR, BitPar efficiently computes complete parse forest representations."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a study of aspects of discourse structure ? specifically discourse devices used to organize information in a sentence ? that significantly impact the quality of machine translation. Our analysis is based on manual evaluations of translations of news from Chinese and Arabic to English. We find that there is a particularly strong mismatch in the notion of what constitutes a sentence in Chinese and English, which occurs often and is associated with significant degradation in translation quality. Also related to lower translation quality is the need to employ multiple explicit discourse connectives (because, but, etc.), as well as the presence of ambiguous discourse connectives in the English translation. Furthermore, the mismatches between discourse expressions across languages significantly impact translation quality.
###Abstract of the cited paper: We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese- English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task? and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. ?This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants
###Context text: In this study we examine how the use of discourse devices to organize information in a sentence ? and the mismatch in their usage across languages ? influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems.[Citation]Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to interpret multi-sentential text while statistical MT systems are trained to translate a single sentence in one language into a single sentence in another. However, discourse devices are at play in the organization of information into complex sentences. The mere definition of sentence may differ across languages.
###Response:",Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments #OTHEREFR and word polysemy #REFR.,Recent research has demonstrated the utility of word sense disambiguation systems in enhancing translation quality in multilingual settings [#REFR].
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The quality of annotated data is crucial for supervised learning. To eliminate errors in single annotated data, a second round of annotation is often used. However, is it absolutely necessary to double annotate every example. We show that it is possible to reduce the amount of the second round of annotation by more than half without sacrificing the performance.
###Abstract of the cited paper: We present a method for automatically detecting errors in a manually marked corpus using anomaly detection. Anomaly detection is a method for determining which elements of a large data set do not conform to the whole. This method fits a probability distribution over the data and applies a statistical test to detect anomalous elements. In the corpus error detection problem, anomalous elements are typically marking errors. We present he results of applying this method to the tagged portion of the Penn Treebank corpus.
###Context text: Our algorithms select the data for repeated labeling in a single batch, which means the selection can be done off-line. This should greatly simplify the application of our approach in a real life annotation project. Our work also borrows from the error detection literature.[Citation]The approaches to error detection include anomaly detection #OTHEREFR by exploiting the fact that errors tend to concentrate among the examples with large weights. Some of these works eliminate the errors #OTHEREFR. Others correct them automatically #OTHEREFR.
###Response:","Researchers have explored error detection for manually tagged corpora in the context of pos-tagging #REFR, dependency parsing #OTHEREFR.","Utilizing a methodology similar to the one presented by #REFR, errors in the annotated dataset were detected through anomaly detection techniques."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We have designed and implemented a text processing system that can extract important information from hundreds of paragraphs per hour and can be transported within weeks to a new domain. The system performs efficiently because it determines the level of processing required to understand a text. This ""skimming"" method identifies urface relations in the input text that are likely to contribute to its interpretation in a domain. This approach differs from previous kimming techniques in that it uses conceptual information as part of bottom-up linguistic processing, thus using linguistic knowledge more fully while limiting grammatical complexity.
###Abstract of the cited paper: The SCISOR. system is a computer program designed to scan naturally occurring texts in constrained omains, extract information, and answer questions about that information. The system currently reads newspapers stories in the domain of corporate mergers and acquisitions. The language analysis trategy used by SCISOR combines full syntactic (bottomup) parsing and conceptual expectation-driven (topdown) parsing. Four knowledge sources, including syntactic and semantic information and domain knowledge, interact in a flexible manner. This integration produces a more robust semantic analyzer designed to deal gracefully with gaps in \]exical and syntactic knowledge, transports easily to new domains, and facilitates the extraction of information from texts.
###Context text: This approach seems to work in highly constrained texts where conceptual knowledge is sufficient tbr determining role relationships. In the domains that we have tested, the pure template-based approach fails because some role relationships are determined almost entirely from linguistic intbrmat.ion such as complernent structure or agreement, l'br example, the target and suitor of corporate mergers are both companies; thus there is little conceptual information (other than the size of the companies) that helps to determine role-filling. In many classes of tactical operations reports, the agent and object are both military forces, thus correct linguistic attachmen~ is essential in this domain as well.[Citation]Like some of the other major text processing systems such as PI{OTEUS, PUNDIT, and qACH'US \[tIobbs, 1986; Grishman and Ilirschmau, 1986\], tile skimming program applies linguistic constraints and maps linguistic structures into conceptual roles. In these other systems, however, the bottom-up approach may cause the program to waste time on irrelevant sections of text. The difference is that these programs do not really use conceplual infof mation until after the parser has generated its candidate structures.
###Response:","Although the overall parsing style of our system integrates template-based and language-based strategies \[#REFR\], the skimming algorithm is actually more bottom-up or language-based.","Drawing on an approach similar to the SCISOR system, our method also combines bottom-up parsing and conceptual expectation-driven parsing but focuses more efficiently on relevant sections of the text [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations. The model is essentially an infinite HMM that infers the number of states from data. Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step. We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction.
###Abstract of the cited paper: Part-of-speech (POS) tag distributions are known to exhibit sparsity ? a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity. This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems.1
###Context text: Unsupervised morphology learning is another popular task that has been extensively studied by many authors. Here we are interested in learning concatenative morphology of words, meaning the substrings of the word corresponding to morphemes that, when concatenated, will give the lexical representation of the word type. For the rest of the paper we will refer to this task as (morphological) segmentation.[Citation]In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis. In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem. We will show that the unsupervised learning of both of these tasks in the same model will lead to better results than learning both tasks separately with the gold standard data of the other task provided.
###Response:",Several unsupervised POS induction systems make use of morphological features #OTHEREFR; #REFR and this approach has been empirically proved to be helpful #OTHEREFR.,The combined learning approach of POS induction and morphological segmentation draws on the efficient POS tagging system that models tag sparsity as highlighted by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Discontinuities occur especially frequently in languages with a relatively free word order, such as German. Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser. In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks. In both treebanks, discontinuities are annotated with crossing branches. Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems.
###Abstract of the cited paper: We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particu- Jar representational strata.
###Context text: [Citation]In a preprocessing step, following common practice, we attach all punctuation to nodes within the tree, since it is not included in the NeGra annotation. In a first pass, using heuristics, we attach all nodes to the in each case highest available phrasal node such that ideally, we do not introduce new crossing branches. In a second pass, parentheses and quotation marks are preferably attached to the same node.
###Response:",Our data sources are the German NeGra #REFR and TIGER #OTHEREFR treebanks.,"In line with the annotation scheme for non-configurational languages elaborated by #REFR, we employed a multi-step preprocessing approach for the NeGra annotation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.
###Abstract of the cited paper: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
###Context text: ring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages. This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with parallel data through which cross-lingual word-cluster constraints are enforced.[Citation]Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%. By incorporating cross-lingual cluster features in a linguistic transfer system, we are for the first time combining SSL and cross-lingual transfer.
###Response:","We show that by augmenting the delexicalized direct transfer system of #REFR with cross-lingual cluster features, we are able to reduce its error by up to 13% relative.","Our approach builds upon the method presented by #REFR, which effectively transferred delexicalized parsers between languages, thereby improving parser accuracy."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice.
###Abstract of the cited paper: We present hree systems for surface natural anguage generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce agrammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate ach word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain.
###Context text: The first widely known data-driven approach to surface realisation, or tactical generation, #OTHEREFR used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker.[Citation]Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions #OTHEREFR. The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing #OTHEREFR. The grammar generates candidate analyses for an underlying representation and the ranker?s task is to predict the contextually appropriate realisation.
###Response:","Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models #REFR; Marciniak and Strube, 2005; Belz, 2005, a.o.).","The surface realisation system NLG2 and NLG3, as discussed in #REFR, influence word choice and order within phrases, improving the contextually appropriate realisation prediction."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a compendium of recent and current projects that utilize crowdsourcing technologies for language studies, finding that the quality is comparable to controlled laboratory experiments, and in some cases superior. While crowdsourcing has primarily been used for annotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a ?correct? response for any given data point.
###Abstract of the cited paper: Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon?s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.
###Context text: Crowdsourcing?s greatest contribution to language studies might be the ability to generate new kinds of data, especially within experimental paradigms.[Citation]For psycholinguistic experiments in particular, we are not so much utilizing ?artificial artificial? intelligence as the plain intelligence and linguistic intuitions of each crowdsourced worker ? the ?voices in the crowd?, so to speak. In many experiments we are studying gradient phenomena where there are no right answers. Even when there is binary response we are often interested in the distribution of responses over many speakers rather than specific data points.
###Response:",The speed and cost benefits for annotation are certainly impressive #OTHEREFR; #REFR but we hope to show that some of the greatest gains are in the very nature of the phenomena that we can now study.,"Applying crowdsourcing technologies, as exemplified by Amazon's Mechanical Turk in a translation task evaluation by #REFR, has proven cost-effective and efficient."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We cast multi-sentence compression as a structured prediction problem. Related sentences are represented by a word graph so that summaries constitute paths in the graph (Filippova, 2010). We devise a parameterised shortest path algorithm that can be written as a generalised linear model in a joint space of word graphs and compressions. We use a large-margin approach to adapt parameterised edge weights to the data such that the shortest path is identical to the desired summary. Decoding during training is performed in polynomial time using loss augmented inference. Empirically, we compare our approach to the state-of-the-art in graph-based multi-sentence compression and observe significant improvements of about 7% in ROUGE F-measure and 8% in BLEU score, respectively.
###Abstract of the cited paper: We consider the task of summarizing a cluster of related sentences with a short sentence which we call multi-sentence compression and present a simple approach based on shortest paths in word graphs. The advantage and the novelty of the proposed method is that it is syntaxlean and requires little more than a tokenizer and a tagger. Despite its simplicity, it is capable of generating grammatical and informative summaries as our experiments with English and Spanish data demonstrate.
###Context text: We cast multi-sentence compression as a structured prediction problem.[Citation]We devise a parameterised shortest path algorithm that can be written as a generalised linear model in a joint space of word graphs and compressions. We use a large-margin approach to adapt parameterised edge weights to the data such that the shortest path is identical to the desired summary. Decoding during training is performed in polynomial time using loss augmented inference.
###Response:",Related sentences are represented by a word graph so that summaries constitute paths in the graph #REFR.,"Our approach draws on the proposed method of representing related sentences as a word graph for multi-sentence compression, as introduced by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we present a new, multilingual data-driven method for coreference resolution as implemented in the SWIZZLE system. The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts, outperformed coreference r solution in each of the individual anguages.
###Abstract of the cited paper: We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations. The tool makes use of part-of-speech tagging and wordalignment programs to extract candidate terms and their translations. Although the extraction programs are far from perfect, it is not too hard for the user to filter out the wheat from the chaff. The extraction algorithms emphasize completeness. Alternative proposals are likely to miss important but infrequent terms/translations. To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes. Termight is currently being used by the translators at ATT Business Translation Services (formerly AT&T Language Line Services).
###Context text: The recent availability of large bilingual corpora has spawned interest in several areas of multilingual text processing.[Citation]In addition, the Multilingual Entity Task (MET) from the TIP- STER program 1 (http://www-nlpir.nist.gov/relatedprojeets/tipster/met.htm) challenged the participants in the Message Understanding Conference (MUC) to extract named entities across everal foreign language corpora, such as Chinese, Japanese and Spanish. In this paper we present a new application of aligned multilinguai texts. Since coreference r solution is a pervasive discourse phenomenon causing performance impediments in current IE systems, we considered a corpus of aligned English and Romanian texts to identify coreferring expressions.
###Response:","Most of the research as focused on bilingual terminology identification, either as parallel multiwords forms #OTHEREFR), technical terminology (e.g. the Termight system #REFR or broad-coverage translation lexicons #OTHEREFR).","Expanding on the semi-automatic tool 'termight' proposed by #REFR, our method enhances the process of identifying coreferring expressions in bilingual corpus."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Building effective spoken dialogue systems (SDS) is currently a complex task requiring expert knowledge. Our tools give control of SDS application development to non-experts, who need only use a Graphical User Interface or GUI to develop state-of-the-art ?Information State Update? (ISU) dialogue systems. Behind the GUI is a set of Advanced Dialogue Tools (ADT) that generate complete SDS based on Business User Resources. These resources include a database and a Process Model that captures the structure of an application, for example, banking or restaurant information. Also generated are speech recognition Language Models and grammars for robust interpretation of spontaneous speech. We will demonstrate how our simple GUI allows developers to easily and quickly create and modify SDS without the need for expensive speech application service providers. This demonstration shows the interface, the ADT components, and discusses some of the research issues involved. We also show an example application built with the tools: a tourist information system running on an ultra-mobile PC.
###Abstract of the cited paper: We demonstrate a multimodal dialogue system using reinforcement learning for in-car scenarios, developed at Edinburgh University and Cambridge University for the TALK project1. This prototype is the first ?Information State Update? (ISU) dialogue system to exhibit reinforcement learning of dialogue strategies, and also has a fragmentary clarification feature. This paper describes the main components and functionality of the system, as well as the purposes and future use of the system, and surveys the research issues involved in its construction. Evaluation of this system (i.e. comparing the baseline system with handcoded vs. learnt dialogue policies) is ongoing, and the demonstration will show both.
###Context text: General aspects of dialogue, such as confirmation and clarification strategies, are handled by the domain-general DM. Values for constraints on transitions and branching in the BPM, for example ?present insurance option if the user is business-class?, are compiled into domain-specific parts of the DM?s update rules. XML format is used for BPMs, and they are compiled into finite state machines consulted by the spoken dialogue system through the BPM module.[Citation]
###Response:",The domaingeneral DM was mostly abstracted from the TALK system #REFR.,"Drawing from prior research, our approach to generating dialogue strategies mirrors the ISU dialogue system with reinforcement learning demonstrated by the TALK project1 (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative. Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning. Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection. We formulate the projection task as a generalization of the linear assignment problem. We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.
###Abstract of the cited paper: Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system.
###Context text: Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents #OTHEREFR.[Citation]Resources like FrameNet #OTHEREFR have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training data. Examples of the annotations provided in FrameNet are given in (1).
###Response:","This is partly due to its relevance for applications ranging from information extraction #OTHEREFR, and the modeling of textual entailment relations #REFR.",The efforts to enhance the performance of these systems through combining semantic information and extracting new knowledge from various resources are well-documented #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we present a novel extension of a forest-to-string machine translation system with a reordering model. We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest. Our approach naturally deals with the ambiguity present in the input parse forest, but, at the same time, takes into account only the parts of the input forest used by the current translation hypothesis. The method provides improvement from 0.6 up to 1.0 point measured by (Ter . Bleu)/2 metric.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: The training corpus consists of 16 million sentence pairs available within the DARPA BOLT Chinese-English task. The corpus includes a mix of newswire, broadcast news, webblog data coming from various sources such as LDC, HK Law, HK Hansard and UN data. The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF).[Citation]Language models are trained on the English side of the parallel corpus, and on monolingual corpora, such as Gigaword #OTHEREFRT07) and Google News, altogether comprising around 10 billion words. We parse the Chinese part of the training data with a modified version of the Berkeley parser Only to some extent, the rule still has to match the input forest, but the reordering model decides based on the sum of paths observed between the root and frontier nodes. #OTHEREFR, then prune the obtained parse forests for each training sentence with the marginal probability-based inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length. We extract tree-to-string translation rules from forest-string sentence pairs using the forest-based GHKM algorithm #OTHEREFR.
###Response:",Bilingual word alignments are trained and combined from two sources: GIZA #REFR and maximum entropy word aligner #OTHEREFR.,The phrase-based translation model presented in our framework is influenced largely by the heuristic learning of phrase translations and lexical weighting approach from the cited study #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Despite being closely related languages, German and English are characterized by important word order differences. Longrange reordering of verbs, in particular, represents a real challenge for state-of-theart SMT systems and is one of the main reasons why translation quality is often so poor in this language pair. In this work, we review several solutions to improve the accuracy of German-English word reordering while preserving the efficiency of phrase-based decoding. Among these, we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: Modeling the German-English language pair is known to be a challenging task for state-of-theart statistical machine translation (SMT) methods. A major factor of difficulty is given by word order differences that yield important long-range reordering phenomena.[Citation]However, they typically fail to predict long reorderings. On the other hand, hierarchical SMT #OTHEREFR can learn reordering patterns by means of discontinuous translation rules, and are therefore considered a better choice for language pairs characterized by massive and hierarchical reordering. Looking at the results of the Workshop of Machine Translation?s last edition #OTHEREFR, no particular SMT approach appears to be clearly dominating.
###Response:","Thanks to specific reordering modeling components, phrase-based SMT #OTHEREFR; #REFR are generally good at handling local reordering phenomena that are not captured inside phrases.","Our work proposes improvements to long-range reordering phenomenons, building upon the phrase-based translation model outlined by #REFR, which already exhibits superior performance over word-based models."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a simple history-based model for sentence generation from LFG f-structures, which improves on the accuracy of previous models by breaking down PCFG independence assumptions so that more f-structure conditioning context is used in the prediction of grammar rule expansions. In addition, we present work on experiments with named entities and other multi-word units, showing a statistically significant improvement of generation accuracy. Tested on section 23 of the Penn Wall Street Journal Treebank, the techniques described in this paper improve BLEU scores from 66.52 to 68.82, and coverage from 98.18% to 99.96%.
###Abstract of the cited paper: Previous stochastic approaches to generation do not include a tree-based representation of syntax. While this may be adequate or even advantageous for some applications, other applications profit from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar. We present initial resuits showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a handcrafted grammar outpertbrms both.
###Context text: In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules.[Citation]Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of #OTHEREFR. Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice #OTHEREFR, and then select the most probable sequence of words via an n-gram language model.
###Response:","These rules can be handcrafted grammar rules, such as those of #OTHEREFR or, alternatively, extracted fully automatically from treebanks #REFR.",Previous research has demonstrated that utilizing tree-based syntax models has resulted in better generation outcomes [#REFR].
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we extend the work on using latent cross-language topic models for identifying word translations across comparable corpora. We present a novel precisionoriented algorithm that relies on per-topic word distributions obtained by the bilingual LDA (BiLDA) latent topic model. The algorithm aims at harvesting only the most probable word translations across languages in a greedy fashion, without any prior knowledge about the language pair, relying on a symmetrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations.
###Abstract of the cited paper: Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community, e.g. (Fung, 1998; Rapp, 1999). Surprisingly, none of those works have systematically investigated the impact of the many parameters controlling their approach. The present study aims at doing just this. As a testcase, we address the task of translating terms of the medical domain by exploiting pages mined from Wikipedia. One interesting outcome of this study is that significant gains can be obtained by using an association measure that is rarely used in practice.
###Context text: Therefore the focus of researchers turned to comparable corpora, which consist of documents with partially overlapping content, usually available in abundance. Thus, it is much easier to build a high-volume comparable corpus. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the similar topic, but strongly varying in style, length and vocabulary, while still sharing a certain amount of main concepts (or topics).[Citation]All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al#OTHEREFR have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, based on improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons.
###Response:","Over the years, several approaches for mining translations from non-parallel corpora have emerged #OTHEREFR; #REFR, all sharing the same Firthian assumption, often called the distributionial hypothesis #OTHEREFR, which states that words with a similar meaning are likely to appear in similar contexts across languages.","Expanding upon the practical trend of utilizing comparable corpora, significant strides were made in the exploration of various parameters impacting context-based projection strategies for term translations, as comprehensively investigated in the study by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Text classification methods for tasks like factoid question answering typically use manually defined string matching rules or bag of words representations. These methods are ineffective when question text contains very few individual words (e.g., named entities) that are indicative of the answer. We introduce a recursive neural network (rnn) model that can reason over such input by modeling textual compositionality. We apply our model, qanta, to a dataset of questions from a trivia competition called quiz bowl. Unlike previous rnn models, qanta learns word and phrase-level representations that combine across sentences to reason about entities. The model outperforms multiple baselines and, when combined with information retrieval methods, rivals the best human players.
###Abstract of the cited paper: We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., ?very?) with a positive polar adjective (e.g., ?good?) produces a phrase (?very good?) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model.
###Context text: Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together.[Citation]Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases #OTHEREFR. Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing #OTHEREFRb). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom #OTHEREFR, both of which are able to generate distributed representations of paragraphs.
###Response:",Many computational models of compositionality focus on learning vector spaces #OTHEREFR; #REFR.,"Drawing on the compositional approach to sentiment analysis, our model also derives meaning from the arrangement of words within a text, paralleling the methods outlined by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar?s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.
###Abstract of the cited paper: We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse. Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals. We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG. We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level. We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG. We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results.
###Context text: It is straightforward to take a projection pi and map a CFG G to its induced projection pi(G). What is less obvious is how the probabilities associated with the rules of G should be mapped. In the case where pi(G) is more coarse than the treebank originally used to train G, and when that treebank is available, it is easy to project the treebank and directly estimate, say, the maximum-likelihood parameters for pi(G).[Citation]However, treebank estimation has several limitations. First, the treebank used to train G may not be available. Second, if the grammar G is heavily smoothed or otherwise regularized, its own distribution over trees may be far from that of the treebank.
###Response:","This is the approach taken by #REFR, where they estimate what in our terms are projections of the raw treebank grammar from the treebank itself.","Our novel coarse-to-fine method improves upon the multilevel PCFG parsing process by effectively computing projections of a grammar without a treebank, addressing the limitations outlined by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper introduces a novel unsupervised constraint-driven learning algorithm for identifying named-entity (NE) transliterations in bilingual corpora. The proposed method does not require any annotated data or aligned corpora. Instead, it is bootstrapped using a simple resource ? a romanization table. We show that this resource, when used in conjunction with constraints, can efficiently identify transliteration pairs. We evaluate the proposed method on transliterating English NEs to three different languages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models.
###Abstract of the cited paper: Named Entity recognition (NER) is an important part of many natural language processing tasks. Most current approaches employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.
###Context text: [Citation]Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target languages and require considerably less data to converge.
###Response:","Transliteration methods typically fall into two categories: generative approaches #OTHEREFR; #REFRa), that try to identify the correct transliteration for a word in the source language given several candidates in the target language.","In contrast to popular machine learning techniques that rely on supervised data for named entity recognition, the cited work presents an innovative method for discovering NEs in resource-free languages based on bilingual corpora and transliteration [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a novel lexicon acquirer that works in concert with the morphological analyzer and has the ability to run in online mode. Every time a sentence is analyzed, it detects unknown morphemes, enumerates candidates and selects the best candidates by comparing multiple examples kept in the storage. When a morpheme is unambiguously selected, the lexicon acquirer updates the dictionary of the analyzer, and it will be used in subsequent analysis. We use the constraints of Japanese morphology and effectively reduce the number of examples required to acquire a morpheme. Experiments show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis.
###Abstract of the cited paper: Unknown words are inevitable at any step of analysis in natural anguage processing. Wc propose a method to extract words from a corl)us and estimate the probability that each word belongs to given parts of speech (POSs), using a distributional analysis. Our experiments have shown that this method is etfective for inferring the POS of unknown words.
###Context text: The number of sentences affected by the augmentation varied considerably (1.04%?15.4%). The initial lexicon of the morphological analyzer lacked morphemes that appeared frequently in some corpora because morphological analysis had been tested mainly with newspaper articles. The precision of acquired morphemes was high (97.4%?99.3%), and the number of examples used for acquisition was as little as 4?9.[Citation]Although demonstrative nouns are closed class morphemes, their katakana forms such as ???? (this) were acquired as nouns. The morphological analyzer assumed that demonstrative pronouns were written in hiragana, e.g., ???,? as they always are in a newspaper. Table 3 shows some acquired morphemes.
###Response:",These results are astonishing considering that #REFR ignored candidates that appeared less than 10 times (because they were unreliable). nouns because their morphological behaviors were the same as those of nouns.,"Our lexicon acquirer echoes the method proposed by #REFR, which effectively infers the part of speech of previously unknown words, enhancing morphological analysis."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Existing concept-color-emotion lexicons limit themselves to small sets of basic emotions and colors, which cannot capture the rich pallet of color terms that humans use in communication. In this paper we begin to address this problem by building a novel, color-emotion-concept association lexicon via crowdsourcing. This lexicon, which we call CLEX, has over 2,300 color terms, over 3,000 affect terms and almost 2,000 concepts. We investigate the relation between color and concept, and color and emotion, reinforcing results from previous studies, as well as discovering new associations. We also investigate cross-cultural differences in color-emotion associations between US and India-based annotators.
###Abstract of the cited paper: Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon?s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.
###Context text: [Citation]The quality of the data obtained from non-expert annotators, also referred to as workers or turkers, was investigated by Snow et al #OTHEREFR. Their empirical results show that the quality of non-expert annotations is comparable to the quality of expert annotations on a variety of natural language tasks, but the cost of the annotation is much lower. There are various quality control strategies that can be used to ensure annotation quality.
###Response:",Amazon Mechanical Turk is a crowdsourcing platform that has been extensively used for obtaining low-cost human annotations for various linguistic tasks over the last few years #REFR.,"Echoing Callison-Burch's findings, Mechanical Turk provides a cost-effective method for high-quality annotation, even in the nuanced context of translation quality evaluation #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we address the issue of automatically assigning information status to discourse entities. Using an annotated corpus of conversational English and exploiting morpho-syntactic and lexical features, we train a decision tree to classify entities introduced by noun phrases as old, mediated, or new. We compare its performance with hand-crafted rules that are mainly based on morpho-syntactic features and closely relate to the guidelines that had been used for the manual annotation. The decision tree model achieves an overall accuracy of 79.5%, significantly outperforming the hand-crafted algorithm (64.4%). We also experiment with binary classifications by collapsing in turn two of the three target classes into one and retraining the model. The highest accuracy achieved on binary classification is 93.1%.
###Abstract of the cited paper: This paper investigates automatic identification of Information Structure (IS) in texts. The experiments use the Prague Dependency Treebank which is annotated with IS following the Praguian approach of Topic Focus Articulation. We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines. We present the performance of decision trees (C4.5), maximum entropy, and rule induction (RIPPER) classifiers on all tectogrammatical nodes. We compare the results against a baseline system that always assigns f(ocus) and against a rule-based system. The best system achieves an accuracy of 90.69%, which is a 44.73% improvement over the baseline (62.66%).
###Context text: To our knowledge, there are no other studies on the automatic assignment of information status in English.[Citation]The Czech treebank is annotated following the Topic-Focus articulation theory #OTHEREFR. The theoretical definitions underlying the Prague Treebank and the corpus we are using are different, with the former giving a more global picture of information structure, and the latter a more entity-specific one. For this reason, and due to the fact that Postolache et al?s experiments are on Czech (with a freer word order than English), comparing results is not straightforward.
###Response:","Recently, #REFR have reported experiments on learning information structure in the Prague TreeBank.","Previous research has explored automatic identification of Information Structure in texts, employing various machine learning methods with significant success rates #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Unknown words are a hindrance to the performance of hand-crafted computational grammars of natural language. However, words with incomplete and incorrect lexical entries pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar. Such lexical entries are hard to detect and even harder to correct. We employ an error miner to pinpoint words with problematic lexical entries. An automated lexical acquisition technique is then used to learn new entries for those words which allows the grammar to parse previously uncovered sentences successfully. We test our method on a large-scale grammar of Dutch and a set of sentences for which this grammar fails to produce a parse. The application of the method enables the grammar to cover 83.76% of those sentences with an accuracy of 86.15%.
###Abstract of the cited paper: The effectiveness of parsers based on manually created resources, namely a grammar and a lexicon, rely mostly on the quality of these resources. Thus, increasing the parser coverage and precision usually implies improving these two resources. Their manual improvement is a time consuming and complex task : identifying which resource is the true culprit for a given mistake is not always obvious, as well as finding the mistake and correcting it. Some techniques, like van Noord (2004) or Sagot and Villemonte de La Clergerie (2006), bring a convenient way to automatically identify forms having potentially erroneous entries in a lexicon. We have integrated and extended such techniques in a wider process which, thanks to the grammar ability to tell how these forms could be used as part of correct parses, is able to propose lexical corrections for the identified entries. We present in this paper an implementation of this process and discuss the main results we have obtained on a syntactic widecoverage French lexicon.
###Context text: Then, they employ LA to learn proper lexical entries for these MWEs and add them to the lexicon of a large-scale HPSG grammar of English #OTHEREFR). This increases parsing coverage by 15% to 22.7% for a test set of 674 sentences containing MWEs and parsed with the PET parser #OTHEREFR. In both studies, however, the combination of error mining and LA is applied to a very specific task whereas our method is a general one.[Citation]The lexicon is used in two grammars? the FRMG #OTHEREFR. The first step in this approach is also the application of an error miner #OTHEREFR which uses a parsed newspaper corpus (about 4.3M words) to pinpoint problematic unigrams. The crucial difference with our method is in the second step.
###Response:",#REFR employ a semi-automatic method to improve a large-scale morphosyntactic lexicon of French #OTHEREFR).,"Applying automated techniques for identifying and correcting erroneous entries in a lexicon, as in #REFR, our method instead focuses on a widespread application, differing mainly in the follow-up phase."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and Tu?Ba-D/Z treebanks. Experiments with the Stanford parser, which uses a factored PCFG and dependency model, show that, contrary to previous claims for other parsers, lexicalization of PCFG models boosts parsing performance for both treebanks. The experiments also show that there is a big difference in parsing performance, when trained on the Negra and on the Tu?Ba- D/Z treebanks. Parser performance for the models trained on Tu?Ba-D/Z are comparable to parsing results for English with the Stanford parser, when trained on the Penn treebank. This comparison at least suggests that German is not harder to parse than its West-Germanic neighbor language English.
###Abstract of the cited paper: We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particu- Jar representational strata.
###Context text: [Citation]A common theme that has emerged from this research is the claim that lexicalization of PCFGs, which has been proven highly beneficial for other languages1 , is detrimental for parsing accuracy of German. In fact, this assumption is by now so widely held that Schiehlen #OTHEREFR. parameter and concentrates instead only on treebank transformations of various sorts in his experiments. Another striking feature of all studies mentioned above are the relatively low parsing F- scores achieved for German by comparison to the scores reported for English, its West-Germanic neighbor, using similar parsers.
###Response:","There have been a number of recent studies on probabilistic treebank parsing of German #OTHEREFR, using the Negra treebank #REFR as their underlying data source.","Contrary to the assumption about non-configurational languages as stated by the cited paper #REFR, our study found that lexicalization of PCFG models significantly improves German parsing performance."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested. However, n-gram based metrics are still today the dominant approach. The main reason is that the advantages of employing deeper linguistic information have not been clarified yet. In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofficient against human judges do not reveal details about the advantages and disadvantages of particular metrics. We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics. Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust metaevaluation performance.
###Abstract of the cited paper: Combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics. Recently, several approaches have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a). Although based on different assumptions, these approaches share the common characteristic of being parametric. Their models involve a number of parameters whose weight must be adjusted. As an alternative, in this work, we study the behaviour of non-parametric schemes, in which metrics are combined without having to adjust their relative importance. Besides, rather than limiting to the lexical dimension, we work on a wide set of metrics operating at different linguistic levels (e.g., lexical, syntactic and semantic). Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation.
###Context text: At the syntactic level, we have used several families of metrics based on dependency parsing (DP) and constituency trees (CP). At the semantic level, we have included three different families which operate using named entities (NE), semantic roles (SR), and discourse representations (DR). A detailed description of these metrics can be found in #OTHEREFR.[Citation]ULC considers a subset of metrics which operate at several linguistic levels. This approach has proven very effective in recent evaluation campaigns. Metric computation has been carried out using the IQMT Framework for Automatic MT Evaluation #OTHEREFR1.
###Response:","Finally, we have also considered ULC, which is a very simple approach to metric combination based on the unnormalized arithmetic mean of metric scores, as described by Gime?nez and Ma`#REFRa).","Drawing on the work of #REFR, our method combines various metrics without the need for adjusting their relative significance, leading to an improved, heterogeneous evaluation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Recent work has shown how a parallel corpus can be leveraged to build syntactic parser for a target language by projecting automatic source parse onto the target sentence using word alignments. The projected target dependency parses are not always fully connected to be useful for training traditional dependency parsers. In this paper, we present a greedy non-directional parsing algorithm which doesn?t need a fully connected parse and can learn from partial parses by utilizing available structural and syntactic information in them. Our parser achieved statistically significant improvements over a baseline system that trains on only fully connected parses for Bulgarian, Spanish and Hindi. It also gave a significant improvement over previously reported results for Bulgarian and set a benchmark for Hindi.
###Abstract of the cited paper: In this paper, we first introduce a new architecture for parsing, bidirectional incremental parsing. We propose a novel algorithm for incremental construction, which can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set.
###Context text: The algorithm described in the previous section uses a weight vector ??w to compute the best arc from the list of candidate arcs.[Citation]Algorithm 3 lists the learning framework for GNPPA. For a training sample with sentence w0 ? ? ? wn, projected partial parses projectedPPs={?(ri) ? ? ? ?(rm)}, unconnected words unConn and weight vector ??w , the builtPPs and candidateArcs are initiated as in algorithm 1. Then the arc with the highest score is selected.
###Response:",This weight vector is learned using a simple Perceptron like algorithm similar to the one used in #REFR.,"The selection of the highest scoring arc is inspired by the bidirectional incremental parsing algorithm introduced in #REFR, which has shown significant accuracy improvements."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic lassifiers for word-sense disambiguation.
###Abstract of the cited paper: This paper presents and evaluates models created according to a schema that provides a description of the joint distribution of the values of sense tags and contextual features that is potentially applicable to a wide range of content words. The models are evaluated through a series of experiments, the results of which suggest hat the schema is particularly well suited to nouns but that it is also applicable to words in other syntactic ategories.
###Context text: Decomposable models are a subset of the class of graphical models #OTHEREFR. Familiar examples of decomposable models are Naive Bayes and n-gram models.[Citation]
###Response:",They are characterized by the following properties #REFRb):,"The study by #REFR presents a schema of models that describe the distribution of sense tags and contextual features, supporting the applicability of such models for word-sense disambiguation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a novel approach to grammatical error correction based on Alternating Structure Optimization. As part of our work, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using various feature sets. Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. Our approach also outperforms two commercial grammar checking software packages.
###Abstract of the cited paper: In this paper we describe a methodology for detecting preposition errors in the writing of non-native English speakers. Our system performs at 84% precision and close to 19% recall on a large set of student essays. In addition, we address the problem of annotation and evaluation in this domain by showing how current approaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issues that complicate evaluation of error detection systems.
###Context text: For a more comprehensive survey, see #OTHEREFR. The seminal work on grammatical error correction was done by Knight and Chander #OTHEREFR on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning #OTHEREFR.[Citation]All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance #OTHEREFRb). However, training data has either been small #OTHEREFRa).
###Response:",Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed #OTHEREFR; #REFR.,"The cited study demonstrates an effective approach in detecting preposition errors in non-native English speaker's writings, achieving 84% precision #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Syntactic word reordering is essential for translations across different grammar structures between syntactically distant languagepairs. In this paper, we propose to embed local and non-local word reordering decisions in a synchronous context free grammar, and leverages the grammar in a chartbased decoder. Local word-reordering is effectively encoded in Hiero-like rules; whereas non-local word-reordering, which allows for long-range movements of syntactic chunks, is represented in tree-based reordering rules, which contain variables correspond to sourceside syntactic constituents. We demonstrate how these rules are learned from parallel corpora. Our proposed shallow Tree-to-String rules show significant improvements in translation quality across different test sets.
###Abstract of the cited paper: This paper presents a maximum entropy word alignment algorithm for Arabic- English based on supervised training data. We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance. The probabilistic model used in the alignment directly models the link decisions. Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests. Performance of the algorithm is contrasted with human annotation performance.
###Context text: Our training data consists of two corpora: the GALE Chinese-English parallel corpus and the LDC handaligned corpus1. The Chinese side of these two corpora were parsed using a constituency parser #OTHEREFR. The average labeled F-measure of the parser is 81.4%.[Citation]Then, phrase-pairs that overlap with our development and test set were extracted from the word alignments #OTHEREFR. Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora. The detailed statistics are shown in Table 1.
###Response:",Parallel sentences were first word-aligned using a MaxEnt aligner #REFR.,The word alignment for our training data was achieved via a maximum entropy algorithm as elaborated in #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The problem we address in this paper is that of providing contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages. For a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus.
###Abstract of the cited paper: We propose a dynamic programming algorithm for calculaing the similarity between two segmeuts of words of the same language. The similarity is considered as a vector whose coordinates refer to the levels of analysis of the segments. This algorithm is extremely efficient for retrieving the best example in Translation Memory systems. The calculus being constructive, it also gives the correspondences between the words of the two segments. This allows the extension of Translation Memory systems towards Example-based Machiue Translation. \]Introduction \[n Translation Memory (TM) or Example-Based lVlachine Translation (EBMT) systems, one of lhe decisive tasks is to retrieve from the database, the example that best approaches the input sentence. In Planas (1999) we proposed a twostep retriewd procedure, where a rapid and rough index-based search gives a short list of example candidates, and a refined matching selects the best candidates from this list. This procedure drastically improves the reusability rate of selected examples to 97% at worst, for our English-Japanese TM prototype; with the classical TM strategy, this rate would constantly decline with the number of non matched words. It also allows a better ecall rate when searching for very similar examples. We describe here the Multi-level Similar Seglnent Matching (MSSM) algorithm on which is based the second step of the above retrieval procedure. This algorithm does not only give the distance between the input and the example source segmeuts, but also indicates which words would inatch together. It uses F different levels of data (surface words, lemlnas, parts of speech (POS), etc.) in a combined and uniform way. The computation of the worst case requires F*m*(n-m+2) operations, where m and n are respectively the lengths of the input and the candidate (m<=n). This leads to a linear behavior when m and n have similar lengths, which is often the case for TM segmentsL Furthermore, because this algorithm gives the exact matching links (along with the level o1' match) between all of the words of the input and the candidate sentence, it prepares the transfer stage of an evolution of TM that we call Shallow Translation. This involves substituting in the corresponding translated candidate (stored in the melnory), the translation of the substituted words, provided that the input and the candidate are ""similar enough"".
###Context text: The set of tools is now under further development. This involves an extension of the English semantic tagger, development of the Russian tagger with the target lexical coverage of 90% of source texts, designing the procedure for retrieval of semantically similar situations and completing the user interface.[Citation]There are two main applications of the proposed methodology. One concerns training translators and advanced foreign language (FL) learners to make them aware of the variety of translation equivalents beyond the set offered by the dictionary. The other application pertains to the development of tools for practising translators.
###Response:",Identification of semantically similar situations can be improved by the use of segmentmatching algorithms as employed in Example- Based MT and translation memories #REFR.,"Building on the work by Planas (1999) which demonstrates an effective dynamic programming algorithm for word segment similarity, our approach enhances translation memory systems[#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a method for bootstrapping a fine-grained, broad-coverage part-of-speech (POS) tagger in a new language using only one personday of data acquisition effort. It requires only three resources, which are currently readily available in 60-100 world languages: (1) an online or hard-copy pocket-sized bilingual dictionary, (2) a basic library reference grammar, and (3) access to an existing monolingual text corpus in the language. The algorithm begins by inducing initial lexical POS distributions from English translations in a bilingual dictionary without POS tags. It handles irregular, regular and semi-regular morphology through a robust generative model using weighted Levenshtein alignments. Unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for fine-grained POS tag spaces. Experiments show high accuracy, fine-grained tag resolution with minimal new human effort.
###Abstract of the cited paper: This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection. Keywords multilingual, text analysis, part-of-speech tagging, noun phrase bracketing, named entity, morphology, lemmatization, parallel corpora
###Context text: Previous work in minimally supervised language learning has defined minimal using several different criteria. Some have assumed only partially tagged training corpora #OTHEREFR for named-entity tagging).[Citation]Ngai and Yarowsky #OTHEREFR observed that an often more practical measure of the degree of supervision is not simply the quantity of annotated words, but the total weighted human labor and resource costs of different modes of supervision (allowing manual rule writing to be compared directly with active learning on a common cost-performance learning curve). In this paper we observe that another useful measure of (minimal) supervision is the additional cost of obtaining a desired functionality from existing commonly available knowledge sources. In particular, we note that for a remarkably wide range of languages, academic libraries, many booksellers and websites offer a foundation of linguistic wisdom in reference grammars and dictionaries.
###Response:","Others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such as the translingual projection of part-of-speech tags, syntactic bracketing and inflectional morphology in #REFR, requiring no direct supervision in the foreign language).",The concept of inducing monolingual part-of-speech taggers from noisy and incomplete initial projections was demonstrated by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.
###Abstract of the cited paper: This paper studies the impact of paraphrases on the accuracy of automatic evaluation. Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method in the context of machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation. We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
###Context text: Paraphrases are alternative ways of expressing the same information. Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications.[Citation]While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora #OTHEREFR. The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase extraction heuristics to obtain bilingual phrase pairs from word alignments.
###Response:","Recent work has shown how paraphrases can improve question answering through query expansion #OTHEREFR, automatic evaluation of translation and summarization by modeling alternative lexicalization #REFR, and machine translation both by dealing with out of vocabulary words and phrases #OTHEREFR.","In line with previous research on the significant role of paraphrases in automatic evaluation accuracy, our approach seeks to improve their structural integrity #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.
###Abstract of the cited paper: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
###Context text: Cross-lingual transfer methods and unsupervised methods have recently been shown to hold promise as a way to at least partially sidestep the demand for labeled data. Unsupervised methods attempt to infer linguistic structure without using any annotated data #OTHEREFR. The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available #OTHEREFR.[Citation]Figure 1: Cross-lingual word cluster features for parsing. Top-left: Cross-lingual (EN-ES) word clustering model. Top-right: Samples of some of the induced cross-lingual word clusters.
###Response:","Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems #OTHEREFR; #REFR. ","Our method, as discussed, employs an algorithm for inducing cross-lingual clusters, similar to the simple dependency parser transfer method proposed by #REFR, that enhances the overall quality of resultant parsers."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees.
###Abstract of the cited paper: We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.
###Context text: Given the amount of variability between the outputs of different parsers and word aligners (or even the same systems with different settings), the best way to improve agreement is to learn a transformation sequence that is specifically tuned for the same annotators (parsers and word aligners) we are evaluating with. In particular, we found that though training on the English Chinese Translation Treebank produces clean, interpretable rules, preliminary experiments showed little to no improvement from using these rules for MT, primarily because actual alignments are not only noisier but also systematically different from gold ones. Thus, all rules used for MT experiments were learned from automatically annotated text.[Citation]We used an MT pipeline that conditions on target-side syntax, so our initial state annotator was the Berkeley Parser #OTHEREFR, trained on a modified English treebank that has been adapted to match standard MT tokenization and capitalization schemes. As mentioned in Section 5, we could, in principle train on all 500k sentences of our MT training data. However, this would be quite slow: each iteration of the training procedure requires iterating through all n training sentences6 once for each of the m candidate transformations, for a total cost of O(nm) where m grows (albeit sublinearly) with n.
###Response:","For our Chinese to English translation experiments, we generated word alignments using the Berkeley Aligner #REFR with default settings.","Drawing on the approach of jointly training asymmetric models for better agreement in symmetric word alignment presented by #REFR, we fine-tuned our transformation sequence for enhanced efficacy."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We perform hyperparameter inference within a model of morphology learning (Goldwater et al., 2011) and find that it affects model behaviour drastically. Changing the model structure successfully avoids the unsegmented solution, but results in oversegmentation instead.
###Abstract of the cited paper: We present V-measure, an external entropybased cluster evaluation measure. V- measure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the ?problem of matching?, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.
###Context text: DP-UNI is between these two extremes. b is consistently between 1 and 3, confirming it has little effect. The effect of the hyperparameters can be seen in the morphology results, shown in Table 2. DP-CHAR is robust across hyperparameter values, finding the same type-based solution with fixed and inferred hyperparameters, while the other models have very different results depending on the hyperparameter settings.[Citation]However, with inferred hyperparameters, this model severely undersegments: it finds the unsegmented maximum likelihood solution, where all tokens are generated from the stem distribution #OTHEREFR. The models with alternate base distributions go to the other extreme, oversegmenting the corpus. As generating new morphemes becomes less probable, the pressure to find the most compact morpheme lexicon grows.
###Response:","ORIG with fixed hyperparameters performs best, with the highest VM score (a clustering measure, #REFR) and a level of segmentation close to the correct one.","Utilizing the V-measure tool presented by #REFR, the oversegmentation tendencies of altering base models were identified, validated, and highlighted."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Conditional random fields (Lafferty et al, 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem?POS tagging given a tagging dictionary and unlabeled text?contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
###Abstract of the cited paper: Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.
###Context text: [Citation]CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation.
###Response:",Conditional random fields #OTHEREFR are quite effective at sequence labeling tasks like shallow parsing #REFR and namedentity extraction #OTHEREFR.,"Building on the work of Lafferty et al, who demonstrated the efficiency of conditional random fields in sequence labeling tasks such as shallow parsing, #REFR applies this understanding to train on unlabeled data."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.
###Abstract of the cited paper: Almost all automatic semantic role labeling (SRL) systems rely on a preliminary parsing step that derives a syntactic structure from the sentence being analyzed. This makes the choice of syntactic representation an essential design decision. In this paper, we study the influence of syntactic representation on the performance of SRL systems. Specifically, we compare constituent-based and dependencybased representations for SRL of English in the FrameNet paradigm. Contrary to previous claims, our results demonstrate that the systems based on dependencies perform roughly as well as those based on constituents: For the argument classification task, dependencybased systems perform slightly higher on average, while the opposite holds for the argument identification task. This is remarkable because dependency parsers are still in their infancy while constituent parsing is more mature. Furthermore, the results show that dependency-based semantic role classifiers rely less on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data.
###Context text: [Citation]SRL proceeds through two main steps: the localization of arguments in a sentence, called boundary detection (BD), and the assignment of the proper role to the detected constituents, that is the argument classification, (AC) step. In #OTHEREFR a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. It incorporates strong dependencies within a comprehensive statistical joint model with a rich set of features over multiple argument phrases.
###Response:","State-of-art approaches to frame-based SRL are based on Support Vector Machines, trained over linear models of syntactic features, e.g. #REFRb), or tree-kernels, e.g. #OTHEREFR.","The cited paper examines the impact of syntactic representation on SRL, revealing that dependency-based systems perform comparably to constituent-based counterparts [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically. The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated.
###Abstract of the cited paper: This paper presents an empirical study on using syntactic and semantic information for Concept Segmentation and Labeling (CSL), a well-known component in spoken language understanding. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We perform extensive experimentation by comparing different tree-based kernels with a variety of representations of the available linguistic information, including semantic concepts, words, POS tags, shallow and full syntax, and discourse trees. The results show that the structured representation with the semantic concepts yields significant improvement over the base CSL parser, much larger compared to learning with an explicit feature vector representation. We also show that shallow syntax helps improve the results and that discourse relations can be partially beneficial.
###Context text: We have also shown that the contributions of syntax and discourse information are cumulative, indicating that this learning framework can be appropriate for the combination of different sources of information. Finally, despite the limited information we used, we achieved better correlation at the segment level than BLEU and other metrics in the WMT12 Metrics task. In the future, we plan to work towards our longterm goal, i.e., including more linguistic information in the SKL framework and showing that this can help.[Citation]We further want to show that the proposed framework is flexible and can include information in the form of quality scores predicted by other evaluation metrics, for which a vector of features would be combined with the structured kernel. Acknowledgments This research is part of the Interactive sYstems for Answer Search (Iyas) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Qatar Foundation.
###Response:","This would also include more semantic information, e.g., in the form of Brown clusters or using semantic similarity between the words composing the structure calculated with latent semantic analysis #REFRb).","Drawing on the study by #REFR, which effectively used semantic concepts and syntax in Concept Segmentation and Labeling, we aim to integrate these insights within our SKL framework."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes our system for shared task 13 ?Word Sense Induction for Graded and Non-Graded Senses? of SemEval-2013. The task is on word sense induction (WSI), and builds on earlier SemEval WSI tasks in exploring the possibility of multiple senses being compatible to varying degrees with a single contextual instance: participants are asked to grade senses rather than selecting a single sense like most word sense disambiguation (WSD) settings. The evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance. We adopt a previously-proposed WSI methodology for the task, which is based on a Hierarchical Dirichlet Process (HDP), a nonparametric topic model. Our system requires no parameter tuning, uses the English ukWaC as an external resource, and achieves encouraging results over the shared task.
###Abstract of the cited paper: We apply topic modelling to automatically induce word senses of a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses. We start by exploring the utility of standard topic models for word sense induction (WSI), with a pre-determined number of topics (=senses). We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the WSI task. We go on to establish state-of-the-art results over two WSI datasets, and apply the proposed model to a novel sense detection task.
###Context text: [Citation]To grade the senses for the instances in the test dataset, we apply the sense probabilities learnt by the topic model as the sense weights without any modification. To illustrate the senses induced by our model, we present the top-10 words of the induced senses for the verb strike in Table 2. Although 13 senses in total are induced and some of them do not seem very coherent, only the first 8 senses ? the more coherent ones ? are observed (i.e., have non-zero probability for any usage) in the test dataset.
###Response:","Following #REFR, we use the default parameters (? = 0.1 and ?0 = 1.0) for HDP.1 For each target word, we apply HDP to induce the senses, and a distribution of senses is produced for each ?document? in the model.","Applying the automatic word sense induction method by #REFR, this study grades senses for instances in a test dataset using sense probabilities gained through the topic model."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We consider the problem of constructing a directed acyclic graph that encodes temporal relations found in a text. The unit of our analysis is a temporal segment, a fragment of text that maintains temporal coherence. The strength of our approach lies in its ability to simultaneously optimize pairwise ordering preferences and global constraints on the graph topology. Our learning method achieves 83% F-measure in temporal segmentation and 84% accuracy in inferring temporal relations between two segments.
###Abstract of the cited paper: In this paper we propose a data intensive approach for inferring sentence-internal temporal relations, which relies on a simple probabilistic model and assumes no manual coding. We explore various combinations of features, and evaluate performance against a goldstandard corpus and human subjects performing the same task. The best model achieves 70.7% accuracy in inferring the temporal relation between two clauses and 97.4% accuracy in ordering them, assuming that the temporal relation is known.
###Context text: Besides its linguistic significance, temporal analysis has important practical implications. In multidocument summarization, knowledge about the temporal order of events can enhance both the content selection and the summary generation processes #OTHEREFR. In question answering, temporal analysis is needed to determine when a particular event occurs and how events relate to each other.[Citation]This paper characterizes the temporal flow of discourse in terms of temporal segments and their ordering. We define a temporal segment to be a fragment of text that does not exhibit abrupt changes in temporal focus #OTHEREFR. A segment may contain more than one event or state, but the key requirement is that its elements maintain temporal coherence.
###Response:",Some of these needs can be addressed by emerging technologies for temporal analysis #OTHEREFR; #REFR.,"Drawing on the data-intensive approach proposed by #REFR, our method optimizes the inference of temporal relations between segments, achieving an overall 84% accuracy."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a model for Chinese poem generation based on recurrent neural networks which we argue is ideally suited to capturing poetic content and form. Our generator jointly performs content selection (?what to say?) and surface realization (?how to say?) by learning representations of individual characters, and their combinations into one or more lines as well as how these mutually reinforce and constrain each other. Poem lines are generated incrementally by taking into account the entire history of what has been generated so far rather than the limited horizon imposed by the previous line or lexical n-grams. Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods.
###Abstract of the cited paper: Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.
###Context text: Following lines are generated by taking into account the representations of all previously generated lines. using recurrent neural networks. Structural, semantic, and coherence constraints are captured naturally in our framework, through learning the representations of individual characters and their combinations. Secondly, generation proceeds by taking into account multi-sentential context rather than the immediately preceding sentence.[Citation]More recently, continuous translation models based on recurrent neural networks have been proposed as a means to map a sentence from the source language to sentences in the target language #OTHEREFR. These models are evaluated on the task of rescoring n-best lists of translations. We use neural networks more directly to perform the actual poem generation task.
###Response:",Our work joins others in using continuous representations to express the meaning of words and phrases #REFR and how these may be combined in a language modeling context #OTHEREFR.,"Our network's design is inspired by the recursive neural network model suggested by #REFR, which effectively learns compositional vector representations."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes the use of ""patternbased"" context-free grammars as a basis for building machine translation (MT) systems, which are now being adopted as personal tools by a broad range of users in the cyberspace society. We discuss major requirements for such tools, including easy customization for diverse domains, the efficiency of the translation algorithm, and scalability (incremental improvement in translation quality through user interaction), and describe how our approach meets these requirements.
###Abstract of the cited paper: The unique properties of lree-adjoining grammars (TAG) present a challenge for the application of 'FAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of natural h'mguage. We present a variant of ""FAGs, called synchronous TAGs, which chmacterize correspondences between languages. ""lq\]e formalism is intended usage is to relate expressions of natural anguages to their associated semantics represented in a logical tbrm language, or to their translates in another natural anguage; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mentioning primarily in passing some computational issues that tu:ise in its interpretation.
###Context text: source and target CFG rules are linked if they 2Where \]G\] stands for the size of grammar G, and n is the length of an input string. 3Lexicalized CFG, or Tree Insertion Grammar #OTHEREFR, has been recently introduced to achieve such efficiency and lexicalization. 4and its inflectional variants - - we will discuss inflections and agreement issues later.[Citation]See also bilingual signs #OTHEREFR for a discussion of the importance of combining the appropriate domain of locality and synchronization. are given the same index "":i"". Linked nonterminal must be derived from a sequence of synchronized pairs. Thus, the first NP (NP:I) in the source rule corresponds to the second NP (NP:I) in the target rule, the Vs in both rules correspond to each other, and the second NP (NP:3) in the source rule corresponds to the first NP (NP:3) in the target rule.
###Response:","5The meaning of the word ""synchronized"" here is exactly the same as in STAG #REFR.","The introduction of synchronous TAGs, as detailed by #REFR, allows for the characterization of correspondences between languages, thus enhancing translation efficiency."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We deal with the question as to whether there exists a polynomial time algorithm for computing the most probable parse tree of a sentence generated by a data-oriented parsing (DOP) model. (Scha, 1990; Bod, 1992, 1993a). Therefore we describe DOP as a stochastic tree-substitution grammar (STSG). In STSG, a tree can be generated by exponentially many derivations involving different elementary trees. The probability of a tree is equal to the sum of the probabilities of all its derivations. We show that in STSG, in contrast with stochastic context-free grammar, the Viterbi algorithm cannot be used for computing a most probable tree of a string. We propose a simple modification of Viterbi which allows by means of a ""se lectrandom"" search to est imate the most probable tree of a string in polynomial time. Experiments with DOP on ATIS show that only in 68% of the cases, the most probable derivation of a string generates the most probable tree of that string. Therefore, the parse accuracy obtained by the most probable trees (96%) is dramatically higher than the parse accuracy obtained by the most probable derivations (65%). It is still an open question whether the most probable tree of a string can be deterministically computed in polynomial time.
###Abstract of the cited paper: 1)ata Oriented Parsing (IX)P) is a model where no abstract rules, but language xt~riences in the ti3ru~ of all ,'malyzed COlpUS, constitute the basis for langnage processing. Analyzing a new input means that the system attempts to find tile most probable way to reconstruct the input out of frugments that alr""c~y exist ill the corpus. Disambiguation occurs as a side-effect. DOP can be implemented by using colivelllional parsing strategies. In~oducfion This paper tommlizes the model for natural Imlgnage introduced m \[Sclm 199o\]. Since that article is written in Dutch, we will translate Some parts of it more or less literally in this introduction. According to Scba, the current radition of language processing systems is based on linguistically motivated competence models of natural Imlguages. ""llte problems that these systems lull iato, suggest file necessity of a more perfommnce oriented model of language processing, that takes into account the statistical properties of real language use. qllerefore Scha proposes a system ritat makes use of an annotated corpus. AnMyzing a new input means that the system attempts to find the most probable way to reconstruct the input out of fragments that already exist in the corpus. The problems with competence grammars that are mentioned in Scha is aiticle, include the explosion of ambiguities, the fact tilat Itunmn judgemeats on grammaticality are not stable, that competence granunars do not account for language h~alge, alld that no existing rule-based grammar gives a descriptively 'adequate characterization of an actual language. According to Scha, tile deveh,pment of a fornml gnatunar fur natural latlguage gets more difficult ,as tire grammar gets larger. When the number of phenotnena one has already takea into account gets larger, the number of iareractions that must be considered when ,me tries to introduce all account of a new pllenomenon grows accordingly. As to tile problem of ,'mtbiguity, it has turned out that as soon as a formal gratmnar clmracterizes a non-trivial part of a natural anguage, almost every input sentence of reasonable length gets ml re\]manageably large number of different structural analyses (and * The author wishes to thank his colleagues at the Department of Computational Linguistics of the Ilaiversity of Amsterdam for many fruitful discussions, and, in particular, Remko Scha, Martin van den Berg, Kwee Tjoe l,iong and Frodenk Somsen for valuable comments on earlier w~'rsions of this paper. semantical interpretations). I ""lids is problenmtic since most of these interpretations ~re not perceived as lVossible by a hunmn language user, while there are no systematic reasons 111 exclude tileln on syutactic or sematltic grounds. Often it is just a ntatter of relative implausibility: tile only reason why a certain iarerpmtarion of a sentence is not perceived, is that aanther interprctatilm is much more plausible. Competence and Performance 'tale lhnriations of the current language procossing systerus are not suprising: riley are the direct consequence of rile fact that these systems implement Chart\]sky is notion of a coutpetence grmnmar. The formal grilnuuars that constitute the subject-nmtter of theoretieal linguistics, aim at characterizing the clnnpetencc of tile langnage user. But the preferences language users have m the case of ambiguous entences, are paradigm instances of perfonatmce phenomena. In order to build effective lauguage processing systems we nmst intplement performanec-grammars, rather than competence gratumars, qlaese performance granmuus houM not only contain information on the structural possibilities of file general I~mgnage system, but also on details of actual language use in a language conmmnity, and of tile language experiences of an individual, which cause this individual to have certain expectations on what kinds of uUerances are going to occur, and what slractures and interpretations these utterances are going to have. Therc is all alternative linguistic tradition tluat has always focused on the concrete details of actual language use: file statistical tradition. In this approach, syntactic structure is usually ignored; only isuperficial' stalistical properties of a large coqms are described: file probability that a certain word is followed by a certain other word, the probability that a certain sequence of two words is followed by a ce~ml word, etc. (Markovcludns, see e.g. \[Bahl 1983\]). This approach bus perforumd succesfully ill certain practical tasks, such ,as selecting the most probable sentence from the outputs of a speech recognition coruptment. It will be clear that this approach is not suitable for mmly other tasks, because no uotion of syntactic structme is used. Aud there are statistical dependencies within the sentences of a corpus, that cam extend over all arbitrarily long sequence of words; this is ignored by file Markov-approach. The challenge is now to develop a theory of language processlag that does justice to tile statistieM ,as well as to tile structural aspects of langange.
###Context text: [Citation]We will limit ourselves in this paper to corpora with purely syntactic annotations. For the semantic dimension of DOP, the reader is referred to #OTHEREFR. Consider the imaginary example corpus consisting of only two trees in figure
###Response:","A Data-Oriented Parsing model #OTHEREFR; #REFR, 1993a) is characterized by a corpus of analyzed language utterances, together with a set of operations that combine sub-analyses from the corpus into new analyses.","Drawing from the Data Oriented Parsing (DOP) model, this study focuses on reconstructing inputs from fragments existing in a syntactically annotated corpus [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling characterlevel analysis of Chinese syntactic structures. In this paper, we investigate the problem of character-level Chinese dependency parsing, building dependency trees over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods.
###Abstract of the cited paper: Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beamsearch. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.
###Context text: It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition #OTHEREFR.[Citation]When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with ?words? being ?characters?. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing.
###Response:","Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager #REFR or the arc-standard #OTHEREFR transition systems.","Drawing from the combined graph-based and transition-based parsing methods for beam-search based parser, significant improvements have been found in such character-level dependency parsing #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). The model?s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models #OTHEREFR learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data.[Citation]However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models.
###Response:","Phrase-based SMT models, such as the alignment template model #REFR, improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering.","This notion was challenged by Marcu and Wong (2002), who presented a joint probability model for phrase-based SMT, offering a deterministic approach to estimating parameters at the phrase level #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Monolingual translation probabilities have recently been introduced in retrieval models to solve the lexical gap problem. They can be obtained by training statistical translation models on parallel monolingual corpora, such as question-answer pairs, where answers act as the ?source? language and questions as the ?target? language. In this paper, we propose to use as a parallel training dataset the definitions and glosses provided for the same term by different lexical semantic resources. We compare monolingual translation models built from lexical semantic resources with two other kinds of datasets: manually-tagged question reformulations and question-answer pairs. We also show that the monolingual translation probabilities obtained (i) are comparable to traditional semantic relatedness measures and (ii) significantly improve the results over the query likelihood and the vector-space model for answer finding.
###Abstract of the cited paper: User logs of search engines have recently been applied successfully to improve various aspects of web search quality. In this paper, we will apply pairs of user queries and snippets of clicked results to train a machine translation model to bridge the ?lexical gap? between query and document space. We show that the combination of a query-to-snippet translation model with a large n-gram language model trained on queries achieves improved contextual query expansion compared to a system based on term correlations.
###Context text: Other kinds of training data have also been proposed. Jeon et al #OTHEREFR automatically clustered semantically similar questions based on their answers. Murdock and Croft #OTHEREFR created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus.[Citation]All in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval #OTHEREFR. Many of the approaches previously described have used parallel data extracted from the retrieval corpus itself. The translation models obtained are therefore domain and collection-specific, which introduces a bias in the evaluation and makes it difficult to assess to what extent the translation model may be re-used for other tasks and document collections.
###Response:",Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages #OTHEREFR or queries and clicked snippets from query logs #REFR.,"Another notable method applied by #REFR utilized pairs of user queries and snippets of clicked results to train a machine translation model, hence aiding in bridging the 'lexical gap'."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.
###Abstract of the cited paper: In word sense disambiguation, choosing the most frequent sense for an ambiguous word is a powerful heuristic. However, its usefulness is restricted by the availability of sense-annotated data. In this paper, we propose an information retrieval-based method for sense ranking that does not require annotated data. The method queries an information retrieval engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance.We also show that the estimated sense frequencies correlate reliably with native speakers? intuitions.
###Context text: When it comes to automatic text understanding, then, it is crucial to take into account the domain specificity of a piece of text, so as to perform a focused and as-precise-as-possible analysis which, in its turn, can enable domain-aware applications such as question answering and information extraction. Domain knowledge also has the potential to improve open-text applications such as summarization #OTHEREFR. Research in Word Sense Disambiguation #OTHEREFR, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now.[Citation]More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carry out the task #OTHEREFR. High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e., WordNet #OTHEREFR. Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains.
###Response:","Many approaches have been devised, including the identification of domain-specific predominant senses #OTHEREFR; #REFR, the development of domain resources #OTHEREFR.","Taking from a heuristic which suggests that the most frequent sense of an ambiguous word is often accurate for WSD, this method was developed to sidestep the need for annotated data #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes a heuristic algorithm capable of automatically assigning a label to each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a computational-semantic lexicon for treatment of lexical ambiguity. Including these labels in the MRD-based lexical database offers several positive ffects. The labels can be used as a coarser sense division so unnecessarily fine sense distinction can be avoided in word sense disambiguation (WSD).The algorithm is based primarily on simple word matching between an MRD definition sentence and word lists of an LLOCE topic. We also describe an implementation f the algorithm for labeling definition sentences in Longman Dictionary of Contemporary English (LDOCE). For this purpose the topics and sets of related words in Longman Lexicon of Contemporary English (LLOCE) are used in this work. Quantitative r sults for a 12-word test set are reported. Our discussion entails how the availability of these labels provides the means for treating such problems as: acquisition of a lexicon capable of providing broad coverage, systematic word sense shifts, lexical underspecification, and acquisition of zero-derivatives.
###Abstract of the cited paper: This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Roget is Thesaurus categories. Roget is categories serve as approximations of conceptual classes. The categories li ted for a word in Roger is index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful evel of sense disambiguatiou. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework. Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unresUicted monolingual text without human intervention. Applied to the 10 million word Grolier is Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature.
###Context text: Sanfilippo and Poznanski #OTHEREFR proposed a so-caUed Dictionary Correlation Kit (DCK) in a dialog-based environment for correlating word senses across pairs of MRDs, LDOCE and LLOCE. Dolan #OTHEREFR described a heuristic approach to forming unlabeled clusters of closely related senses in a MRD. The clustering program relies on LDOCE domain code, grammar code, and 25 types of semantic relations exu'acted from definitions.[Citation]The author suggested that he method can also apply to dictionary definitions. Krovetz #OTHEREFR described a simple algorithm based on overlap of defining words to identify related senses between morphological variants. The author eported that the success rate was over 80%.
###Response:",#REFR described a WSD method and an implementation based on Roget' s Thesaurus and the training material of the 10-rnillion-word Grolier' s Encyclopedia.,"Our approach, similar to the statistical models described by #REFR, aims to disambiguate word senses by matching MRD definition sentences with LLOCE topic word lists."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a method that speeds up a classifier trained with many conjunctive features: combinations of (primitive) features. The key idea is to precompute as partial results the weights of primitive feature vectors that appear frequently in the target NLP task. A trie compactly stores the primitive feature vectors with their weights, and it enables the classifier to find for a given feature vector its longest prefix feature vector whose weight has already been computed. Experimental results for a Japanese dependency parsing task show that our method speeded up the SVM and LLM classifiers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6.
###Abstract of the cited paper: We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f -score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.
###Context text: [Citation]One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing #OTHEREFR.
###Response:","Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications, and it is inadequate to process Web-scale corpora for knowledge acquisition #OTHEREFR or semi-supervised learning #REFR.","The technique employed derives insights from an effective method of self-training a two-phase parser-reranker system, which exhibits a bootstrapping potential for parsing tasks with a discriminative reranker [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Following (McDonald et al, 2005), we present an application of a maximum spanning tree algorithm for a directed graph to non-projective labeled dependency parsing. Using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003), we discriminatively trained our parser in an on-line fashion. After just one epoch of training, we were generally able to attain average results in the CoNLL
###Abstract of the cited paper: We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.
###Context text: [Citation]Based on the analysis of the Arabic parsing results, we have proposed a bottomup non-projective labeled dependency parsing algorithm that allows us to use features dependent on more than one edge, with very little disadvantage compared to the original algorithm.
###Response:",We have extended non-projective unlabeled dependency parsing #REFR to a very simple non-projective labeled dependency and showed that the parser performs reasonably well with small number of features and just one iteration of training.,"Applying the methodology of McDonald et al (2005), we utilize a maximum spanning tree algorithm for effective non-projective labeled dependency parsing[#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art Penn Treebank (PTB) parser. An accuracy comparison is performed by converting the CCG derivations into PTB trees. We show that the conversion is extremely difficult to perform, but are able to fairly compare the parsers on a representative subset of the PTB test section, obtaining results for the CCG parser that are statistically no different to those for the Berkeley parser.
###Abstract of the cited paper: This paper presents a methodology for the comparative performance analysis of the parsers developed for different grammar frameworks. For such a comparison, we need a common representation format of the parsing results since the representation of the parsing results depends on the grammar frameworks; hence they are not directly comparable to each other. We first convert the parsing result to a shallow CFG analysis by using an automatic tree converter based on synchronous grammars. The use of such a shallow representation as a common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations. We compared an HPSG parser with several CFG parsers in our experiment and found that meaningful differences among the parsers? performance can still be observed by such a shallow representation.
###Context text: [Citation]The conclusion is that such conversions are surprisingly difficult. Clark and Curran #OTHEREFR for details of the two parsers. shows that converting gold-standard CCG derivations into the GRs in DepBank resulted in an F- score of only 85%; hence the upper bound on the performance of the CCG parser, using this evaluation scheme, was only 85%. Given that the current best scores for the PTB parsing task are over 90%, any loss from the conversion process needs to be considered carefully if a fair comparison with PTB parsers is to be achieved.
###Response:","There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. #OTHEREFR; #REFR.","In reference to the comparative methodology for parser performance analysis, it becomes evident that the conversion process to a common format is indeed challenging #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: An approach to automatic detection of syllable structure is presented. We demonstrate a novel application of EM-based clustering to multivariate data, exempli ed by the induction of 3- and 5-dimensional probabilistic syllable classes. The qualitative evaluation shows that the method yields phonologically meaningful syllable classes. We then propose a novel approach to grapheme-to-phoneme conversion and show that syllable structure represents valuable information for pronunciation systems.
###Abstract of the cited paper: A finite-state method, based on leftmost longestmatch replacement, is presented for segmenting words into graphemes, and for converting raphemes into phonemes. A small set of hand-crafted conversion rules for Dutch achieves a phoneme accuracy of over 93%. The accuracy of the system is further improved by using transformation-based learning. The phoneme accuracy of the best system (using a large rule and a 'lazy' variant of Brill is algoritm), trained on only 40K words, reaches 99%.
###Context text: This TTS system relies on an annotation of morphological structure for the words in its lexicon and it performs a morphological analysis of unknown words #OTHEREFR; the pronunciation rules draw on this structural information. These comparative results emphasize the value of phonotactic knowledge and information on syllable structure and morphological structure for g2p conversion. In a comparison across languages, a word accuracy rate of 75.3% for our 5-dimensional German syllable model is slightly higher than the best data-driven method for English with 72% #OTHEREFR.[Citation]We are con dent that a judicious combination of phonological information of the type employed in our feasibility study with standard techniques such as g2p alignment of training data will produce a pronunciation system with a word accuracy that matches the one reported by Bouma #OTHEREFR. We believe, however, that for an optimally performing system as is desired for TTS, an even more complex design will have to be adopted. In many languages, including English, German and Dutch, access to morphological and phonological information is required to reliably predict the pronunciation of words; this view is further evidenced by the performance of the Bell Labs system, which relies on precisely this type of information.
###Response:","Recently, #REFR has reported a word accuracy of 92.6% for Dutch, using a `lazy' training strategy on data aligned with the correct phoneme string, and a hand-crafted system that relied on a large set of rule templates and a many-to-one mapping of characters to graphemes preceding the actual g2p conversion.",The high phoneme accuracy achieved through the use of a finite-state method as demonstrated by Bouma's successful grapheme-to-phoneme conversion model supports our proposition #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe our experience in developing a discourse-annotated corpus for community-wide use. Working in the framework of Rhetorical Structure Theory, we were able to create a large annotated resource with very high consistency, using a well-defined methodology and protocol. This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, discourse-specific applications.
###Abstract of the cited paper: We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computational model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones.
###Context text: It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. . Previous research on annotating texts with rhetorical structure trees #OTHEREFR has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. .[Citation]We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: . The leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units .
###Response:",Previous research has shown that RST trees can play a crucial role in building natural language generation systems #OTHEREFR and text summarization systems #REFR; can be used to increase the naturalness of machine translation outputs #OTHEREFR.,"Drawing on empirical findings demonstrating distinct discourse structures in Japanese and English translations #REFR, we refined our annotation methods."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe a novel neural network architecture for the problem of semantic role labeling. Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al, 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost.
###Abstract of the cited paper: In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.
###Context text: They proposed to take advantage of the syntactic tree structure that can be predicted by a parser, such as Charniak?s parser #OTHEREFR. Their aim is, given a node in the parse tree, to assign a semantic role label to the words that are the children of that node. They extract several key types of features from the parse tree to be used in a statistical model for prediction.[Citation]These features include: . The parts of speech and syntactic labels of words and nodes in the tree. . The node?s position (left or right) in relation to the verb. .
###Response:","These same features also proved crucial to subsequent approaches, e.g. #REFR.","The statistical model they used is built upon the machine learning algorithm proposed by Gildea and Jurafsky (2002), which improved the performance of earlier classifiers #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we study the impact of a group of features extracted automatically from machine-generated parse trees on coreference resolution. One focus is on designing syntactic features using the binding theory as the guideline to improve pronoun resolution, although linguistic phenomenon such as apposition is also modeled. These features are applied to the Arabic, Chinese and English coreference resolution systems and their effectiveness is evaluated on data from the Automatic Content Extraction (ACE) task. The syntactic features improve the Arabic and English systems significantly, but play a limited role in the Chinese one. Detailed analyses are done to understand the syntactic features? impact on the three coreference systems.
###Abstract of the cited paper: The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.
###Context text: A perfect coreference system gets 100% ACE-Value while a system outputting many false-alarm entities could get a negative value. The default weights in ACE-Value emphasize names, and severely discount pronouns: the relative importance of a pronoun is two orders of magnitude less than that of a name. So the ACE-Value will not be able to accurately reflect a system?s improvement on pronouns2.[Citation]The F-measure is computed by first aligning system and reference entities such that the number of common mentions is maximized and each system entity is constrained to align with at most one reference entity, and vice versa. For example, suppose that a reference document contains three entities: {[m1], [m2, m3], [m4]} while a system outputs four entities: {[m1, m2], [m3], [m5], [m6]}, where {mi : i = 1, 2, ? ? ? , 6} are mentions, then the best alignment from reference to system would be [m1] ? [m1, m2], [m2, m3] ? [m3] and other entities are not aligned. The number of common mentions of the best alignment is 2 2Another possible choice is the MUC F-measure #OTHEREFR.
###Response:","For this reason, we compute an unweighted entity-constrained mention F-measure #REFR and report all contrastive experiments with this metric.",The alignment method was introduced in the Constrained Entity-Alignment F-Measure (CEAF) methodology as described by #REFR to better assess coreference resolution.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Unknown words are a hindrance to the performance of hand-crafted computational grammars of natural language. However, words with incomplete and incorrect lexical entries pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar. Such lexical entries are hard to detect and even harder to correct. We employ an error miner to pinpoint words with problematic lexical entries. An automated lexical acquisition technique is then used to learn new entries for those words which allows the grammar to parse previously uncovered sentences successfully. We test our method on a large-scale grammar of Dutch and a set of sentences for which this grammar fails to produce a parse. The application of the method enables the grammar to cover 83.76% of those sentences with an accuracy of 86.15%.
###Abstract of the cited paper: Unknown words are a major issue for large-scale grammars of natural language. We propose a machine learning based algorithm for acquiring lexical entries for all forms in the paradigm of a given unknown word. The main advantages of our method are the usage of word paradigms to obtain valuable morphological knowledge, the consideration of different contexts which the unknown word and all members of its paradigm occur in and the employment of a full-blown syntactic parser and the grammar we want to improve to analyse these contexts and provide elaborate syntactic constraints. We test our algorithm on a large-scale grammar of Dutch and show that its application leads to an improved parsing accuracy.
###Context text: However, the error miner identifies this word as the reason for the parsing failure of 9 sentences. A manual examination reveals that the word is used as a neuter noun in these cases? het afwater (the drainage). Since there is no noun entry in the lexicon, Alpino was not able to produce full-span analyses.[Citation]This method has already been successfully applied to the task of learning lexical entries for unknown words and, as the error miner, it can be used ?out of the box?. LA correctly predicts a neuter noun entry for afwater and the addition of this entry to the lexicon enables Alpino to cover the 9 problematic sentences from the Mediargus corpus. It should be noted that since our approach cannot differentiate between incomplete and incorrect entries, no entry in the lexicon is modified.
###Response:","After the error miner identifies afwater as a problematic word, we employ our machine learning based LA method presented in #REFR to learn new entries for this word.","Building on the machine learning algorithm proposed by #REFR, we were able to effectively identify and correct inaccurate lexical entries."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Anatomical entities such as kidney, muscle and blood are central to much of biomedical scientific discourse, and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts. Although a number of resources and methods addressing aspects of the task have been introduced, there have so far been no annotated corpora for training and evaluating systems for broad-coverage, open-domain anatomical entity mention detection. We introduce the AnEM corpus, a domainand species-independent resource manually annotated for anatomical entity mentions using a fine-grained classification system. The corpus texts are selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientific literature. We demonstrate the use of the corpus through an evaluation of the broad-coverage MetaMap tagger and a CRF-based system trained on the corpus data, considering also a combination of these two methods. The combined system demonstrates a promising level of performance, approaching 80% F-score for mention detection for a relaxed matching criterion. The corpus and other introduced resources are available under open licences from http:// www.nactem.ac.uk/anatomy/.
###Abstract of the cited paper: We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org
###Context text: Primary annotation was created by a PhD biologist with extensive experience in domain information extraction and text annotation (TO). The use of any relevant resources, such as the full article being annotated or species-specific anatomy ontologies in the OBO foundry, was encouraged for resolving unclear or ambiguous cases during annotation. Initial annotation was produced entirely manually.[Citation]The annotations were created using the BRAT annotation tool #OTHEREFR. To evaluate the annotation consistency, we performed an inter-annotator agreement (IAA) experiment. After brief training with annotation guidelines provided by the primary annotator, a random 10% of the corpus was independently annotated by a PhD computer scientist with experience in domain text annotation and anatomy ontologies (SP).
###Response:","To further assure the quality of the annotation, a series of automatic tests was performed and used as the basis of a further manual round of revision.4 Annotation guidelines were initially created based on those created by our previous domain-specific effort #REFRa) and revised throughout the annotation effort to document specific decisions made during annotation.","The initial manual annotation was facilitated with the help of the brat rapid annotation tool, as introduced by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition (ASR) hypotheses to a spoken dialogue system (SDS). Our novel approach is to use a simple statistical User Simulation (US) for this task, which measures the likelihood that the user would say each hypothesis in the current context. Such US models are now common in machine learning approaches to SDS, are trained on real dialogue data, and are related to theories of ?alignment? in psycholinguistics. We use a US to predict the user?s next dialogue move and thereby re-rank n-best hypotheses of a speech recognizer for a corpus of 2564 user utterances. The method achieved a significant relative reduction of Word Error Rate (WER) of 5% (this is 44% of the possible WER improvement on this data), and 62% of the possible semantic improvement (Dialogue Move Accuracy), compared to the baseline policy of selecting the topmost ASR hypothesis. The majority of the improvement is attributable to the User Simulation feature, as shown by Information Gain analysis.
###Abstract of the cited paper: This paper investigates the problem of bootstrapping a statistical dialogue manager without access to training data and proposes a new probabilistic agenda-based method for simulating user behaviour. In experiments with a statistical POMDP dialogue system, the simulator was realistic enough to successfully test the prototype system and train a dialogue policy. An extensive study with human subjects showed that the learned policy was highly competitive, with task completion rates above 90%.
###Context text: We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses.[Citation]In our model, the system?s ?belief? b(h) in a recognition hypothesis h is factored in two parts: the observation probability P (o|h) (approximated by the ASR confidence score) and the User Simulation probability P (h|us,C) of the hypothesis: b(h) = P (o|h).P (h|us,C) (1) where us is the state of the User Simulation in context C. The context is simply a window of dialogue acts in the dialogue history, that the US is sensitive to (see section 3). The paper is organized as follows.
###Response:","These User Simulations are now commonly used in statistical learning approaches to dialogue management #OTHEREFR; #REFR, but they have not been used for context-sensitive ASR before.","Drawing inspiration from a study that utilized a probabilistic agenda-based user behavior simulator for dialogue management without training data, our model uses User Simulations for improved predictions #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that syntax provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns. Results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90%. We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles.
###Abstract of the cited paper: We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers? judgments of text readability. This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text. We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.
###Context text: Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences #OTHEREFR.[Citation]Early theories #OTHEREFR posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on.
###Response:",Other work has shown that co-occurrence of words #OTHEREFR and discourse relations #REFR also predict coherence.,"In their study, the authors confirmed that discourse relations strongly influence text readability, demonstrating a linkage to the intentional discourse structure #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper we present a methodology for extracting subcategorisation frames based on an automatic LFG f-structure annotation algorithm for the Penn-II Treebank. We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG categorybased subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs. Our approach does not predefine frames, associates probabilities with frames conditional on the lemma, distinguishes between active and passive frames, and fully reflects the effects of long-distance dependencies in the source data structures. We extract 3586 verb lemmas, 14348 semantic form types (an average of 4 per lemma) with 577 frame types. We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource.
###Abstract of the cited paper: We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate hat a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount 1.
###Context text: Consequently our F-score is also low (Schulte im Walde (op cit.) records an F-score of 57.24%). Experiments 2a and 3a are similar to Experiments 2 and 3 respectively except they include the specific particle associated with each PART. 5.1.1 Directional Prepositions There are a number of possible reasons for our low recall scores for Experiment 3 in Table 5.[Citation]We have extracted frames from one domain (the WSJ) whereas COM- LEX was built using examples from the San Jose Mercury News, the Brown Corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorisation frames than our induced lexicon.
###Response:",It is a well-documented fact #REFR that subcategorisation frames (and their frequencies) vary across domains.,"Our methodology, though divergent, aligns with the technique discussed by #REFR in constructing subcategorization dictionaries from varying textual corpora."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets.
###Abstract of the cited paper: A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more longdistance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to rescore word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.
###Context text: Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks #OTHEREFR.[Citation]The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward networks as well as conventional n-gram models, both of which are limited to fixed-length contexts. Building on the success of recurrent architectures, we base our joint language and translation model on an extension of the recurrent neural network language model #OTHEREFR that introduces a layer of additional inputs (?2).
###Response:","In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling #OTHEREFR; #REFR, outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition #OTHEREFR.","With this, our approach resonates with the two-pass re-scoring strategy proposed by #REFR, which also leverages the capabilities of RNN-LM to capture long-distance dependencies."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL?00 and ?03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections.
###Abstract of the cited paper: This paper proposes a framework for semi-supervised structured output learning (SOL), specifically for sequence labeling, based on a hybrid generative and discriminative approach. We define the objective function of our hybrid model, which is written in log-linear form, by discriminatively combining discriminative structured predictor(s) with generative model(s) that incorporate unlabeled data. Then, unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation. Experiments on named entity recognition (CoNLL-2003) and syntactic chunking (CoNLL-2000) data show that our hybrid model significantly outperforms the stateof-the-art performance obtained with supervised SOL methods, such as conditional random fields (CRFs).
###Context text: We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields #OTHEREFR.[Citation]
###Response:","As our approach for incorporating unlabeled data, we basically follow the idea proposed in #REFR.","Drawing on the hybrid generative and discriminative approach proposed by #REFR, we leverage unlabeled data to enhance our model's discriminant functions."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In recent years, there has been widespread interest in compositional distributional semantic models (cDSMs), that derive meaning representations for phrases from their parts. We present an evaluation of alternative cDSMs under truly comparable conditions. In particular, we extend the idea of Baroni and Zamparelli (2010) and Guevara (2010) to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature, so that all models can be tested under the same training conditions. The linguistically motivated functional model of Baroni and Zamparelli (2010) and Coecke et al(2010) emerges as the winner in all our tests.
###Abstract of the cited paper: We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
###Context text: [Citation]Equipped with a general estimation approach, we thoroughly evaluated the cDSMs in a comparable setting. The linguistically motivated Lexfunc model of Baroni and Zamparelli #OTHEREFR composition method (of course, the composition method is only one aspect of Socher et als architecture). All other composition methods behaved inconsistently.
###Response:","We extended the unsupervised corpus-extracted phrase approximation method of Guevara #OTHEREFR and #REFR to estimate all known state-of-the-art cDSMs, using closedform solutions or simple iterative procedures in all cases.","Drawing on the adjective-noun composition approach proposed by the cited author, who represents nouns as vectors and adjectives as matrices, we developed our own evaluation methodology #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents an approach to normalize documents in constrained domains. This approach reuses resources developed for controlled document authoring and is decomposed into three phases. First, candidate content representations for an input document are automatically built. Then, the content representation that best corresponds to the document according to an expert of the class of documents is identified. This content representation is finally used to generate the normalized version of the document. The current version of our prototype system is presented, and its limitations are discussed.
###Abstract of the cited paper: Tiffs lmper concerns an approach to Machine Translation whieJJ differs from the typical istandard' approaches crucially in.that it does not rely on the prior existence of a source text as a basis of the translation. Our approach can be characterised as an 'intelligent secretary with knowledge of the foreign language', which helps monolingual users to formulate the desired target-language text in the context of a (keyboard) dialogue translation systems. Keywords: Machine translation; natural language interface; dialogue
###Context text: The authoring of documents in constrained domains and their translation into other languages is a very important activity in industrial settings. In some cases, the distinction between technical writers and technical translators has started to blur, so as to minimize the time and efforts needed to obtain multilingual documents.[Citation]A related paradigm emerged in the 90s #OTHEREFR, whereby a technical author is responsible for providing the content of a document and a generation system produces multilingual versions of it. Updating documents is then done by updating the document content, and only some postediting may take place instead of full translation by a human translator. Systems implementing this paradigm range from template-based multilingual document 1This is a reedition of the original article.
###Response:",The paradigm of translation for monolinguals introduced by Kay in 1973 #OTHEREFR; #REFR).,"This echoes the approach discussed by #REFR, where the translation process does not hinge on the pre-existence of a source document, but rather involves interactive content creation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a case study of the methodology of using information extracted from interlinear glossed text (IGT) to create of actual working HPSG grammar fragments using the Grammar Matrix focusing on one language: Chintang. Though the results are barely measurable in terms of coverage over running text, they nonetheless provide a proof of concept. Our experience report reflects on the ways in which this task is non-trivial and on mismatches between the assumptions of the methodology and the realities of IGT as produced in a large-scale field project.
###Abstract of the cited paper: In this paper we explore the potential for identifying computationally relevant typological features from a multilingual corpus of language data built from readily available language data collected off the Web. Our work builds on previous structural projection work, where we extend the work of projection to building individual CFGs for approximately 100 languages. We then use the CFGs to discover the values of typological parameters such as word order, the presence or absence of definite and indefinite determiners, etc. Our methods have the potential of being extended to many more languages and parameters, and can have significant effects on current research focused on tool and resource development for low-density languages and grammar induction from raw corpora.
###Context text: The canonical form of an IGT instance includes a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line (typically in a resource-rich language). The bootstrapping process starts with word alignment of the language line and translation line with the help of the gloss line. Then the translation line is parsed and the parse tree is projected to the language line using the alignments #OTHEREFR.[Citation]Our work extends this methodology to the construction of precision grammars.
###Response:",The projected trees can be used to answer linguistic questions such as word order #REFR or bootstrap parsers #OTHEREFR.,"Drawing on prior research, we identified typological features from multilingual corpus data to build the necessary CFGs for various languages #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Deterministic parsing has emerged as an effective alternative for complex parsing algorithms which search the entire search space to get the best probable parse tree. In this paper, we present an online large margin based training framework for deterministic parsing using Nivre?s Shift-Reduce parsing algorithm. Online training facilitates the use of high dimensional features without creating memory bottlenecks unlike the popular SVMs. We participated in the CoNLL Shared Task-2007 and evaluated our system for ten languages. We got an average multilingual labeled attachment score of 74.54 % (with 65.50% being the average and 80.32% the highest) and an average multilingual unlabeled attachment score of 80.30% (with 71.13% being the average and 86.55% the highest).
###Abstract of the cited paper: The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.
###Context text: The system with online learning and Nivre?s parsing algorithm was trained on the data released by CoNLL Shared Task Organizers for all the ten languages #OTHEREFR.[Citation]The evaluation metrics are Unlabeled Attachment Score(UAS) and Labeled Attachment Score(LAS). The results of our system with various feature models are listed in Table 11. The history information in ?3 contributed to a marginal improvement in accuracy of Hungarian, Italian and Turkish.
###Response:",We evaluated our system using the standard evaluation script provided by the organizers #REFR.,"The data utilized for the training was derived from multiple treebanks, each corresponding to a distinct language, meticulously created by the organizers of the 2007 CoNLL Shared task #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: While automatic metrics of translation quality are invaluable for machine translation research, deeper understanding of translation errors require more focused evaluations designed to target specific aspects of translation quality. We show that Word Sense Disambiguation (WSD) can be used to evaluate the quality of machine translation lexical choice, by applying a standard phrase-based SMT system on the SemEval2010 Cross-Lingual WSD task. This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system.
###Abstract of the cited paper: It is possible to reduce the bulk of phrasetables for Statistical Machine Translation using a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score. In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrasetable smoothing is employed.
###Context text: Table 5: Impact of translation candidate selection on PBSMT performance each source phrase in the phrase-table. The main PBSMT system uses t = 50 translation candidates per source phrase. Limiting that number to 20 and increasing it to 100 both have a very small impact on CLWSD.[Citation]This pruning strategy aims to drastically decrease the size of the phrase-table without degrading translation performance by removing noisy phrase pairs.
###Response:","Second, we prune the phrase-table using a statistical significance test to measure #REFR.","The process of reducing phrasetable bulk, as described by #REFR, aligns with this strategy to remove irrelevant phrase pairs while maintaining quality."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Many NLP applications rely on type systems to represent higher-level classes. Domain-specific ones are more informative, but have to be manually tailored to each task and domain, making them inflexible and expensive. We investigate a largely unsupervised approach to learning interpretable, domain-specific entity types from unlabeled text. It assumes that any common noun in a domain can function as potential entity type, and uses those nouns as hidden variables in a HMM. To constrain training, it extracts co-occurrence dictionaries of entities and common nouns from the data. We evaluate the learned types by measuring their prediction accuracy for verb arguments in several domains. The results suggest that it is possible to learn domain-specific entity types from unlabeled data. We show significant improvements over an informed baseline, reducing the error rate by 56%.
###Abstract of the cited paper: We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.
###Context text: Alternatively, unsupervised approaches #OTHEREFR can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Furthermore, it requires us to define the number of clusters beforehand. Ideally, we would like to learn domain-specific types directly from data.[Citation]Recently, Hovy et al #OTHEREFR proposed an approach that uses co-occurrence patterns to find entity type candidates, and then learns their applicability to relation arguments by using them as latent variables in a first-order HMM. However, they only evaluate their method using human sensibility judgements for one domain. While this shows that the types are coherent, it does not tell us much about their applicability.
###Response:","To this end, patternbased approaches have long been used to induce type systems #REFR.","Echoing the findings presented by #REFR, we incorporate a method which autonomously learns hyponymy lexical relationships from unrestricted text, demonstrating extensive applicability."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper discusses the establishment and implementation of a curriculum for teaching NLP. At the core are two classes which involve some theoretical background, extensive hands-on experience with state-ofthe-art technologies, and practical application in the form of an intensive programming project. Issues involving interdisciplinary coordination, curriculum design, and challenges in teaching this discipline are discussed.
###Abstract of the cited paper: Automatic part of speech tagging is an area of natural anguage processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically earns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.
###Context text: Students from the CS department have access to CS and Linguistics servers where class-related resources can be used. Students also have access to the campus supercomputer when necessary for NLP projects, under the instructor?s supervision. Sample non-trivialhands-on and programming assignments are given weekly.[Citation]Tutorials are provided for for any necessary remedial work that the student might need or desire in such topics as using the Emacs editor, using Unix shell scripts, or writing Perl or Tcl scripts. Final project: A final programming project is required, similar in scope to that described above for the humanities course: close coordination with the instructor, meeting milestones, documenting and demonstrating the final product, and producing a write-up of the significance and contributions of the result. Of course, a much higher standard is required of these advanced students.
###Response:","They include such topics as: work with various corpus manipulation and annotation tools, use of various POS taggers and their comparison #REFR, development of morphophonological rules in PC-Kimmo #OTHEREFR.","Emphasizing the importance of practical applications, the course utilizes rule-based part of speech tagging techniques outlined in #REFR for programming projects."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English. Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language.
###Abstract of the cited paper: This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.
###Context text: Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news #OTHEREFR. While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons.[Citation]In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification. This is reported in studies of manual annotation of phrases #OTHEREFR. Second, an NLP application may seek a wide range of types of subjectivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments.
###Response:","First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity #OTHEREFR; #REFR.","The method for analyzing subjectivity proposed by #REFR, which distinguishes neutral and polar expressions before determining their polarity, influenced our focus on subjectivity classification."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.
###Abstract of the cited paper: We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.
###Context text: Transition-based dependency parsing #OTHEREFR utilize a deterministic shift-reduce process for making structural predictions.[Citation]Recent research has addressed two potential disadvantages of systems like MaltParser. In the aspect of decoding, beam-search #OTHEREFR have been applied to improve upon greedy one-best search, and positive results were reported. In the aspect of training, global structural learning has been used to replace local learning on each decision #OTHEREFR, although the effect of global learning has not been separated out and studied alone.
###Response:","Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local features, as exemplified by the comparison between MaltParser and MSTParser #OTHEREFR; #REFR.","In a comparative analysis of models in data-driven dependency parsing, different types of errors produced by each model were identified, offering new opportunities for parser improvement #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Annotated corpora play a significant role in many NLP applications. However, annotation by humans is time-consuming and costly. In this paper, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes. We demonstrate the effectiveness of our approach in the context of one form of unbalanced task: annotation of transcribed human-human dialogues for presence/absence of uncertainty. In two data sets, our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy. The method is able to reduce human annotation effort by about 80% without a significant loss in data quality, as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations.
###Abstract of the cited paper: It is not always clear how the differences in intrinsic evaluation metrics for a parser or classifier will affect the performance of the system that uses it. We investigate the relationship between the intrinsic evaluation scores of an interpretation component in a tutorial dialogue system and the learning outcomes in an experiment with human users. Following the PARADISE methodology, we use multiple linear regression to build predictive models of learning gain, an important objective outcome metric in tutorial dialogue. We show that standard intrinsic metrics such as F-score alone do not predict the outcomes well. However, we can build predictive performance functions that account for up to 50% of the variance in learning gain by combining features based on standard evaluation scores and on the confusion matrix entries. We argue that building such predictive models can help us better evaluate performance of NLP components that cannot be distinguished based on F-score alone, and illustrate our approach by comparing the current interpretation component in the system to a new classifier trained on the evaluation data.
###Context text: To obtain a higher quality annotated corpus, it is necessary to spend more time and money on data annotation. For this reason, one often has to accept some tradeoff between data quality and human effort. A significant proportion of corpora are unbalanced, where the distribution of class categories are heavily skewed towards one or a few categories.[Citation]In this paper, we present a semi-automated annotation method that can reduce annotation effort for the class of binary unbalanced corpora. Here is our proposed annotation scheme: the first step is to build a high-recall classifier with some initial annotated data with an acceptable accuracy via a cost-sensitive approach. The second step is to apply this classifier to the rest of the unlabeled data, where the data are then classified with positive or negative labels.
###Response:","Unbalanced corpora are common in a number of different tasks, such as emotion detection #OTHEREFR; #REFR, text classification #OTHEREFR, and so on1.","Reflecting the exploration of the relationship between intrinsic evaluation scores and learning outcomes in the tutorial dialogue system by #REFR, we have implemented this consideration into our methodology."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences. This paper further analyzes aspects of learning that impact performance. We argue that previously proposed approaches of training a Human- Likeness classifier is not as well correlated with human judgments of translation quality, but that regression-based learning produces more reliable metrics. We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics.
###Abstract of the cited paper: This paper studies the impact of paraphrases on the accuracy of automatic evaluation. Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method in the context of machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation. We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
###Context text: In addition to the well-known word error rate #OTHEREFR. Second, similarity can be expressed in terms of common word sequences. Since the introduction of BLEU #OTHEREFR the basic n-gram precision idea has been augmented in a number of ways.[Citation]Finally, researchers have begun to look for similarities at a deeper structural level. For example, Liu and Gildea #OTHEREFR developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. With this wide array of metrics to choose from, MT developers need a way to evaluate them.
###Response:",Metrics in the Rouge family allow for skip n-grams #OTHEREFRa); #REFR take paraphrasing into account; metrics such as METEOR #OTHEREFR in that word class information is used.,"Incorporating the findings from the referenced study, another augmentation is using paraphrasing techniques in machine translation evaluation, in order to refine the accuracy of automatic evaluation #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Modern statistical machine translation systems may be seen as using two components: feature extraction, that summarizes information about the translation, and a log-linear framework to combine features. In this paper, we propose to relax the linearity constraints on the combination, and hence relaxing constraints of monotonicity and independence of feature functions. We expand features into a non-parametric, non-linear, and high-dimensional space. We extend empirical Bayes reward training of model parameters to meta parameters of feature generation. In effect, this allows us to trade away some human expert feature design for data. Preliminary results on a standard task show an encouraging improvement.
###Abstract of the cited paper: We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation.
###Context text: In recent years, statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation #OTHEREFR. The conditional log-linear feature combination framework #OTHEREFR is remarkably simple and effective in practice.[Citation]Because of their simplicity, however, log-linear models impose some constraints on how new information may be inserted into the system to achieve the best results. In other words, new information needs to be parameterized carefully into one or more real valued feature functions. Therefore, that requires some human knowledge and understanding.
###Response:","Therefore, recent efforts #REFR have concentrated on feature design ? wherein more intelligent features may be added.",Our study challenges these constraints by exploring non-linear feature combinations which provides a more flexible framework for machine translation as explored in #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Usually unsupervised dependency parsing tries to optimize the probability of a corpus by modifying the dependency model that was presumably used to generate the corpus. In this article we explore a different view in which a dependency structure is among other things a partial order on the nodes in terms of centrality or saliency. Under this assumption we model the partial order directly and derive dependency trees from this order. The result is an approach to unsupervised dependency parsing that is very different from standard ones in that it requires no training data. Each sentence induces a model from which the parse is read off. Our approach is evaluated on data from 12 different languages. Two scenarios are considered: a scenario in which information about part-of-speech is available, and a scenario in which parsing relies only on word forms and distributional clusters. Our approach is competitive to state-of-the-art in both scenarios.
###Abstract of the cited paper: We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages. Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages. During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also automatically refine the syntactic categories given in our coarsely tagged input. Across six languages our approach outperforms state-of-theart unsupervised methods by a significant margin.1
###Context text: Our results are on average significantly better than DMV PR-AS 140 (2.5%), and better than DMV PR- AS 140 on 8/12 languages. E-DMV PR-AS 140 is slightly better than our model on average (1.3%), but we still obtain better results on 6/12 languages. Our results are a lot better than IBM-M3.[Citation]
###Response:","#REFR report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish.","The success of our model could be attributed to the integration of language-independent rules commonly found across multiple languages, as described in #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. While most SMT systems use unsupervised algorithms (e.g. GIZA++) for training word alignment, supervised methods, which exploit a small amount of human-aligned data, have become increasingly popular recently. This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. We used two unsupervised aligners, GIZA++ and HMM, and one supervised aligner, ITG, in this study. To avoid language and genre specific conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). Results show that the two classes of algorithms achieve the same level of MT performance. Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. Significant improvements (around 1.0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. The improvements are consistent across languages and genres.
###Abstract of the cited paper: We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.
###Context text: [Citation]GIZA++ is an unsupervised method based on models 1-5 of Brown et al #OTHEREFR. Given a sentence pair e ? f , it seeks the alignment a that maximizes the probability P (f, a|e). As in most previous studies using GIZA++, we ran GIZA++ in both directions, from e to f and from f to e, and symmetrized the bidirectional alignments into one, using a method similar to the grow-diagonal-final method described in Och and Ney #OTHEREFR.
###Response:","We used three aligners in this work: GIZA++ #OTHEREFR, jointly trained HMM #REFR, and ITG #OTHEREFR.","The unsupervised alignment method employed by GIZA++, along with its bidirectional application and subsequent symmetrization, aligns closely with the models put forth by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition (NER) can outperform existing approaches that handle non-local dependencies, while being much more computationally efficient. NER systems typically use sequence models for tractable inference, but this makes them unable to capture the long distance structure present in text. We use a Conditional Random Field (CRF) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF. Using features capturing non-local dependencies from the same document, our approach yields a 12.6% relative error reduction on the F1 score, over state-of-theart NER systems using local-information alone, when compared to the 9.3% relative error reduction offered by the best systems that exploit non-local information. Our approach also makes it easy to incorporate non-local information from other documents in the test corpus, and this gives us a 13.3% error reduction over NER systems using local-information alone. Additionally, our running time for inference is just the inference time of two sequential CRFs, which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference.
###Abstract of the cited paper: This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentencebased classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.
###Context text: Malouf #OTHEREFR condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a previous sentence of the same document. This violates the Markov property and therefore instead of finding the maximum likelihood sequence over the entire document (exact inference), they label one sentence at a time, which allows them to condition on the maximum likelihood sequence of previous sentences. While this approach is quite effective for enforcing label consistency in many NLP tasks, it permits a forward flow of information only, which can result in loss of valuable information.[Citation]This approach has the advantage of allowing the training procedure to automatically learn good weights for these ?global? features relative to the local ones. However, it is hard to extend this to incorporate other types of non-local structure.
###Response:","#REFR propose a solution to this problem: for each token, they define additional features based on known information, taken from other occurrences of the same token in the document.","In contrast, our method employs a two-stage process using a CRF based NER system and combines local and non-local information, outperforming such limited scope approaches like those presented by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a semantic class prediction model of Chinese twocharacter compound words based on a character ontology, which is set to be a feasible conceptual knowledge resource grounded in Chinese characters. The experiment we conduct yields satisfactory results which turn out to be that the task of semantic prediction of two-character words could be greatly facilitated using Chinese characters as a knowledge resource.
###Abstract of the cited paper: Tim paper describes a similarity-based model to present he morphological rules for Chinese compound nouns. This representation model serves functions of 1) as the morphological rules of the compounds, 2) as a mean to evaluate the properness of a compound construction, and 3) as a mean to disambiguate the semantic ambiguity of the nlorphological head of a compound noun. An automatic semantic lassil'ication system for Chinese unknown compounds i thus implemented based on the model. Experiments and on'or analyses arc also presented.
###Context text: So far as we know, no evaluation in the previous works was done. This might be due to many reasons: (1) the different scale of experiment (how many words are in the test data?), (2) the selection of syntactic category (VV, VN or NN?) of morphemic components, and (3) the number of morphemic components involved (two or threecharacter words?).. etc. Hence it is difficult to compare our results to other models.[Citation]
###Response:","Among the current similar works, Table 4 shows that our system outperforms Chen#OTHEREFR in VV compounds, and approximates the #REFR in NN compounds.","One comparable research is the one by #REFR, which presented a model dealing with morphological rules and semantic ambiguity for Chinese compound nouns."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe an algorithm for recovering non-local dependencies in syntactic dependency structures. The patternmatching approach proposed by Johnson (2002) for a similar task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in (Johnson, 2002).
###Abstract of the cited paper: Alter presenting a novel O(n a) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical atfinity mode\] where words struggle to modify each other, (b) a sense tagging model where words tluctuate randomly in their selectional preferences, and (e) a. generative model where the speaker fleshes ()tit each word is syntactic and concep{.ual structure without regard to the implications :for the hearer. W(! also give preliminary empirical results from evaluating the three models' p;Lrsing performance on annotated Wall Street Journal trMning text (derived fi'om the Penn Treebank). in these results, the generative model performs significantly better than the others, and does about equally well at assigning pa.rtof-speech tags.
###Context text: This simple heuristics does not allow us to handle all reduced relative clauses, because some of them correspond to PPs or NPs rather than VPs, but the latter are quite rare in the Treebank. The second important change to Buchholz? script concerns the structure of VPs. For every verb cluster, we choose the main verb as the head of the cluster, and leave modal and auxiliary verbs as dependents of the main verb.[Citation]As will be described below, this allows us to ?factor out? tense and modality of finite clauses from our patterns, making the patterns more general.
###Response:",A similar modification was used by #REFR for the study of dependency parsing models.,"In harmony with the novel parsing algorithm presented by #REFR, this adjustment caters to the syntactic structure of VPs, enhancing pattern generality."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present wo approaches for syntactic and semantic transfer based on LFG f-structures and compare the results with existing co-description and restriction operator based approaches, focusing on aspects of ambiguity preserving transfer, complex cases of syntactic structural mismatches as well as on modularity and reusability. The two transfer approaches are interfaced with an existing, implemented transfer component (Verbmobi1), by translating f-structures into a term language, and by interfacing fstructure representations with an existing semantic based transfer approach, respectively.
###Abstract of the cited paper: This article presents a new semanticbased transfer approach developed and applied within the Verbmobil Machine Translation project. We give an overview of the declarative transfer formalism together with its procedural realization. Our approach is discussed and compared with several other approaches from the MT literature. The results presented in this article have been implemented and integrated into the Verbmobil system.
###Context text: We have presented two alternative architectures for transfer in LFG.[Citation]In the case of syntactic transfer, transfer is defined on term representations of f-structures. In the case of semantic transfer, transfer is defined on UDRS translations of f-structures. F- structure, term and UDRS correspondences are defined in the Appendix.
###Response:","In both cases, transfer is driven by the transfer module developed and implemented by #REFRa).",Our methodologies draw from the semantic-based transfer approach developed in the Verbmobil Machine Translation project [#REFR].
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a named entity recognition (NER) system for extracting product attributes and values from listing titles. Information extraction from short listing titles present a unique challenge, with the lack of informative context and grammatical structure. In this work, we combine supervised NER with bootstrapping to expand the seed list, and output normalized results. Focusing on listings from eBay?s clothing and shoes categories, our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.
###Abstract of the cited paper: Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE?s novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.
###Context text: Traditional named entity recognition #OTHEREFR. Recently there has been a surge of interest in extracting product attributes from online data due to the rapid growth of E-Commerce. Current work in this domain focuses on mining product reviews and descriptions from retailer websites.[Citation]In this paper, we focus on mining short product listing titles, which poses unique challenges. Short listings are typical in classified ads where each seller is given limited space (in terms of words) to describe the product. On eBay, product listing titles cannot exceed 55 characters in length.
###Response:",Such text data tend to be long and generate enough context for the target task #OTHEREFR; #REFR.,"In contrast to the OPINE system which mines reviews for product features #REFR, our work emphasizes on extracting product attributes from compact listing titles."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This study is conducted in the area of multidocument summarization, and develops a literature review framework based on a deconstruction of human-written literature review sections in information science research papers. The first part of the study presents the results of a multi-level discourse analysis to investigate their discourse and content characteristics. These findings were incorporated into a framework for literature reviews, focusing on their macro-level document structure and the sentence-level templates, as well as the information summarization strategies. The second part of this study discusses insights from this analysis, and how the framework can be adapted to automatic summaries resembling human written literature reviews. Summaries generated from a partial implementation are evaluated against human written summaries and assessors? comments are discussed to formulate recommendations for future work.
###Abstract of the cited paper: We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference/journal papers) as input, a related work summarization system creates a topic-biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in set of keywords arranged in a hierarchical fashion that describes a target paper?s topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation.
###Context text: Some other scientific summarization systems aim to model information relationships accurately without concerning themselves with summary structure. Centrifuser, a framework for summarizing medical literature #OTHEREFR produced a multidocument, query-focused indicative summary highlighting the similarities and differences between source documents. The topic tree for the final summary was constructed offline by clustering a large number of documents, thus it was not suitable for real-time user queries.[Citation]To sum up, these scientific summarization systems are typically delimited by their scalability and generalizability for multiple documents and domains. Newer approaches in scientific paper summarization rely on preselected information cited in other papers to judge whether information is influential or not, and generate a multi-document summary of a topic #OTHEREFR. A system for generating literature surveys through citations was proposed by Mohammad et al#OTHEREFR which applied superficial analysis of research paper citation sentences to suggest model sentences; the present study describes parallel efforts to refine a summarization framework after extensive discourse analysis.
###Response:","In a related recent approach, #REFR presented preliminary results from automatically generating related work sections for a target paper by taking a hierarchical topic tree as an input; however, the requirement of a pre-conceived topic tree limits the scalability of this system.","In a similar vein, the ReWoS system introduced the concept of automated related work summarization for specific target papers, demonstrating a potential expansion of the summarization field and its methodologies #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We explore the intersection of rule-based and statistical approaches in machine translation, with a particular focus on past and current work here at Microsoft Research. Until about ten years ago, the only machine translation systems worth using were rule-based and linguistically-informed. Along came statistical approaches, which use large corpora to directly guide translations toward expressions people would actually say. Rather than making local decisions when writing and conditioning rules, goodness of translation was modeled numerically and free parameters were selected to optimize that goodness. This led to huge improvements in translation quality as more and more data was consumed. By necessity, the pendulum is swinging towards the inclusion of linguistic features in MT systems. We describe some of our statistical and non-statistical attempts to incorporate linguistic insights into machine translation systems, showing what is currently working well, and what isn?t. We also look at trade-offs in using linguistic knowledge (?rules?) in preor post-processing by language pair, with a particular eye on the return on investment as training data increases in size.
###Abstract of the cited paper: This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.
###Context text: The point of data cleaning heuristics is to increase the value of training data. Each data point that is noisy increases the chance of learning something that could be distracting or harmful. Likewise, each data point that is cleaned reduces the level of data sparsity (e.g., through normalizations or substitutions) and improves the chances that the models will be more robust.[Citation]Crucially, most data munging is done through heuristics, or rules, although thresholds or constraints can be tuned by data. A more sophisticated example of data cleaning is described in #OTHEREFR where the authors used machine learning methods for measuring quality estimation to select the ?best? portions of a corpus. So, rather than training their SMT on an entire corpus, they trained an estimator that selected the best portions, and used only those.
###Response:","Although it has been shown that increasing the amount of training data for SMT improves results #REFR, not all data is beneficial, and clean data is best of all.","In a similar pursuit of refining translation systems, #REFR has demonstrated how extensive statistical language modeling, operating on large volumes of data, effectively improves machine translation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In recent years, sentiment analysis in social media has attracted a lot of research interest and has been used for a number of applications. Unfortunately, research has been hindered by the lack of suitable datasets, complicating the comparison between approaches. To address this issue, we have proposed SemEval-2013 Task 2: Sentiment Analysis in Twitter, which included two subtasks: A, an expression-level subtask, and B, a messagelevel subtask. We used crowdsourcing on Amazon Mechanical Turk to label a large Twitter training dataset alg with additional test sets of Twitter and SMS messages for both subtasks. All datasets used in the evaluation are released to the research community. The task attracted significant interest and a total of 149 submissions from 44 teams. The bestperforming team achieved an F1 of 88.9% and 69% for subtasks A and B, respectively.
###Abstract of the cited paper: People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms cotraining, increasing F1 by 25% over ten common entity types. Our NLP tools are available at: http:// github.com/aritter/twitter_nlp
###Context text: Twitter is the most common micro-blogging site on the Web, and we used it to gather tweets that express sentiment about popular topics.[Citation]Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective, positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark the position of its start and end in the text boxes below.
###Response:","We first extracted named entities using a Twitter-tuned NER system #REFR from millions of tweets, which we collected over a one-year period spanning from January 2012 to January 2013; we used the public streaming Twitter API to download tweets.","Our method of gathering sentiment-expressing tweets was informed by the approach outlined in #REFR, which effectively captures the informal language commonly encountered on Twitter."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Word alignment plays a central role in statistical MT (SMT) since almost all SMT systems extract translation rules from word aligned parallel training data. While most SMT systems use unsupervised algorithms (e.g. GIZA++) for training word alignment, supervised methods, which exploit a small amount of human-aligned data, have become increasingly popular recently. This work empirically studies the performance of these two classes of alignment algorithms and explores strategies to combine them to improve overall system performance. We used two unsupervised aligners, GIZA++ and HMM, and one supervised aligner, ITG, in this study. To avoid language and genre specific conclusions, we ran experiments on test sets consisting of two language pairs (Chinese-to-English and Arabicto-English) and two genres (newswire and weblog). Results show that the two classes of algorithms achieve the same level of MT performance. Modest improvements were achieved by taking the union of the translation grammars extracted from different alignments. Significant improvements (around 1.0 in BLEU) were achieved by combining outputs of different systems trained with different alignments. The improvements are consistent across languages and genres.
###Abstract of the cited paper: This paper presents a new approach to combining outputs of existing word alignment systems. Each alignment link is represented with a set of feature functions extracted from linguistic features and input alignments. These features are used as the basis of alignment decisions made by a maximum entropy approach. The learning method has been evaluated on three language pairs, yielding significant improvements over input alignments and three heuristic combination methods. The impact of word alignment on MT quality is investigated, using a phrase-based MT system.
###Context text: The other method is to combine the outputs of different MT systems trained using different aligners. Assuming different systems make independent errors, system combination can generate a better translation than those of individual systems through voting #OTHEREFR. Our work differs from previous work in two ways.[Citation]In contrast, our work is based on the assumption that perfect word alignment is impossible due to the intrinsic difficulty of the problem, and it is more effective to resolve translation ambiguities at later stages of the MT pipeline. A main focus of much previous work on word alignments is on theoretical aspects of the proposed algorithms. In contrast, the nature of this work is purely empirical.
###Response:","Past studies of combining alternative alignments focused on minimizing alignment errors, usually by merging alternative alignments for a sentence pair into a single alignment with the fewest number of incorrect alignment links #REFR.","We build upon the approach popularized by #REFR, which emphasizes the use of linguistic feature extraction and maximum entropy to improve word alignment outputs."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The lack of large amounts of readily available, explicitly represented knowledge has long been recognized as a barrier to applications requiring semantic knowledge such as machine translation and question answering. This problem is analogous to that facing machine translation decades ago, where one proposed solution was to use human translators to post-edit automatically produced, low quality translations rather than expect a computer to independently create high-quality translations. This paper describes an attempt at implementing a semantic parser that takes unrestricted English text, uses publically available computational linguistics tools and lexical resources and as output produces semantic triples which can be used in a variety of tasks such as generating knowledge bases, providing raw material for question answering systems, or creating RDF structures. We describe the TEXTCAP system, detail the semantic triple representation it produces, illustrate step by step how TEXTCAP processes a short text, and use its results on unseen texts to discuss the amount of post-editing that might be realistically required.
###Abstract of the cited paper: We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.
###Context text: Word sense disambiguation #OTHEREFR, often based on term frequency analyses of large annotated corpora, can help localize search in a particular area of a knowledge base to find the most related concepts and instances. Semantic role labelers #OTHEREFR annotate what role each entity has in relation to its local man verb, and can provide additional clues for disambiguating words and locating them in an ontological space. In addition to lexical and semantic tasks, multi-sentence linguistic analysis such as discourse segmentation and parsing is needed to semantically label the roles of verb phrases in relation to one other.[Citation]
###Response:",#REFR presented a statistical system that automatically produces an analysis of the rhetorical structure that holds between sets of sentences or clauses at the paragraph level.,The need for discourse segmentation and parsing further establishes the relevance of probabilistic models for identifying discourse units and building sentence-level discourse trees as introduced by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we first identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to generate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach.
###Abstract of the cited paper: This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.
###Context text: The relevance score on each node is determined by a SVM classifier. Multiple SVM classifiers are learned on the hierarchy, one distinct classifier for a node. The reviews that are stored in the node and its child-nodes are used as training samples.[Citation]Classifying the opinions: Given a set of testing questions, we first distinguish the opinion questions from the factual ones #OTHEREFR. Since the opinion questions often contain one or more sentiment terms, we classify them by employing the sentiment terms in the sentiment lexicon provided from MPQA project #OTHEREFR. Subsequently, we learn a SVM sentiment classifier to determine the opinion polarity of the opinion questions.
###Response:","We employ the features of noun terms, and sentiment terms in the sentiment lexicon provided by MPQA project #REFR.","The sentiment analysis on those opinion questions is inspired by a technique proposed by #REFR, which determines initial expression polarity before disambiguating it."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Translation systems are complex, and most metrics do little to pinpoint causes of error or isolate system differences. We use a simple technique to discover induction errors, which occur when good translations are absent from model search spaces. Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the search spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences.
###Abstract of the cited paper: Translation model size is growing at a pace that outstrips improvements in computing power, and this hinders research on many interesting models. We show how an algorithmic scaling technique can be used to easily handle very large models. Using this technique, we explore several large model variants and show an improvement
###Context text: [Citation]A ruleset licenses the steps by which a source string f1...fI may be rewritten as a target string e1...eJ . A parameterization defines a weight function over every sequence of rule applications. In a phrase-based model, the ruleset is simply the unweighted phrase table, where each phrase pair fi...fi?/ej ...ej? states that phrase fi...fi? in the source can be rewritten as ej ...ej? in the target.
###Response:","A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization #REFR.","In handling larger translation models, an algorithmic scaling technique has been proven effective, showcasing improvements with several model variants #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: To support empirical study of online privacy policies, as well as tools for users with privacy concerns, we consider the problem of aligning sections of a thousand policy documents, based on the issues they address. We apply an unsupervised HMM; in two new (and reusable) evaluations, we find the approach more effective than clustering and topic models.
###Abstract of the cited paper: We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.
###Context text: Given the corpus of privacy policies described in ?2, we designed a model to efficiently infer an alignment of policy sections. While we expect that different kinds of websites will likely address different privacy issues, we believe that many policies will discuss roughly the same set of issues. Aligning the policies is a first step in a larger effort to #OTHEREFR.[Citation]We estimate an HMM-like model on our corpus, exploiting similarity across privacy policies to the extent it is evident in the data. In our formulation, each hidden state corresponds to an issue or topic, characterized by a distribution over words and bigrams appearing in privacy policy sections addressing that issue. The transition distribution captures tendencies of privacy policy authors to organize these sections in similar orders, though with some variation.
###Response:","We are inspired by multiple sequence alignment methods in computational biology #OTHEREFR and by #REFR, who described a hidden Markov model (HMM) for document content where each state corresponds to a distinct topic and generates sentences relevant to that topic according to a language model.","After evaluating various models, we implemented an unsupervised Hidden Markov Model suggested by #REFR, utilizing its efficiency in organizing diverse topics and maintaining content order."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a general approach for translating Chinese unknown words (UNK) for SMT. This approach takes advantage of the properties of Chinese word composition rules, i.e., all Chinese words are formed by sequential characters. According to the proposed approach, the unknown word is re-split into a subword sequence followed by subword translation with a subwordbased translation model. ?Subword? is a unit between character and long word. We found the proposed approach significantly improved translation quality on the test data of NIST MT04 and MT05. We also found that the translation quality was further improved if we applied named entity translation to translate parts of unknown words before using the subword-based translation.
###Abstract of the cited paper: We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level. The model is evaluated on the Europarl corpus for German-English and Finnish- English translation and shows improvements over state-of-the-art phrase-based models.
###Context text: Of course, this simple way produces a lower quality of translations if there are a lot of UNKs in the test data, especially for using a Chinese word segmenter that produces many UNKs. The translation of UNKs need to be solved by a special method. The translation of Chinese unknown words seems more difficult than other languages because Chinese language is a non-inflected language.[Citation]Using machine transliteration can resolve part of UNK translation #OTHEREFR. But this approach is effective for translating phonetically related unknown words, not for other types. No unified approach for translating Chinese unknown words has been proposed.
###Response:","Unlike other languages #REFR, Chinese UNK translation cannot use information from stem and inflection analysis.","Considering the challenge of Chinese UNKs, we adopted a novel method using Chinese word composition rules and subword translation, inspired by the backoff model for translation of unseen word forms #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role of category relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.
###Abstract of the cited paper: In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods. In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
###Context text: Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels.[Citation]In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143).
###Response:","For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. #OTHEREFR; #REFRa), can be applied.",Our approach draws from the demonstrated utility of tree kernels in automatic learning of natural language applications as shown by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Subjectivity word sense disambiguation (SWSD) is automatically determining which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD has been shown to improve the performance of contextual opinion analysis, but only on a small scale and using manually developed integration rules. In this paper, we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis.
###Abstract of the cited paper: This paper investigates a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. We provide empirical evidence that SWSD is more feasible than full word sense disambiguation, and that it can be exploited to improve the performance of contextual subjectivity and sentiment analysis systems.
###Context text: [Citation]We train a different supervised SWSD classifier for each target word separately. This means the overall SWSD system consists of as many SWSD classifiers as there are target words. We utilize the same machine learning features as in #OTHEREFR, which are commonly used in Word Sense Disambiguation (WSD).
###Response:",We follow the same approach as in #REFR to build our SWSD system.,"Drawing on the work of #REFR, we employ the concept of Subjectivity Word Sense Disambiguation (SWSD) to automatically differentiate between subjective and objective use of words."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a novel method for Japanese dependency analysis, which is usually reduced to the construction of a dependency tree. In deterministic approaches to this task, dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed. Conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent-child relation or not. However, tree structures include relations between two nodes other than the parent-child relation. Therefore, we use ancestor-descendant relations in addition to parent-child relations, so that the added redundancy helps errors be corrected. Experimental results show that the proposed method achieves higher accuracy.
###Abstract of the cited paper: A deternfinistic finite state transducer is a fast device fbr analyzing strings. It takes O(n) time to analyze a string of length n. In this 1)al)er, an application of this technique to Japanese dependency analysis will be described. We achieved the speed at; a small cost in accuracy. It takes about 0.1.7 nfillisecond to analyze one sentence (average length is 10 bunsetsu, 1)ased on Pent:tahiti 650MHz PC, Linux) and we actually observed the analysis time to be proportional to the sentence length. Thb accuracy is about; 81% even though very little lexical information is used. This is about 17% and 9% better than the default and a simple system, respectively. We believe the gap between our pertbrmm:ce and the best current 1)erforlnm:ce on the stone task, about 7%, can be filled by introducing lexical or sen:antic infornmtion.
###Context text: We used Kyoto University text corpus #OTHEREFR for training and test data. The articles on January 1st through 8th (7,958 sentences) were used as training data, and the articles on January 9th (1,246 sentences) as test data.[Citation]We used SVMs as the algorithm of learning and analyzing the relations between nodes. We used the third degree polynomial kernel function and set the soft margin parameter C to 1, which is exactly the same setting as in Kudo and Matsumoto #OTHEREFR. We can obtain the real-valued score in step 3 of the algorithm, which is the output of the separating function.
###Response:",The dataset is the same as in leading works #REFR.,"Drawing from the method outlined in the cited work, our model uses deterministic finite state transducers for swift string analysis during Japanese dependency evaluation #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Chinese word segmentation (CWS) is a necessary step in Chinese-English statistical machine translation (SMT) and its performance has an impact on the results of SMT. However, there are many settings involved in creating a CWS system such as various specifications and CWS methods. This paper investigates the effect of these settings to SMT. We tested dictionarybased and CRF-based approaches and found there was no significant difference between the two in the qualty of the resulting translations. We also found the correlation between the CWS F-score and SMT BLEU score was very weak. This paper also proposes two methods of combining advantages of different specifications: a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated. We found these approaches were very effective in improving quality of translations.
###Abstract of the cited paper: We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.
###Context text: We used each of the training corpora provided to create a CWS scheme and evaluated the performance of the schemes on our test data. This enabled us to examine the effect of CWS specifications on SMT. We used a Chinese word segmentation tool, Achilles, to implement word segmentation.[Citation]The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff. Moreover, this tool meets our need to test the effect of the two kinds of CWS approaches for SMT. We can easily train a dictionary-based and a CRF-based CWS by using this tool.
###Response:",Part of the work using this tool was described by #REFR.,"Our segmentation tool, Achilles, incorporates the techniques proposed by #REFR, achieving superior performance in Chinese word segmentation through its subword-based tagging and confidence measure approach."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem, yet with many potential applications. In this paper, we exploit two independent observations about term translations: (a) terms are often formed by corresponding sub-lexical units across languages and (b) a term and its translation tend to appear in similar lexical context. Based on the first observation, we develop a new character n-gram compositional method, a logistic regression classifier, for learning a string similarity measure of term translations. According to the second observation, we use an existing context-based approach. For evaluation, we investigate the performance of compositional and context-based methods on: (a) similar and unrelated languages, (b) corpora of different degree of comparability and (c) the translation of frequent and rare terms. Finally, we combine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals.
###Abstract of the cited paper: We describe a machine learning approach, a Random Forest (RF) classifier, that is used to automatically compile bilingual dictionaries of technical terms from comparable corpora. We evaluate the RF classifier against a popular term alignment method, namely context vectors, and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accurately translate unknown terms. The dictionary extraction method described in this paper is freely available .
###Context text: Furthermore, context vector approaches are sensitive to the frequency of terms. For frequent terms, distributional semantics methods exhibit robust performance since the corresponding context is more informative. Chiao and Zweigenbaum #OTHEREFR reported an accuracy of 91% for the top 20 candidates when translating terms that occur 100 times or more.[Citation]Our work is more closely related to a second class of term alignment methods that exploits the internal structure of terms between a source and a target language. Compositional translation algorithms are based on the principal of compositionality #OTHEREFR, which claims that the translation of the whole is a function of the translation of its parts. Lexical #OTHEREFR compositional algorithms are knowledgerich approaches that proceed in two steps, namely generation and selection.
###Response:","However, the performance of context vectors drastically decreases for lower frequency terms #REFR.",The methodology outlined in our research differs from previous studies such as the application of the Random Forest classifier to compile bilingual dictionaries #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability?an extraneous variable that is seldom controlled for?on experimental outcomes, and make recommendations for reporting results more accurately.
###Abstract of the cited paper: Och?s (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time.
###Context text: However, they only briefly mention the implications of the instability on significance. Cer et al #OTHEREFR explored regularization of MERT to improve generalization on test sets.[Citation]Cer et al #OTHEREFR analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function.
###Response:",#REFR explored strategies for selecting better random ?restart points? in optimization.,"Building upon Och's 2003 research on statistical machine translation models, the advantages of using multiple randomized starting points in MERT were evaluated which outperformed the traditional model in all aspects #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents suggested semantic representations for different types of referring expressions in the format of Minimal Recursion Semantics and sketches syntactic analyses which can create them compositionally. We explore cross-linguistic harmonization of these representations, to promote interoperability and reusability of linguistic analyses. We follow Borthen and Haugereid (2005) in positing COG-ST (?cognitive status?) as a feature on the syntax-semantics interface to handle phenomena associated with definiteness. Our proposal helps to unify the treatments of definiteness markers, demonstratives, overt pronouns and null anaphora across languages. In languages with articles, they contribute an existential quantifier and the appropriate value for COG-ST. In other languages, the COG-ST value is determined by an affix. The contribution of demonstrative determiners is decomposed into a COG-ST value, a quantifier, and proximity information, each of which can be contributed by a different kind of grammatical construction in a given language. Along with COG-ST, we posit a feature that distinguishes between pronouns (and null anaphora) that are sensitive to the identity of the referent of their antecedent and those that are sensitive to its type.
###Abstract of the cited paper: We present a new way to simplify the construction of precise broad-coverage grammars, employing typologicallymotivated, customizable extensions to a language-independent core grammar. Each ?module? represents a salient dimension of cross-linguistic variation, and presents the grammar developer with simple choices that result in automatically generated language-specific software. We illustrate the approach for several phenomena and explore the interdependence of the modules.
###Context text: In future work, we plan to expand the range of these analyses to cover phenomena such as Irish demonstratives taking scope over coordinated noun phrases and crosslinguistic variation in the marking of generics as definite or indefinite. On the basis of these analyses, we plan to develop libraries for the Grammar Matrix customization system covering the topics discussed here.[Citation]On the basis of the user?s responses to the questionnaire, the customization system compiles a working starter grammar out of the Matrix core grammar and analyses stored in libraries. The new libraries will cover argument optionality (both general pro-drop and lexically-licensed), as well as demonstratives of different syntactic types (pronouns, determiners, adjectives and affixes), the marking of definiteness, and definiteness agreement. Acknowledgments We would like to thank Toshiyuki Ogihara, Laurie Poulson, Jeanette Gundel, Jennifer Arnold, Francesca Gola, and the reviewers for STEP 2008 for helpful comments and discussion.
###Response:",The Grammar Matrix customization system #REFR presents the linguist-user with a typological questionnaire which elicits information about the language to be described.,"In building these libraries, we will leverage the approach proposed by #REFR for constructing precise broad-coverage grammars with customizable extensions."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a model using associative processors (APs) for real-time spoken language translation. Spoken language translation requires (1) an accurate translation and (2) a realtime response. We have already proposed a model, TDMT (Transfer-Driven Machine Translation), that translates a sentence utilizing examples effectively and performs accurate structural disambiguation and target word selection. This paper will concentrate on the second requirement. In TDMT, example-retrieval (ER), i.e., retrieving examples most similar to an input expression, is the most dominant part of the total processing time. Our study has concluded that we only need to implement the ER for expressions including a frequent word on APs. Experimental results how that the ER can be drastically speeded up. Moreover, a study on communications between APs demonstrates the scalability against vocabulary size by extrapolation. Thus, our model, TDMT on APs, meets the vital requirements of spoken language translation.
###Abstract of the cited paper: Transfer-Driven Machine Translation (TDMT) is presented as a method which drives the translation processes according to the nature of the input. In TDMT, transfer knowledge is the central knowledge of translation, and various kinds aml levels of knowledge are cooperatively applied to input sentences. TDMT effectively utilizes an example-based framework for transfer and analysis knowledge. A consistent framework of examples makes the cooperation between transfer and analysis effective, and efficient ranslation is achieved. The TDMT prototype system, which translates Japanese spoken dialogs into English, has shown great promise.
###Context text: Each process must be accelerated in order to achieve real-time response. This paper focuses on the second process, spoken language translation, which requires (1) an accurate translation and (2) a real-time response. We have already proposed a model that utilizes examples and translates a sentence by combining pieces of transfer knowledge, i.e., target language xpressions that correspond to source language xpressions that cover the sentence jointly.[Citation]A prototype system of TDMT which translates a Japanese spoken sentence into English, has performed accurate structural disambiguation and target word selection 1. This paper will focus on the second requirement. First, we will outline TDMT and analyze its computational cost.
###Response:",The model is called Transfer- Driven Machine Translation (TDMT) #REFR (see subsection 2.1 for details).,"The method for this was developed in our previous work involving the application of Transfer-Driven Machine Translation (TDMT), as detailed by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Recent work has seen the emergence of a common framework for parsing categorial grammar (CG) formalisms that fall within the 'type-logical' tradition (such as the Lambek calculus and related systems), whereby some method of linear logic theorem proving is used in combination with a system of labelling that ensures only deductions appropriate to the relevant grammatical ogic are allowed. The approaches realising this framework, however, have not so far addressed the task of incremental parsing - - a key issue in earlier work with 'flexible' categorial grammars. In this paper, the approach of (Hepple, 1996) is modified to yield a linear deduction system that does allow flexible deduction and hence incremental processing, but that hence also suffers the problem of ispurious ambiguity'. This problem is avoided via normalisation.
###Abstract of the cited paper: Recent work within the field of Categorial Grammar has seen the development of approaches that allow different modes of logical behaviour to be displayed within a single system, something corresponding to making available differing modes of linguistic description. Earlier attempts to achieve this goal have employed modal operators called structural modalities, whose use presents a number of problems. I pro-. pose an alternative approach, involving coexistence and interrelation of different sublogics, that eliminates the need for structural modalities, whilst maintaining the descriptive power they provide.
###Context text: Hepple #OTHEREFR introduces a linear deduction method, involving compilation to first order formulae, which can be combined with various labelling disciplines. These approaches, however, are not directed toward incremental processing. In what follows, we show how the method of #OTHEREFR can be modified to allow processing which has a high degree of incrementality.[Citation]We show how this problem is solved via normalisation.
###Response:","These modifications, however, give a system which suffers 2See, for example, the formalisms developed in #OTHEREFR, #REFR. the problem of woulderivational equivalence', also called ispurious ambiguity', i.e. allowing multiple proofs which assign the same reading for some combination, a fact which threatens processing efficiency.",Adopting the alternative sublogics approach proposed in #REFR allows the elimination of structural modalities and enables a higher degree of incrementality.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A method for resolving the ellipses that appear in Japanese dialogues i proposed. This method resolves not only the subject ellipsis, but also those in object and other grammatical cases. In this approach, a machine-learning algorithm is used to select he attributes necessary for a resolution. A decision tree is built, and used as the actual ellipsis resolver. The results of blind tests have shown that the proposed method was able to provide a resolution accuracy of 91.7% for indirect objects, and 78.7% for subjects with a verb predicate. By investigating the decision tree we found that topic-dependent a tributes are necessary to obtain high performance resolution, and that indispensable attributes vary according to the grammatical case. The problem of data size relative to decision-tree training is also discussed.
###Abstract of the cited paper: This paper proposes a method to resolve the reference of deictic Japanese zero pronouns which can be implemented in a practical machine translation system. This method focuses on semantic and pragmatic onstraints uch as semantic constraints on cases, modal expressions, verbal semantic attributes and conjunctions to determine the deictic reference of Japanese zero pronouns. This method is highly effective because the volume of knowledge that must be prepared beforehand is not very large and its precision of resolution is good. This method was implemented in the Japanese-to-English machine translation system, ALT-J/E. According to a window test for 175 zero pronouns with deictic referent in a sentence set for the evaluation of Japaneseto-English machine translation systems, all of zero pronouns could be resolved consistently and correctly.
###Context text: However, its application to complex sentences has not been established because most studies have only investigated its effectiveness with successive simple sentences. Several studies of this problem have been made using the empirical approach. Among them, Murata and Nagao #OTHEREFR proposed a scoring approach where each constraint is manually scored with an estimation of possibility, and the resolution is conducted by totaling the points each candidate receives.[Citation]They claimed that 100% of the ellipses with exophoric referents could be resolved, but the experiment was a closed test with only a few samples. These approaches always require some effort to decide the scoring or the preference of provided constraints. Aone and Bennett #OTHEREFR applied a machinelearning technique to anaphora resolution in written texts.
###Response:","On the other hand, #REFR proposed a resolving algorithm for Japanese exophoric ellipses of written texts, utilizing semantic and pragmatic onstraints.","In contrast, our proposed method employs machine learning to resolve ellipses in various grammatical cases, borrowing from the approach used in reference resolution for Japanese zero pronouns #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Statistical part-of-st)eeeh(POS) taggers achieve high accuracy and robustness when based oil large, scale maimally tagged eorl)ora. Ilowever, enhancements of the learning models are necessary to achieve better 1)erforma.nce. We are develol)ing a learning tool for a Jalmnese morphological analyzer called Ch, aScn. Currently we use a fine-grained POS tag set with about 500 tags. To al)l)ly a normal trigram model on the tag set, we need unrealistic size of eorl)ora. Even, for a hi-gram model, we eanno~, 1)ret)are a ll loderate size of an mmotated corpus, when we take all the tags as distinct. A usual technique to Col)e with such fine-grained tags is to reduce the size of the tag set 1)y grouping the set of tags into equivalence classes. We introduce the concept of position-wise 9rouping where the tag set is t)artitioned into dill'el'lint equivalence classes at each t)osition in the. conditional 1)rohabilities in the Markov Model. Moreover, to eoi)e with the data Sl)arsen(?ss prot)lem caused 1) 3, exceptional t)henon> ena, we introduce several other techniques uch as word-level statistics, smoothing of word-level an(l P()S-level statistics and a selective tri-gram model. To help users determine probabilistic 1)arameters, we introduce an error-driven method for the pm'mneter selection. We then give results of exl)eriments to see the effect of the tools applied to an existing Jat)anese morphological nalyzer.
###Abstract of the cited paper: We present an implementation f a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.
###Context text: [Citation]Schmid used tile equivaleuce classes for smoothing. Their classes define not a partition of POS tags, but mixtures of some POS tags #OTHEREFR. Brill proposed a transfbrmation-based method.
###Response:",Cutting introduced grouping of words into equiva.lence classes based on the set of possible tags to reduce the number of the parameters #REFR .,"In their work, the authors displayed an effective implementation of a part-of-speech tagger grounded on a hidden Markov model #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classifier in target language. Starting from the union of small training data in target language and large translated examples in other languages, the Transfer AdaBoost algorithm is applied to iteratively reduce the influence of low quality translated examples. Alternatively, starting only from the training data in target language, the Transfer Self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set. These two algorithms are applied to sentenceand document-level cross lingual opinion analysis tasks, respectively. The evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data.
###Abstract of the cited paper: Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language. In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages. Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages. Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.
###Context text: Some CLOA works used bilingual dictionaries #OTHEREFR to align the expressions between source and target languages. These works are puzzled by the limited aligned opinion resources. Alternatively, some works used machine translation system to do the opinion expression alignment.[Citation]Wan #OTHEREFR combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. These works directly used all of the translation of annotated corpus in source language as the training data for target language without considering the following two problems: (1) the machine translation errors propagate to following CLOA procedure; (2) The annotated corpora from different languages are collected from different domains and different writing styles which lead the training and testing data having different feature spaces and distributions. Therefore, the performances of these supervised learning algorithms are affected.
###Response:",#REFR proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language.,"In consolidating subjectivity analysis across languages, research has shown the viability of utilizing machine translation to generate resources #REFR. This approach, however, is not without potential pitfalls."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we address the role of syntactic parsing for distributional similarity. On the one hand, we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers. On the other hand, we explore whether single unsupervised parsers, or their combination, can contribute to better distributional similarities, or even replace supervised parsing as a preprocessing step for word similarity. We evaluate distributional thesauri against manually created taxonomies both for English and German for five unsupervised parsers. While for English, a supervised parser is the best single parser in this evaluation, we find an unsupervised parser to work best for German. For both languages, we show significant improvements in word similarity when combining features from supervised and unsupervised parsers. To our knowledge, this is the first work where unsupervised parsers are systematically evaluated extrinsically in a semantic task, and the first work to show that unsupervised parsing can complement and even replace supervised parsing, when used as a pre-processing feature.
###Abstract of the cited paper: This work investigates the variation in a word?s distributionally nearest neighbours with respect to the similarity measure used. We identify one type of variation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations).
###Context text: Lin #OTHEREFR and Roget?s Thesaurus. Using WordNet, he defines context (synsets a word occurs in Wordnet or subsets when using Roget?s Thesaurus) and then builds a gold standard thesaurus using a similarity measure on these contexts. Then he evaluates his automatically computed Distributional Thesaurus (DT) with respect to the gold standard thesauri.[Citation]Curran #OTHEREFR created a gold standard thesaurus by manually extracting entries from several English thesauri for 70 words. His automatically generated DTs are evaluated against this gold standard thesaurus. All these systems employ context representations based on syntactic parsing for computing word similarity.
###Response:",#REFR evaluate various similarity measures based on 1000 frequent and 1000 infrequent target terms.,This work is in parallel with previous research which studied the variation in distributionally nearest neighbours of words based on similarity measures #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The use of semantic information to improve IR is a long-standing goal. This paper presents a novel Document Expansion method based on a WordNet-based system to find related concepts and words. Expansion words are indexed separately, and when combined with the regular index, they improve the results in three datasets over a state-of-the-art IR engine. Considering that many IR systems are not robust in the sense that they need careful finetuning and optimization of their parameters, we explored some parameter settings. The results show that our method is specially effective for realistic, non-optimal settings, adding robustness to the IR engine. We also explored the effect of document length, and show that our method is specially successful with shorter documents.
###Abstract of the cited paper: This paper presents and compares WordNetbased and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.
###Context text: For that, we use a technique based on random walks over the graph representation of WordNet concepts and relations. We represent WordNet as a graph as follows: graph nodes represent WordNet concepts (synsets) and dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges. We used version 3.0, with all relations provided, including the gloss relations.[Citation]Given a document and the graph-based representation of WordNet, we obtain a ranked list of WordNet concepts as follows:
###Response:",This was the setting obtaining the best results in a word similarity dataset as reported by #REFRb).,"In drawing from the findings of #REFR, which compared WordNet-based and distributional similarity approaches, we adapted their robust methods for our own system."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Adjectival modification, particularly by expressions that have been treated as higherorder modifiers in the formal semantics tradition, raises interesting challenges for semantic composition in distributional semantic models. We contrast three types of adjectival modifiers ? intersectively used color terms (as in white towel, clearly first-order), subsectively used color terms (white wine, which have been modeled as both firstand higher-order), and intensional adjectives (former bassist, clearly higher-order) ? and test the ability of different composition strategies to model their behavior. In addition to opening up a new empirical domain for research on distributional semantics, our observations concerning the attested vectors for the different types of adjectives, the nouns they modify, and the resulting noun phrases yield insights into modification that have been little evident in the formal semantics literature to date.
###Abstract of the cited paper: We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
###Context text: One of the most appealing aspects of so-called distributional semantic models #OTHEREFR. However, in order to have a complete theory of natural language meaning, these models must be supplied with or connected to a compositional semantics; otherwise, we will have no account of the recursive potential that natural language affords for the construction of novel complex contents.[Citation]Although the proposed operations have shown varying degrees of success in a number of tasks such as detecting phrase similarity and paraphrasing, it remains unclear to what extent they can account for the full range of meaning composition phenomena found in natural language. Higher-order modification (that is, modification that cannot obviously be modeled as property intersection, in contrast to firstorder modification, which can) presents one such challenge, as we will detail in the next section. The goal of this paper is twofold.
###Response:","In the last 4-5 years, researchers have begun to introduce compositional operations on distributional semantic representations, for instance to combine verbs with their arguments or adjectives with nouns #OTHEREFR; #REFR1.","Building on the approach proposed by #REFR, where adjective-noun composition is modeled using linear functions over noun vectors, we aim to further explore and challenge semantic interpretations in language."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model. We train a state-of-the-art MT system using MERT on many parameterizations of each metric and evaluate the resulting models on the other metrics and also using human judges. In accordance with popular wisdom, we find that it?s important to train on the same metric used in testing. However, we also find that training to a newer metric is only useful to the extent that the MT model?s structure and features allow it to take advantage of the metric. Contrasting with TER?s good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: [Citation]Learning such a model cannot be done using gradient methods since the value of the objective function only depends on the translation model?s argmax for each sentence in the tuning set. Typically, this optimization is performed as a series of line searches that examines the value of the evaluation metric at critical points where a new translation argmax becomes preferred by the model. Since the model score assigned to each candidate translation varies linearly with changes to the model parameters, it is possible to efficiently find the global minimum along any given search direction with only O(n2) operations when n-best lists are used.
###Response:",MERT is the standard technique for obtaining a machine translation model fit to a specific evaluation metric #REFR.,"In line with the phrase-based translation model proposed by #REFR, we found that model performance varies with different evaluation metrics and human preferences."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper improves the use of pseudowords as an evaluation framework for selectional preferences. While pseudowords originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic fit to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-ofthe-art by 13% absolute on a newspaper domain.
###Abstract of the cited paper: We present a discriminative method for learning selectional preferences from unlabeled text. Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations. We train a Support Vector Machine classifier to distinguish the positive from the negative instances. We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances. The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments. Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37% more pronouns correctly in a pronoun resolution experiment.
###Context text: While pseudo-words are now less often used for word sense disambigation, they are a common way to evaluate selectional preferences, models that measure the strength of association between a predicate and its argument filler, e.g., that the noun lunch is a likely object of eat. Selectional preferences are useful for NLP tasks such as parsing and semantic role labeling #OTHEREFR. Since evaluating them in isolation is difficult without labeled data, pseudoword evaluations can be an attractive evaluation framework.[Citation]However, evaluation design varies across research groups. This paper studies the evaluation itself, showing how choices can lead to overly optimistic results if the evaluation is not designed carefully. We show in this paper that current methods of applying pseudo-words to selectional preferences vary greatly, and suggest improvements.
###Response:",Pseudo-word evaluations are currently used to evaluate a variety of language modeling tasks #OTHEREFR; #REFR.,"Incorporating insights from a methodology that learned selectional preferences using a discriminative model and unlabeled text #REFR, we revised pseudoword evaluation for better performance."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Traditionally, machine learning approaches for information extraction require human annotated data that can be costly and time-consuming to produce. However, in many cases, there already exists a database (DB) with schema related to the desired output, and records related to the expected input text. We present a conditional random field (CRF) that aligns tokens of a given DB record and its realization in text. The CRF model is trained using only the available DB and unlabeled text with generalized expectation criteria. An annotation of the text induced from inferred alignments is used to train an information extractor. We evaluate our method on a citation extraction task in which alignments between DBLP database records and citation texts are used to train an extractor. Experimental results demonstrate an error reduction of 35% over a previous state-of-the-art method that uses heuristic alignments.
###Abstract of the cited paper: With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs.
###Context text: In this paper, we mainly deal with information extraction from text fragments that closely resemble structured records. Examples of such texts include citation strings in research papers, contact addresses on person homepages and apartment listings in classified ads. Pattern matching and rule-based approaches for IE #OTHEREFR that only use specific patterns, and delimiter and font-based cues for segmentation are prone to failure on such data because these cues are generally not broadly reliable.[Citation]However, these methods require labeled training data, such as annotated text, which is often scarce and expensive to produce. In many cases, however, there already exists a database with schema related to the desired output, and records that are imperfectly rendered in the available unlabeled text. This database can serve as a source of significant supervised guidance to machine learning methods.
###Response:",Statistical machine learning methods such as hidden Markov models #OTHEREFR; #REFR have become popular approaches to address the text extraction problem.,"Building upon the work of #REFR, which successfully applied Conditional Random Fields (CRFs) for information extraction tasks, we have adapted this approach to effectively handle unlabeled data."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The amount of cognitive effort required to process a word has been argued to depend on the word?s effect on the uncertainty about the incoming sentence, as quantified by the entropy over sentence probabilities. The current paper tests this hypothesis more thoroughly than has been done before by using recurrent neural networks for entropy-reduction estimation. A comparison between these estimates and wordreading times shows that entropy reduction is positively related to processing effort, confirming the entropy-reduction hypothesis. This effect is independent from the effect of surprisal.
###Abstract of the cited paper: A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times.
###Context text: A set of language models was trained on a corpus of POS tags of sentences.[Citation]Subsequent to training, the models were made to generate estimates of surprisal and entropy reductions ?Hsufn and ?Hsentn over a test corpus. These estimates were then compared to reading times measured over the words of the same test corpus. This section presents the data sets that were used, language-model details, and the evaluation metric.
###Response:","The advantage of using POS tags rather than words is that their probabilities can be estimated much more accurately and, consequently, more accurate prediction of wordreading time is possible #OTHEREFR; #REFR.","Our approach to estimating surprisal and entropy reductions stems from the methods introduced by Hale, who pioneered the use of a singular incremental parser in his psycholinguistic models [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Paraphrase generation is an important task that has received a great deal of interest recently. Proposed data-driven solutions to the problem have ranged from simple approaches that make minimal use of NLP tools to more complex approaches that rely on numerous language-dependent resources. Despite all of the attention, there have been very few direct empirical evaluations comparing the merits of the different approaches. This paper empirically examines the tradeoffs between simple and sophisticated paraphrase harvesting approaches to help shed light on their strengths and weaknesses. Our evaluation reveals that very simple approaches fare surprisingly well and have a number of distinct advantages, including strong precision, good coverage, and low redundancy.
###Abstract of the cited paper: We address the text-to-text generation problem of sentence-level paraphrasing ? a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
###Context text: Bhagat and Ravichandran #OTHEREFR proposed a similar approach that used noun phrase chunks as contexts and locality sensitive hashing to reduce the dimensionality of the context vectors. Despite their simplicity, such techniques are susceptible to a number of issues stemming from the distributional assumption. For example, such approaches have a propensity to assign large scores to antonyms and other semantically irrelevant phrases.[Citation]Amongst the most effective recent work, Bannard and Callison-Burch #OTHEREFR show how different English translations of the same entry in a statistically-derived translation table can be viewed as paraphrases. The recent work by Zhao et al #OTHEREFR uses a generalization of DIRT- style patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be available for all languages, domains, or applications.
###Response:",The second line of research uses comparable or bilingual corpora as the ?pivot? that binds paraphrases together #OTHEREFR; #REFR.,"This challenge was addressed by #REFR, who implemented a method for sentence-level paraphrasing using multiple-sequence alignment, delivering superior performance to baseline models."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations.
###Abstract of the cited paper: We describe an approach to automatically learn reordering rules to be applied as a preprocessing step in phrase-based machine translation. We learn rules for 8 different language pairs, showing BLEU improvements for all of them, and demonstrate that many important order transformations (SVO to SOV or VSO, headmodifier, verb movement) can be captured by this approach.
###Context text: Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree transformation are rewrite rules. These typically involve some condition under which the transformation can be applied (e.g., a noun and an adjective found in the same clause) and the transformation itself (e.g., move the adjective after the noun).[Citation]Another type of tree transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values.
###Response:",These rules can be designed manually #OTHEREFR; #REFR.,"The study by #REFR presents an automatic learning approach for reordering rules, demonstrating improvements in phrase-based translation for several language pairs."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Given the increasing need to process massive amounts of textual data, efficiency of NLP tools is becoming a pressing concern. Parsers based on lexicalised grammar formalisms, such as TAG and CCG, can be made more efficient using supertagging, which for CCG is so effective that every derivation consistent with the supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning.
###Abstract of the cited paper: This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms.
###Context text: One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations #OTHEREFR. One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence #OTHEREFR.[Citation]The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efficient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser?s CCG combinatory rules, with no chart pruning whatsoever.
###Response:","#REFR adapt the technique of supertagging #OTHEREFR to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word.","Drawing on previous work, it has been established that this intricate process, including supertagging, substantially increases the parser's speed and minimizes the derivation space #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe an ongoing work in information extraction which is seen as a text normalization task. The normalized representation can be used to detect paraphrases in texts. Normalization and paraphrase detection tasks are built on top of a robust analyzer for English and are exclusively achieved using symbolic methods. Both grammar development rules and information extraction rules are expressed within the same formalism and are developed in an integrated way. The experiment we describe in the paper is evaluated and presents encouraging results.
###Abstract of the cited paper: We des((tile tile design of Comlex Syntax, a co,nputational lexicon providing detailed syntactic iuformation ff)r approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.
###Context text: The morpho-syntactic normalizer is a general module that is neither corpusnor application-dedicated. It consists of hand-made rules that apply to the syntactic representation produced by our parser. It uses well known syntactic equivalences such as passiveactive transformation and verb alternations proposed in Levin.[Citation]For instance the utterance Antimony ores are mixed with other metals is finally represented with a set of normalized syntactic relations expressing that the normalized subject (SUBJ-N) of the verb mix is unknown, and that mix has two second actants (OBJ-N) ore and metal : SUBJ-N(mix,SOMEONE) OBJ-N(mix,ore) OBJ-N(mix,metal) For this example, both passive transformation and reciprocal alternation transformation have been applied on the set of dependencies produced by the general parser. Deep syntactic rules are expressed using the same formalism than general syntactic rules presented in the previous section. For instance the following rule construct an OBJ-N (Normalized object) dependency between the surface syntactic subject and a verb in a passive form3. if ( SUBJ(#1,#2) & VDOMAIN[passive](#1,#3) ) OBJ-N(#3,#2) Unlike Rose??s approach #OTHEREFR which also developed a deep syntactic analyzer, this is done exclusively by hand-made rules based on the previous calculated dependencies on the one hand and syntactic and morphological properties of the nodes involved in the dependencies on the other hand.
###Response:",It also exploits the classification given by the COMLEX lexicon #REFR in order to calculate the deep-subject of infinitive verbs.,"Our approach draws on the complex syntax design in the cited work, which provides a comprehensive syntactic lexicon for English headwords #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a method of improving the performance of dialog act tagging in identifying minority classes by using per-class feature optimization and a method of choosing the class based not on confidence, but on a cascade of classifiers. We show that it gives a minority class F-measure error reduction of 22.8%, while also reducing the error for other classes and the overall error by about 10%.
###Abstract of the cited paper: We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate the effectiveness of various features and machine learners for this task. While a simple bag-of-words approach provides a solid baseline, we find that adding information from dialogue structure and inter-utterance dependency provides some increase in performance; learners that account for sequential dependencies (CRFs) show the best performance. We report our results from testing using a corpus of chat dialogues derived from online shopping customer-feedback data.
###Context text: Dialog act #OTHEREFR, have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions #OTHEREFR).[Citation]Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, individual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment #OTHEREFR.
###Response:","Recently, studies have explored dialog act tagging in written interactions such as emails #OTHEREFR; #REFRb), instant messaging #OTHEREFR.","Building on the research of classifying dialog acts in online chat forums, we refined this system by implementing minority class tags and per-class feature optimization #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Creating labeled training data for relation extraction is expensive. In this paper, we study relation extraction in a special weakly-supervised setting when we have only a few seed instances of the target relation type we want to extract but we also have a large amount of labeled instances of other relation types. Observing that different relation types can share certain common structures, we propose to use a multi-task learning method coupled with human guidance to address this weakly-supervised relation extraction problem. The proposed framework models the commonality among different relation types through a shared weight vector, enables knowledge learned from the auxiliary relation types to be transferred to the target relation type, and allows easy control of the tradeoff between precision and recall. Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods.
###Abstract of the cited paper: This paper proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction. It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities, while removing the noisy information from the syntactic parse tree, eventually leading to a dynamic syntactic parse tree. This paper also explores entity features and their combined features in a unified parse and semantic tree, which integrates both structured syntactic parse information and entity-related semantic information. Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans, and the composite kernel combining this tree kernel with a linear state-of-the-art feature-based kernel, achieves the so far best performance.
###Context text: Zhou et al #OTHEREFR studied various features and feature combinations for relation extraction. We systematically explored the feature space for relation extraction #OTHEREFRb) . Kernel methods allow a large set of features to be used without being explicitly extracted.[Citation]However, in both feature-based and kernel-based studies, availability of sufficient labeled training data is always assumed. Chen et al #OTHEREFR explored semi-supervised learning for relation extraction using label propagation, which makes use of unlabeled data. Zhou et al #OTHEREFR proposed a hierarchical learning strategy to address the data sparseness problem in relation extraction.
###Response:","A number of relation extraction kernels have been proposed, including dependency tree kernels #OTHEREFR; #REFR.","In this research, we confront the challenge of limited labeled training data for relation extraction, building upon the dynamic syntactic parse tree and entity feature integration methods of the previous study #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Previous work on treebank parsing with discontinuous constituents using Linear Context-Free Rewriting systems (LCFRS) has been limited to sentences of up to 30 words, for reasons of computational complexity. There have been some results on binarizing an LCFRS in a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible. Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy. The resulting parser has been applied to a discontinuous treebank with favorable results.
###Abstract of the cited paper: We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particu- Jar representational strata.
###Context text: Discontinuity in constituent structures (cf. figure 1 & 2) is important for a variety of reasons.[Citation]Other reasons are phenomena such as extraposition and word-order freedom, which arguably require discontinuous annotations to be treated systematically in phrase-structures #OTHEREFR. Empirical investigations demonstrate that discontinuity is present in non-negligible amounts: around 30% of sentences contain discontinuity in two German treebanks #OTHEREFR. Recent work on treebank parsing with discontinuous constituents #OTHEREFR shows that it is feasible to directly parse discontinuous constituency annotations, as given in the German Negra (Skut et al SBARQ SQ VP WHNP MD NP VB .
###Response:","For one, it allows a tight correspondence between syntax and semantics by letting constituent structure express argument structure #REFR.",The annotation scheme and tool developed for non-configurational languages illustrates the importance of discontinuities in constituent structures #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segmentand at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.
###Abstract of the cited paper: Independence between sentences is an assumption deeply entrenched in the models and algorithms used for statistical machine translation (SMT), particularly in the popular dynamic programming beam search decoding algorithm. This restriction is an obstacle to research on more sophisticated discourse-level models for SMT. We propose a stochastic local search decoding method for phrase-based SMT, which permits free document-wide dependencies in the models. We explore the stability and the search parameters of this method and demonstrate that it can be successfully used to optimise a document-level semantic language model.
###Context text: From its foundations, Statistical Machine Translation (SMT) had two defining characteristics: first, translation was modeled as a generative process at the sentence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models.[Citation]Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards. The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts.
###Response:","Recently, there have been two promising research directions for improving SMT and its evaluation: #OTHEREFR, and (b) by going beyond the sentence-level, e.g., translating at the document level #REFR.","The experiments conducted by #REFR build upon the work of #OTHEREFR, who have challenged the tradition of sentence-level independence in SMT by implementing document-level dependencies."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: SenseClusters is a freely?available open? source system that served as the University of Minnesota, Duluth entry in the SENSEVAL-4 sense induction task. For this task SenseClusters was configured to construct representations of the instances to be clustered using the centroid of word cooccurrence vectors that replace the words in an instance. These instances are then clustered using k?means where the number of clusters is discovered automatically using the Adapted Gap Statistic. In these experiments SenseClusters did not use any information outside of the raw untagged text that was to be clustered, and no tuning of the system was performed using external corpora.
###Abstract of the cited paper: SenseClusters is a freely available system that clusters similar contexts. It can be applied to a wide range of problems, although here we focus on word sense and name discrimination. It supports several different measures for automatically determining the number of clusters in which a collection of contexts should be grouped. These can be used to discover the number of senses in which a word is used in a large corpus of text, or the number of entities that share the same name. There are three measures based on clustering criterion functions, and another on the Gap Statistic.
###Context text: It consists of 295 words; in addition, all punctuation, single letter words, and numbers (with the exception of years) were eliminated. Each of the contexts that contain a particular target word is represented by a single vector that is the average (or the centroid) of all the co-occurrence vectors found for the words that make up the context. This results in a context by feature matrix, where the features are the words that occur with the words in the contexts (i.e., second order co? occurrences).[Citation]The premise of this method is to create a randomized sample of data with the same characteristics of the observed data (i.e., the contexts to be clustered). This is done by fixing the marginal totals of the context by feature matrix and then generating randomized values that are consistent with those marginal totals. This creates a matrix that is can be viewed as being from the same population as the observed data, except that the data is essentially noise (because it is randomly generated).
###Response:","The k?means algorithm is used for clustering the contexts, where the number of clusters is automatically discovered using the Adapted Gap Statistic #REFR.","The system used to execute this method is SenseClusters, which employs k?means and the Adapted Gap Statistic to automatically determine the number of clusters, as described in #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This article presents an approach to interpret the content of documents in constrained domains at the level of communicative goals. The kind of knowledge used contains descriptions of wellformed document contents and texts that can be produced from them. The automatic analysis of text content is followed by an interactive negotiation phase involving an expert of the class of documents. Motivating reasons are given for an application of this approach, document normalization, and an implemented system is briefly introduced.1
###Abstract of the cited paper: In this paper, we describe a first prototype of a pattern-based analyzer developed in the context of a speech-to-speech translation project using a pivot-based approach (the pivot is called IF). The chosen situation involves a French client talking to an Italian travel agent (both in their own language) to organize a stay in the Trentino area. An IF consists of a dialogue act, and a list, possibly empty, of argument values. The analyzer applies a ""phrase spotting"" mechanism on the output of the speech recognition module. It finds well-formed phrases corresponding to argument values. A dialogue act is then built according to the instantiated arguments and some other features of the input. The current version of the prototype has been involved in an evaluation campaign on an unseen corpus of four dialogues consisting of 235 speech turns. The results are given and commented in the last part of the paper. We think they pave the way for future enhancements to both the coverage and the development methodology. R?sum. Dans cet article, nous d?crivons la premi?re version d'un analyseur fond? sur des patrons dans le contexte d'un projet de traduction de parole utilisant une technique de traduction par pivot (le pivot est appel. IF). Dans la situation choisie, un client fran?ais parle avec un agent italien (chacun dans sa langue maternelle) pour organiser un s?jour dans la r?gion du Trentin en Italie. Une IF se compose d'un acte de dialogue et d'une liste, ?ventuellement vide, de valeurs d'arguments. L'analyseur met en ?uvre un m?canisme de reconnaissance de syntagmes sur la sortie du module de reconnaissance de la parole. Cela permet de trouver des syntagmes bien form?s qui correspondent ? des valeurs d'arguments. L'acte de parole est alors construit en utilisant les arguments instanci?s ainsi que d'autres caract?ristiques de l'entr?e. Cette version du prototype a ?t? mis en ?uvre lors d'une ?valuation sur un corpus de quatre dialogues, non utilis?s pour le d?veloppement, compos? de 235 tours de parole du client. Les r?sultats sont donn?s dans la derni?re section de cet article. Nous pensons qu'ils ouvrent la voix pour de futures am?liorations de la couverture ainsi que de la m?thodologie de d?veloppement.
###Context text: communicative content of a document in constrained domain Several approaches have already been experimented to analyze the content of documents in constrained domains, which can vary depending on the amount of surface analysis of the text.[Citation]Another type of approach first performs a syntactic analysis of the text, from which semantic dependencies can be extracted. The system presented in #OTHEREFR derives normalized predicates encoding the meaning of documents from semantic dependencies found by a robust parser. This allows obtaining identical semantic interpretations for paraphrases such as ProductX is a colorless, non flammable liquid and ProductX is a liquid that has no colour and that does not burn easily.
###Response:","One type of approach uses information extraction techniques such as pattern matching that use strong predictions on the content and attempt to fill templates derived from a model of the domain (e.g. #REFR), thus not giving too much importance to syntactic structure.","A related method, demonstrated by #REFR, lays out an approach where a pattern-based analyzer identifies well-formed phrases which reflect specific argument values."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: [Citation]We start by using a to find a projection from each target word ei onto a source span, represented as blue vertical lines in Figure 1. Similarly, source words project onto target spans (red horizontal lines in Figure 1). pi(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa. Figure 1 contains an example for d = 2.
###Response:","The mapping from a word alignment to the set of licensed phrase pairs pi(a) is based on the standard rule extraction procedures used in most modern statistical systems #REFR, but extended to handle possible links #OTHEREFR.","Drawing on the phrase-based model proposed by #REFR, we initiated our approach by finding a projection for each target and source word."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: As tokenization is usually ambiguous for many natural languages such as Chinese and Korean, tokenization errors might potentially introduce translation mistakes for translation systems that rely on 1-best tokenizations. While using lattices to offer more alternatives to translation systems have elegantly alleviated this problem, we take a further step to tokenize and translate jointly. Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on the target side simultaneously. By integrating tokenization and translation features in a discriminative framework, our joint decoder outperforms the baseline translation systems using 1-best tokenizations and lattices significantly on both Chinese- English and Korean-Chinese tasks. Interestingly, as a tokenizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers.
###Abstract of the cited paper: Training a statistical machine translation starts with tokenizing a parallel corpus. Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization. Moreover, morphologically rich languages such as Korean present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear. Both rule-based solutions and statistical solutions are currently used. In this paper, we present unsupervised methods to solve tokenization problem. Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation.
###Context text: Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing #OTHEREFR several consecutive words together.[Citation]In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work. Our work also has connections to multilingual tokenization #OTHEREFR.
###Response:",Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus #REFR.,"The method of using unsupervised methods to solve tokenization issues, as detailed in #REFR, could enhance the efficiency of our joint decoder in translation tasks."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We propose a general method for reranker construction which targets choosing the candidate with the least expected loss, rather than the most probable candidate. Different approaches to expected loss approximation are considered, including estimating from the probabilistic model used to generate the candidates, estimating from a discriminative model trained to rerank the candidates, and learning to approximate the expected loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task.
###Abstract of the cited paper: Statisti(:a,1 signiticance testing of (litl'erelmeS in v;~hl(`-s of metri(:s like recall, i)rccision and batau(:(~(l F-s(:()rc is a ne(:(`-ssary t)art of eml)irical ual;ural language 1)ro(:essing. Unfortunately, we lind in a set of (;Xl)erinlc\]d;s (;hal; many (:oreinertly used tesl;s ofte, n underest imate t.he s ignif icancc an(l so are less likely to detect differences that exist 1)el;ween ditl'ercnt techniques. This undel'esi;imation comes from an in(let)endcn('(~ a,-;SUlnl)tion that is often violated. \~fe l)oint out some useful l;e,%s (;hal; (lo nol; make this assuml)- lion, including computationally--intcnsive rand()mizat,ion 1;cs|;s.
###Context text: This result might indicate that the LLK- Learn model still can be useful for different problems where discriminative learning gives more advantage over generative approaches. These experimental results demonstrate that the loss approximation reranking approaches proposed in this paper demonstrate significant improvement over the baseline models, achieving about the same relative error reduction as previously achieved with data-defined kernels #OTHEREFR. This improvement is despite the fact that the loss function is already used in the definition of the training criteria for all the models except SSN.[Citation]
###Response:","It is also interesting to note that the best result on the validation set for estimation 4We measured significance of all the experiments in this paper with the randomized significance test #REFR. of the loss with data-defined kernels (12) and (13) was achieved when the parameter A is close to the inverse of the first component of the learned decision vector, which confirms the motivation for these kernels.","This upward trend may be due to the adoption of statistical significance testing methods as discussed in #REFR, which highlight differences among various techniques."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.
###Abstract of the cited paper: We compare and contrast the strengths and weaknesses of a syntax-based machine translation model with a phrase-based machine translation model on several levels. We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations.
###Context text: Rule Coverage Marcu et al #OTHEREFR. For example, the following rule ? <(hong)Chinese, (DT(the) JJ(red))English> is not a valid string-to-tree transfer rule since the red is a partial constituent. A number of techniques have been proposed to improve rule coverage. #OTHEREFR introduced artificial constituent nodes dominating the phrase of interest.[Citation]For example, it cannot handle the above example. DeNeefe et al #OTHEREFR showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules.
###Response:","The binarization method used by #REFR can cover many non-constituent rules also, but not all of them.","In this research's string-to-dependency framework, we build upon the syntax-based model critique and improvements discussed in #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this position paper, we make several observations about the state of the art in automatic word sense disambiguation. Motivated by these observations, we offer several specific proposals to the community regarding improved evaluation criteria, common training and testing resources, and the definition of sense inventories.
###Abstract of the cited paper: Previous work \[Gale, Church and Yarowsky, 1992\] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test his empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99% accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92% using combined models of very local context.
###Context text: The task can generally be accomplished successfully using only tag-level models without lexical sensitivities besides the priors. . Standard annotated corpora of adequate size have long been available. Table 1: Some properties of the POS tagging task.[Citation]
###Response:","In contrast, approaches to WSD attempt o take advantage of many different sources of information #OTHEREFR); it seems possible to obtain benefit from sources ranging from local collocational clues #REFR to membership in semantically or topically related word classes #OTHEREFR; and disambignation seems highly lexically sensitive, in effect requiring specialized isamhignators for each polysemous word.","Building on the findings of Gale, Church, and Yarowsky (1992), we assert that a polysemous word typically demonstrates a single sense per collocation, facilitating disambiguation [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We connect two scenarios in structured learning: adapting a parser trained on one corpus to another annotation style, and projecting syntactic annotations from one language to another. We propose quasisynchronous grammar (QG) features for these structured learning tasks. That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment. Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to a synchronous grammar, which would insist on some form of structural parallelism. In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence. On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments. Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone. When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. . The first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst. We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
###Abstract of the cited paper: One may need to build a statistical parser for a new language, using only a very small labeled treebank together with raw text. We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features, as in recent models for scoring dependency parses (McDonald et al, 2005). Drawing on Abney?s (2004) analysis of the Yarowsky algorithm, we perform bootstrapping by entropy regularization: we maximize a linear combination of conditional likelihood on labeled data and confidence (negative Re?nyi entropy) on unlabeled data. In initial experiments, this surpassed EM for training a simple feature-poor generative model, and also improved the performance of a feature-rich, conditionally estimated model where EM could not easily have been applied. For our models and training sets, more peaked measures of confidence, measured by Re?nyi entropy, outperformed smoother ones. We discuss how our feature set could be extended with cross-lingual or cross-domain features, to incorporate knowledge from parallel or comparable corpora during bootstrapping.
###Context text: For many target languages, however, we do not have the luxury of a large parsed ?source corpus? in the language, even one in a different style or domain as above. Thus, we may seek other forms of data to augment our small target corpus.[Citation]But we can also try to transfer syntactic information from a parsed source corpus in another language. This is an extreme case of out-of-domain data. This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation.
###Response:",One option would be to leverage unannotated text #OTHEREFR; #REFR.,"Drawing on the bootstrapping and entropy regularization techniques discussed by McDonald et al., we can augment our small target corpus [#REFR]. This aids in transferring syntactic information from alternate language corpora."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes our submission for the *SEM shared task of Semantic Textual Similarity. We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47.
###Abstract of the cited paper: We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.
###Context text: [Citation]We evaluated multiple types of preprocessing per unsupervised metric and chose different ones depending on the metric. Word-level semantic similarities, used for soft comparisons and affective feature extraction, were computed over a corpus of 116 million web snippets collected by posing one query for every word in the Aspell spellchecker (asp, ) vocabulary to the Yahoo! search engine. Word-level emotional ratings in continuous valence and arousal scales were produced by a model trained on the ANEW dataset and using contextual similarities.
###Response:","Initially all sentences are pre-processed by the CoreNLP #OTHEREFR; #REFR suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.","""[#REFR] (Our model) leveraged the principles of conditional log linear models and fine-grained modeling of unknown word features, as proposed by the new part-of-speech tagger [#OTHEREFR]."""
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a first efficient implementation of a weighted deductive CYK parser for Probabilistic Linear Context- Free Rewriting Systems (PLCFRS), together with context-summary estimates for parse items used to speed up parsing. LCFRS, an extension of CFG, can describe discontinuities both in constituency and dependency structures in a straightforward way and is therefore a natural candidate to be used for data-driven parsing. We evaluate our parser with a grammar extracted from the German NeGra treebank. Our experiments show that datadriven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality.
###Abstract of the cited paper: We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particu- Jar representational strata.
###Context text: [Citation]In a preprocessing step, following common practice #OTHEREFR, we attach punctuation (not included in the NeGra annotation) as follows: In a first pass, using heuristics, we attach punctuation as high as possible while avoiding to introduce new crossing branches. In a second pass, parentheses and quotation marks preferably attach to the same node. Grammatical function labels on the edges are discarded.
###Response:",Our data source is the German NeGra treebank #REFR.,"Adopting the annotation scheme from the cited study #REFR, we incorporated linguistic alterations suitable for non-configurational languages in the preprocessing stage."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Semantic similarity measures have focused on individual word senses. However, in many applications, it may be informative to compare the overall sense distributions for two different contexts. We propose a new method for comparing two probability distributions over WordNet, which captures in a single measure the aggregate semantic distance of the component nodes, weighted by their probability. Previous such measures compute only the distributional distance, and do not take into account the semantic similarity between Word- Net senses across the distributions. To incorporate semantic similarity, we calculate the (dis)similarity between two probability distributions as a weighted distance ?travelled? from one to the other through the WordNet hierarchy. We evaluate the measure by applying it to the acquisition of verb argument alternation knowledge, and find that overall it outperforms existing distance measures.
###Abstract of the cited paper: We present a class-based approach to building a verb lexicon that makes explicit the close association between syntax and semantics for Levin classes. We have used Lexicalized Tree Adjoining Grammars to capture the syntax associated with each verb class and have augmented the trees to include selectional restrictions. In addition, semantic predicates are associated with each tree, which allow for a colnpositional interpretation.
###Context text: SPD is calculated as a tree distance that aggregates the individual semantic distances between nodes in the hierarchy, weighted by their probability in the two sense profiles. SPD can be calculated between two probability distributions over any hierarchy that supports a user-supplied semantic distance function. (In fact, the two sense profiles need not strictly be probability distributions?the measure is well-defined as long as the sum of the values of the two sense profiles is equal.) We demonstrate our method on a problem that arises in lexical acquisition, of determining whether two different argument positions across syntactic usages of a verb are assigned the same semantic role. For example, even though the truck shows up in two different syntactic positions, it is the Destination of the action in both of the sentences I loaded the truck with hay and I loaded hay onto the truck.[Citation]We focus on this problem to illustrate how our general method works, and how it aids in a particular natural language learning task. As in McCarthy #OTHEREFR, we cast argument alternation detection as a comparison of sense profiles across two different argument positions of a verb. Our method differs, however, in two important respects.
###Response:","Automatic detection of such argument alternations is important to acquisition of verb lexical semantics #REFR, and moreover, may play a role in automatic processing of language for applied tasks, such as question-answering #OTHEREFR.","Drawing from the class-based approach to verb lexicon building outlined by #REFR, our technique incorporates semantic similarities between WordNet senses, enriching the syntax-semantics association."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We investigate in this paper the adequate unit of analysis for Arabic Mention Detection. We experiment different segmentation schemes with various feature-sets. Results show that when limited resources are available, models built on morphologically segmented data outperform other models by up to 4F points. On the other hand, when more resources extracted from morphologically segmented data become available, models built with Arabic TreeBank style segmentation yield to better results. We also show additional improvement by combining different segmentation schemes.
###Abstract of the cited paper: In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality. Our results show that given large amounts of training data, splitting off only proclitics performs best. However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation. Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
###Context text: Consequently, the Arabic data is sparser than other languages, such as English, and it is necessary to ?segment? the words into several units of analysis in order to achieve a good performance. #OTHEREFR data. On the other hand, #OTHEREFR data using Arabic TreeBank (ATB) segmentation. In all published works, authors do not mention a specific motivation for the segmentation scheme they have adopted.[Citation]They report that the best results were obtained when the ATB-like segmentation was used. We explore here the four known and linguistically-motivated sorts of segmentation: punctuation separation, ATB, morphological and character-level segmentations. To our knowledge, this is the first paper which investigates different segmentation schemes to define the unit of analysis which best fits Arabic MD.
###Response:","Only for the Machine Translation task, #REFR report several results using different Arabic segmentation schemes.","In line with the findings in #REFR, diverse word-level preprocessing decisions for Arabic led to variations in SMT quality, significantly impacting performance."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues. To achieve high parsing performance for spoken monologues, it could prove effective to simplify the structure by dividing a sentence into suitable language units. This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation. In this method, the dependency parsing is executed in two stages: at the clause level and the sentence level. First, the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause. Next, the dependencies over clause boundaries are identified stochastically, and the dependency structure of the entire sentence is thus completed. An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences.
###Abstract of the cited paper: This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguating dependencies between subordinate clauses. Estimated dependencies of subordinate clauses successfully increase the precision of an existing statistical dependency analyzer.
###Context text: To resolve the syntactic ambiguity of a long sentence, some of them have focused attention on the ?clause.. First, there are the studies that focused attention on compound clauses #OTHEREFR. These tried to improve the parsing accuracy of long sentences by identifying the boundaries of coordinate structures.[Citation]Furthermore, Kim et al #OTHEREFR divided a sentence into ?S(ubject)-clauses,? which were defined as a group of words containing several predicates and their common subject. The above studies have attempted to reduce the parsing ambiguity between specific types of clauses in order to improve the parsing accuracy of an entire sentence. On the other hand, our method utilizes all types of clauses without limiting them to specific types of clauses.
###Response:","Next, other research efforts utilized the three categories into which various types of subordinate clauses are hierarchically classified based on the ?scope-embedding preference? of Japanese subordinate clauses #OTHEREFR; #REFR.","In addition, studies presented in #REFR exploit the scope embedding preference of subordinate clauses, significantly improving the precision in dependency analysis."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we present a study of a novel summarization problem, i.e., summarizing the impact of a scientific publication. Given a paper and its citation context, we study how to extract sentences that can represent the most influential content of the paper. We propose language modeling methods for solving this problem, and study how to incorporate features such as authority and proximity to accurately estimate the impact language model. Experiment results on a SIGIR publication collection show that the proposed methods are effective for generating impact-based summaries.
###Abstract of the cited paper: Scientific papers revolve around citations, and for many discourse level tasks one needs to know whose work is being talked about at any point in the discourse. In this paper, we introduce the scientific attribution task, which links different linguistic expressions to citations. We discuss the suitability of different evaluation metrics and evaluate our classification approach to deciding attribution both intrinsically and in an extrinsic evaluation where information about scientific attribution is shown to improve performance on Argumentative Zoning, a rhetorical classification task.
###Context text: Such an extraction-based definition of summarization has also been quite common in most existing general summarization work #OTHEREFR. By definition, in order to generate an impact summary of a paper, we must look at how other papers cite the paper, use this information to infer the impact of the paper, and select sentences from the original paper that can reflect the inferred impact. Note that we do not directly use the sentences from the citation context to form a summary.[Citation]Formally, let d = (s0, s1, ..., sn) be a paper tobe summarized, where si is a sentence. We referto a sentence (in another paper) in which there is an explicit citation of d as a citing sentence of d. When a paper is cited, it is often discussed consecutively in more than one sentence near the citation, thus intuitively we would like to consider a window of sentences centered at a citing sentence; the window size would be a parameter to set.
###Response:","This is because in citations, the discussion of the paper cited is usually mixed with the content of the paper citing it, and sometimes also with discussion about other papers cited #REFR.","While this approach borrows elements from the concept of scientific attribution as studied by #REFR, it refines and extends the methodology to focus on the impact assessment of a given paper."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model?s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.
###Abstract of the cited paper: Determining the polarity of a sentimentbearing expression requires more than a simple bag-of-words approach. In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity. In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%). We also find that ?contentword negators?, not widely employed in previous work, play an important role in determining expression-level polarity. Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered.
###Context text: Most previous work is centered around a given sentiment lexicon or building one via heuristics #OTHEREFR. In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams #OTHEREFR propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment.[Citation]Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, #OTHEREFR showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as ?once in a life time?.
###Response:","The work of #OTHEREFR; #REFR focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as ?prevent cancer?, where prevent reverses the negative polarity of cancer.","Similar to our method, the study by #REFR also acknowledges the importance of compositional semantics in achieving a higher accuracy for sentiment analysis tasks."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we propose a novel method for semi-supervised learning of nonprojective log-linear dependency parsers using directly expressed linguistic prior knowledge (e.g. a noun?s parent is often a verb). Model parameters are estimated using a generalized expectation (GE) objective function that penalizes the mismatch between model predictions and linguistic expectation constraints. In a comparison with two prominent ?unsupervised? learning methods that require indirect biasing toward the correct syntactic structure, we show that GE can attain better accuracy with as few as 20 intuitive constraints. We also present positive experimental results on longer sentences in multiple languages.
###Abstract of the cited paper: We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively.
###Context text: Computing the above takes O(n3) time, the time required to compute single edge marginals. In this paper, we use the O(n5) exact method, though we find that the accuracy attained by approximate training is usually within 5% of the exact method. If G is not edge-factored, then we need to compute a marginal over three or more edges, making exact training intractable.[Citation]In this paper g is binary and normalized by its total count in the corpus. The expectation of g is then the probability that it indicates a true edge.
###Response:",An appealing alternative to a similar approximation to the above would use loopy belief propagation to efficiently approximate the marginals #REFR.,"In line with recent research that formulates dependency parsing via globally constrained graphical models, approximate training can be advantageous for its efficiency and nearly comparable accuracy (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality. Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.
###Abstract of the cited paper: We use logical inference techniques for recognising textual entailment. As the performance of theorem proving turns out to be highly dependent on not readily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment. Finally, we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap; the resulting hybrid model achieves high accuracy on the RTE testset, given the state of the art. Our results also show that the different techniques that we employ perform very differently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature.
###Context text: There is a term corresponding to each node and arc, and the resolution steps at the core of weighted abduction theorem proving consider matching an individual node of the hypothesis (e.g. rose(e1)) with something from the text (e.g. fell(e1)), just as in the graph-matching approach. The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms?as in Moldovan et al #OTHEREFR. Then the theorem prover may generate intermediate forms in the proof, but, nevertheless, individual terms are resolved locally without reference to global context.[Citation]While in principle this approach does not suffer from the limitations we describe below, in practice it has not borne much fruit. Because few problem sentences can be accurately translated to logical form, and because logical entailment is a strict standard, recall tends to be poor. The simple graph matching formulation of the problem belies three important issues.
###Response:","Finally, a few efforts #OTHEREFR; #REFR have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover.","The cited approach of combining logical inference techniques with model building and machine learning also seeks to bypass the issues we highlight, albeit with varying successes on different RTE corpus subsets #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The lexical transfer phase is the most crucial step in MT because most of difficult problems are caused by lexical differences between two languages. In order to treat lexical issues systematically in transfer-based MT systems, we introduce the concept of bilingual-sings which are defined by pairs of equivalent monolingual signs. The bilingual signs not only relate the local linguistic structures of two languages but also play a central role in connecting the linguistic processes of translation with knowledge based inferences. We also show that they can be effectively used to formulate appropriate questions for disambiguating ""transfer ambiguities"", which is crucial in interactive MT systems.
###Abstract of the cited paper: We propose an architecture for the organisation of linguistic knowledge which allows to (1) separately formulate generalizations for different ypes of linguistic information, and (2) state interrelations between partial information belonging to different levels of description. We use typed feature structures for encoding linguistic knowledge. We show the application of this representational device for the architecture of linguistic knowledge sources for nmltilingum generation. As an example, we describe the use of interacting collocational nd syntactic onstraints in the generation of French and German sentences.
###Context text: However, many cases have been observed , where lexical transfer causes tructural changes. It ',is also the ease that objects or events describable by ~single words in one language are described by phrases or clauses in other languages ( ee section 3). We may expect hat classes of objects/events which can be expressed by single words in one language correspond to natural classes of objects/events, he classes whose truth conditions are naturally captured by single predicates in logical forms.[Citation]That is, we define a single bilingual sign which corresponds to a complex linguistic object in one language, if the other language xpresses the same ""meaning"" by a single word. As \[Sadler, 1990\] pointed out, compared with other methods using arbitrary predicates in meaning representation, our method is well-motivated in selecting basic predicates. In fact, the required fineness of distinction of word senses depends highly on the target language #OTHEREFR\]).
###Response:","Therefore, we prepare single bilingual signs for expressing their truth conditions if at least one of the languages has lexical items \[#REFR\].","This approach aligns with the proposed architecture for organising linguistic knowledge, separating generalizations and interrelations for various linguistic levels #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-todependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets.
###Abstract of the cited paper: In statistical machine translation, decoding without any reordering constraint is an NP-hard problem. Inversion Transduction Grammars (ITGs) exploit linguistic structure and can well balance the needed flexibility against complexity constraints. Currently, translation models with ITG constraints usually employs the cube-time CYK algorithm. In this paper, we present a shift-reduce decoding algorithm that can generate ITG-legal translation from left to right in linear time. This algorithm runs in a reduce-eager style and is suited to phrase-based models. Using the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets.
###Context text: In addition, their algorithm produces phrasal dependency parse trees while the leaves of our dependency trees are words, making dependency language models can be directly used. Shift-reduce parsing has been successfully used in phrase-based decoding but limited to adding structural constraints. Galley and Manning #OTHEREFR propose a shift-reduce algorithm to integrate a hierarchical reordering model into phrase-based systems.[Citation]Our work differs from theirs by going further to incorporate linguistic syntax into phrase-based decoding. Along another line, a number of authors have developed incremental algorithms for syntaxbased models #OTHEREFR. Watanabe et al #OTHEREFR introduce an Earlystyle top-down parser based on binary-branching Greibach Normal Form.
###Response:",#REFR use shift-reduce parsing to impose ITG #OTHEREFR constraints on phrase permutation.,The methodology presented in our research expands upon the shift-reduce decoding algorithm for phrase-based models proposed by the authors in #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
###Abstract of the cited paper: We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages. Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages. During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also automatically refine the syntactic categories given in our coarsely tagged input. Across six languages our approach outperforms state-of-theart unsupervised methods by a significant margin.1
###Context text: For all data sets we used the predefined training and testing splits. Our approach relies on a consistent set of partof-speech tags across languages and treebanks. For this we used the universal tagset from Petrov et al. #OTHEREFR, which includes: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all tag).[Citation]For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 #OTHEREFR.
###Response:",Similar tagsets are used by other studies on grammar induction and projection #REFR.,"Utilizing the approach by #REFR, our method enhanced dependency parsing across multiple languages using syntactic universals and language-independent rules."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a novel application of a supervised topic model to do entity relation detection (ERD). We adapt Maximum Entropy Discriminant Latent Dirichlet Allocation (MEDLDA) with mixed membership for relation detection. The ERD task is reformulated to fit into the topic modeling framework. Our approach combines the benefits of both, maximum-likelihood estimation (MLE) and max-margin estimation (MME), and the mixed membership formulation enables the system to incorporate heterogeneous features. We incorporate different features into the system and perform experiments on the ACE 2005 corpus. Our approach achieves better overall performance for precision, recall and Fmeasure metrics as compared to SVM-based and LLDA-based models.
###Abstract of the cited paper: Relation extraction is the task of finding semantic relations between entities from text. The state-of-the-art methods for relation extraction are mostly based on statistical learning, and thus all have to deal with feature selection, which can significantly affect the classification performance. In this paper, we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces. We present a general definition of feature spaces based on a graphic representation of relation instances, and explore three different representations of relation instances and features of different complexities within this framework. Our experiments show that using only basic unit features is generally sufficient to achieve state-of-the-art performance, while overinclusion of complex features may hurt the performance. A combination of features of different levels of complexity and from different sentence representations, coupled with task-oriented feature pruning, gives the best performance.
###Context text: Chan and Roth #OTHEREFR employ constraints using an integer linear programming (ILP) framework. Using this, they apply rich linguistic and knowledgebased constraints based on coreference annotations, a hierarchy of relations, syntacto-semantic structure, and knowledge from Wikipedia. In our work, we focus on capturing the latent semantics of the text between the NEs.[Citation]Syntactic features such as POS tags and dependency path between entities; semantic features such as Word-Net relations, semantic parse trees and types of NEs; and structural features such as which entity came first in the sentence have been found useful for ERD. We too observe the utility of informative features for this task. However, exploration of the feature space is not the main focus of this work.
###Response:",A variety of features have been explored for ERD in previous research #OTHEREFR; #REFR.,"Our model, incorporating different features for more effective relation detection, draws inspiration from the systematic feature space exploration proposed by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample.
###Abstract of the cited paper: The resolution of lexical ambiguity is important for most natural language processing tasks, and a range of computational techniques have been proposed for its solution. None of these has yet proven effective on a large scale. In this paper, we describe a method for lexical disambiguation of text using the definitions in a machine-readable dictionary together with the technique of simulated annealing. The method operates on complete sentences and attempts to select the optimal combinations ofword senses for all the words in the sentence simultaneously. The words in the sentences may be any of the 28,000 headwords in Longman is Dictionary of Contemporary English (LDOCE) and are disambiguated relative to the senses given in LDOCE. Our initial results on a sample set of 50 sentences are comparable to those of other researchers, and the fully automatic method requires no hand coding of lexical entries, or hand tagging of text.
###Context text: We also found that the system correctly identified one of the correct senses 83.4% of the time. Assuming that our tagger will perform to a similar level over all content words in our corpus if test data was avilable, and we have no evidence to the contrary, this figure equates to 92.8% correct agging over all words in text (since, in our corpus, 42% of words tokens are ambiguous in LDOCE). Comparative valuation is generally difficult in word sense disambiguation due to the variation in approach and the evaluation corpora.[Citation]Neither McRoy nor Veronis & Ide provide a quantative evaluation of their system and so our performance annot be easily compared with theirs. Mahesh et. al. claim high levels of sense tagging accuracy (about 89%), but our results are not directly comparable since its authors explicitly reject the conventional markup-trainingtest method used here. Cowie et. al. used LDOCE and so we can compare results using the same set of senses.
###Response:","However, it is fair to compare our work against other approaches which have attempted to disambiguate all content words in a text against some standard lexical resource, such as #REFR, #OTHEREFR.","Our method aligns with the technique introduced by #REFR, utilizing dictionary definitions and a computational technique for large scale lexical disambiguation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively.
###Abstract of the cited paper: In this paper we consider the problem of analysing sentence-level discourse structure. We introduce discourse chunking (i.e., the identification of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing. Our experiments show that the proposed modelling approach yields results comparable to state-of-the-art while exploiting knowledge-lean features and small amounts of discourse annotations. We also demonstrate how discourse chunking can be successfully applied to a sentence compression task.
###Context text: For this boundary position, the 6-gram including the three words before and the three words after the boundary is included as a feature; additionally, all n-grams for n < 6 such that either wi or wi+1 (or both) is in the n-gram are included as features. In other words, all n-grams in a six word window of boundary position i are included as features, except those that include neither wi nor wi+1 in the n-gram. The identical feature templates are used with POS-tag and shallow tag sequences as well, to define tag n-gram features.[Citation]Their n-gram feature definitions were different #OTHEREFR. In addition, they used a rulebased clauser that we did not. Despite such differences, this feature set is quite close to what is described in that paper.
###Response:","This feature set is very close to that used in #REFR, but not identical.","Drawing on a similar discourse structure methodology, our strategy mirrors the discourse chunking approach undertaken by #REFR, albeit with a few adjustments."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present an algorithm for incremental statistical parsing with Parallel Multiple Context-Free Grammars (PMCFG). This is an extension of the algorithm by Angelov (2009) to which we added statistical ranking. We show that the new algorithm is several times faster than other statistical PMCFG parsing algorithms on real-sized grammars. At the same time the algorithm is more general since it supports non-binarized and non-linear grammars. We also show that if we make the search heuristics non-admissible, the parsing speed improves even further, at the risk of returning sub-optimal solutions.
###Abstract of the cited paper: Parallel Multiple Context-Free Grammar (PMCFG) is an extension of context-free grammar for which the recognition problem is still solvable in polynomial time. We describe a new parsing algorithm that has the advantage to be incremental and to support PMCFG directly rather than the weaker MCFG formalism. The algorithm is also top-down which allows it to be used for grammar based word prediction.
###Context text: [Citation]When the item is complete, it will yield the forest of all trees that derive the sub-string covered by the item. For example, when the first constituent for category Conj is completely parsed, the forest will contain the single production in (1). The inside weight for the active item is the currently best known estimation for the lowest weight of a tree in the forest.
###Response:",The inside weight w i and the outside weight w o in the active items deserve more attention since this is the only difference compared to #REFR.,"The item completion process outlined by our study is an advancement of a parsing algorithm that supports PMCFG directly, as presented in #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.
###Abstract of the cited paper: We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively.
###Context text: In the aspect of training, global structural learning has been used to replace local learning on each decision #OTHEREFR, although the effect of global learning has not been separated out and studied alone. In this short paper, we study a third aspect in a statistical system: feature definition. Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy.[Citation]Transitionbased parsing, by contrast, can easily accommodate arbitrarily complex representations involving nonlocal features. Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing #OTHEREFRa). We explore considerably richer feature representations and show that they improve parsing accuracy significantly.
###Response:",Various recent attempts have been made to include non-local features into graph-based dependency parsing #REFR.,"Drawing on the approach of formulating dependency parsing as a graphical model with global constraints as described by #REFR, we further refine feature templates."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present an unsupervised approach to reconstructing ancient word forms. The present work addresses three limitations of previous work. First, previous work focused on faithfulness features, which model changes between successive languages. We add markedness features, which model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of Proto- Oceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date.
###Abstract of the cited paper: Alignment of phonetic sequences i a necessary step in many applications in computational phonology. After discussing various approaches to phonetic alignment, I present a new algorithm that combines a number of techniques developed for sequence comparison with a scoring scheme for computing phonetic similarity on the basis of multivalued features. The algorithm performs better on cognate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature.
###Context text: The model also supports the addition of prior linguistic knowledge. This takes the form of feature templates with more internal structure. We performed experiments with an additional feature template: STRUCT-FAITHFULNESS is a structured version of FAITHFULNESS, replacing x and y with their natural classes N?(x) and N?(y) where ? indexes types of classes, ranging over {manner, place, phonation, isOral, isCentral, height, backness, roundedness}.[Citation]We compared the performance of the system with and without STRUCT-FAITHFULNESS to check if the algorithm can recover the structure of natural classes in an unsupervised fashion. We found that with 2 or 4 observed languages, FAITHFULNESS underperformed STRUCT-FAITHFULNESS, but for larger trees, the difference was not significant. FAITH- FULNESS even slightly outperformed its structured cousin with 16 observed languages.
###Response:",This feature set is reminiscent of the featurized representation of #REFR.,"Building on #OTHEREFR's work on phonetic sequence alignment, we employed their multivalued feature-based scoring scheme to compute phonetic similarity during our experiments[#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text. The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort.
###Abstract of the cited paper: Most event extraction systems are trained with supervised learning and rely on a collection of annotated documents. Due to the domain-specificity of this task, event extraction systems must be retrained with new annotated data for each domain. In this paper, we propose a bootstrapping solution for event role filler extraction that requires minimal human supervision. We aim to rapidly train a state-of-the-art event extraction system using a small set of ?seed nouns? for each event role, a collection of relevant (in-domain) and irrelevant (outof-domain) texts, and a semantic dictionary. The experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the MUC-4 data set, and achieves performance levels comparable to supervised training with 700 manually annotated documents.
###Context text: First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort.[Citation]However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task #OTHEREFR. Filatova et al#OTHEREFR identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE #OTHEREFR limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text.
###Response:","For example, a popular approach to reduce annotation effort is bootstrapping from seed examples #OTHEREFR; #REFR.","In response to this, a bootstrapping approach for event extraction that minimizes human supervision has been introduced #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature. This paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models, such as tensors and matrices, can be used to simulate different aspects of predicate logic. This paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors. It provides tensor interpretations of the set of logical connectives required to model propositional calculi. It suggests a variant of these tensor calculi capable of modelling quantifiers, using few non-linear operations. It finally discusses the relation between these variants, and how this relation should constitute the subject of future work.
###Abstract of the cited paper: Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.
###Context text: This emerging sub-field of natural language semantic modelling seeks to combine two seemingly orthogonal approaches to modelling the meaning of words and sentences, namely formal semantics and distributional semantics. These approaches, summarised in Section 2, differ in that formal semantics, on the one hand, provides a neatly compositional picture of natural language meaning, reducing sentences to logical representations; one the other hand, distributional semantics accounts for the ever-present ambiguity and polysemy of words of natural language, and provides tractable ways of learning and comparing word meanings based on corpus data. Recent efforts, some of which are briefly reported below, have been made to unify both of these approaches to language modelling to produce compositional distributional models of semantics, leveraging the learning mechanisms of distributional semantics, and providing syntax-sensitive operations for the production of representations of sentence meaning obtained through combination of corpus-inferred word meanings.[Citation]While these developments are promising with regard to the goal of obtaining learnable-yetstructured sentence-level representations of language meaning, part of the motivation for unifying formal and distributional models of semantics has been lost. The compositional aspects of formal semantics are combined with the corpus-based empirical aspects of distributional semantics in such models, yet the logical aspects are not. But it is these logical aspects which are so appealing in formal semantic models, and therefore it would be desirable to replicate the inferential powers of logic within compositional distributional models of semantics.
###Response:","These efforts have been met with some success in evaluations such as phrase similarity tasks #OTHEREFR; #REFR, sentiment prediction #OTHEREFR.","In this context, the work of Coecke et al. (2010) must be considered, who successfully incorporated empirical distributional methods in modelling compositional meaning for sentences using data from the BNC [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins. State-of-the-art methods for arc filtering use separate classifiers to make pointwise decisions about the tree; they label tokens with roles such as root, leaf, or attaches-tothe-left, and then filter arcs accordingly. Because these classifiers overlap substantially in their filtering consequences, we propose to train them jointly, so that each classifier can focus on the gaps of the others. We integrate the various pointwise decisions as latent variables in a single arc-level SVM classifier. This novel framework allows us to combine nine pointwise filters, and adjust their sensitivity using a shared threshold based on arc length. Our system filters 32% more arcs than the independently-trained classifiers, without reducing filtering speed. This leads to faster parsing with no reduction in accuracy.
###Abstract of the cited paper: In this paper, we investigate structured models for document-level sentiment classification. When predicting the sentiment of a subjective document (e.g., as positive or negative), it is well known that not all sentences are equally discriminative or informative. But identifying the useful sentences automatically is itself a difficult learning problem. This paper proposes a joint two-level approach for document-level sentiment classification that simultaneously extracts useful (i.e., subjective) sentences and predicts document-level sentiment based on the extracted sentences. Unlike previous joint learning methods for the task, our approach (1) does not rely on gold standard sentence-level subjectivity annotations (which may be expensive to obtain), and (2) optimizes directly for document-level performance. Empirical evaluations on movie reviews and U.S. Congressional floor debates show improved performance over previous approaches.
###Context text: Figure 1: The dotted arc can be filtered by labeling any of the boxed roles as True; i.e., predicting that the head the3 is not the head of any arc, or that the modifier his6 attaches elsewhere. Role truth values, derived from the gold-standard tree (in grey), are listed adjacent to the boxes, in parentheses. classifier is given credit for eliminating an arc. The identity of the responsible classifier is modeled as a latent variable, which is filled in during training using a latent SVM (LSVM) formulation.[Citation]In our framework, each classifier learns to focus on the cases where the other classifiers are less confident. Furthermore, the integrated approach directly optimizes for arc-filtering accuracy (rather than token-labeling fidelity). We trade-off filtering precision/recall using two hyperparameters, while the previous approach trained classifiers for eight different tasks resulting in sixteen hyperparameters.
###Response:","Our use of an LSVM to assign credit during joint training differs substantially from previous LSVM applications, which have induced latent linguistic structures #OTHEREFR or sentence labels #REFR.","This integrated approach aligns well with the methodology utilized in document-level sentiment classification, where sentences are jointly processed to optimize overall performance #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure?s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation.
###Abstract of the cited paper: We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.
###Context text: Dependency parsing comes in two main forms: Graph algorithms and Greedy algorithms.[Citation]Each parser has its advantages and disadvantages, but the accuracy overall is approximately the same. The types of errors made by each parser, however, are very different. MST- Parser is globally trained for an optimal solution and this has led it to get the best results on longer sentences.
###Response:",The two most popular algorithms are McDonald?s MST- Parser #REFR and Nivre?s Malt- Parser #OTHEREFR.,"The MST Parser's superior performance on longer sentences can be attributed to its formulation as a maximum spanning tree search in weighted dependency parsing, as shown by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a simple, robust and (almost) unsupervised dictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector) to automatically generate polarity lexicons. We show that qwn-ppv outperforms other automatically generated lexicons for the four extrinsic evaluations presented here. It also shows very competitive and robust results with respect to manually annotated ones. Results suggest that no single lexicon is best for every task and dataset and that the intrinsic evaluation of polarity lexicons is not a good performance indicator on a Sentiment Analysis task. The qwn-ppv method allows to easily create quality polarity lexicons whenever no domain-based annotated corpora are available for a given language.
###Abstract of the cited paper: Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has properties that support the Polyanna Hypothesis. Using the General Inquirer as gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above.
###Context text: Another manually corrected lexicon #OTHEREFR positively annotated words respectively. Among the automatically built lexicons, Turney and Littman #OTHEREFR proposed a minimally supervised algorithm to calculate the polarity of a word depending on whether it co-ocurred more with a previously collected small set of positive words rather than with a set of negative ones. Agerri and Garc??a Serrano presented a very simple method to extract the polarity information starting from the quality synset in WordNet #OTHEREFR.[Citation]They produce several lexicons the best of which, MSOL(ASL and GI) contains 51K and 76K entries respectively and uses the full General Inquirer as seeds. They performed both intrinsic and extrinsic evaluations using the MPQA 1.1 corpus. Finally, there are two approaches that are somewhat closer to us, because they are based on Word- Net and graph-based methods.
###Response:","#REFR developed a method in which they first identify (by means of affixes rules) a set of positive/negative words which act as seeds, then used a Roget-like thesaurus to mark the synonymous words for each polarity type and to generalize from the seeds.","In a similar vein, the cited work proposes an approach for creating a high-coverage semantic orientation lexicon, leveraging a Roget-like thesaurus and affixes, while demonstrating its effectiveness over traditional WordNet-based lexicons #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespecific and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance.
###Abstract of the cited paper: This paper investigates two strategies for improving coreference resolution: (1) training separate models that specialize in particular types of mentions (e.g., pronouns versus proper nouns) and (2) using a ranking loss function rather than a classification function. In addition to being conceptually simple, these modifications of the standard single-model, classification-based approach also deliver significant performance improvements. Specifically, we show that on the ACE corpus both strategies produce f -score gains of more than 3% across the three coreference evaluation metrics (MUC, B3, and CEAF).
###Context text: Given an anaphor ai and a set of candidate antecedents C = {C1,C2, ...,Ck}, the problem of anaphora resolution is to choose the best candidate antecedent for ai.[Citation]The advantage of the candidate-ranking model over the mention-pair model is that it overcomes the strong independence assumption made in mentionpair models and evaluates how good a candidate is relative to all other candidates. We train our model as follows. If the anaphor is a this-issue anaphor, the set C is extracted using the candidate extraction algorithm from Section 4.1.
###Response:",We follow the candidate-ranking model proposed by #REFR.,Our approach applies strategies inspired by previous work that demonstrated improved coreference resolution through separate models and use of a ranking loss function over classification [#REFR].
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present three novel methods of compactly storing very large n-gram language models. These methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time, at speeds comparable to modern language modeling toolkits. Our basic approach generates an explicit minimal perfect hash function, that maps all n-grams in a model to distinct integers to enable storage of associated values. Extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs, including variable length coding of values and the use of tiered structures that partition the data for more efficient storage. We apply our approach to storing the full Google Web1T n-gram set and all 1-to-5 grams of the Gigaword newswire corpus. For the 1.5 billion n-grams of Gigaword, for example, we can store full count information at a cost of 1.66 bytes per n-gram (around 30% of the cost when using the current stateof-the-art approach), or quantized counts for 1.41 bytes per n-gram. For applications that are tolerant of a certain class of relatively innocuous errors (where unseen n-grams may be accepted as rare n-grams), we can reduce the latter cost to below 1 byte per n-gram.
###Abstract of the cited paper: A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements fall significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we present a general framework for deriving smoothed language model probabilities from BFs. We investigate how a BF containing n-gram statistics can be used as a direct replacement for a conventional n-gram model. Recent work has demonstrated that corpus statistics can be stored efficiently within a BF, here we consider how smoothed language model probabilities can be derived efficiently from this randomised representation. Our proposal takes advantage of the one-sided error guarantees of the BF and simple inequalities that hold between related n-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities. We use these models as replacements for a conventional language model in machine translation experiments.
###Context text: [Citation]These methods store language models in relatively little space by not actually keeping the ngram key in the structure and by allowing a small probability of returning a false positive, i.e. so that for an n-gram that is not in the model, there is a small risk that the model will return some random probability instead of correctly reporting that the ngram was not found. These structures do not allow enumeration over the n-grams in the model, but for many applications this is not a requirement and their space advantages make them extremely attractive. Two major approaches have been used for storing language models: Bloom Filters and Bloomier Filters.
###Response:",Recent randomized language models #REFR make use of Bloom filter like structures to map n-grams to their associated probabilities or counts.,"Drawing from the work by #REFR, Bloom Filters offer a feasible solution for storing n-gram statistics in compact space while dealing with an acceptable rate of false positives."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a novel filtration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed wellformed dependency restriction is used to filter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement, and the new feature brings another improvement to the baseline system, especially on larger corpus.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: [Citation]Source word f aligns with target word e, according to the IBM model 1, the lexical translation probability is p(e|f). However, in the sense of dependency relationship, we believe that the generation of the target word e, is not only triggered by the aligned source word f , but also associated with f ?s head word f ?. Therefore, the lexical translation probability becomes p(e|f ? f ?), which of course allows for a more fine-grained lexical choice of the target word.
###Response:",#REFR introduced the concept of lexical weighting to check how well words of the phrase translate to each other.,"Following the phrase-based translation model proposed by #REFR, this study further refines the lexical translation probability, taking into account the dependency relationship of the source words."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we show that linguistically motivated pronunciation rules can improve phone and word recognition results for Modern Standard Arabic (MSA). Using these rules and the MADA morphological analysis and disambiguation tool, multiple pronunciations per word are automatically generated to build two pronunciation dictionaries; one for training and another for decoding. We demonstrate that the use of these rules can significantly improve both MSA phone recognition and MSA word recognition accuracies over a baseline system using pronunciation rules typically employed in previous work on MSA Automatic Speech Recognition (ASR). We obtain a significant improvement in absolute accuracy in phone recognition of 3.77%?7.29% and a significant improvement of 4.1% in absolute accuracy in ASR.
###Abstract of the cited paper: We present a diacritization system for written Arabic which is based on a lexical resource. It combines a tagger and a lexeme language model. It improves on the best results reported in the literature.
###Context text: Some diacritics are lexical (where word meaning varies) and others are inflectional (where nominal case or verbal mood varies). Inflectional diacritics are typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always produce these inflections correctly or at all.[Citation]In this paper, we use the MADA #OTHEREFR. MADA, which uses the Buckwalter Arabic morphological Analyzer databases #OTHEREFR, provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the special symbol ?{? for Hamzat-Wasl.
###Response:",Much work has been done on automatic Arabic diacritization #OTHEREFR; #REFR.,"In developing our methodology, we also incorporated insights from a previously proposed system for written Arabic diacritization based on a lexical resource #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Topic models have great potential for helping users understand document corpora. This potential is stymied by their purely unsupervised nature, which often leads to topics that are neither entirely meaningful nor effective in extrinsic tasks (Chang et al 2009). We propose a simple and effective way to guide topic models to learn topics of specific interest to a user. We achieve this by providing sets of seed words that a user believes are representative of the underlying topics in a corpus. Our model uses these seeds to improve both topicword distributions (by biasing topics to produce appropriate seed words) and to improve document-topic distributions (by biasing documents to select topics related to the seed words they contain). Extrinsic evaluation on a document clustering task reveals a significant improvement when using seed information, even over other models that use seed information na??vely.
###Abstract of the cited paper: We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system?s error trends.
###Context text: Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus.[Citation]Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents #OTHEREFR. We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3).
###Response:",This kind of supervision is similar to the seeding in bootstrapping literature #OTHEREFR or prototype-based learning #REFR.,"Drawing on the prototype-driven learning principle investigated by #REFR, we have integrated seed words that directly mirror these canonical examples in our model."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes a Chinese word segmentation system that is based on majority voting among three models: a forward maximum matching model, a conditional random field (CRF) model using maximum subword-based tagging, and a CRF model using minimum subwordbased tagging. In addition, it contains a post-processing component to deal with inconsistencies. Testing on the closed track of CityU, MSRA and UPUC corpora in the third SIGHAN Chinese Word Segmentation Bakeoff, the system achieves a F-score of 0.961, 0.953 and 0.919, respectively.
###Abstract of the cited paper: Keh- J iann Chen Sh ingl luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese sentences are composed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences i to identify the words. The difficulties of identifying words include (l) the identification of complex words, such as Determinative-Measure, reduplications, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmentations. In this paper, we propose the possible solutions for the above difficulties. We adopt a matching algorithm with 6 different heuristic rules to resolve the ambiguities and achieve an 99.77% of the success rate. The statistical data supports that the maximal matching algorithm is the most effective heuristics.
###Context text: The maximum matching algorithm is a greedy segmentation approach. It proceeds through the sentence, mapping the longest word at each point with an entry in the dictionary.[Citation]The maximum matching approach is simple and efficient, and it results in high in-vocabulary accuracy; However, the small size of the dictionary, which is obtained only from the training data, is a major bottleneck for this approach to be applied by itself.
###Response:","In our system, the well-known forward maximum matching algorithm #REFR is implemented.","As noted by Chen et al., the first step of processing Chinese sentences involves identifying words, a process deftly managed by utilizing a maximal matching algorithm [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product?s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HL- SOT approach is easily generalized to labeling a mix of reviews of more than one products.
###Abstract of the cited paper: Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then collapsed into a single set of 7, 813 unique words. For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement.
###Context text: Then there came up with research works shifting focus from overall document sentiment to sentiment analysis based on product attributes #OTHEREFR. Document overall sentiment analysis is to summarize the overall sentiment in the document. Research works related to document overall sentiment analysis mainly rely on two finer levels sentiment annotation: word-level sentiment annotation and phrase-level sentiment annotation.[Citation]The phrase-level sentiment annotation focuses sentiment annotation on phrases not words with concerning that atomic units of expression is not individual words but rather appraisal groups #OTHEREFR. In #OTHEREFR, the concepts of prior polarity and contextual polarity were proposed. This paper presented a system that is able to automatically identify the contextual polarity for a large subset of sentiment expressions.
###Response:",The wordlevel sentiment annotation is to utilize the polarity annotation of words in each sentence and summarize the overall sentiment of each sentimentbearing word to infer the overall sentiment within the text #OTHEREFR; #REFR.,"In a similar vein, a method for extracting sentiment-bearing adjectives using the Sentiment Tag Extraction Program (STEP) was proposed to facilitate document sentiment analysis #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes the Cunei Machine Translation Platform and how it was used in the WMT ?10 German to English and Czech to English translation tasks.
###Abstract of the cited paper: We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.
###Context text: The phrase alignment features are then components of the model for each translation instance. While the calculations are not exactly the same, conceptually this work is modeled after #OTHEREFR. For each source-side match in the corpus, an alignment matrix is loaded for the complete sentence in which the match resides.[Citation]Intuitively, when a source phrase is aligned to a target phrase, this implies that the remainder of the source sentence that is not specified by the source phrase is aligned to the remainder of the target sentence not specified by the target phrase. Separate features compute the probability that the word alignments for tokens within the phrase are concentrated within the phrase boundaries and that the word alignments for tokens outside the phrase are concentrated outside the phrase boundaries. In addition, words with no alignment links or weak alignments links demonstrate uncertainty in modeling.
###Response:",This alignment matrix contains scores for all word correspondences in the sentence pair and can be created using GIZA++ #OTHEREFR or the Berkeley aligner #REFR.,The alignment method we used to optimize the data likelihood and agreement between models is strongly influenced by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper introduces the KyotoEBMT Example-Based Machine Translation framework. Our system uses a tree-to-tree approach, employing syntactic dependency analysis for both source and target languages in an attempt to preserve non-local structure. The effectiveness of our system is maximized with online example matching and a flexible decoder. Evaluation demonstrates BLEU scores competitive with state-of-the-art SMT systems such as Moses. The current implementation is intended to be released as open-source in the near future.
###Abstract of the cited paper: A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrasebased translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.
###Context text: hypothesis construction An important characteristic of our system is that we do not extract and store translation rules in advance: the alignment of translation examples is performed offline. However, for a given input sentence i, the steps for finding examples partially matching i and extracting their translation hypotheses is an online process. This approach could be considered to be more faithful to the original EBMT approach advocated by Nagao #OTHEREFR.[Citation]It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several benefits. The first is that we are not required to impose a limit on the size of translation hypotheses.
###Response:","It has already been proposed for phrase-based #OTHEREFR, hierarchical #REFR, and syntax-based #OTHEREFR systems.","Our method's efficiency in handling large translation rulesets is parallel to the system described by #REFR, utilizing on-the-fly rule lookup for greater proficiency."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Availability of source information for semantic tagging (or disambignating) words in corpora is problematic. A framework to produce a semantically tagged corpus in a domain specific perspective using as source a general purpose taxonomy (i.e. Word- Net) is here proposed. The tag set is derived from higher level Wordnet synsets. A methodology aiming to support semantic bootstrapping in a NLP application is defined. Results from large scale experiments are reported 1.
###Abstract of the cited paper: I:n this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results colnparing peffo> mange of this algorithm with other corpus-based approaches to this problem.
###Context text: Hindle and Rooths,1993) show evident limitations in coverage and efficacy to deal with complex forms. In #OTHEREFR weak performances are reported for ambiguities with more that two attachment sites. These last are very frequent in a language like Italian where prepositional phrases play a role similar to English compounds.[Citation]Smaller training data set can be used and also unknown collocates are deal with, if they are able to trigger the proper semantic generalizations. The method proposed in this paper suggests and provides evidences that processing a corpus, first, to tune a general purpose taxonomy to the underlying domain and, then, sense disambiguating word occurrences according to the derived semantic lassification is feasible. The reference information (i.e. the Wordnet axonomy) isa well-known sharable resource with an explicit semantics (i.e. the hyperonimy/hyponimy hierarchy): this has a beneficial effect on the possibility to extract further lexical phenomenon (e.g.
###Response:","Class-based approaches #OTHEREFR and #REFR are more promising: the implied clustering also tackles the data sparseness difficulties, but mainly they produce selectional constraints hat have a direct semantic interpretation.",This approach aligns with recent corpus-based methods to tackle prepositional phrase attachment disambiguation as described by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We present a number of semi-supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger, yet still relatively small, set of unlabelled sentences. We take two popular dependency parsers ? one graph-based and one transition-based ? and compare results for both. Results show that using semisupervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy. We also try to use morphological information in a targeted way and fail to see any improvements.
###Abstract of the cited paper: Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications. However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain.
###Context text: In their parser co-training experiments, Steedman et al#OTHEREFR sample training items based on the confidence of the individual parsers (as approximated by parse probability). In Active Learning research, the Query By Committee selection method #OTHEREFRb)). Steedman et al#OTHEREFR discuss a sample selection approach based on differences between parsers ? if parser A and parser B disagree on an analysis, parser A can be improved by being retrained on parser B?s analysis, and vice versa.[Citation]Sample selection can be used with both selftraining and co-training. We restrict our attention to co-training since our previous experiments have demonstrated that it has more potential than selftraining. In the following set of experiments, we explore the role of both parser agreement and parser disagreement in sample selection in co-training.
###Response:","In contrast, #REFR show that parser agreement is a strong indicator of parse quality, and in parser domain adaptation, Sagae and Tsujii #OTHEREFR use agreement between parsers to choose which automatically parsed target domain items to add to the training set.","In line with the study by #REFR, we aim to predict the parser performance in new domains through certain domain characteristics, which can enhance the sample selection process in co-training."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis. A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking. The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class. We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets. Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features. We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach.
###Abstract of the cited paper: We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French. The architectures are based on PCFGs with latent variables, graph-based dependency parsing and transition-based dependency parsing, respectively. We also study the in?uence of three types of lexical information: lemmas, morphological features, and word clusters. The results show that all three systems achieve competitive performance, with a best labeled attachment score over 88%. All three parsers bene?t from the use of automatically derived lemmas, while morphological features seem to be less important. Word clusters have a positive effect primarily on the latent variable parser.
###Context text:  Table 3 shows labeled attachment score (LAS) results for our baseline parser (Lemmas) and four lexical generalization configurations.[Citation]Punctuation tokens are not scored, and significance is calculated using Dan Bikel?s randomized parsing evaluation comparator14, at significance level p=0.05. 13Proper nouns tend to have sparse counts, but for computational reasons we did not include them in our distributional thesaurus construction. We thus chose to simply generalize them Parse Evaluation Set LAS Configuration FTB Test EMEA Dev EMEA Test Europarl Forms 86.85 84.08 85.41 86.01 Lemmas 87.30 84.34 85.41 86.26 RC 87.32 84.28 85.71* 86.28 PKNL 87.46 84.63* 85.82* 86.26 RS 87.34 84.48 85.54 86.34 PKPS 87.41 84.63* 85.68* 86.22
###Response:","For comparison, we also include results for a setting that only uses word forms (Forms), which was the baseline for previous work on French dependency parsing #REFRb).",Our research implements lexical generalization configurations as demonstrated in the study of comparative performance on dependency structures for French #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Much work on information extraction has successfully used gazetteers to recognise uncommon entities that cannot be reliably identified from local context alone. Approaches to such tasks often involve the use of maximum entropy-style models, where gazetteers usually appear as highly informative features in the model. Although such features can improve model accuracy, they can also introduce hidden negative effects. In this paper we describe and analyse these effects and suggest ways in which they may be overcome. In particular, we show that by quarantining gazetteer features and training them in a separate model, then decoding using a logarithmic opinion pool (Smith et al, 2005), we may achieve much higher accuracy. Finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified.
###Abstract of the cited paper: Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.
###Context text: [Citation]Discriminative models offer a significant advantage over their generative counterparts by allowing the specification of powerful, possibly non-independent features which would be difficult to tractably encode in a generative model. In a task such as NER, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has not be seen before. In these cases, a gazetteer or dictionary of possible entity identifiers is often useful.
###Response:","In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing #OTHEREFR, noun phrase chunking #REFR and information extraction from research papers #OTHEREFR.","As demonstrated by the successful implementation of Conditional Random Fields for sequence labelling tasks, these models can outperform generative ones in terms of performance [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Traditionally, machine learning approaches for information extraction require human annotated data that can be costly and time-consuming to produce. However, in many cases, there already exists a database (DB) with schema related to the desired output, and records related to the expected input text. We present a conditional random field (CRF) that aligns tokens of a given DB record and its realization in text. The CRF model is trained using only the available DB and unlabeled text with generalized expectation criteria. An annotation of the text induced from inferred alignments is used to train an information extractor. We evaluate our method on a citation extraction task in which alignments between DBLP database records and citation texts are used to train an extractor. Experimental results demonstrate an error reduction of 35% over a previous state-of-the-art method that uses heuristic alignments.
###Abstract of the cited paper: We present two machine learning approaches to information extraction from semi-structured documents that can be used if no annotated training data are available, but there does exist a database filled with information derived from the type of documents to be processed. One approach employs standard supervised learning for information extraction by artificially constructing labelled training data from the contents of the database. The second approach combines unsupervised Hidden Markov modelling with language models. Empirical evaluation of both systems suggests that it is possible to bootstrap a field segmenter from a database alone. The combination of Hidden Markov and language modelling was found to perform best at this task.
###Context text: Settles and Craven #OTHEREFR provide a comparison of various active learning strategies for sequence labeling tasks. We have shown, however, that in domains where a database can provide significant supervision, one can bootstrap accurate extractors with very little human effort. Another area of research, related to the task described in our paper, is learning extractors from database records.[Citation]Both Agichtein and Ganti #OTHEREFR train a language model for each database column. The language modeling approach is sensitive to word re-orderings in text and other variability present in real-world text (e.g., abbreviation). We allow for word and field re-orderings through alignments and model complex transformations through feature functions.
###Response:",These records are also known as field books and reference sets in literature #REFR.,"The methods we used for information extraction, as proposed by the cited authors #REFR, can successfully utilize a database as a source of supervision, greatly reducing the need for human annotation."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) ? more than 5% higher than the previous state-of-the-art.
###Abstract of the cited paper: We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries ? such as English determiners ? resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.
###Context text: Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams #OTHEREFRb).[Citation]But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo #OTHEREFR, which hit exact solutions eventually.
###Response:",That parsing model has since been extended to make unsupervised learning more feasible #OTHEREFR; #REFRb).,"Despite these challenges, our work leverages networked local optimizers that enhance search scope, inspired by Dependency and Boundary models for unsupervised parsing #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Recent advances in large-scale, broad coverage part-of-speech tagging and syntactic parsing have been achieved in no small part due to the availability of large amounts of online, human-annotated corpora. In this paper, I argue that a large, human sensetagged corpus is also critical as well as necessary to achieve broad coverage, high accuracy word sense disambiguation, where the sense distinction is at the level of a good desk-top dictionary such as WORD- NET. Using the sense-tagged corpus of 192,800 word occurrences reported in (Ng and Lee, 1996), I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier. I also estimate the amount of human sense-tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most-frequent-sense classifier. Finally, I suggest hat intelligent example selection techniques may significantly reduce the amount of sense-tagged corpus needed and offer this research problem as a fruitful area for word sense disambiguation research.
###Abstract of the cited paper: This paper proposes benchmarks for systems of automatic sense identification. A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guessing heuristic, a most-frequent heuristic, and a co-occurrence h uristic. When no information about sense-frequencies was available, the guessing heuristic using the numbers of alternative senses in WordNet was correct 45% of the time. When statistics for sensefrequancies were derived from a semantic concordance, the assumption that each word is used in its most frequently occurring sense was correct 69% of the time; when that figure was calculated for polysemous words alone, it dropped to 58%. And when a cooccur~nce heuristic took advantage of prior occurrences of words together in the same sentences, little improvement was observed. The semantic concordance is still too small to estimate the potential limits of a co-occurrence h uristic.
###Context text: These examples clearly demonstrate he utility of WSD in practical NLP applications. In this paper, by word sense disambiguation, I mean identifying the correct sense of a word in context such that the sense distinction is at the level of a good desk-top dictionary like WORDNET #OTHEREFR. I only focus on content word disambiguation (i.e., words in the part of speech noun t, verb, adjective and adverb).[Citation]When the task is to resolve word senses to the fine-grain distinction of WORD- NET senses, the accuracy figures achieved are generally not very high #OTHEREFR. This indicates that WSD is a challenging task and much improvement is still needed. However, if one were to resolve word sense to the level of homograph, or coarse sense distinction, then quite high accuracy can be achieved #OTHEREFR.
###Response:",This is also the task addressed by other WSD research such as #OTHEREFR; #REFR.,"Drawing from earlier work, particularly the insightful benchmarks for automatic sense identification established by Ng and Lee (1996) #REFR, in-depth exploration of sense identification strategies is conducted."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In recent years there is much interest in word cooccurrence r lations, such as n-grams, verbobject combinations, or cooccurrence within a limited context. This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric. Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models.
###Abstract of the cited paper: Manual acquisition of semantic onstraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect, semantic onstraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scherne was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun ""it"" in sentences that were randomly selected from the corpus. Ttle results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic onstraints and thus provide a basis {'or a useful disambiguat.ion tool.
###Context text: Statistical data on word cooccurrence relations play a major role in many corpus based approaches for natural anguage processing. Different types of cooccurrence r lations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc.) or the cooccurrence of two words within a limited distance in the context.[Citation]A major problem for the above applications i how to estimate the probability of cooccurrences that were not observed in the training corpus. Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus #OTHEREFR. Since applications often have to compare alternative hypothesized cooccurrences, it is important to distinguish between those unobserved cooccurrences that are likely to occur in a new piece of text and those that are not.
###Response:","Statistical data about these various cooccurrence r lations is employed for a variety of applications, uch as speech recognition #OTHEREFR; #REFR.","Following the automatic scheme introduced by #REFR for collecting statistics on cooccurrence patterns, this study applies a similar methodology to estimate probabilities of unobserved cooccurrences."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes a novel statistical namedentity (i.e. ""proper name"") recognition system built around a maximum entity framework. By working v,ithin the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves cores that exceed the highest comparable scores thus-far published.
###Abstract of the cited paper: We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and / as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.
###Context text: M.E. has been successfully applied to many other tasks in computational linguistics. Some recent work for which there are solid comparable benchmarks i the work of Adwait Ratnaparkhi at the University of Pennsylvania.[Citation]Other recent work has applied M.E. to language modeling #OTHEREFR. M.E. was first applied to named entity recognition at the MUC-7 conference by #OTHEREFR. Note that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition.
###Response:","He has achieved state-of-the art results by applying M.E. to parsing #OTHEREFR, and sentence-boundary detection #REFR.","In particular, Ratnaparkhi pioneered a model for identifying sentence boundaries in raw text using M.E. with significant success #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: A great deal of work has been done demonstrating the ability of machine learning algorithms to automatically extract linguistic knowledge from annotated corpora. Very little work has gone into quantifying the difference in ability at this task between a person and a machine. This paper is a first step in that direction.
###Abstract of the cited paper: Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to ""bootstrapping"" the manual tagging process, with the goal of reducing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate ""named entities"" demonstrates that this approach can approximately double the production rate. As an ~ benefit, the combined efforts of machine and user produce domainspecific annotation rules that can be used to annotate similar texts automatically through the Alembic NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session.
###Context text: Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment ofmachine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora.[Citation]Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo- *and Woman. ple could also derive an effective rule list manually from an annotated corpus. In this paper we explore how quickly and effectively relatively untrained people can extract linguistic generalities from a corpus as compared to a machine.
###Response:","Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging #OTHEREFR, message understanding #REFR, discourse tagging #OTHEREFR.","The Alembic Workbench has proven effective in aiding manual corpus tagging, which can optimize machine learning in language processing tasks (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Crowdsourcing is a viable mechanism for creating training data for machine translation. It provides a low cost, fast turnaround way of processing large volumes of data. However, when compared to professional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality control is necessary for crowdsourcing to work well. In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals. We develop graphbased ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals.
###Abstract of the cited paper: Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon?s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.
###Context text: Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site into different languages using volunteers #OTHEREFR. DuoLingo turns translation into an educational game, and translates web content using its language learners #OTHEREFR.[Citation]This setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward #OTHEREFR. A natural approach for trying to shore up the skills of weak bilinguals is to pair them with a native speaker of the target language to edit their translations. We review relevant research from NLP and human-computer interaction (HCI) on collaborative translation processes in Section 2.
###Response:","Rather than relying on volunteers or gamification, NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk (MTurk) platform #REFR.","Our research expands on the work of #REFR, who effectively harnessed non-expert annotators on Amazon's Mechanical Turk for inexpensive, high-quality translation evaluations."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system. These models provide properly formulated, non-deficient, probability distributions over reordered phrase sequences. They are implemented by Weighted Finite State Transducers. We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering. Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks. We also show that the procedure scales as the bitext size is increased.
###Abstract of the cited paper: In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible reorderings in an appropriate way, we obtain a polynomial-time search algorithm. We investigate different reordering constraints for phrase-based statistical machine translation, namely the IBM constraints and the ITG constraints. We present efficient dynamic programming algorithms for both constraints. We evaluate the constraints with respect to translation quality on two Japanese?English tasks. We show that the reordering constraints improve translation quality compared to an unconstrained search that permits arbitrary phrase reorderings. The ITG constraints preform best on both tasks and yield statistically significant improvements compared to the unconstrained search.
###Context text: Our current work is inspired by the block #OTHEREFR in which reordering allows neighboring blocks to swap. This is described as a sequence of orientations (left, right, neutral) relative to the monotone block order. Model parameters are blockspecific and estimated over word aligned trained bitext using simple heuristics.[Citation]In these cases, 0c 4c 5c 0d 1d 1v 2v 3v 4v 5v 6v 7v 1f 2f 3f 4f 5f 6f 7f 8f 9f 2d 3d 4d 5d 2c 3c1c x 1 x 2 x 3 x 4 x 5 1e 5e 7e2e 3e 4e 6e 9e8e u 1 u 2 u 3 u 4 u 5 y 1 y 5y 4y 3y 2 doivent de_25_%exportationsgrains fl?chir exportations grains de_25_%doivent fl?chir les exportations de les exportations de grains doivent fl?chir de 25 % grains doivent fl?chir de_25_% 1.exportations doiventgrains fl?chir de_25_% grain exports are_projected_to by_25_% grain exports are projected to fall by 25 %Sentence fall Source Language Target Language Sentence Figure 1: TTM generative translation process; here, I = 9,K = 5, R = 7, J = 9. reordering is not governed by an explicit probabilistic model over reordered phrases; a language model is employed to select the translation hypothesis. We also note the prior work of Wu #OTHEREFR, closely related to Tillmann?s model.
###Response:",Other researchers #OTHEREFR; #REFR have reported performance gains in translation by allowing deviations from monotone word and phrase order.,"Drawing on the research by the cited study #REFR, we found that implementing reordering constraints can significantly improve translation quality."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. Instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. Based on this intuition, we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph. The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small.
###Abstract of the cited paper: Previous work on ordering events in text has typically focused on local pairwise decisions, ignoring globally inconsistent labels. However, temporal ordering is the type of domain in which global constraints should be relatively easy to represent and reason over. This paper presents a framework that informs local decisions with two types of implicit global constraints: transitivity (A before B and B before C implies A before C) and time expression normalization (e.g. last month is before yesterday). We show how these constraints can be used to create a more densely-connected network of events, and how global consistency can be enforced by incorporating these constraints into an integer linear programming framework. We present results on two event ordering tasks, showing a 3.6% absolute increase in the accuracy of before/after classification over a pairwise model.
###Context text: Time is an important dimension of any information space and can be useful in information retrieval, question-answering systems and timeline summaries. In the applications involving temporal analysis, document timestamps are very useful. For instance, temporal information retrieval models take into consideration the document?s creation time for document retrieval and ranking #OTHEREFR for better dealing with time-sensitive queries; some infor- ?Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users? needs.[Citation]Unfortunately, many documents on the web do not have a credible timestamp, as Chambers #OTHEREFR reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al#OTHEREFR.
###Response:","In addition, timeline summarization techniques #OTHEREFR and some event-event ordering models #REFR also rely on the timestamps.","In response to this need, we have developed an event-based time label propagation model for accurately predicting document timestamps #REFR, advancing from previous pairwise decision models #OTHEREFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
###Abstract of the cited paper: In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model. This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models ? which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.
###Context text: It is worth noting that Tetreault #OTHEREFR to identify and remove candidate antecedents that are not referentially accessible to an anaphoric pronoun in his heuristic pronoun resolvers. It would be interesting to incorporate this idea into a learning-based resolver. There are also features that do not fall into any of the preceding categories.[Citation]Memorization features have been used as binary-valued features indicating the presence or absence of their words #OTHEREFRb). An anaphoricity feature indicates whether an NP to be resolved is anaphoric, and is typically computed using an anaphoricity classifier #OTHEREFR, hand-crafted patterns (Daume. III and Marcu, 2005), and automatically acquired patterns #OTHEREFR.
###Response:","For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance #REFR.","On the topic of essential features, recent research emphasized the significance of robust feature selection in developing a coreference model #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper presents a system capable of automatically acquiring subcategorization frames (SCFs) for French verbs from the analysis of large corpora. We applied the system to a large newspaper corpus (consisting of 10 years of the French newspaper ?Le Monde?) and acquired subcategorization information for 3267 verbs. The system learned 286 SCF types for these verbs. From the analysis of 25 representative verbs, we obtained 0.82 precision, 0.59 recall and 0.69 F-measure. These results are comparable with those reported in recent related work.
###Abstract of the cited paper: We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate hat a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount 1.
###Context text: [Citation]The first experiments were performed on English but since the beginning of 2000s the approach has been successfully applied to various other languages. For example, #OTHEREFR has induced a subcategorization lexicon for German verbs from a lexicalized PCFG. Our approach is quite similar to the work done in Cambridge.
###Response:",Experiments have been made on the automatic acquisition of subcategorization frames since mid 1990s #OTHEREFR; #REFR.,"Drawing inspiration from a system that constructs a subcategorization dictionary from textual corpora in English, our French counterpart was developed [#REFR]."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Articles in the Penn TreeBank were identified as being reviews, summaries, letters to the editor, news reportage, corrections, wit and short verse, or quarterly profit reports. All but the latter three were then characterised in terms of features manually annotated in the Penn Discourse TreeBank ? discourse connectives and their senses. Summaries turned out to display very different discourse features than the other three genres. Letters also appeared to have some different features. The two main findings involve (1) differences between genres in the senses associated with intra-sentential discourse connectives, inter-sentential discourse connectives and inter-sentential discourse relations that are not lexically marked; and (2) differences within all four genres between the senses of discourse relations not lexically marked and those that are marked. The first finding means that genre should be made a factor in automated sense labelling of non-lexically marked discourse relations. The second means that lexically marked relations provide a poor model for automated sense labelling of relations that are not lexically marked.
###Abstract of the cited paper: The central questions are: How useful is information about part-of-speech frequency for text categorisation. Is it feasible to limit word features to content words for text classifications. This is examined for 5 domain and 4 genre classification tasks using LIMAS, the German equivalent of the Brown corpus. Because LIMAS is too heterogeneous, neither question can be answered reliably for any of the tasks. However, the results suggest that both questions have to be examined separately for each task at hand, because in some cases, the additional information can indeed improve performance.
###Context text: Or one might investigate how patterns of attribution vary in different genres, since this is relevant to subjectivity in text. Other aspects of genre may be even more significant for language technology. For example, whereas the first sentence of a news article might be an effective summary of its contents ? e.g. #OTHEREFR Of course, to exploit these differences, it is important to be able to automatically identify what genre or genres a text belongs to.[Citation]Of particular interest in this regard is whether other news corpora, such as the New York Times Annotated Corpus #OTHEREFRT19) manifest similar properties to theWSJ in their different genres. If so, then genre-specific extrapolation from the WSJ Corpus may enable better performance on a wider range of corpora. Acknowledgments I thank my three anonymous reviewers for their useful comments.
###Response:","Fortunately, there is a growing body of work on genre-based text classification, including #OTHEREFR; #REFR.","Understanding the usefulness of part-of-speech frequency in text classification, as explored by #REFR, may aid in automatic genre identification."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The frame-semantic parsing task is challenging for supervised techniques, even for those few languages where relatively large amounts of labeled data are available. In this preliminary work, we consider unsupervised induction of frame-semantic representations. An existing state-of-the-art Bayesian model for PropBank-style unsupervised semantic role induction (Titov and Klementiev, 2012) is extended to jointly induce semantic frames and their roles. We evaluate the model performance both quantitatively and qualitatively by comparing the induced representation against FrameNet annotations.
###Abstract of the cited paper: We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task.
###Context text: Since we focus in this study on verbal predicates only, the first stage would be trivial and the second stage could be handled with heuristics as in much of previous work on unsupervised SRL #OTHEREFR. Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as #OTHEREFR.[Citation]From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference.
###Response:","This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks #REFR.","Drawing inspiration from the unsupervised semantic parsing approach of learning a semantic parser via Markov logic, as demonstrated by #REFR, we formulated our frame-semantic parsing strategy."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: The task of identifying redundant information in documents that are generated from multiple sources provides a significant challenge for summarization and QA systems. Traditional clustering techniques detect redundancy at the sentential level and do not guarantee the preservation of all information within the document. We discuss an algorithm that generates a novel graph-based representation for a document and then utilizes a set cover approximation algorithm to remove redundant text from it. Our experiments show that this approach offers a significant performance advantage over clustering when evaluated over an annotated dataset.
###Abstract of the cited paper: Selecting important information while accounting for repetitions is a hard task for both summarization and question answering. We propose a formal model that represents a collection of documents in a two-dimensional space of textual and conceptual units with an associated mapping between these two dimensions. This representation is then used to describe the task of selecting textual units for a summary or answer as a formal optimization task. We provide approximation algorithms and empirically validate the performance of the proposed model when used with two very different sets of features, words and atomic events.
###Context text: However, this approach does not attempt to identify sub-sentential redundancy. Alternative approaches to identifying redundancy use clustering at the sentence level #OTHEREFR to remove sentences that are largely repetitive; however, as noted earlier, this is not well-suited to the redundancy task. The use of sentence simplification in conjunction with clustering #OTHEREFR could help alleviate this problem by effectively clustering smaller units, but this issue cannot be avoided unless sentences are simplified to atomic elements of information.[Citation]However, this approach uses tf-idf to approximate concepts and thus doesn?t explicitly identify redundant text. Our work draws on this approach but extends it to identify all detectable redundancies within a document set. Another approach does identify small subsentential units of information within text called ?Basic Elements? and uses these for evaluating summarizations #OTHEREFR.
###Response:","Other research has introduced the notion of identifying concepts in the input text #REFR, using a set cover algorithm to attempt to include as many concepts as possible.","In this regard, the formal model presented by #REFR, which uses a two-dimensional representation to capture repetitions in text, has influenced our method of detecting redundancies at all levels within a document."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. We find that this hypothesis only holds when it is applied to relevant dimensions. We propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion.
###Abstract of the cited paper: We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.
###Context text: A target word is represented as a vector in a high-dimensional space in which the dimensions are context items (for example, other words) and the coordinates of the vector indicate the target?s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations.[Citation]For example, ?[NP ] and other [NP ]? implies that NP is a hypernym of NP . Pattern-based approaches have been applied to meronymy #OTHEREFR. Pantel and Pennachiotti #OTHEREFR generalize the idea to a wide variety of relations.
###Response:","Early work on automatically inducing semantic relations between words, starting with #REFR, uses textual patterns.","Building on the automatic acquisition of hyponymy lexical relation from unrestricted text described by #REFR, we applied these insights to our work on distributional inclusion."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Non-verbal modalities such as gesture can improve processing of spontaneous spoken language. For example, similar hand gestures tend to predict semantic similarity, so features that quantify gestural similarity can improve semantic tasks such as coreference resolution. However, not all hand movements are informative gestures; psychological research has shown that speakers are more likely to gesture meaningfully when their speech is ambiguous. Ideally, one would attend to gesture only in such circumstances, and ignore other hand movements. We present conditional modality fusion, which formalizes this intuition by treating the informativeness of gesture as a hidden variable to be learned jointly with the class label. Applied to coreference resolution, conditional modality fusion significantly outperforms both early and late modality fusion, which are current techniques for modality combination.
###Abstract of the cited paper: The regular occurrence of disfluencies is a distinguishing characteristic of spontaneous speech. Detecting and removing such disfluencies can substantially improve the usefulness of spontaneous speech transcripts. This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information sources. Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events, transformationbased learning is used to detect edit disfluencies and conversational fillers. Results are reported on human and automatic transcripts of conversational telephone speech.
###Context text: Figure 1: An example where gesture helps to disambiguate meaning. ity combination technique that they consider trains a single classifier with all modalities combined into a single feature vector; this is sometimes called ?early fusion.. Shriberg et al also consider training separate classifiers and combining their posteriors, either through weighted addition or multiplication; this is sometimes called ?late fusion.. Late fusion is also employed for gesture-speech combination in #OTHEREFR.[Citation]Toyama and Horvitz #OTHEREFR introduce a Bayesian network approach to modality combination for speaker identification. As in late fusion, modalityspecific classifiers are trained independently. However, the Bayesian approach also learns to predict the reliability of each modality on a given instance, and incorporates this information into the Bayes net.
###Response:","Experiments in both #OTHEREFR and #REFR find no conclusive winner among early fusion, additive late fusion, and multiplicative late fusion.","Our approach introduces conditional modality fusion for resolving co-references, offering an advancement over early and late fusion methods described by Shriberg et al #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: In this paper, we introduce a system, Sentence Planning Using Description, which generates collocations within the paradigm of sentence planning. SPUD simultaneously constructs he semantics and syntax of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential and lexical interactions between multiple descriptions in a sentence, At the same time, it exploits linguistically motivated, eclarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic hoices.
###Abstract of the cited paper: We simplify previous work in the development of algorithms for the generation of referring expre~ sions while at the same time taking account of psycholinguistic findings and transcript data. The result is a straightforward algorithm that is computationally tractable, sensitive to the preferences of human users, and reasonably domain-independent. We provide a specification of the resources a host system must provide in order to make use of the algorithm, and describe an implementation used in the IDAS system.
###Context text: These specifications can include idiosyncratic semantic and pragmatic information; grammatical processes like tense marking apply normally. In this section, we describe how SPUD can be made to use words in other conventional combinations. Our proposal involves three steps.[Citation]Second, in keeping with ontological promiscuity #OTHEREFR, we represent he importance of attributes by the salience of events and states in the discourse model--these states and events now have the same status in the discourse model as any other entities. Finally, we extend SPUD is evaluation of alternatives, o that it describes the most salient entities possible, and uses basic-level terms wherever possible. By associating entities not just with salient attributes but also with salient actions and salient figurations, we capture collocations, semantic ollocations and idiomatic compositionality using a uniform mechanism.
###Response:","First, as in #REFR, we stipulate that some attributes of entities are more important than others, and that some words more naturally describe those attributes.","First, we incorporate the methodology from #REFR, simplifying the generation of referring expressions and aligning with psycholinguistic findings."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Active learning has been applied to different NLP tasks, with the aim of limiting the amount of time and cost for human annotation. Most studies on active learning have only simulated the annotation scenario, using prelabelled gold standard data. We present the first active learning experiment for Word Sense Disambiguation with human annotators in a realistic environment, using fine-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task.
###Abstract of the cited paper: We consider the impact Active Learning (AL) has on effective and efficient text corpus annotation, and report on reduction rates for annotation efforts ranging up until 72%. We also address the issue whether a corpus annotated by means of AL ? using a particular classifier and a particular feature set ? can be re-used to train classifiers different from the ones employed by AL, supplying alternative feature sets as well. We, finally, report on our experience with the AL paradigm under real-world conditions, i.e., the annotation of large-scale document corpora for the life sciences.
###Context text: Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers.[Citation]The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we do not know whether the results of experiments performed using hand-corrected data carry over to real-world scenarios in which individual human annotators produce noisy annotations.
###Response:","Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance #OTHEREFR; #REFR.","This concern is echoed in a study by #REFR, which highlighted the challenges of applying active learning in real-world conditions, particularly for large-scale document annotation tasks."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Extracting sentences that contain important information from a document is a form of text summarization. The technique is the key to the automatic generation of summaries similar to those written by humans. To achieve such extraction, it is important to be able to integrate heterogeneous pieces of information. One approach, parameter tuning by machine learning, has been attracting a lot of attention. This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To confirm the method?s performance, we conduct experiments that compare our method to three existing methods. Results on the Text Summarization Challenge (TSC) corpus show that our method offers the highest accuracy. Moreover, we clarify the different features effective for extracting different document genres.
###Abstract of the cited paper: We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMsbased systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches.
###Context text: However, most machine learning methods overfit the training data when many features are given. Therefore, we need to select features carefully. Support Vector Machines #OTHEREFR is robust even when the number of features is large.[Citation]In this paper, we present an important sentence extraction technique based on SVMs. We verified the technique against the Text Summarization Challenge #OTHEREFR corpus.
###Response:","Therefore, SVMs have shown good performance for text categorization #OTHEREFR, chunking #REFR, and dependency structure analysis #OTHEREFR.","Our approach aligns with the successful application of SVMs in the identification of English base phrases, as reported by #REFR, which showcases SVMs' high generalization performance."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper proposes a probabilistic model for associative anaphora resolution in Japanese. Associative anaphora is a type of bridging anaphora, in which the anaphor and its antecedent are not coreferent. Our model regards associative anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora.
###Abstract of the cited paper: This paper proposes a method to automatically construct Japanese nominal case frames. The point of our method is the integrated use of a dictionary and example phrases from large corpora. To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames. The constructed case frames were evaluated by hand, and were confirmed to be good quality. Experimental results of indirect anaphora resolution also indicated the effectiveness of our approach.
###Context text: [Citation]Whereas verbal case frame construction uses arguments of each verb #OTHEREFR, nominal case frame construction basically uses adnominal constituents of each noun. However, while the meaning of a verbal argument can be distinguished by the postposition, such as ?ga? (nominative), ?wo? (accusative), and ?ni? (dative), the meaning of an adnominal constituent can not be distinguished easily, because most adnominal constituents appear with the same postposition ?no? (of). Thus, we first conduct a semantic analysis of adnominal constituents, and then construct NCFs using the results as follows:
###Response:","First, we briefly introduce our method for constructing NCFs from raw corpora proposed in #REFR.",Our approach to associative anaphora resolution leverages the novel method for constructing nominal case frames in Japanese as described by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Covering as many phenomena as possible is a traditional goal of parser development, but the broader a grammar is made, the blunter it may become, as rare constructions influence the behaviour on simple sentences that were already solved correctly. We observe the effects of intentionally removing support for specific constructions from a broad-coverage grammar of German. We show that accuracy of analysing sentences from the NEGRA corpus can be improved not only for sentences that do not need the extra coverage, but even when including those that do.
###Abstract of the cited paper: This paper presents the XTAG system, agrammar development tool based on the Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic grammar for English. The various components of the system are discussed and preliminary evaluation results from the parsing of various corpora are given. Results from the comparison of X3AG against the IBM statistical parser and the Alvey Natural Language Tool parser are also given.
###Context text: [Citation]With the advent of empirical methods and the corresponding evaluationmetrics, however, this view changed considerably. #OTHEREFR was among the first who noted that the relationship between coverage and statistical parsing quality is a more complex one. Adding new rules to the grammar, i.e. increasing its coverage, does not only allow the parser to deal with more phenomena, hence more sentences; at the same time it opens up new possibilities for abusing the newly introduced rules to mis-analyse constructions which were already treated properly before. As a consequence, a net reduction in parsing quality might be observed for simple statistical reasons, since the gain usually is obtained for relatively rare phenomena,while the adverse effects might well affect frequent ones. #OTHEREFR uses this observation to argue in favour of stochastic models which attempt to choose the optimal structural interpretation instead of only providing a list of equally probable alternatives.
###Response:","Traditionally, broad coverage has always been considered to be a desirable property of a grammar: the more linguistic phenomena are treated properly by the grammar, the better results can be expected when applying it to unrestricted text #OTHEREFR; #REFR).","The XTAG system, a wide-coverage syntactic grammar tool for English, presents an example of this balancing act between breadth and precision as noted in #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.
###Abstract of the cited paper: We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.
###Context text: The standard approach to word alignment employs directional Markov models that align the words of a sentence f to those of its translation e, such as IBM Model 4 #OTHEREFR. Machine translation systems typically combine the predictions of two directional models, one which aligns f to e and the other e to f #OTHEREFR. Combination can reduce errors and relax the one-to-many structural restriction of directional models.[Citation]This paper presents a model-based alternative to aligner combination. Inference in a probabilistic model resolves the conflicting predictions of two directional models, while taking into account each model?s uncertainty over its output. This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model.
###Response:","Common combination methods include the union or intersection of directional alignments, as well as heuristic interpolations between the union and intersection like grow-diag-final #REFR.","In examining the effectiveness of phrase-based translation models, #REFR noted their usefulness in managing the uncertainty of directional models and providing improved alignment results."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Although the performance of SMT systems has improved over a range of different linguistic phenomena, negation has not yet received adequate treatment. Previous works have considered the problem of translating negative data as one of data sparsity (Wetzel and Bond (2012)) or of structural differences between source and target language with respect to the placement of negation (Collins et al. (2005)). This work starts instead from the questions ofwhat is meant by negation and what makes a good translation of negation. These questions have led us to explore the use of semantics of negation in SMT ? specifically, identifying core semantic elements of negation (cue, event and scope) in a source-side dependency parse and reranking hypotheses on the n-best list produced after decoding according to the extent to which an hypothesis realises these elements. The method shows considerable improvement over the baseline as measured by BLEU scores and Stanford?s entailmentbased MT evaluation metric (Pad? et al. (2009)).
###Abstract of the cited paper: This paper describes an efficient method to extract large n-best lists from a word graph produced by a statistical machine translation system. The extraction is based on the k shortest paths algorithm which is efficient even for very large k. We show that, although we can generate large amounts of distinct translation hypotheses, these numerous candidates are not able to significantly improve overall system performance. We conclude that large n-best lists would benefit from better discriminating models.
###Context text: In the case of negation, N-best list re-ranking allows us to assess whether a system is able to correctly translate the elements of negation, while failing to place the best hypothesis on these grounds at the top of the n-best list. The current work follows the same approach as other n-best list re-rankers #OTHEREFR) but using negation as the additional feature. Negation is here defined as the degree of overlap of cue, event and scope between the hypothesis translation and the source sentence.[Citation]This is because in order to assess the degree of overlap between the scope in the source and the hypothesis sentence, a n-gram based score is used which conveys the same information as that of the language model score in the log-linear model. Moreover, our re-ranking exploits lexical translation probabilities, thereby resembling a simple translation model.
###Response:","Following #REFR, we use an n-best list of 10000 sentences but we do not initially tune the negation feature using MERT or interpolate it with other features.",Our approach utilizes the efficient method of extracting large n-best lists from a word graph in a statistical machine translation system as proposed by #REFR.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: We consider multilingual semantic parsing ? the task of simultaneously parsing semantically equivalent sentences from multiple different languages into their corresponding formal semantic representations. Our model is built on top of the hybrid tree semantic parsing framework, where natural language sentences and their corresponding semantics are assumed to be generated jointly from an underlying generative process. We first introduce a variant of the joint generative process, which essentially gives us a new semantic parsing model within the framework. Based on the different models that can be developed within the framework, we then investigate several approaches for performing the multilingual semantic parsing task. We present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families.
###Abstract of the cited paper: We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.
###Context text: Substantial research efforts have focused on building monolingual semantic parsing systems. We survey in this section several of them.[Citation]KRISP #OTHEREFR to recursively map contiguous word sequences into semantic units to construct a tree structure. The SCISSOR model #OTHEREFR performs integrated semantic and syntactic parsing. The model parses natural language sentences into semantically augmented parse trees whose nodes consist of both semantic and syntactic labels and then builds semantic representations based on such augmented trees.
###Response:",WASP #REFR is a model motivated by statistical synchronous parsing-based machine translation #OTHEREFR.,"In a similar vein, WASP presented a novel statistical approach to semantic parsing that relied on translation techniques#REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Assigning a positive or negative score to a word out of context (i.e. a word?s prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words? prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.
###Abstract of the cited paper: This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.
###Context text: The SO-CAL entries #OTHEREFR were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label. These ratings were further validated through crowdsourcing. Other resources, such as the General Inquirer lexicon #OTHEREFR, provide a binomial classification (either positive or negative) of sentiment-bearing words.[Citation]In the following we describe in detail the two resources we used for our experiments, namely ANEW for the regression experiments and the General Inquirer (GI) for the classification ones.
###Response:",The resource presented in #REFR uses a similar binomial annotation for single words; another interesting resource is WordNetAffect #OTHEREFR but it labels words senses and it cannot be used for the prior polarity validation task.,"Our work builds on the approach outlined by #REFR, which automates the identification of sentiment expressions and determines their polarity, thereby improving upon the baseline results."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.
###Abstract of the cited paper: We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words? argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.
###Context text: However, weighting schemes introduce a large number of parameters, which we wanted to avoid. 2Each annotator was allowed to give up to three paraphrases per target in context. As a consequence, the number of gold paraphrases per target sentence varies. potential paraphrases of T , the task is to predict which of the paraphrases are applicable in s.[Citation]In our exemplar setting, we select a contextually adequate subset of contexts in which T has been observed, using T ? = act(T, s) as a generalized representation of meaning of target T in the context of s. Previous approaches used all of P as a representation for a paraphrase candidate P . However, P includes also irrelevant exemplars, while for a paraphrase to be judged as good, it is sufficient that one plausible reading exists.
###Response:",Previous approaches #OTHEREFR; #REFR have performed this task by modifying the type vector for T to the context s and then comparing the resulting vector T ? to the type vector of a paraphrase candidate P .,"We adapted a method for computing vector space representations for meaning of word occurrences in context, as done by #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This paper studies the problem of identifying erroneous/correct sentences. The problem has important applications, e.g., providing feedback for writers of English as a Second Language, controlling the quality of parallel bilingual sentences mined from the Web, and evaluating machine translation results. In this paper, we propose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising.
###Abstract of the cited paper: Recognition errors hinder the proliferation of speech recognition (SR) systems. Based on the observation that recognition errors may result in ungrammatical sentences, especially in dictation application where an acceptable level of accuracy of generated documents is indispensable, we propose to incorporate two kinds of linguistic features into error detection: lexical features of words, and syntactic features from a robust lexicalized parser. Transformation-based learning is chosen to predict recognition errors by integrating word confidence scores with linguistic features. The experimental results on a dictation data corpus show that linguistic features alone are not as useful as word confidence scores in detecting errors. However, linguistic features provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 12.30% and improve the F measure by 53.62%.
###Context text: As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method #OTHEREFR is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS.[Citation]They also require specifying error tags that can tell the specific errors and their corrections in the training corpus. The phrasal Statistical Machine Translation #OTHEREFR. This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature.
###Response:",The method #OTHEREFR aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in #REFR to learn rules to detect errors for speech recognition outputs.,Incorporating linguistic features as utilized in SR error detection methods studied by #REFR can significantly ameliorate the identification of erroneous sentences.
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overfit. The standard approach to regularising CRFs involves a prior distribution over the model parameters, typically requiring search over a hyperparameter space. In this paper we address the overfitting problem from a different perspective, by factoring the CRF distribution into a weighted product of individual ?expert. CRF distributions. We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs). We apply the LOP-CRF to two sequencing tasks. Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.
###Abstract of the cited paper: With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs.
###Context text: [Citation]In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction #OTHEREFR.
###Response:","In recent years, conditional random fields #OTHEREFR and information extraction from research papers #REFR.","[Citation] In the context of regularizing Conditional Random Fields, prior work has explored various priors and feature classes for improved regularization #REFR."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: This work focuses on the empirical investigation of distributional models for the automatic acquisition of frame inspired predicate words. While several semantic spaces, both word-based and syntaxbased, are employed, the impact of geometric representation based on dimensionality reduction techniques is investigated. Data statistics are accordingly studied along two orthogonal perspectives: Latent Semantic Analysis exploits global properties while Locality Preserving Projection emphasizes the role of local regularities. This latter is employed by embedding prior FrameNet-derived knowledge in the corresponding non-euclidean transformation. The empirical investigation here reported sheds some light on the role played by these spaces as complex kernels for supervised (i.e. Support Vector Machine) algorithms: their use configures, as a novel way to semi-supervised lexical learning, a highly appealing research direction for knowledge rich scenarios like FrameNet-based semantic parsing.
###Abstract of the cited paper: Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNetbased models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined.
###Context text: The resulting embedding is therefore expected to determine a new space where regions for LU of a given frame can be more easily discovered. Experiments have been run using the resulting spaces for task dependent kernels in a SVM learning setting. The application of the FrameNet KB on the 100 best represented frames showed that a combined use of the global and local models made available by LSA and LPP, respectively, achieves the best results, as the 67.3% of LUs recovers the same frames of the annotated dictionary.[Citation]Future work is required to increase the level of constraints made available from the semisupervised setting of LPP: syntactic information, as well as role-related evidence, can be both accommodated by the adjacency constraints imposed for LPP. This constitutes a significant area of research towards a comprehensive semisupervised model of frame semantics, entirely based on manifold learning methods, of which this study on LSA and LPP is just a starting point. Acknowledgement We want to acknowledge Prof.
###Response:",This is a significant improvement with respect to previous results achieved by the pure distributional model reported in #REFR.,"This achievement corroborates with the literature that examined the application of distributional and WordNet-based models to the FrameNet enhancement, achieving notable accuracy and coverage (#REFR)."
"You are a research paper writing assistant, proficient in generating citation sentences based on the user's paper in a given context. I will provide you with the abstract of the citing paper, the abstract of the cited paper, and the context for generating the citation sentence at the [Citation] location within the text. In the citation sentence, #REFR represents the citation of this paper, while #OTHEREFR represents the citation of another paper.The citation sentence should be between 25-30 words.
The citing paper refers to the paper the user is writing, the cited paper is the reference paper, and the context refers to the surrounding text for generating the citation sentence. The generated citation sentence needs to be logically coherent within the context, while also reflecting the actual content of the citing paper and the cited paper. The context text will provide the linguistic environment and purpose of the citation sentence, which is crucial to consider when generating it. It's essential to ensure that the generated citation sentence is not only logically sound and linguistically appropriate when combined with the context but also faithfully reflects the core content of both the citing and cited papers.
###Abstract of the citing paper: Dependency parsing with high-order features results in a provably hard decoding problem. A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems. In contrast, we explore, analyze, and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing: a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing; b) we show that, as a decoding algorithm, the greedy method surpasses dual decomposition in second-order parsing; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets.
###Abstract of the cited paper: In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
###Context text: Dependency parsing is typically guided by parameterized scoring functions that involve rich features exerting refined control over the choice of parse trees.[Citation]Much of the recent work on parsing has focused on solving these problems using powerful optimization techniques. In this paper, we follow a different strategy, arguing that a much simpler inference strategy suffices. In fact, we demonstrate that a randomized greedy method of inference surpasses the state-of-the-art performance in dependency parsing. .
###Response:","As a consequence, finding the highest scoring parse tree is a provably hard combinatorial inference problem #REFR.","These scoring functions were extended to include higher-order feature representations in MST framework as examined by McDonald et al., overcoming computational intractability through approximate parsing algorithms[#REFR]."
