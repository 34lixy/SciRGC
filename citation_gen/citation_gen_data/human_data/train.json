{"citing_paper_id": "W02-1905", "cited_paper_id": "A00-1023", "citing_paper_abstract": "This paper presents a novel approach to extracting phrase-level answers in a question answering system. This approach uses structural support provided by an integrated Natural Language Processing (NLP) and Information Extraction (IE) system. Both questions and the sentence-level candidate answer strings are parsed by this NLP/IE system into binary dependency structures. Phrase-level answer extraction is modelled by comparing the structural similarity involving the question-phrase and the candidate answerphrase. There are two types of structural support. The first type involves predefined, specific entity associa tions such as Affiliation, Position, Age for a person entity. If a question asks about one of these associations, the answer-phrase can be determined as long as the system decodes such pre-defined dependency links correctly, despite the syntactic difference used in expressions between the question and the candidate answer string. The second type involves generic grammatical relationships such as V-S (verb-subject), V-O (verbobject). Preliminary experimental results show an improvement in both precision and recall in extracting phrase-level answers, compared with a baseline system which only uses Named Entity constraints. The proposed methods are particularly effective in cases where the question-phrase does not correspond to a known named entity type and in cases where there are multiple candidate answer-phrases satisfying the named entity constraints.", "cited_paper_abstract": "This paper discusses an information extraction (IE) system, Textract, in natural language (NL) question answering (QA) and examines the role of IE in QA application. It shows: (i) Named Entity tagging is an important component for QA, (ii) an NL shallow parser provides a structural basis for questions, and (iii) high-level domain independent IE can result in a QA breakthrough.", "citation": "It is worth noticing that in our experiment, the structural support used for answer-point identification only checks the binary links involving the asking point and the candidate answer points, instead of full template matching as proposed in #REFR.", "context": "V- S matches V-S, AGE matches AGE, etc.); (iii) match the type of asking point and answer point (e.g. NePerson asking point matches NePerson and its sub-types NeMan and NeWoman; ?how? matches manner-modifier; etc.): either through direct link or indirect link based on conjunctive link (ConjLink) or equivalence link (S-P, subjectpredicative or appositive relations between two NPs). Step (1) and Step (2) attempt to leverage the structural support from parsing and high-level information extraction beyond NE.[Citation]Full template matching is best exemplified by the following example. If the incoming question is Who won the Nobel Prize in 1991, and the candidate answer string is John Smith won the Nobel Prize in 1991, the question template and answer template are shown below: win V-S: NePerson [Who] V-O: NP [the Nobel Prize] H-M: NeYear [1991] win V-S: NePerson [John Smith] V-O: NP [the Nobel Prize] H-M: NeYear [1991] The template matching will match the asking point Who with the answer point John Smith because for all the dependency links in the trees, the information is all compatible (in this case, exact match). This is the ideal case of full template matching and guarantees the high precision of the extracted answer point."}
{"citing_paper_id": "P07-2053", "cited_paper_id": "A00-1031", "citing_paper_abstract": "In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000). We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT?s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages. We present HunPos1, a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.", "cited_paper_abstract": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs ignificantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.", "citation": "Even without a formal survey it is clear that TnT #REFR is used widely in research labs throughout the world: Google Scholar shows over 400 citations.", "context": "[Citation]For research purposes TnT is freely available, but only in executable form (closed source). Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer- 1http://mokk.bme.hu/resources/hunpos/ ing system. Though taggers based on dependency networks #OTHEREFR, and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer."}
{"citing_paper_id": "P10-1130", "cited_paper_id": "A00-1031", "citing_paper_abstract": "We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning?s Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-theart by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus ? nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP.", "cited_paper_abstract": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs ignificantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.", "citation": "We kept only sentence-like runs of words (satisfying punctuation and capitalization constraints), POS-tagged with TnT #REFR.", "context": "We built a large (see Table 1) but messy data set, WEB . English-looking web-pages, pre-crawled by a search engine. To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system.[Citation]"}
{"citing_paper_id": "S12-1031", "cited_paper_id": "A00-1031", "citing_paper_abstract": "We investigate the effects of adding semantic annotations including word sense hypernyms to the source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar. The semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger. We test this using two test corpora in different domains with various sources of training data. The best reduces error rate in dependency F- score by 1% on average, while some methods produce substantial decreases in performance.", "cited_paper_abstract": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs ignificantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.", "citation": "We POS-tag using TnT #REFR and lemmatise using WordNet?s native lemmatiser.", "context": "Configurations using this are designated HP, for ?hypernym path?. 3.4.2 Disambiguating senses We return now to the question of determination of the synset for a given token. One frequentlyused and robust strategy is to lemmatise and POS- tag each token, and assign it the first-listed sense from WordNet (which may or may not be based on actual frequency counts).[Citation]This yields a leaf-level synset, making it suitable as a source of annotations for both SS and HP. We denote this ?WNF? for ?WordNet First? (shown in parentheses after SS or HP). Secondly, to evaluate whether a more informed approach to sense-tagging helps beyond the naive WNF method, in the ?SST? method, we use the outputs of SuperSense Tagger #OTHEREFR, which is optimised for assigning the supersenses described above, and can outperform a WNF- style baseline on at least some datasets."}
{"citing_paper_id": "W13-3609", "cited_paper_id": "A00-1031", "citing_paper_abstract": "We describe a system for detecting and correcting instances of a small class of frequently occurring grammatical error types in a corpus of essays which have been manually annotated for these errors. Our system employs a precise broad-coverage grammar of English which has been augmented with a set of mal-rules and malentries to explicitly license certain types of erroneous expressions. The derivation tree produced by a parser using this grammar identifies the location and type of an error in an ill-formed sentence, enabling a postprocessing script to use the tree and the inventory of error types to delete and/or insert tokens in order to produce a corrected version of the original sentence.", "cited_paper_abstract": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs ignificantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.", "citation": "In addition to using the manually constructed 37,000- word lexicon included in the ERG, we accommodate unknown words by mapping POS tags produced by TnT #REFR to generic lexical entry types on the fly.", "context": "Both the grammar and the parser are open-source resources developed and distributed as part of the DELPH-IN consortium (www.delph-in.net). We use the English Resource Grammar, described below, which we have augmented with both rules and lexical entries that license instances of certain error types, using the mal-rule approach of #OTHEREFR. For parsing each sentence with this grammar, we use the relatively efficient PET parser #OTHEREFR, along with a parse-ranking method based on a model trained on a manually disambiguated treebank, so far consisting only of parses of well-formed sentences.[Citation]The bottom-up chart parser then exhaustively applies the rules of the grammar to the lexical entries introduced by the tokens in the input sentence, producing a packed forest of analyses (derivations) ranked by likelihood, and then presents the most likely derivation for postprocessing. The post-processor is a script which uses the derivation tree to identify the type and location of each error, and then takes appropriate action, which in the course is an instructional message to the student, and in this shared task is a corrected version of the original sentence."}
{"citing_paper_id": "W13-2416", "cited_paper_id": "A00-1039", "citing_paper_abstract": "In this paper we present a semi-automatic approach for acqusition of lexico-syntactic knowledge for event extraction in two Slavic languages, namely Bulgarian and Czech. The method uses several weaklysupervised and unsupervised algorithms, based on distributional semantics. Moreover, an intervention from a language expert is envisaged on different steps in the learning procedure, which increases its accuracy, with respect to unsupervised methods for lexical and grammar learning.", "cited_paper_abstract": "Information Extraction (IE) systems are commonly based on pattern matching. Adapting an IE system to a new scenario entails the construction of a new pattern base---a timeconsuming and expensive process. We have implemented a system for finding patterns automatically from un-annotated text. Starting with a small initial set of seed patterns proposed by the user, the system applies an incremental discovery procedure to identify new patterns. We present experiments with evaluations which show that the resulting patterns exhibit high precision and recall.", "citation": "Most of the work up to now has aimed at English #OTHEREFR and #REFR), however #OTHEREFR presented automatic learning of event extraction patterns for Russian, English and Italian.", "context": "There are different approaches for event extraction.[Citation]Our work is based on weakly supervised algorithms for learning of semantic classes and patterns, presented in #OTHEREFR; these approaches are based on distributional semantics. There are different other methods which use this paradigm: A concept and pattern learning Web agent, called NELL #OTHEREFR. Parallel learning of semantic classes and patterns was presented in #OTHEREFR."}
{"citing_paper_id": "P06-2019", "cited_paper_id": "A00-1043", "citing_paper_abstract": "The ability to compress sentences while preserving their grammaticality and most of their meaning has recently received much attention. Our work views sentence compression as an optimisation problem. We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art.", "cited_paper_abstract": "We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose. The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals. Reduction can significantly improve the conciseness of automatic summaries.", "citation": "Examples include text summarisation #REFR, subtitle generation from spoken transcripts #OTHEREFR.", "context": "A mechanism for automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications.[Citation]Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion #OTHEREFR. More formally, given an input sentence of words W = w1,w2, . . . ,wn, a compressionis formed by removing any subset of these words."}
{"citing_paper_id": "W04-2309", "cited_paper_id": "A00-2001", "citing_paper_abstract": "In this paper we hypothesise that Denial of Expectation (DofE) across turns in dialogue signalled by ?but? can involve a range of different expectations, i.e., not just causal expectations, as argued in the literature. We will argue for this hypothesis and outline a methodology to distinguish the relations these denied expectations convey. Finally we will demonstrate the practical utility of this hypothesis by showing how it can improve generation of appropriate responses to DofE and decrease the likelihood of misunderstandings based on incorrectly interpreting these underlying cross-speaker relations.", "cited_paper_abstract": "This paper describes an implementation f some key aspects of a theory of dialogue processing whose main concerns are to provide models of GROUND- ING and of the role of DISCOURSE OBLIGATIONS in an agent is deliberation processes. Our system uses the TrindiKit dialogue move engine toolkit, which assumes a model of dialogue in which a participan. t is knowledge is characterised in terms of INFORMA- TION STATES which are subject o various kinds of updating mechanisms.", "citation": "We address how this information might be modelled in the PTT #OTHEREFR model of dialogue by adapting the Information State (IS) #REFR in order to facilitate more responsive generation from the system upon hearing the DofE.", "context": "Although this methodology requires human judgment to assess the results of the substitution tests, it is a first step towards distinguishing underlying relations in DofE.[Citation]"}
{"citing_paper_id": "P14-3003", "cited_paper_id": "A00-2002", "citing_paper_abstract": "Translation of discourse relations is one of the recent efforts of incorporating discourse information to statistical machine translation (SMT). While existing works focus on disambiguation of ambiguous discourse connectives, or transformation of discourse trees, only explicit discourse relations are tackled. A greater challenge exists in machine translation of Chinese, since implicit discourse relations are abundant and occur both inside and outside a sentence. This thesis proposal describes ongoing work on bilingual discourse annotation and plans towards incorporating discourse relation knowledge to a Chinese- English SMT system with consideration of implicit discourse relations. The final goal is a discourse-unit-based translation model unbounded by the traditional assumption of sentence-to-sentence translation.", "cited_paper_abstract": "We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computational model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones.", "citation": "Earlier studies of discourse relations in MT includes #REFR, which proposed a discourse transfer model to re-construct the target discourse tree from the source discourse tree, parsed by the (RST).", "context": "[Citation]However, incorporation to an SMT system was not discussed in the work. Recent works focus on the translation of ambiguous DCs, such as ?since? in the temporal sense vs. ?since? in the reason sense. This is achieved by annotating the DCs in the training data by ?translation spotting?, which is to manually align the DCs of the source text to their translation in the target text, either occurring as DCs or other expressions #OTHEREFR."}
{"citing_paper_id": "N12-1064", "cited_paper_id": "A00-2004", "citing_paper_abstract": "This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset.", "cited_paper_abstract": "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.", "citation": "For the C99 algorithm #REFR, named (C99LDA) when using topic IDs, the text is divided into minimal units on sentence boundaries.", "context": "[Citation]A similarity matrix Sm?m is computed, where m denotes the number of units (sentences). Every element sij is calculated using the cosine similarity between unit i and j. Next, a rank matrix R is computed to improve the contrast of S: Each element rij contains the number of neighbors of sij that have lower similarity scores then sij itself."}
{"citing_paper_id": "P06-1004", "cited_paper_id": "A00-2004", "citing_paper_abstract": "We consider the task of unsupervised lecture segmentation. We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion. Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies. Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.", "cited_paper_abstract": "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.", "citation": "For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks #REFR.", "context": "Not surprisingly, text segmentation has been extensively investigated over the last decade. Following the first unsupervised segmentation approach by Hearst #OTHEREFR, most algorithms assume that variations in lexical distribution indicate topic changes. When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately.[Citation]The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle. This is evident in the performance of existing unsupervised algorithms on less structured datasets, such as spoken meeting transcripts #OTHEREFR. Therefore, a more refined analysis of lexical distribution is needed."}
{"citing_paper_id": "W03-1306", "cited_paper_id": "A00-2009", "citing_paper_abstract": "Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GE- NIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score.", "cited_paper_abstract": "This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results.", "citation": "The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing and information retrieval #OTHEREFR; #REFR.", "context": "[Citation]Here we briefly review the naive Bayes model. Let ~x be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector ~x belongs to the class ck."}
{"citing_paper_id": "C04-1010", "cited_paper_id": "A00-2018", "citing_paper_abstract": "This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "This is different from the original IB1 algorithm, as described in Aha et al. #OTHEREFR; #REFR.", "context": "The data set used for experimental evaluation is the standard data set from the Wall Street Journal section of the Penn Treebank, with sections 2?21 3Given the parsing algorithm, N can never have a head or a right dependent in the current configuration. 4In TiMBL, the value of k in fact refers to k nearest distances rather than k nearest neighbors, which means that, even with k = 1, the nearest neighbor set can contain several instances that are equally distant to the test instance.[Citation]The data has been converted to dependency trees using head rules #OTHEREFR. We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins #OTHEREFR. This permits us to make exact comparisons with the parser of Yamada and Matsumoto #OTHEREFR."}
{"citing_paper_id": "C04-1204", "cited_paper_id": "A00-2018", "citing_paper_abstract": "This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations. We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "However, the accuracy of those parsers was still below PCFG parsers #OTHEREFR; #REFR in terms of the PARSEVAL score, i.e., labeled bracketing accuracy of CFG-style parse trees.", "context": "Thus, we now have a basis for integrating statistical language modeling with deep linguistic analysis. To date, accurate parsers have been developed for LTAG #OTHEREFR. Those studies have opened up the application of deep linguistic analysis to practical use.[Citation]Since one advantage of deep parsers is that they can output a sort of semantic representation, e.g. predicateargument structures, several studies have reported the accuracy of predicate-argument relations #OTHEREFR. However, their evaluation employed a treebank developed for a specific grammar formalism. Hence, those results cannot be compared fairly with parsers based on other formalisms including PCFG parsers."}
{"citing_paper_id": "C10-1151", "cited_paper_id": "A00-2018", "citing_paper_abstract": "There often exist multiple corpora for the same natural language processing (NLP) tasks. However, such corpora are generally used independently due to distinctions in annotation standards. For the purpose of full use of readily available human annotations, it is significant to simultaneously utilize multiple corpora of different annotation standards. In this paper, we focus on the challenge of constituent syntactic parsing with treebanks of different annotations and propose a collaborative decoding (or co-decoding) approach to improve parsing accuracy by leveraging bracket structure consensus between multiple parsing decoders trained on individual treebanks. Experimental results show the effectiveness of the proposed approach, which outperforms stateof-the-art baselines, especially on long sentences.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "For example, for constituent syntactic parsing #OTHEREFR; #REFR in Chinese, in addition to the most popular treebank Chinese Treebank #OTHEREFR.", "context": "However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks.[Citation]For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations."}
{"citing_paper_id": "N10-1003", "cited_paper_id": "A00-2018", "citing_paper_abstract": "We show that the automatically induced latent variable grammars of Petrov et al (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-ofthe-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "These constraints can be lexicalized #OTHEREFR; #REFR, unlexicalized #OTHEREFR.", "context": "For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information.[Citation]The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al #OTHEREFR, where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy."}
{"citing_paper_id": "P06-1006", "cited_paper_id": "A00-2018", "citing_paper_abstract": "Syntactic knowledge is important for pronoun resolution. Traditionally, the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically. In the paper, we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution. Specifically, we utilize the parse trees directly as a structured feature and apply kernel functions to this feature, as well as other normal features, to learn the resolution classifier. In this way, our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features. The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "As mentioned, the above reported results were based on #REFR?s parser.", "context": "[Citation]It would be interesting to examine the influence of different parsers on the resolution performance. For this purpose, we also tried the parser by Collins #OTHEREFR (Mode II)7, and the results are shown in Table 6. We can see that Charniak #OTHEREFR?s achieves better results for NWire."}
{"citing_paper_id": "P06-2067", "cited_paper_id": "A00-2018", "citing_paper_abstract": "In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating subcategorization cues from written and spoken language. Although Bikel?s parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language. Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "Robust statistical syntactic parsers, made possible by new statistical techniques #OTHEREFR; #REFR and by the availability of large, hand-annotated training corpora such as WSJ #OTHEREFR, have had a major impact on the field of natural language processing.", "context": "[Citation]There are many ways to make use of parsers? output. One particular form of data that can be extracted from parses is information about subcategorization. Subcategorization data comes in two forms: subcategorization frame (SCF) and subcategorization cue (SCC)."}
{"citing_paper_id": "P09-2020", "cited_paper_id": "A00-2018", "citing_paper_abstract": "We present a syntactic and lexically based discourse segmenter (SLSeg) that is designed to avoid the common problem of over-segmenting text. Segmentation is the first step in a discourse parser, a system that constructs discourse trees from elementary discourse units. We compare SLSeg to a probabilistic segmenter, showing that a conservative approach increases precision at the expense of recall, while retaining a high F-score across both formal and informal texts.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "Finally, two parsers are compared for their effect on segmentation quality: Charniak #REFR and Stanford #OTHEREFR.", "context": "Sundance is a shallow parser which provides clause segmentation on top of a basic word-tagging and phrase-chunking system. Since Sundance clauses are also too fine-grained for our purposes, we use a few simple rules to collapse clauses that are unlikely to meet our definition of EDU. The baseline segmenter in Table 1 inserts segment boundaries before and after all instances of S, SBAR, SQ, SINV, SBARQ from the syntactic parse (text spans that represent full clauses able to stand alone as sentential units).[Citation]"}
{"citing_paper_id": "P11-1065", "cited_paper_id": "A00-2018", "citing_paper_abstract": "While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "The parses of the source sentences employed by our system during training and decoding are created with the Charniak parser #REFR.", "context": "Both the baseline and our method decode Training English to French German Dutch Chinese set size BLEU NIST BLEU NIST BLEU NIST BLEU NIST 200K josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540 lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595** 400K josh-base 29.58 7.3033 18.86 5.8818 22.25 6.2949 23.24 6.7402 lts 29.83 7.4000** 19.49** 5.9374** 22.92** 6.3727** 25.16** 6.9005** Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score improvements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two. with a 3-gram language model smoothed with modified Knesser-Ney discounting #OTHEREFR, trained on around 1M sentences per target language.[Citation]We compare against a state-of-the-art hierarchical translation #OTHEREFR baseline, based on the Joshua translation system under the default training and decoding settings (josh-base). Apart of evaluating against a state-of-the-art system, especially on the English-Chinese language pair, the comparison has an added interesting aspect. The heuristically trained baseline takes advantage of ?gap rules? to reorder based on lexical context cues, but makes very limited use of the hierarchical structure above the lexical surface."}
{"citing_paper_id": "W01-0521", "cited_paper_id": "A00-2018", "citing_paper_abstract": "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser is probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "The parsing models of #REFR and Collins #OTHEREFR add more complex features to the parsing model that we use as our baseline.", "context": "In fact, they are of surprisingly little bene t even for matched training and test data | removing them from the model entirely reduces performance by less than 0.5% on the standard WSJ parsing task. Our selective pruning technique allows for a more ne grained tuning of parser model size, and would be particularly applicable to cases where large amounts of training data are available but memory usage is a consideration. In our implementation, pruning allowed models to run within 256MB that, unpruned, required larger machines.[Citation]An area for future work is investigation of the degree to which such features apply across corpora, or, on the other hand, further tune the parser to the peculiarities of the Wall Street Journal. Of particular interest are the automatic clusterings of lexical co-occurrences used in Charniak #OTHEREFR. Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser is use."}
{"citing_paper_id": "W04-0308", "cited_paper_id": "A00-2018", "citing_paper_abstract": "Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "Parsers that attempt to disambiguate the input completely ? full parsing ? typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis #OTHEREFR; #REFR.", "context": "The first is mainly practical and has to do with real-time applications such as speech recognition, which require a continually updated analysis of the input received so far. The second reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental #OTHEREFR. However, most state-of-the-art parsing methods today do not adhere to the principle of incrementality, for different reasons.[Citation]Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input ? partial parsing ? are usually deterministic and construct the final analysis in one pass over the input #OTHEREFR. But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason."}
{"citing_paper_id": "W04-2407", "cited_paper_id": "A00-2018", "citing_paper_abstract": "This paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text. Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed. The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank. The evaluation shows that memory-based learning gives a signficant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "Thus, the Penn Treebank of American English #OTHEREFR; #REFR.", "context": "It is standard practice in data-driven approaches to natural language parsing to use treebanks both for training and evaluation.[Citation]One problem when developing a parser for Swedish is that there is no comparable large-scale treebank available for Swedish. For the experiments reported in this paper we have used a manually annotated corpus of written Swedish, created at Lund University in the 1970?s and consisting mainly of informative texts from official sources #OTHEREFR. Although the original annotation scheme is an eclectic combination of constituent structure, dependency structure, and topological fields #OTHEREFR, it has proven possible to convert the annotated sentences to dependency graphs with fairly high accuracy."}
{"citing_paper_id": "W08-0308", "cited_paper_id": "A00-2018", "citing_paper_abstract": "We propose three enhancements to the treeto-string (TTS) transducer for machine translation: first-level expansion-based normalization for TTS templates, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU- 4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "The translation is from English to Chinese, and #REFR?s parser, trained on the Penn Treebank, is used to generate the syntax trees for the English side.", "context": "We used 74,597 pairs of English and Chinese sentences in the FBIS data set as our experimental data, which are further divided into 500 test sentence pairs, 500 development sentence pairs and 73597 training sentence pairs. The test set and development set are selected as those sentences having fewer than 25 words on the Chinese side.[Citation]The weights of the MT components are optimized based on the development set using a gridbased line search. The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 #OTHEREFR. Huang et al #OTHEREFR used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source."}
{"citing_paper_id": "W13-1701", "cited_paper_id": "A00-2018", "citing_paper_abstract": "We investigate the utility of linguistic features for automatically differentiating between children with varying combinations of two potentially comorbid neurodevelopmental disorders: autism spectrum disorder and specific language impairment. We find that certain manual codes for linguistic errors are useful for distinguishing between diagnostic groups. We investigate the relationship between coding detail and diagnostic classification performance, and find that a simple coding scheme is of high diagnostic utility. We propose a simple method to automate the pared down coding scheme, and find that these automatic codes are of diagnostic utility.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "The first two are minor: 1) we substitute the Charniak- Johnson reranking parser #OTHEREFR for Charniak?s original parser #REFR, and 2) we use the scikit multinomial naive bayes classifier #OTHEREFR.", "context": "0.929). The statistical detector, however, requires each error identified by any of the sub-detectors to be manually identified in the training data. We reimplement both the rule based and statistical detectors proposed by Hassanali and Liu, and apply it to our data, with three modifications.[Citation]The third difference is that we use these detectors to identify SALT error codes rather than the errors these classifiers were originally built to detect. The mapping of the original errors to SALT error codes is given in Table 5. To clarify, if we are training the ?Missing Verb? detector, then any utterance with an [OW] code is taken to be a positive example."}
{"citing_paper_id": "W13-4913", "cited_paper_id": "A00-2018", "citing_paper_abstract": "We present an empirical study on constructing a Japanese constituent parser, which can output function labels to deal with more detailed syntactic information. Japanese syntactic parse trees are usually represented as unlabeled dependency structure between bunsetsu chunks, however, such expression is insufficient to uncover the syntactic information about distinction between complements and adjuncts and coordination structure, which is required for practical applications such as syntactic reordering of machine translation. We describe a preliminary effort on constructing a Japanese constituent parser by a Penn Treebank style treebank semi-automatically made from a dependency-based corpus. The evaluations show the parser trained on the treebank has comparable bracketing accuracy as conventional bunsetsu-based parsers, and can output such function labels as the grammatical role of the argument and the type of adnominal phrases.", "cited_paper_abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trMned and tested on the previously established \\[5,9,10,15,17\\] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus \\[9\\]. The major technical innovation is tire use of a \"ma~ximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head is pre-terminal before guessing the lexical head.", "citation": "We take an approach to deriving a grammar from manually annotated corpora by training probabilistic models like current statistical constituent parsers of de facto standards #OTHEREFR ; #REFR.", "context": "This drawback complicates operating syntactically meaningful units in such applications as statistical machine translation, which needs to recognize syntactic units in building a translation model (e.g. tree-to-string and tree-to-tree) and in preordering source language sentences. Semantic analysis, such as predicate-argument structure analysis, is usually done as a pipeline process after syntactic analysis #OTHEREFR ); but in Japanese, the discrepancy between syntactic and semantic units cause difficulties integrating semantic analysis with syntactic analysis. Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output grammatical functions as semisemantic information, e.g., grammatical or semantic roles of arguments and gapping types of relative clauses.[Citation]We used a constituent-based treebank that Uematsu et al#OTHEREFR converted from an existing bunsetsubased corpus as a base treebank, and retag the nonterminals and transform the tree structures in described in Section 3. We will present the results of evaluations of the parser trained with the treebank in Section 4, and show some analyses in Section 5."}
{"citing_paper_id": "P06-2004", "cited_paper_id": "A00-2021", "citing_paper_abstract": "We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment. The supervised component is Collins? parser, trained on the Wall Street Journal. The unsupervised component gathers lexical statistics from an unannotated corpus of newswire text. We find that the combined system only improves the performance of the parser for small training sets. Surprisingly, the size of the unannotated corpus has little effect due to the noisiness of the lexical statistics acquired by unsupervised learning.", "cited_paper_abstract": "This paper describes a method for estimating conditional probability distributions over the parses of \"unification-based\" grammars which can utilize auxiliary distributions that are estimated by other means. We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic \"Unificationbased\" Grammars (SUBGs). While we apply this estimator to a Stochastic Lexical- Functional Grammar, the method is general, and should be applicable to stochastic versions of HPSGs, categorial grammars and transformational grammars.", "citation": "Other work combining supervised and unsupervised learning for parsing includes #OTHEREFR, #REFR, and #OTHEREFR.", "context": "[Citation]These papers present integrated formal frameworks for incorporating information learned from unlabeled corpora, but they do not explicitly address PP and RC attachment. The same is true for uncorrected colearning in #OTHEREFR. Conversely, no previous work on PP and RC attachment has integrated specialized ambiguity resolution into parsing."}
{"citing_paper_id": "D07-1028", "cited_paper_id": "A00-2023", "citing_paper_abstract": "We present a simple history-based model for sentence generation from LFG f-structures, which improves on the accuracy of previous models by breaking down PCFG independence assumptions so that more f-structure conditioning context is used in the prediction of grammar rule expansions. In addition, we present work on experiments with named entities and other multi-word units, showing a statistically significant improvement of generation accuracy. Tested on section 23 of the Penn Wall Street Journal Treebank, the techniques described in this paper improve BLEU scores from 66.52 to 68.82, and coverage from 98.18% to 99.96%.", "cited_paper_abstract": "This paper presents a new approach to statistical sentence generation in which Mternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation ffers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.", "citation": "One generation technique is to first generate all possible sentences, storing them in a word lattice #OTHEREFR or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator #REFR, and then select the most probable sequence of words via an n-gram language model.", "context": "These rules can be handcrafted grammar rules, such as those of #OTHEREFR. Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of #OTHEREFR. Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar.[Citation]Increasingly syntax-based information is being incorporated directly into the generation model. For example, Carroll and Oepen #OTHEREFR describe a sentence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest. A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entropy model."}
{"citing_paper_id": "W07-2216", "cited_paper_id": "A00-2030", "citing_paper_abstract": "In this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results. This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.", "cited_paper_abstract": "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.", "citation": "It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework, including information extraction #REFR and syntax-based machine translation #OTHEREFR.", "context": "On the non-projective side, algorithms are either greedy-recursive in nature (i.e., the Chu-Liu- Edmonds algorithm) or based on the calculation of the determinant of a matrix (i.e., the partition function and edge expectations). Thus, the existence of bottom-up chart parsing algorithms for projective dependency parsing provides many advantages. As mentioned above, it permits simple augmentation techniques to incorporate non-local information such as arity constraints and Markovization.[Citation]The complexity results given here suggest that polynomial chart-parsing algorithms do not exist for the non-projective case. Otherwise we should be able to augment them and move beyond edgefactored models without encountering intractability ? just like the projective case. An interesting line of research is to investigate classes of non-projective structures that can be parsed with chart-parsing algorithms and how these classes relate to the languages parsable by other syntactic formalisms."}
{"citing_paper_id": "W02-0105", "cited_paper_id": "A00-2032", "citing_paper_abstract": "This paper describes a new Cornell University course serving as a nonprogramming introduction to computer science, with natural language processing and information retrieval forming a crucial part of the syllabus. Material was drawn from a wide variety of topics (such as theories of discourse structure and random graph models of the World Wide Web) and presented at some technical depth, but was massaged to make it suitable for a freshman-level course. Student feedback from the first running of the class was overall quite positive, and a grant from the GE Fund has been awarded to further support the course?s development and goals.", "cited_paper_abstract": "Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and grammar or on pre-segmented data. In contrast, we introduce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological nalyzers over a variety of error metrics. I Introduction Because Japanese is written without delimiters between words) accurate word segmentation to recover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al, 1996; Fung, 1998). Typically, Japanese word segmentation is performed by morphological nalysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segreenters for several reasons. First and most importantly, kanji sequences often contain domain terms and proper nouns: Fung (1998) notes that 50-85% of the terms in various technical dictio- ~The analogous situation i English would be if words were written without spaces between them. Sequence l ngth # of characters % of corpus 1 - 3 kanji 20,405,486 25.6 4 - 6 kanji 12,743,177 16.1 more than 6 kanji 3,966,408 5.1 Total 37,115,071 46.8  Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,406 characters total. naries are composed at least partly of kanji. Such words tend to be missing from general-purpose lexicons, causing an unknown word problem for morphological nalyzers; yet, these terms are quite important for information retrieval, information extraction, and text summarization, making correct segmentation f these terms critical. Second, kanji sequences often consist of compound nouns, so grammatical constraints are not applicable. For instance, the sequence sha-chohlkenlgyoh-mulbuchoh (presidentlandlbusinesslgeneral m nager = \"a president as well as a general manager of business\") could be incorrectly segmented as: shachohlken-gyohlmulbu-choh (presidentl subsidiary business\\[Tsutomu \\[a name\\]\\[general manager); since both alternatives are four-noun sequences, they cannot be distinguished by part-of-speech information alone. Finally, heuristics based on changes in character type obviously do not apply to kanji-only sequences. Although kanji sequences are difficult to segment, they can comprise a significant portion of Japanese text, as shown in Figure 1. Since sequences of more than 3 kanji generally consist of more than one word, at least 21.2% of 1993 Nikkei newswire consists of kanji sequences requiring segmentation. Thus, accuracy on kanji sequences i an important aspect of the total segmentation process. As an alternative to lexico-grammatical and supervised approaches, we propose a simple, efficient segmentation method which learns mostly from very large amounts of unsegmented training data, thus avoiding the costs of building a lexicon or grammar or hand-segmenting large amounts of training data. Some key advantages of this method are: . No Japanese-specific rules are employed, enhancing portability to other languages. . A very small number of pre-segmented training examples (as few as 5 in our experiments) are needed for good performance, as long as large amounts of unsegmented data are available. . For long kanji strings, the method produces results rivalling those produced by Juman 3.61 (Kurohashi and Nagao, 1998) and Chasen 1.0 (Matsumoto et al, 1997), two morphological analyzers in widespread use. For instance, we achieve 5% higher word precision and 6% better morpheme r call.", "citation": "We also discussed an intuitive algorithm for Japanese segmentation drawn from one of my own recent research collaborations #REFR, and how word statistics were applied to determining the authorship of the Federalist Papers #OTHEREFR.", "context": "In fact, because we had previously investigated generative models for the Web, it was natural to consider Miller?s #OTHEREFR ?monkeys? model which demonstrates that very simple generative models can account for Zipf?s law. Next, we looked at methods taking advantage of statistical regularities, including the IBM Candide statistical machine translation system, following Knight?s #OTHEREFR tutorial and treating probabilities as weights. It was interesting to point out parallels with the hubs and authorities algorithm ? both are iterative update procedures with auxiliary information (alignments in one case, hubs in the other).[Citation]We concluded with an examination of human statistical learning, focusing on recent evidence indicating that human infants can use statistics when learning to segment continuous speech into words #OTHEREFR. The Turing test [2 lectures] Finally, we ended the course with a consideration of intelligence in the large. In particular, we focused on Turing?s #OTHEREFR ?Chinese Room? rebuttal that fluent language behavior is not a sufficient indication of intelligence."}
{"citing_paper_id": "N01-1014", "cited_paper_id": "A00-2038", "citing_paper_abstract": "I present a method of identifying cognates in the vocabularies of related languages. I show that a measure of phonetic similarity based on multivalued features performs better than ?orthographic? measures, such as the Longest Common Subsequence Ratio (LCSR) or Dice?s coefficient. I introduce a procedure for estimating semantic similarity of glosses that employs keyword selection and WordNet. Tests performed on vocabularies of four Algonquian languages indicate that the method is capable of discovering on average nearly 75% percent of cognates at 50% precision.", "cited_paper_abstract": "Alignment of phonetic sequences i a necessary step in many applications in computational phonology. After discussing various approaches to phonetic alignment, I present a new algorithm that combines a number of techniques developed for sequence comparison with a scoring scheme for computing phonetic similarity on the basis of multivalued features. The algorithm performs better on cognate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature.", "citation": "The implementation of ALINE is described in #REFR.", "context": "The phonetic module calculates phonetic similarity using either ALINE or a straightforward method such as LCSR, DICE, or truncation. The truncation coefficient is obtained by dividing the length of the common prefix by the average of the lengths of the two words being compared. The similarity score returned by ALINE is also normalized, so that it falls in the range ffi 0 1 .[Citation]For the calculation of a WordNet-based semantic similarity score, I initially used the length of the shortest path between synsets, measured in the num- Rank Similarity level Score Coverage 1 gloss identity 1.00 .618 2 gloss synonymy 0.70 .020 3 keyword identity 0.50 .105 4 gloss hyponymy 0.50 .023 5 keyword synonymy 0.35 .012 6 keyword hyponymy 0.25 .021 7 gloss meronymy 0.10 .002 8 keyword meronymy 0.05 .000 9 none detected 0.00 .199 Table 3: Semantic similarity levels. ber of IS-A links.4 However, I found the effect of considering paths longer than one link to be negligible. Moreover, the process of determining the link distances between all possible pairs of glosses, separately for each pair, was too time-consuming."}
{"citing_paper_id": "W05-0805", "cited_paper_id": "A00-2038", "citing_paper_abstract": "In this paper, we present an approach to automatically revealing phonological correspondences within historically related languages. We create two bilingual pronunciation dictionaries for the language pairs German-Dutch and German- English. The data is used for automatically learning phonological similarities between the two language pairs via EM- based clustering. We apply our models to predict from a phonological German word the phonemes of a Dutch and an English cognate. The similarity scores show that German and Dutch phonemes are more similar than German and English phonemes, which supplies statistical evidence of the common knowledge that German is more closely related to Dutch than to English. We assess our approach qualitatively, finding meaningful classes caused by historical sound changes. The classes can be used for language learning.", "cited_paper_abstract": "Alignment of phonetic sequences i a necessary step in many applications in computational phonology. After discussing various approaches to phonetic alignment, I present a new algorithm that combines a number of techniques developed for sequence comparison with a scoring scheme for computing phonetic similarity on the basis of multivalued features. The algorithm performs better on cognate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature.", "citation": "#REFR presents an algorithm to align phonetic sequences by computing the similarities of these words.", "context": "The word triple Text-tekst-text ([tEkst] in German, Dutch and English) can be easily recognized as a cognate; recognizing Pfeffer-peper-pepper ([pfE][f@r]- [pe:][p@r])-[pE][p@r*]), however, requires more knowledge about sound changes within the languages. The algorithms developed for machine translation search for similarities on the orthographic level, whereas some approaches to comparative and synchronic linguistics put their focus on similarities of phonological sequences. Covington #OTHEREFR, for instance, suggests different algorithms to align the phonetic representation of words of historical languages.[Citation]Nerbonne and Heeringa #OTHEREFR use phonetic transcriptions to measure the phonetic distance between different dialects. The above mentioned approaches presuppose either parallel texts of different languages for machine translation or manually compiled lists of transcribed cognates/words for analyzing synchronic or diachronic word pairs. Unfortunately, transcribed bilingual data are scarce and it is labor-intensive to collect these kind of corpora."}
{"citing_paper_id": "W07-1308", "cited_paper_id": "A00-2038", "citing_paper_abstract": "This paper discusses the reconstruction of the Elamite language?s phonology from its orthography using the Gradual Learning Algorithm, which was re-purposed to ?learn? underlying phonological forms from surface orthography. Practical issues are raised regarding the difficulty of mapping between orthography and phonology, and Optimality Theory?s neglected Lexicon Optimization module is highlighted.", "cited_paper_abstract": "Alignment of phonetic sequences i a necessary step in many applications in computational phonology. After discussing various approaches to phonetic alignment, I present a new algorithm that combines a number of techniques developed for sequence comparison with a scoring scheme for computing phonetic similarity on the basis of multivalued features. The algorithm performs better on cognate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature.", "citation": "The algorithm used in this study is a similarity-based approach, not unlike ALINE #REFR.", "context": "For an annotation graph to be ?aligned?, every grapheme must be licensed by some part of the phonology, and every phoneme must be represented in the orthography. Without such a licensing relationship, it is impossible to make the comparisons needed to count constraint violations. There is considerable previous work in the area of alignment, most recently summarized by Kondrak and Sherif #OTHEREFR.[Citation]It differs in some significant respects, notably the use of binary features. Determining the eligibility of two phonemes for matching requires a distance function. The approach taken was to assign a weight to each phonological feature, and to calculate the distance as the sum of the weights of all features that differ between the two phonemes."}
{"citing_paper_id": "W06-3502", "cited_paper_id": "A00-2041", "citing_paper_abstract": "In this paper we discuss issues related to speeding up parsing with wide-coverage unification grammars. We demonstrate that state-of-the-art optimisation techniques based on backbone parsing before unification do not provide a general solution, because they depend on specific properties of the grammar formalism that do not hold for all unification based grammars. As an alternative, we describe an optimisation technique that combines ambiguity packing at the constituent structure level with pruning based on local features.", "cited_paper_abstract": "This paper describes AUTOSEM, a robust semantic interpretation framework that can operate both at parse time and repair time. The evaluation demonstrates that AUTOSEM achieves a high level of robustness efficiently and without requiring any hand coded knowledge dedicated to repair.", "citation": "Several deep, wide-coverage parsers are currently available #OTHEREFR; #REFR, but many of these have not been designed with issues related to interpretation in a dialogue context in mind.", "context": "While typical sentences in dialogue contexts are shorter than in expository text domains, longer utterances are important in discussion oriented domains. For example, in educational applications of dialogue it is important to elicit deep explanation from students and then offer focused feedback based on the details of what students say. The choice of instructional dialogue as a target application influenced the choice of parser we needed to use for interpretation in a dialogue system.[Citation]The TRIPS grammar #OTHEREFR is a wide-coverage unification grammar that has been used very successfully in several task-oriented dialogue systems. It supports interpretation of fragments and lexical semantic features (see Section 2 for a more detailed discussion), and provides additional robustness through ?robust? rules that cover common grammar mistakes found in dialogue such as missing articles or incorrect agreement. These enhancements help parsing dialogue #OTHEREFR."}
{"citing_paper_id": "W96-0417", "cited_paper_id": "A92-1009", "citing_paper_abstract": "Comparisons are typically employed to distinguish similar entities, or to illustrate a property of an entity by referring to another com- ? monly known entity which shares that property. Based on an analysis of a corpus of encyclopaedia texts, we define three types of comparisons and outline some strategies for applying these in the generation of entity descriptions. We describe how these comparison strategies are used within the PEBA-II hypertext generation system to generate descriptions of animals.", "cited_paper_abstract": "The Intelligent Documentation Advisory System generates on-line documentation and help messages from a domain knowledge base, using natural-language (NL) generation techniques. This paper gives an overview of IDAS, with particular emphasis on: (1) its architecture and the types of questions it is capable of answering; (2) its KR and NL generation systems, and lessons we have learned in designing them; and (3) its hypertext-like user interface, and the benefits such an interface brings.", "citation": "As has been noted by others (see, for example, \\[#REFR\\] and \\[Moore 1989\\]), the dynamic generation of hypertext also permits the user to effectively drive the text generation system, alleviating from the system some of the responsibility of reasoning about what to present o the user.", "context": "Our aim is to produce texts which introduce new concepts by reference to existing knowledge the user is assumed to have, thus employing the user model to greater advantage; past research #OTHEREFR\\]) has concentrated on avoiding the production of texts which repeat what the user already knows. By mak ing use of a discourse model, we can also generate comparisons that take account of entities that have been mentioned in the previous discourse. This is particularly important in the context of the dynamic construction of hypertext documents from an underlying representation: by employing text generation techniques, we can produce context-dependent descriptions which vary depending on the information which has already been presented to the user, thus overcoming some of the limitations of hypertext documents which have been constructed simply by breaking an existing linear text into pieces.[Citation]In Section 2, we provide an overview of the PEBA-II system; in Section 3, we provide a definition of comparison and identify three types of comparison on the basis of a corpus analysis; and in Section 4 we describe how the corresponding discourse strategies are implemented in PEBA-I I . Section 5 ends the paper by pointing to some future research directions."}
{"citing_paper_id": "A97-1014", "cited_paper_id": "A92-1018", "citing_paper_abstract": "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particu- Jar representational strata.", "cited_paper_abstract": "We present an implementation f a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.", "citation": "Labels Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. #REFR and #OTHEREFR).", "context": "[Citation]For a phrase Q with children of type T . . . . . . T~: and grammatical fimctions G , , . . . , (7~:, we use the lexical probabilities PO(GiITi) and the contextual (trigram) probabilities PQ(T; \\[Ti-,, Ti-~ ) The lexical and contextual probabilities are determined separately for each type of phrase. During annotation, the highest rated granmlatical fimction labels Gi a.re calculated using the Viterbi algorithnr and a.ssigned to the structure, i.e., we. <'Mculate k argma.x H PQ(T, IT,-1, ~_~,) ."}
{"citing_paper_id": "P99-1018", "cited_paper_id": "A92-1021", "citing_paper_abstract": "We present a corpus-based study of the sequential ordering among premodifiers in noun phrases. This information is important for the fluency of generated text in practical applications. We propose and evaluate three approaches to identify sequential order among premodifiers: direct evidence, transitive closure, and clustering. Our implemented system can make over 94% of such ordering decisions correctly, as evaluated on a large, previously unseen test corpus.", "cited_paper_abstract": "Automatic part of speech tagging is an area of natural anguage processing where statistical techniques have been more successful than rulebased methods. In this paper, we present asimple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements o the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.", "citation": "We use a part-of-speech tagger \\[#REFR\\] and a finite-state grammar to extract s implex NPs .", "context": "The result of our analysis is embodied in a function, compute_order(A, B) which returns the sequential ordering between two premoditiers, word A and word B. To identify orderings among premodifiers, premodifier sequences are extracted from simplex NPs. A simplex NP is a maximal noun phrase that includes premodifiers uch as determiners and possessives but not post-nominal constituents uch as prepositional phrases or relative clauses.[Citation]The noun phrases we extract start with an optional determiner (DT) or possessive pronoun (PRP$) , followed by a sequence of cardinal numbers (CDs), adjectives (JJs), nouns (NNs), and end with a noun. We include cardinal numbers in NPs to capture the ordering of numerical information such as age and amounts . Gerunds (tagged as VBG) or past participles (tagged as VBN), such as \"heated\" in \"heated debate\", are considered as adjectives if the word in front of them is a determiner, possessive pronoun, or adjective, thus separating adjectival and verbal forms that are conflared by the tagger."}
{"citing_paper_id": "P97-1004", "cited_paper_id": "A94-1019", "citing_paper_abstract": "A system for the automatic production of controlled index terms is presented using linguistically-motivated techniques. This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unificationbased shallow-level parser using transformational rules over syntactic patterns. The contribution of this research is the successful combination of parsing over a seed term list coupled with derivational morphology to achieve greater coverage of multi-word terms for indexing and retrieval. Final results are evaluated for precision and recall, and implications for indexing and retrieval are discussed.", "cited_paper_abstract": "Both full-text information retrieval and large scale parsing require text preprocessing to identify strong lexical associations in textual databases. In order to associate linguistic felicity with computational efficiency, we have conceived FASTR a unification-based parser supporting large textual and grammatical databases. The grammar is composed of term rules obtained by tagging and lemmatizing term lists with an online dictionary. Through FASTR, large terminological data can be recycled for text processing purposes. Great stress is placed on the handling of term variations through metarules which relate basic terms to their semantically close morphosyntactic variants. The quality of terminological extraction and the computational efficiency of FASTR are evaluated through a joint experiment with an industrial documentation center. The processing of two large technical corpora shows that the application is scalable to such industrial data and that accounting for term variants results in an increase of recall by 20%. Although automatic indexing is the most straightforward application of FASTR, it can be extended fruitfully to terminological cquisition and compound interpretation.", "citation": "The same system has been effectively applied both to English and French, although this paper focuses on French (see #REFR for the case of syntactic variants in English).", "context": "Second, the term list is dynamically expanded through syntactic transformations which allow the retrieval of term variants. For example, genic expressions, genes were expressed, expression of this gene, etc. are extracted as variants of gene expression. This system relies on a full-fledged unification formalism and thus is well adapted to a fine-grained identification of terms related in syntactically and morphologically complex ways.[Citation]All evaluation experiments were performed on two corpora: a training corpus \\[ECI\\] #OTHEREFR used for evaluation. \\[ECI\\] is a subset of the European Corpus Initiative data composed of 1.3 million words of the French newspaper \"Le Monde\"; \\[AGR\\] is a set of abstracts of scientific papers in the agricultural domain from INIST/CNRS (1.1 million words). A list of terms is associated with each corpus: the terms corresponding to \\[ECI\\] were automatically extracted by LEXTER #OTHEREFR and the terms corresponding to \\[AGR\\] were extracted from the AGROVOC term list owned by INIST/CNRS. The following section describes methods for grouping multi-word term variants; Section 4 presents a linguistically-motivated method for lexical analysis (inflectional analysis, part of speech tagging, and derivational analysis); Section 5 explains term expansion methods: constructions with a local parse through syntactic transformations preserving dependency relations; Section 6 illustrates the empirical tuning of linguistic rules; Section 7 presents an evaluation of the results in terms of precision and recall."}
{"citing_paper_id": "H05-1045", "cited_paper_id": "A97-1029", "citing_paper_abstract": "Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g., detecting polarity and strength). We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments. We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al, 2001) and a variation of AutoSlog (Riloff, 1996a). While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns. Our results show that the combination of these two methods performs better than either one alone. The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.", "cited_paper_abstract": "This paper presents a statistical, earned approach to finding names and other nonrecursive ntities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.", "citation": "This task goes beyond Named Entity recognition (e.g., #REFR) because it requires the recognition of role relationships.", "context": "The goal of information extraction (IE) systems is to extract information about events, including the participants of the events.[Citation]For example, an IE system that extracts information about corporate acquisitions must distinguish between the company that is doing the acquiring and the company that is being acquired. Similarly, an IE system that extracts information about terrorism must distinguish between the person who is the perpetrator and the person who is the victim. We hypothesized that IE techniques would be wellsuited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent."}
{"citing_paper_id": "P05-1006", "cited_paper_id": "A97-1029", "citing_paper_abstract": "We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes. Our system performs at the best published accuracy on the English verbs of Senseval-2. We also experiment with using the gold-standard predicateargument labels from PropBank for disambiguating fine-grained WordNet senses and course-grained PropBank framesets, and show that disambiguation of verb senses can be further improved with better extraction of semantic roles.", "cited_paper_abstract": "This paper presents a statistical, earned approach to finding names and other nonrecursive ntities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.", "citation": "In addition, an automatic named entity tagger #REFR was run on the sentences to map proper nouns to a small set of semantic classes.1", "context": "In particular, our goal was to see the extent to which sense-tagging of verbs could be improved by adding features that capture information about predicate-arguments and selectional restrictions. We used the Mallet toolkit #OTHEREFR for learning maximum entropy models with Gaussian priors for all our experiments. In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger #OTHEREFR.[Citation]"}
{"citing_paper_id": "P98-1045", "cited_paper_id": "A97-1030", "citing_paper_abstract": "Implemented methods for proper names recognition rely on large gazetteers of common proper nouns and a set of heuristic rules (e.g. Mr. as an indicator of a PERSON entity type). Though the performance of current PN recognizers is very high (over 90%), it is important o note that this problem is by no means a \"solved problem\". Existing systems perform extremely well on newswire corpora by virtue of the availability of large gazetteers and rule bases designed for specific tasks (e.g. recognition of Organization and Person entity types as specified in recent Message Understanding Conferences MUC). However, large gazetteers are not available for most languages and applications other than newswire texts and, in any case, proper nouns are an open class. In this paper we describe a context-based method to assign an entity type to unknown proper names (PNs). Like many others, our system relies on a gazetteer and a set of context-dependent heuristics to classify proper nouns. However, due to the unavailability of large gazetteers in Italian, over 20% detected PNs cannot be semantically tagged. The algorithm that we propose assigns an entity type to an unknown PN based on the analysis of syntactically and semantically similar contexts already seen in the application corpus. The performance of the algorithm is evaluated not only in terms of precision, fo l lowing the tradit ion of MUC conferences, but also in terms of Information Gain, an information theoretic measure that takes into account the complexity of the classification task.", "cited_paper_abstract": "Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the manyto-many mapping between ames and their referents. We analyze the types of ambiguity - - structural and semantic - - that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center.", "citation": "The only exception is in #REFR where the reported performance for the sole semantic disambiguation task of PNs is 79%.", "context": "In #OTHEREFR we also show how to extend our method to incrementally update the initial gazzeteer. The performance of the proposed algorithm is more than satisfactory. A comparison with existing systems is difficult because in the literature global PN recognition performances are reported, without consider ing the semantic classification of unknowns as a subtask.[Citation]In that paper, however, semantic disambiguation is performed among a lower number of classes 5. The performance of our system is clearly affected by the dimension of the initial seed gazetteer and contextual rules. If the sets ESLA and ESLB are large enough, obviously more examples of similar contexts are found, even for unknown PNs with a single occurrence."}
{"citing_paper_id": "W03-0428", "cited_paper_id": "A97-1030", "citing_paper_abstract": "We discuss two named-entity recognition models which use characters and character -grams either exclusively or as an important part of their data representation. The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features. Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features.", "cited_paper_abstract": "Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the manyto-many mapping between ames and their referents. We analyze the types of ambiguity - - structural and semantic - - that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center.", "citation": "A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features #OTHEREFR, #REFR.", "context": "For most sequence-modeling tasks with word-level evaluation, including named-entity recognition and part-ofspeech tagging, it has seemed natural to use entire words as the basic input features. For example, the classic HMM view of these two tasks is one in which the observations are words and the hidden states encode class labels. However, because of data sparsity, sophisticated unknown word models are generally required for good performance.[Citation]One then treats the unknown word as a collection of such features. Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge. Here, we examine the utility of taking character sequences as a primary representation."}
{"citing_paper_id": "W09-1607", "cited_paper_id": "A97-1042", "citing_paper_abstract": "In this paper, we describe a sentence position based summarizer that is built based on a sentence position policy, created from the evaluation testbed of recent summarization tasks at Document Understanding Conferences (DUC). We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008. Our experiments also show that such a method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task.", "cited_paper_abstract": "This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications uch as information retrieval, routing, and text summarization.", "citation": "Ultimately, as #REFR suggest, the position method can only take us certain distance.", "context": "We also distinguish small and large documents to obtain the position policy. We described the Sub-optimal Sentence Position Policy (SPP) based on pyramid annotation data and implemented the SPP as an algorithm to show that a position policy thus formed is a good representative of the genre and thus performs way above median performance. We further describe the baselines used in summarization evaluation and discuss the need to bring back baseline 2 (or the ?SPP algorithm?) as an official baseline for update summarization task.[Citation]It has a limited power of resolution (the sentence) and its limited method of identification (the position in a text). Which is why we intend to use it as a baseline. Currently, as we can see the algorithm generates a generic summary, it doesn?t consider the topic or query to generate a query-focused summary."}
{"citing_paper_id": "W02-1706", "cited_paper_id": "A97-1052", "citing_paper_abstract": "We describe the use of a suite of highly flexible XML-based NLP tools in a project for processing and interpreting text in the medical domain. The main aim of the paper is to demonstrate the central role that XML mark-up and XML NLP tools have played in the analysis process and to describe the resultant annotated corpus of MEDLINE abstracts. In addition to the XML tools, we have succeeded in integrating a variety of non-XML ?off the shelf. NLP tools into our pipelines, so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage of a hand-crafted grammar that generates logical forms. And second, we investigate how they contribute to automatic lexical semantic acquisition processes.", "cited_paper_abstract": "We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate hat a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount 1.", "citation": "Grammar Our second method of acquiring verb grammatical relations uses the statistical parser developed by #REFR which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above.", "context": "[Citation]The statistical parser, known as the Tag Sequence Grammar (TSG), uses a hand-crafted grammar where the lexical entries are for POS tags rather than words themselves. Thus it is strings of tags that are parsed rather than strings of words. The statistical part of the system is the parse ranking component where probabilities are associated with transitions in an LR parse table."}
{"citing_paper_id": "W10-1612", "cited_paper_id": "A97-1052", "citing_paper_abstract": "IRASubcat is a language-independent tool to acquire information about the subcategorization of verbs from corpus. The tool can extract information from corpora annotated at various levels, including almost raw text, where only verbs are identified. It can also aggregate information from a pre-existing lexicon with verbal subcategorization information. The system is highly customizable, and works with XML as input and output format. IRASubcat identifies patterns of constituents in the corpus, and associates patterns with verbs if their association strength is over a frequency threshold and passes the likelihood ratio hypothesis test. It also implements a procedure to identify verbal constituents that could be playing the role of an adjunct in a pattern. Thresholds controlling frequency and identification of adjuncts can be customized by the user, or else they are given a default value.", "cited_paper_abstract": "We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate hat a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount 1.", "citation": "That?s why automatic acquisition of subcategorization frames has been an active research area since the mid-90s #OTHEREFR; #REFR.", "context": "Verbs can be characterized by their behavior in a big corpus of the language. Thus, lexicographers only need to validate, correct or complete this digested information about the behavior of verbs. Moreover, the starting information can have higher coverage and be more unbiased than if it is manually constructed.[Citation]However, most of the approaches have been adhoc for particular languages or particular settings, like a determined corpus with a given kind of annotation, be it manual or automatic. To our knowledge, there is no system to acquire subcategorization information from corpora that is flexible enough to work with different languages and levels of annotation of corpora. We present IRASubcat, a tool that acquires information about the behaviour of verbs from corpora."}
{"citing_paper_id": "W02-1203", "cited_paper_id": "C00-1004", "citing_paper_abstract": "We report on the role of the Urdu grammar in the Parallel Grammar (ParGram) project (Butt et al, 1999; Butt et al, 2002).1 The ParGram project was designed to use a single grammar development platform and a unified methodology of grammar writing to develop large-scale grammars for typologically different languages. At the beginning of the project, three typologically similar European grammars were implemented. The addition of two Asian languages, Urdu and Japanese, has shown that the basic analysis decisions made for the European languages can be applied to typologically distinct languages. However, the Asian languages required the addition of a small number of new standard analyses to cover constructions and analysis techniques not found in the European languages. With these additional standards, the ParGram project can now be applied to other typologically distinct languages.", "cited_paper_abstract": "Statistical part-of-st)eeeh(POS) taggers achieve high accuracy and robustness when based oil large, scale maimally tagged eorl)ora. Ilowever, enhancements of the learning models are necessary to achieve better 1)erforma.nce. We are develol)ing a learning tool for a Jalmnese morphological analyzer called Ch, aScn. Currently we use a fine-grained POS tag set with about 500 tags. To al)l)ly a normal trigram model on the tag set, we need unrealistic size of eorl)ora. Even, for a hi-gram model, we eanno~, 1)ret)are a ll loderate size of an mmotated corpus, when we take all the tags as distinct. A usual technique to Col)e with such fine-grained tags is to reduce the size of the tag set 1)y grouping the set of tags into equivalence classes. We introduce the concept of position-wise 9rouping where the tag set is t)artitioned into dill'el'lint equivalence classes at each t)osition in the. conditional 1)rohabilities in the Markov Model. Moreover, to eoi)e with the data Sl)arsen(?ss prot)lem caused 1) 3, exceptional t)henon> ena, we introduce several other techniques uch as word-level statistics, smoothing of word-level an(l P()S-level statistics and a selective tri-gram model. To help users determine probabilistic 1)arameters, we introduce an error-driven method for the pm'mneter selection. We then give results of exl)eriments to see the effect of the tools applied to an existing Jat)anese morphological nalyzer.", "citation": "An efficient, broad-coverage morphology was also available for Japanese #REFR and was integrated into the grammar.", "context": "Without this type of resource, it is difficult to build large-scale grammars, especially for languages with substantial morphology. For the original three languages, such morphologies were readily available. As they had been developed for information extraction applications instead of deep grammar applications, there were some minor problems, but the coverage of these morphologies is excellent.[Citation]This has aided in the Japanese grammar rapidly achieving broad coverage. It has also helped control ambiguity because in the case of Japanese, the morphology determines the part of speech of each word in the string with very little ambiguity. While some morphological analyzers already exist for Hindi,3 e.g., as part of the tools developed at the Language Technologies Research Centre (LTRC), IIT Hyderabad (http://www.iiit.net/ltrc/index.html), they are not immediately compatible with the XLE grammar development platform, nor is it clear that the morphological analyses they produce conform to the standards and methods developed within the ParGram project."}
{"citing_paper_id": "N09-3004", "cited_paper_id": "C00-1007", "citing_paper_abstract": "In this paper, we present five models for sentence realisation from a bag-of-words containing minimal syntactic information. It has a large variety of applications ranging from Machine Translation to Dialogue systems. Our models employ simple and efficient techniques based on n-gram Language modeling. We evaluated the models by comparing the synthesized sentences with reference sentences using the standard BLEU metric(Papineni et al, 2001). We obtained higher results (BLEU score of 0.8156) when compared to the state-of-art results. In future, we plan to incorporate our sentence realiser in Machine Translation and observe its effect on the translation accuracies.", "cited_paper_abstract": "Previous stochastic approaches to generation do not include a tree-based representation of syntax. While this may be adequate or even advantageous for some applications, other applications profit from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar. We present initial resuits showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a handcrafted grammar outpertbrms both.", "citation": "These concepts are then realized into words resulting in a bag of words with syntactic relations #REFR.", "context": "We propose models that take 1http://www.isi.edu/natural-language/mteval/html/412.html this information as input and produce the target sentence. We can also use our sentence realiser as an ordering module in other approaches such as #OTHEREFR, where the goal is to order an unordered bag (of treelets in this case) with dependency links. In Natural Language Generation applications such as Dialogue systems etc, the set of concepts and the dependencies between the concepts is obtained first which is known as text planning.[Citation]This is known as sentence planning. In the end, the surface string can be obtained by our models. In this paper, we do not test our models with any of the applications mentioned above."}
{"citing_paper_id": "P08-1022", "cited_paper_id": "C00-1007", "citing_paper_abstract": "In lexicalized grammatical formalisms, it is possible to separate lexical category assignment from the combinatory processes that make use of such categories, such as parsing and realization. We adapt techniques from supertagging ? a relatively recent technique that performs complex lexical tagging before full parsing (Bangalore and Joshi, 1999; Clark, 2002) ? for chart realization in OpenCCG, an open-source NLP toolkit for CCG. We call this approach hypertagging, as it operates at a level ?above? the syntax, tagging semantic representations with syntactic lexical categories. Our results demonstrate that a hypertagger-informed chart realizer can achieve substantial improvements in realization speed (being approximately twice as fast) with superior realization quality.", "cited_paper_abstract": "Previous stochastic approaches to generation do not include a tree-based representation of syntax. While this may be adequate or even advantageous for some applications, other applications profit from using as much syntactic knowledge as is available, leaving to a stochastic model only those issues that are not determined by the grammar. We present initial resuits showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a handcrafted grammar outpertbrms both.", "citation": "We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of #REFR.", "context": "[Citation]To our knowledge, we are the first to report tagging results in the semantic-to-syntactic direction. We have also shown that, by integrating this hypertagger with a broad-coverage CCG chart realizer, considerably faster realization times are possible (approximately twice as fast as compared with a realizer that performs simple lexical look-ups) with higher BLEU, METEOR and exact string match scores. Moreover, the hypertagger-augmented realizer finds more than twice the number of complete realizations, and further analysis revealed that the realization quality (as per modified BLEU and ME- TEOR) is higher in the cases when the realizer finds a complete realization."}
{"citing_paper_id": "P13-2116", "cited_paper_id": "C00-1025", "citing_paper_abstract": "Tabular information in text documents contains a wealth of information, and so tables are a natural candidate for information extraction. There are many cues buried in both a table and its surrounding text that allow us to understand the meaning of the data in a table. We study how natural-language tools, such as part-of-speech tagging, dependency paths, and named-entity recognition, can be used to improve the quality of relation extraction from tables. In three domains we show that (1) a model that performs joint probabilistic inference across tabular and natural language features achieves an F1 score that is twice as high as either a puretable or pure-text system, and (2) using only shallower features or non-joint inference results in lower quality.", "cited_paper_abstract": " Table is a very common presentation scheme, but few papers touch on table extraction in text data mining. This paper l'ocuscs on mining tables from large-scale HTML texts. Table filtering, recognition, interpretation, and presentation arc discussed. Heuristic rules and cell similarities arc employed to identify tables. The F-measure ot' table recognition is 86.50%. We also propose an algorithm to capture attribute-value r lationships alnong table cells. Finally, more structured ata is extracted and presented.", "citation": "Extracting information from tables has been discussed by different communities in the last decade, including NLP #OTHEREFR; #REFR, artificial intelligence #OTHEREFR.", "context": "In contrast, in our approach linguistic features are quite useful. The above approaches use context features that can be extracted without POS tagging or linguistic parsing. One aspect of our work is to demonstrate that traditional NLP tools can enhance the quality of table extraction.[Citation]This body of work considers only features derived from tables and does not examine richer NLP features as we do. While joint inference is popular, it is not clear when a joint inference system outperforms a more traditional NLP pipeline. Recent studies have reached a variety of conclusions: in some, joint inference helps extraction quality #OTHEREFR."}
{"citing_paper_id": "C14-1044", "cited_paper_id": "C00-1027", "citing_paper_abstract": "Linguistic accommodation is a recognised indicator of social power and social distance. However, different individuals will vary their language to different degrees, and only a portion of this variance will be due to accommodation. This paper presents the Zelig Quotient, a method of normalising linguistic variation towards a particular individual, using an author?s other communications as a baseline, thence to derive a method for identifying accommodation-induced variation with statistical significance. This work provides a platform for future efforts towards examining the importance of such phenomena in large communications datasets.", "cited_paper_abstract": "Repetition is very common. Adaptive language models, which allow probabilities to change or adapt after seeing just a few words of a text, were introduced in speech recognition to account for text cohesion. Suppose a document mentions Noriega once. What is the chance that he will be mentioned again? if the first instance has probability p, then under standard (bag-of words) independence assumptions, two instances ought to have probability p2, but we find the probability is actually closer to p/2. The first mention of a word obviously depends on frequency, but surprisingly, the second does not. Adaptation depends more on lexical content han fl'equency; there is more adaptation for content words (proper nouns, technical terminology and good keywords for information retrieval), and less adaptation for function words, cliches and ordinary first names.", "citation": "#REFR developed a method for determining lexical adaptation in text, by examining the probability of one word appearing in the later half of a document when it appears in the earlier half.", "context": "Several computational measures of linguistic accommodation already exist. These measures typically capture the extent to which language use increases in similarity or becomes ?adapted? either within a piece of text or between individuals in dialog.[Citation]This method has been used and extended by other researchers in the examination of lexical adaptation over time #OTHEREFR. Along similar lines, linguistic style matching #OTHEREFR. Ireland et al. #OTHEREFR used LSM techniques to study the predictive value of stylistic similarity in a social setting."}
{"citing_paper_id": "P05-1061", "cited_paper_id": "C00-1030", "citing_paper_abstract": "A complex relation is any n-ary relation in which some of the arguments may be be unspecified. We present here a simple two-stage method for extracting complex relations between named entities in text. The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text.", "cited_paper_abstract": "\\~e report the results of a study into the use of a linear interpolating hidden Marker model (HMM) for the task of extra.('ting lxw\\]mi(:al |;erminology fl:om MEDLINE al)stra('ts and texl;s in the molecular-bioh)gy domain. Tiffs is the first stage isl a. system that will exl;ra('l; evenl; information for automatically ut)da.ting 1)ioh)gy databases. We trained the HMM entirely with 1)igrams based (m lexical and character features in a relatively small corpus of 100 MED- LINE abstract;s that were ma.rked-ul) l)y (lomain experts wil;h term (:lasses u(:h as t)rol;eins and DNA. I.Jsing cross-validation methods we a(:\\]fieved a,n \\].e-score of 0.73 and we (',xmnine the ('ontrilmtion made by each 1)art of the interl)olation model to overconfing (la.ta Sl)arsen('.ss.", "citation": "However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes #REFR and genomic variations #OTHEREFRb) from biomedical literature.", "context": "Successful early named-entity taggers were based on finite-state generative models #OTHEREFR. More recently, discriminatively-trained models have been shown to be more accurate than generative models #OTHEREFR. Both kinds of models have been developed for tagging entities such as people, places and organizations in news material.[Citation]The next logical step for IE is to begin to develop methods for extracting meaningful relations involving named entities. Such relations would be extremely useful in applications like question answering, automatic database generation, and intelligent document searching and indexing. Though not as well studied as entity extraction, relation extraction has still seen a significant amount of work."}
{"citing_paper_id": "W00-1404", "cited_paper_id": "C00-1036", "citing_paper_abstract": "The use of XML-based authoring tools is swiftly becoming a standard in the world of technical documentation. An XML document is a mixture of structure (the tags) and surface (text between the tags). The structure reflects the choices made by the author during the top-down stepwise refinement of the document under control of a DTD grammar. These choices are typically choices of meaning which are independent of the language in which the document is rendered, and can be seen as a kind of interlingua for the class of documents which is modeled by the DTD. Based on this remark, we advocate a radicalization of XML authoring, where the semantic content of the document is accounted for exclusively in terms of choice structures, and where appropriate rendering/realization mechanisms are responsible for producing the surface, possibly in several languages imultaneously. In this view, XML authoring has strong connections to natural language generation and text authoring. We describe the IG (Interaction Grammar) formalism, an extension of DT- D is which permits powerful inguistic manipulations, and show its application to the production of multilingual versions of a certain class of pharmaceutical documents.", "cited_paper_abstract": "Typical al)proaches to XML authoring view a XML docurnent as a mixture of structure (the tags) and surlhce (texl between the tags). We advoeale a radical approach where the surface disappears from lhe XML documenl altogether to be handled exclusively by rendering mechanisms. This move is based on the view that the author is choices when authoring XML docutnciHs are best seen as language-i~eutral semantic decisions, that lhe SlftlC- lure can then be viewed as inlerlingual content, and that the textual oulpul should be derived from this co)lien\\[ by language-sl~ecific realization mechanisms, lhus assimilating XML aufllol'ing lo Mullilingual Document Amhof ing. However, slandard XMI, tools have imporlant lhnitations when used for such a ptu'pose: (1) they are weak at propagating semanlic dependencies belween dil'ferenl parts of the st,'ucture, and, (2) current XMI. rendering tools are ill-suited for handling the grammatical combination of lextual units. We present two relalcd proposals for overcoming these limitalions: one (GI:) origitmting in the Iradilion of malhemalical proof edilors and conslruct ivc type lhcery, the other (IG), a special i?at ion o f l)elinite Clause (_\\]ranllllars strongly inspired by (iF.", "citation": "Such a view, which is argued for in a related paper #REFR, emphasizes the link application of MDA to a certain domain of pharmaceutical documents.", "context": "By selecting one of these options #OTHEREFRa), which permit some typing of the surface (float, boolean, string, etc.), some degree of control is becoming more feasible. document. By contrast, a PCDATA string is language-specific.and ill-suited for multilingual applications. These remarks point to a possible radical view of XML authoring that advocates that surface strings be altogether eliminated from the document content, and that author choices be all under the explicit control of the DTD and reflected in the document structure.[Citation]"}
{"citing_paper_id": "N13-1059", "cited_paper_id": "C00-1044", "citing_paper_abstract": "We examine predicative adjectives as an unsupervised criterion to extract subjective adjectives. We do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives, i.e. another highly subjective subset of adjectives that can be extracted in an unsupervised fashion. In order to prove the robustness of this extraction method, we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons (as a gold standard).", "cited_paper_abstract": "Subjectivity is a pragmatic, sentence-level feature that has important implications for texl processing applicalions such as information exlractiou and information iclricwd. We study tile elfeels of dymunic adjectives, semantically oriented adjectives, and gradable ad.ieclivcs on a simple subjectivity classiiicr, and establish lhat lhcy arc strong predictors of subjectivity. A novel trainable mclhod thai statistically combines two indicators of gradability is presented and ewlhlalcd, complementing exisling automatic Icchniques for assigning orientation labels.", "citation": "It has been stated in previous work that if some adjective can build a comparative (e.g. nicer) or a superlative (e.g. nicest), then this adjective tends to be subjective #REFR.", "context": "As an alternative extraction method, we consider morpho-syntactically gradable adjectives. Gradable adjectives, such as nice or small, are adjectives ?that can be inflected to specify the degree or grade of something? (Wiktionary1).[Citation]We employ the property of gradability, since, firstly, it is very predictive towards subjectivity and, secondly, it is the only other unsupervised criterion currently known to extract subjective adjectives. For the extraction of gradable adjectives, we rely, on the one hand, on the part-of-speech labels JJR (comparative) and JJS (superlative). On the other hand, we also consider adjectives being modified by either more or most."}
{"citing_paper_id": "I08-2130", "cited_paper_id": "C00-1070", "citing_paper_abstract": "Part-Of-Speech(POS) tagging is the essential basis of Natural language processing(NLP). In this paper, we present an algorithm that combines a variety of context features, e.g. the POS tags of the words next to the word a that needs to be tagged and the context lexical information of a by Canonical Belief Network to together determine the POS tag of a. Experiments on a Chinese corpus are conducted to compare our algorithm with the standard HMM-based POS tagging and the POS tagging software ICTCLAS3.0. The experimental results show that our algorithm is more effective.", "cited_paper_abstract": "Since most previous works tbr HMM-1)ased tagging consider only part-ofsl)eech intbrmation in contexts, their models (:minor utilize lexical inforlnatiol~ which is crucial tbr resolving some morphological tmfl)iguity. In this paper we introduce mliformly lexicalized HMMs fin: i)artofst)eech tagging in 1)oth English and \\](ore, an. The lexicalized models use a simplified back-off smoothing technique to overcome data Sl)arsehess. In experiment;s, lexi(:alized models a(:hieve higher accuracy than non-lexicifliz(~d models and the l)ack-off smoothing metho(l mitigates data sparseness 1)etter (;ban simple smoothing methods.", "citation": "Statistic-based algorithms based on Belief Network#OTHEREFR, Lexicalized HMM#REFR and Maximal-Entropy model#OTHEREFR use the statistical information of a manually tagged corpus as background knowledge to tag new sentences.", "context": "Also, Chinese word segmentation and POS tagging can be done at the same time#OTHEREFR. There are two main approaches for POS tagging: rule-based and statistical algorithms#OTHEREFR. Rule based POS tagging methods extratct rules from training corpus and use these rules to tag new sentences#OTHEREFR.[Citation]For example, the verb is mostly followed by a noun, an adverb or nothing, so if we are sure that a word a is a verb, we could say the word b following a has a large probability to be a noun. This could be helpful specially when b has a lot of possible POS tags or it is an unknown word. Formally, this process relates to Pr(noun|verb), Pr(adverb|verb) and Pr(nothing|verb), that can be estimated from the training corpus."}
{"citing_paper_id": "P06-4007", "cited_paper_id": "C00-1072", "citing_paper_abstract": "This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A ? known as predictive questioning ? which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario.", "cited_paper_abstract": "In order to produce, a good summary, one has to identify the most relevant portions of a given text. We describe in this t)at)er a method for automatically training tel)it, signatures--sets of related words, with associated weights, organized around head topics and illustrate with signatm'es we cre- ;tt.ed with 6,194 TREC collection texts over 4 selected tot)ics. We descril)e the l)ossible integration of' tolli(: signatures with ontoh)gies and its evaluaton on an automate(l text summarization system.", "citation": "Under this approach, topic representations like those introduced in #REFR and #OTHEREFR are used to identify a set of text passages that are relevant to a user?s domain of interest.", "context": "First introduced in #OTHEREFRb), a predictive questioning approach to automatic question-answering assumes that Q/A systems can use the set of documents relevant to a user?s query in order to generate sets of questions ? known as predictive questions ? that anticipate a user?s information needs.[Citation]Topic-relevant passages are then semantically parsed (using a PropBank-style semantic parser) and submitted to a question generation module, which uses a set of syntactic rewrite rules in order to create natural language questions from the original passage. Generated questions are then assembled into question-answer pairs ? known as QUABs ? with the original passage serving as the question?s ?answer?, and are then returned to the user. For example, two of the predictive question-answer pairs generated from the documents returned for question Q0, ?What has been the impact of job outsourcing programs on India?s relationship with the U.S.??, are presented in Table 1."}
{"citing_paper_id": "P11-2118", "cited_paper_id": "C00-1072", "citing_paper_abstract": "We investigate the relevance of hierarchical topic models to represent the content of Web gists. We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. Our first approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA. We evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.", "cited_paper_abstract": "In order to produce, a good summary, one has to identify the most relevant portions of a given text. We describe in this t)at)er a method for automatically training tel)it, signatures--sets of related words, with associated weights, organized around head topics and illustrate with signatm'es we cre- ;tt.ed with 6,194 TREC collection texts over 4 selected tot)ics. We descril)e the l)ossible integration of' tolli(: signatures with ontoh)gies and its evaluaton on an automate(l text summarization system.", "citation": "Lastly, while most recent developments have been based on unsupervised data, it is also worth mentioning earlier approaches like Topic Signatures #REFR where words (or phrases) characteristic of a topic are identified using a statistical test of dependence.", "context": "While the inferred models are usually flat, in that no explicit relationship exists among topics, more complex, non-parametric, representations have been proposed to elicit the hierarchical structure of various datasets #OTHEREFR. Our purpose here is more specialized and similar to that of Labeled LDA #OTHEREFR where the set of topics associated with a document is known a priori. In both cases, document labels are mapped to constraints on the set of topics on which theotherwise unalteredtopic inference algorithm is to be applied.[Citation]Our first model extends this approach to the hierarchical setting, building actual topic models based on the selected vocabulary."}
{"citing_paper_id": "W09-2205", "cited_paper_id": "C00-1085", "citing_paper_abstract": "This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains. The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al, 2006) and Self-training (Abney, 2007; McClosky et al., 2006). A preliminary evaluation favors the use of SCL over the simpler self-training techniques.", "cited_paper_abstract": "We argue that some of the computational complexity associated with estimation of stochastic attributevalue grammars can be reduced by training upon an informative subset of the full training set. Results using the parsed Wall Street Journal corpus show that in some circumstances, it is possible to obtain better estimation results using an informative sample than when training upon all the available material. Further experimentation demonstrates that with unlexicalised models, a Gaussian prior can reduce overfitting. However, when models are lexiealised and contain overlapping features, overfitting does not seem to be a problem, and a Gmlssian prior makes minimal difference to performance. Our approach is applicable for situal;ions when there are an infeasibly large mnnber of parses in the training set, or else for when recovery of these parses fl'om a packed representation is itself comi)utationally expensive.", "citation": "To train the MaxEnt model, parameters are estimated based on informative samples #REFR.", "context": "Experimental Design The system used in this study is Alpino, a two-stage dependency parser for Dutch #OTHEREFR. The first stage consists of a HPSG-like grammar that constitutes the parse generation component. The second stage is a Maximum Entropy (MaxEnt) parse selection model.[Citation]A parse is added to the training data with a score indicating its ?goodness? #OTHEREFR. The score is obtained by comparing it with the gold standard (if available; otherwise the score is approximated through parse probability). The source domain is the Alpino Treebank #OTHEREFR (newspaper text; approx. 7,000 sentences; 145k tokens)."}
{"citing_paper_id": "P07-1041", "cited_paper_id": "C00-2126", "citing_paper_abstract": "We investigate the factors which determine constituent order in German clauses and propose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these.", "cited_paper_abstract": "In this paper we describe a method of acquiring word order fl'om corpora. Word order is defined as the order of modifiers, or the order of phrasal milts called 'bunsetsu' which depend on the stone modifiee. The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus. This model shows us to what extent each piece of information contributes to deciding the word order mid which word order tends to be selected when several kinds of information conflict. The contribution rate of each piece of information in deciding word order is eiIiciently learned by a model within a maximum entropy framework. The performance of this traiimd model can be ewfluated by checking how many instances of word order stletted by the model agree with those in the original text. In this paper, we show t, hat even a raw corpits that has not been tagged can be used to train the model, if it is first analyzed by a parser. This is possible because the word order of the text in the corpus is correct.", "citation": "The work of #REFR is done on the free word order language Japanese.", "context": "Unfortunately, they did not implement their algorithm, and it is hard to judge how well the system would perform on real data. Harbusch et al #OTHEREFR present a generation workbench, which has the goal of producing not the most appropriate order, but all grammatical ones. They also do not provide experimental results.[Citation]They determine the order of phrasal units dependent on the same modifiee. Their approach is similar to ours in that they aim at regenerating the original order from a dependency parse, but differs in the scope of the problem as they regenerate the order of modifers for all and not only for the top clausal node. Using a maximum entropy framework, they choose the most probable order from the set of all permutations of n words by the following formula: P (1|h) = P ({Wi,i+j = 1|1 ? i ? n? 1, 1 ? j ? n? i}|h) ? n?1 Y i=1 n?i Y j=1 P (Wi,i+j = 1|hi,i+j) = n?1 Y i=1 n?i Y j=1 PME(1|hi,i+j) (1) For each permutation, for every pair of words , they multiply the probability of their being in the correct2 order given the history h."}
{"citing_paper_id": "P07-1041", "cited_paper_id": "C00-2126", "citing_paper_abstract": "We investigate the factors which determine constituent order in German clauses and propose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these.", "cited_paper_abstract": "In this paper we describe a method of acquiring word order fl'om corpora. Word order is defined as the order of modifiers, or the order of phrasal milts called 'bunsetsu' which depend on the stone modifiee. The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus. This model shows us to what extent each piece of information contributes to deciding the word order mid which word order tends to be selected when several kinds of information conflict. The contribution rate of each piece of information in deciding word order is eiIiciently learned by a model within a maximum entropy framework. The performance of this traiimd model can be ewfluated by checking how many instances of word order stletted by the model agree with those in the original text. In this paper, we show t, hat even a raw corpits that has not been tagged can be used to train the model, if it is first analyzed by a parser. This is possible because the word order of the text in the corpus is correct.", "citation": "For the fourth baseline (UCHIMOTO), we utilized a maximum entropy learner (OpenNLP8) and reimplemented the algorithm of #REFR.", "context": "[Citation]For every possible permutation, its probability is estimated according to Formula (1). The binary classifier, whose task was to predict the probability that the order of a pair of constituents is correct, was trained on the following features describing the verb or hc ? the head of a constituent c9: vlex, vpass, vmod the lemma of the root of the clause (non-auxiliary verb), the voice of the verb and the number of constituents to order; lex the lemma of hc or, if hc is a functional word, the lemma of the word which depends on it; pos part-of-speech tag of hc; 6E.g. DefDet, Coords, Possr, werden 7We use the CMU Toolkit #OTHEREFR."}
{"citing_paper_id": "W01-1404", "cited_paper_id": "C00-2135", "citing_paper_abstract": "Existing studies show that a weighted context-free transduction of reasonable quality can be effectively learned from examples. This paper investigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language.", "cited_paper_abstract": "This paper describes a method to find phraselevel translation patterns from parallel corpora by applying dependency structure analysis. We use statistical dependency parsers to determine dependency relations between base phrases in a seN;ence. Our method is tested with a business expression corpus containing 10000 English Japanese sentence pairs and achieved approximately 90 % accuracy in extracting bilingual correspondences. The result shows that the use of dependency relation helps to acquire interesting translation patterns.", "citation": "Some of these studies have concentrated on finite-state or extended finite-state machinery, such as #OTHEREFR; #REFR, and yet other studies cannot be comfortably assigned to either of these two frameworks, such as #OTHEREFR.", "context": "Several studies have investigated automatic or partly automatic learning of transductions for machine translation.[Citation]In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center for Artificial Intelligence (DFKI). order between two languages, and the selection of appropriate lexical items. Furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful."}
{"citing_paper_id": "E12-1029", "cited_paper_id": "C00-2136", "citing_paper_abstract": "Most event extraction systems are trained with supervised learning and rely on a collection of annotated documents. Due to the domain-specificity of this task, event extraction systems must be retrained with new annotated data for each domain. In this paper, we propose a bootstrapping solution for event role filler extraction that requires minimal human supervision. We aim to rapidly train a state-of-the-art event extraction system using a small set of ?seed nouns? for each event role, a collection of relevant (in-domain) and irrelevant (outof-domain) texts, and a semantic dictionary. The experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the MUC-4 data set, and achieves performance levels comparable to supervised training with 700 manually annotated documents.", "cited_paper_abstract": "In developing an Infbrmation Extraction tIE) system tbr a new class of events or relations, one of the major tasks is identifying the many ways in which these events or relations may be expressed in text. This has generally involved the manual analysis and, in some cases, the annotation of large quantities of text involving these events. This paper presents an alternative approach, based on an automatic discovery procedure, ExDIsCO, which identifies a set; of relewmt documents and a set of event patterns from un-annotated text, starting from a small set of \"seed patterns.\" We evaluate ExDIScO by comparing the pertbrmance of discovered patterns against that of manually constructed systems on actual extraction tasks.", "citation": "Classical methods are either pattern-based #OTHEREFR; #REFR or classifierbased #OTHEREFR).", "context": "Event extraction techniques have largely focused on detecting event ?triggers? with their arguments for extracting role fillers.[Citation]Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision #OTHEREFR). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance #OTHEREFR)."}
{"citing_paper_id": "D11-1050", "cited_paper_id": "C00-2137", "citing_paper_abstract": "This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjective-noun phrases, such as e.g. hot in hot summer denoting the attribute TEMPERATURE, rather than TASTE. We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.", "cited_paper_abstract": "Statisti(:a,1 signiticance testing of (litl'erelmeS in v;~hl(`-s of metri(:s like recall, i)rccision and batau(:(~(l F-s(:()rc is a ne(:(`-ssary t)art of eml)irical ual;ural language 1)ro(:essing. Unfortunately, we lind in a set of (;Xl)erinlc\\]d;s (;hal; many (:oreinertly used tesl;s ofte, n underest imate t.he s ignif icancc an(l so are less likely to detect differences that exist 1)el;ween ditl'ercnt techniques. This undel'esi;imation comes from an in(let)endcn('(~ a,-;SUlnl)tion that is often violated. \\~fe l)oint out some useful l;e,%s (;hal; (lo nol; make this assuml)- lion, including computationally--intcnsive rand()mizat,ion 1;cs|;s.", "citation": "Where appropriate, we test differences in the performance of various model configurations for statistical significance in a randomized permutation test #REFR, using the sigf tool #OTHEREFR.", "context": "Evaluation measures. We evaluate against two gold standards consisting of adjective-noun phrases (or adjective-noun pairs) and their associated attribute meanings. We report precision, recall and f1-score.[Citation]Baselines. We compare our models against two baselines, PATTVSM and DEPVSM. PATTSVM is reconstructed from Hartung and Frank #OTHEREFR."}
{"citing_paper_id": "W04-2614", "cited_paper_id": "C00-2148", "citing_paper_abstract": "Current lexical semantic representations for natural language applications view verbs as simple predicates over their arguments. These structures are too coarse-grained to capture many important generalizations about verbal argument structure. In this paper, I specifically defend the following two claims: verbs have rich internal structure expressible in terms of finer-grained primitives of meaning, and at least for some languages, verbal meaning is compositionally derived from these primitive elements. I primarily present evidence from Mandarin Chinese, whose verbal system is very different from that of English. Many empirical facts about the typology of verbs in Mandarin cannot be captured by a ?flat? lexical semantic representation. These theoretical results hold important practical consequences for natural language processing applications.", "cited_paper_abstract": "We present a class-based approach to building a verb lexicon that makes explicit the close association between syntax and semantics for Levin classes. We have used Lexicalized Tree Adjoining Grammars to capture the syntax associated with each verb class and have augmented the trees to include selectional restrictions. In addition, semantic predicates are associated with each tree, which allow for a colnpositional interpretation.", "citation": "While I may disagree with the technical details, I believe that the approach taken by #REFR is on the right path.", "context": "The important lessons are the theoretical constraints imposed by verbal typology on lexical semantic representations designed for language applications. More specifically: (28) a. verbs have rich internal structure expressible in terms of finer-grained primitives of meaning, and b. at least for some languages, verbal meaning is compositionally derived from these primitive elements. These claims imply that a PropBank or FrameNet approach to lexical semantics will not be sufficient for many language applications, at least for languages such as Mandarin Chinese.[Citation]Due to the productivity of verbal phenomena in Mandarin, it is impossible to exhaustively enumerate all felicitous predicates? verbal meaning, therefore, must be compositionally derived from primitive elements. This however, does not mean that PropBank or FrameNet are not useful; quite the contrary! Existing semantic resources serve as the foundation from which we can bootstrap finer-grained semantic representations. While the approach Palmer and Wu #OTHEREFR take to lexical semantics captures many selectional restrictions and finer-grained facets of meaning, it still does not model the arbitrary productivity of verbal compounds."}
{"citing_paper_id": "P13-3021", "cited_paper_id": "C00-2163", "citing_paper_abstract": "We present experiments using a new unsupervised approach to automatic text simplification, which builds on sampling and ranking via a loss function informed by readability research. The main idea is that a loss function can distinguish good simplification candidates among randomly sampled sub-sentences of the input sentence. Our approach is rated as equally grammatical and beginner reader appropriate as a supervised SMT-based baseline system by native speakers, but our setup performs more radical changes that better resembles the variation observed in human generated simplifications.", "cited_paper_abstract": "In this paper, we t)resent and compare various alignnmnt models for statistical machine translation. We propose to measure tile quality of an aligmnent model using the quality of the Viterbi alignment comt)ared to a manually-produced alignment and describe a refined mmotation scheme to produce suitable reference alignments. We also con,pare the impact of different; alignment models on tile translation quality of a statistical machine translation system.", "citation": "Following Coster and Kauchak #OTHEREFR, with GIZA++ wordalignment #REFR and phrase tables learned from the sentence aligned portion of the DSim corpus.", "context": "To train an SMT system, a large resource of aligned parallel text and a language model of the target language are needed. We combined the 25 million words Danish Korpus 20001 with the entire 1.75 million words unaligned DSim corpus #OTHEREFR to build the language model2. Including both corpora gives better coverage and assigns lower average ppl and a simlar difference in average ppl between the two sides of a held out part of the DSim corpus compared to using only the simplified part of DSim for the language model.[Citation]1http://korpus.dsl.dk/korpus2000/ engelsk_hovedside 2The LM was a 5-gram Knesser-Ney smoothed lowercase model, built using IRSTLM #OTHEREFR"}
{"citing_paper_id": "P06-1078", "cited_paper_id": "C02-1054", "citing_paper_abstract": "This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature. The ASR confidence feature indicates whether each word has been correctly recognized. The NER model is trained using ASR results with named entity (NE) labels as well as the corresponding transcriptions with NE labels. In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of textbased NER to ASR results in NER F- measure by improving precision. These results show that the proposed method is effective in NER for noisy inputs.", "cited_paper_abstract": "Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster. This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selection method and an efficient training method.", "citation": "On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy #OTHEREFR, support vector machines (SVMs) #REFR, and conditional random fields #OTHEREFR.", "context": "Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models #OTHEREFR.[Citation]Zhai et al #OTHEREFR applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf #OTHEREFR proposed an HMM- based NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word hypotheses in the ASR results."}
{"citing_paper_id": "D12-1111", "cited_paper_id": "C02-1061", "citing_paper_abstract": "Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus ? a word sense along with its synonyms and antonyms ? is treated as a ?document,? and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure.", "cited_paper_abstract": "For meaning representations in NLP, we focus our attention on thematic aspects and conceptual vectors. The learning strategy of conceptual vectors relies on a morphosyntaxic analysis of human usage dictionary definitions linked to vector propagation. This analysis currently doesn?t take into account negation phenomena. This work aims at studying the antonymy aspects of negation, in the larger goal of its integration into the thematic analysis. We present a model based on the idea of symmetry compatible with conceptual vectors. Then, we define antonymy functions which allows the construction of an antonymous vector and the enumeration of its potentially antinomic lexical items. Finally, we introduce a measure which evaluates how a given word is an acceptable antonym for a term.", "citation": "We follow #REFR in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis.", "context": "Work in this area includes #OTHEREFR. Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general #OTHEREFR. In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated.[Citation]However, in contrast to the logical axes used previously, we desire that antonyms should lie at the opposite ends of a sphere lying in a continuous and automatically induced vector space. To generate this vector space, we present a novel method for assigning both negative and positive values to the TF-IDF weights used in latent semantic analysis. To determine these signed values, we exploit the information present in a thesaurus."}
{"citing_paper_id": "C04-1127", "cited_paper_id": "C02-1070", "citing_paper_abstract": "In this paper, we discuss the performance of crosslingual information extraction systems employing an automatic pattern acquisition module. This module, which creates extraction patterns starting from a user?s narrative task description, allows rapid customization to new extraction tasks. We compare two approaches: (1) acquiring patterns in the source language, performing source language extraction, and then translating the resulting templates to the target language, and (2) translating the texts and performing pattern discovery and extraction in the target language. We demonstrate an average of 8-10% more recall using the first approach. We discuss some of the problems with machine translation and their effect on pattern discovery which lead to this difference in performance.", "cited_paper_abstract": "Information extraction (IE) systems are costly to build because they require development texts, parsing tools, and specialized dictionaries for each application domain and each natural language that needs to be processed. We present a novel method for rapidly creating IE systems for new languages by exploiting existing IE systems via crosslanguage projection. Given an IE system for a source language (e.g., English), we can transfer its annotations to corresponding texts in a target language (e.g., French) and learn information extraction rules for the new language automatically. In this paper, we explore several ways of realizing both the transfer and learning processes using off-theshelf machine translation systems, induced word alignment, attribute projection, and transformationbased learning. We present a variety of experiments that show how an English IE system for a plane crash domain can be leveraged to automatically create a French IE system for the same domain.", "citation": "#REFR present several approaches to cross-lingual information extraction (CLIE).", "context": "[Citation]They describe the use of ?cross-language projection? for CLIE, exploiting the word alignment of documents in one language and the same documents translated into a different language by a machine translation (MT) system. They conducted experiments between two relatively close languages, English and French. In the experiment reported here, we will explore CLIE for two more disparate languages, English and Japanese."}
{"citing_paper_id": "P06-1038", "cited_paper_id": "C02-1114", "citing_paper_abstract": "We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning. We utilize meta-patterns of highfrequency words and content words in order to discover pattern candidates. Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets. Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNetbased evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported.", "cited_paper_abstract": "This paper presents an unsupervised method for assembling semantic knowledge from a part-ofspeech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word.", "citation": "Many Information Extraction papers discover relationships between words using syntactic patterns #OTHEREFR. #REFR discover categories using two hard-coded symmetric patterns, and are thus the closest to us.", "context": "Both use information obtained by parsing. #OTHEREFR reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in #OTHEREFR using coclustering.[Citation]They also introduce an elegant graph representation that we adopted. They report good results. However, they require POS tagging of the corpus, use only two hard-coded patterns (?x and y?, ?x or y?), deal only with nouns, and require non-trivial computations on the graph."}
{"citing_paper_id": "W03-0910", "cited_paper_id": "C02-1132", "citing_paper_abstract": "This paper presents a methodology for using the argument structure of sentences, as encoded by the PropBank project, to develop clusters of verbs with similar meaning and usage. These clusters can be favorably compared to the classes developed by the VerbNet project. The most interesting cases are those where the clustering methodology suggests new members for VerbNet classes which will then be associated with the semantic predicates for that class.", "cited_paper_abstract": "We evaluate probabilistic models of verb argument structure trained on a corpus of verbs and their syntactic arguments. Models designed to represent patterns of verb alternation behavior are compared with generic clustering models in terms of the perplexity assigned to held-out test data. While the specialized models of alternation do not perform as well, closer examination reveals alternation behavior represented implicitly in the generic models.", "citation": "Various attempts have been made to automatically cluster verbs into semantically meaningful classes, using the Levin class as a gold standard for evaluation #REFR.", "context": "Various attempts worldwide have begun focussing on the argument structure of verbs as part of developing dependency grammars. The PropBank project at Penn #OTHEREFR, and others. The FrameNet project at Berkeley #OTHEREFR has classi ed many words in terms of their relation to a relatively small number of core semantic concepts such as `commerce' and `judgment'.[Citation]In the next two sections, we provide background on VerbNet and PropBank which play central roles in the cluster methodology presented here."}
{"citing_paper_id": "D10-1035", "cited_paper_id": "C02-1154", "citing_paper_abstract": "Multi-category bootstrapping algorithms were developed to reduce semantic drift. By extracting multiple semantic lexicons simultaneously, a category?s search space may be restricted. The best results have been achieved through reliance on manually crafted negative categories. Unfortunately, identifying these categories is non-trivial, and their use shifts the unsupervised bootstrapping paradigm towards a supervised framework. We present NEG-FINDER, the first approach for discovering negative categories automatically. NEG-FINDER exploits unsupervised term clustering to generate multiple negative categories during bootstrapping. Our algorithm effectively removes the necessity of manual intervention and formulation of negative categories, with performance closely approaching that obtained using negative categories defined by a domain expert.", "cited_paper_abstract": "We present an algorithm, Nomen, for learning generalized names in text. Examples of these are names of diseases and infectious agents, such as bacteria and viruses. These names exhibit certain properties that make their identi cation more complex than that of regular proper names. Nomen uses a novel form of bootstrapping to grow sets of textual instances and of their contextual patterns. The algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously. We present results of the algorithm on a large corpus. We also investigate the relative merits of several evaluation strategies.", "citation": "Multi-category bootstrapping algorithms, such as BASILISK #OTHEREFR, NOMEN #REFR, and WMEB #OTHEREFR, aim to reduce semantic drift by extracting multiple semantic categories simultaneously.", "context": "When bootstrapping semantic lexicons, polysemous or erroneous terms and/or patterns that weakly constrain the semantic class are eventually extracted. This often causes semantic drift ? when a lexicon?s intended meaning shifts into another category during bootstrapping #OTHEREFR. For example, female names may drift into gemstones when the terms Ruby and Pearl are extracted.[Citation]These algorithms utilise information about other semantic categories in order to reduce the categories from drifting towards each other. This framework has recently been extended to extract different relations from text #OTHEREFR."}
{"citing_paper_id": "C02-1056", "cited_paper_id": "C02-1163", "citing_paper_abstract": "One of the key issues in spoken language translation is how to deal with unrestricted expressions in spontaneous utterances. This research is centered on the development of a Chinese paraphraser that automatically paraphrases utterances prior to transfer in Chinese-Japanese spoken language translation. In this paper, a pattern-based approach to paraphrasing is proposed for which only morphological analysis is required. In addition, a pattern construction method is described through which paraphrasing patterns can be efficiently learned from a paraphrase corpus and human experience. Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated.", "cited_paper_abstract": "A machine translation model has been proposed where an input is translated through both source-language and target-language paraphrasing processes. We have implemented our prototype model for the Japanese-Chinese language pair. This paper describes our core idea of translation, where a source language paraphraser and a language transfer cooperates in translation by exchanging information about the source input.", "citation": "To resolve this problem, we have proposed a paraphrasing approach in which the utterances are automatically paraphrased prior to transfer #OTHEREFR; #REFR.", "context": "In spoken language translation one of the key issues is how to deal with unrestricted expressions in spontaneous utterances.[Citation]The paraphrasing process aims to bridge the gap between the unrestricted expressions in the input and the limited expressions that the transfer can translate. In fact, paraphrasing actions are often seen in daily communication. When a listener cannot understand what a speaker said, the speaker usually says it again using other words, i.e., he paraphrases."}
{"citing_paper_id": "E06-2030", "cited_paper_id": "C02-2021", "citing_paper_abstract": "We report work1 in progress on adding affect-detection to an existing program for virtual dramatic improvisation, monitored by a human director. To partially automate the directors? functions, we have partially implemented the detection of emotions, etc. in users? text input, by means of pattern-matching, robust parsing and some semantic analysis. The work also involves basic research into how affect is conveyed by metaphor.", "cited_paper_abstract": "A detailed approach has been developed for core aspects of the task of understanding a broad class of metaphorical utterances. The utterances in question are those that depend on known metaphorical mappings but that nevertheless contain elements not mapped by those mappings. A reasoning system has been implemented that partially instantiates the theoretical approach. The system, called ATT-Meta, will be demonstrated. The paper briefly indicates how the system works, and outlines some specific aspects of the system, approach and the overall project.", "citation": "Our developing approach to metaphor handling in the affect detection module is partly to look for stock metaphorical phraseology and straightforward variants of it, and partly to use a simple version of the more open-ended, reasoning-based techniques taken from the ATT-Meta project #REFR.", "context": "We have also encountered quite creative use of metaphor in e-drama. For example, in a school-bullying improvisation that occurred, Mayid had already insulted Lisa by calling her a ?pizza?, developing a previous ?pizza-face? insult. Mayid then said ?I?ll knock your topping off, Lisa? ? a theoretically intriguing spontaneous creative elaboration of the ?pizza? metaphor.[Citation]ATT-Meta includes a general-purpose reasoning engine, and can potentially be used to reason about emotion in relation to other factors in a situation. In turn, the realities of metaphor usage in e-drama sessions are contributing to our basic research on metaphor processing."}
{"citing_paper_id": "C10-2166", "cited_paper_id": "C02-2025", "citing_paper_abstract": "Treebank annotation is a labor-intensive and time-consuming task. In this paper, we show that a simple statistical ranking model can significantly improve treebanking efficiency by prompting human annotators, well-trained in disambiguation tasks for treebanking but not necessarily grammar experts, to the most relevant linguistic disambiguation decisions. Experiments were carried out to evaluate the impact of such techniques on annotation efficiency and quality. The detailed analysis of outputs from the ranking model shows strong correlation to the human annotator behavior. When integrated into the treebanking environment, the model brings a significant annotation speed-up with improved inter-annotator agreement.?", "cited_paper_abstract": "The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank. While several mediumto large-scale treebanks exist for English (and for other major languages), pre-existing publicly available resources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the treebank hard-wires a small, predefined range of ways in which information can be extracted from the treebank, and (iv) representations in existing treebanks are static and over the (often yearor decade-long) evolution of a large-scale treebank tend to fall behind the development of the field. LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself. Since October 2001, the project is working to build the foundations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license.", "citation": "One common characteristic of modern treebanking efforts ? especially, in so-called dynamic treebanking platforms (cf., for instance, #REFR and http://redwoods.stanford. edu), like the one we are describing and referring to extensively in the following, is that the candidate trees are constructed automatically by the grammar, and then manually disambiguated by human annotators.", "context": "[Citation]In doing so, linguistically rich annotation is built efficiently with minimum manual labor. In order to further improve the manual disambiguation efficiency, systems like [incr tsdb#OTHEREFR compute the difference between candidate analyses. Instead of looking at the huge parse forest, the treebank annotators select or reject the features that distinguish between different parses, until no ambiguity remains (either one analysis is accepted from the parse forest, or all of them are rejected)."}
{"citing_paper_id": "D07-1050", "cited_paper_id": "C02-2025", "citing_paper_abstract": "We present results that show that incorporating lexical and structural semantic information is effective for word sense disambiguation. We evaluated the method by using precise information from a large treebank and an ontology automatically created from dictionary sentences. Exploiting rich semantic and structural information improves precision 2?3%. The most gains are seen with verbs, with an improvement of 5.7% over a model using only bag of words and n-gram features.", "cited_paper_abstract": "The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank. While several mediumto large-scale treebanks exist for English (and for other major languages), pre-existing publicly available resources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the treebank hard-wires a small, predefined range of ways in which information can be extracted from the treebank, and (iv) representations in existing treebanks are static and over the (often yearor decade-long) evolution of a large-scale treebank tend to fall behind the development of the field. LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself. Since October 2001, the project is working to build the foundations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license.", "citation": "The actual annotation process uses the same tools as the Redwoods treebank of English #REFR.", "context": "CLASS ?292:driver? (? ?4:person?) WORDNET motorman1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur? formed parses. The flip side to this is that any sentences which the parser cannot parse remain unannotated, at least unless we were to fall back on full manual mark-up of their analyses.[Citation]There were 4 parses for the definition sentence shown in Figure 1. The correct parse, shown as a phrase structure tree, is shown in Figure 2. The two sources of ambiguity are the conjunction and the relative clause."}
{"citing_paper_id": "W07-1217", "cited_paper_id": "C02-2025", "citing_paper_abstract": "This paper presents an approach to partial parse selection for robust deep processing. The work is based on a bottom-up chart parser for HPSG parsing. Following the definition of partial parses in (Kasper et al, 1999), different partial parse selection methods are presented and evaluated on the basis of multiple metrics, from both the syntactic and semantic viewpoints. The application of the partial parsing in spontaneous speech texts processing shows promising competence of the method.", "cited_paper_abstract": "The LinGO Redwoods initiative is a seed activity in the design and development of a new type of treebank. While several mediumto large-scale treebanks exist for English (and for other major languages), pre-existing publicly available resources exhibit the following limitations: (i) annotation is mono-stratal, either encoding topological (phrase structure) or tectogrammatical (dependency) information, (ii) the depth of linguistic information recorded is comparatively shallow, (iii) the design and format of linguistic representation in the treebank hard-wires a small, predefined range of ways in which information can be extracted from the treebank, and (iv) representations in existing treebanks are static and over the (often yearor decade-long) evolution of a large-scale treebank tend to fall behind the development of the field. LinGO Redwoods aims at the development of a novel treebanking methodology, rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself. Since October 2001, the project is working to build the foundations for this new type of treebank, to develop a basic set of tools for treebank construction and maintenance, and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license.", "citation": "For example, the parse disambiguation model described in #REFR uses a maximum entropy approach to model the conditional probability of a parse for a given input sequence P (t|w).", "context": "Generally speaking, the weights of the edges in the shortest path approach represent the quality of the local analyses and their likelihood of appearing in the analysis of the entire input. This is an interesting parallel to the parse selection models for the full analyses, where a goodness score is usually assigned to the full analysis.[Citation]A similar approach has also been reported in #OTHEREFR. For a given partial parse ? = {t1, . . . , tk}, ? = {w1, . . . , wk} is a segmentation of the input sequence so that each local analysis ti ? ? corresponds to a substring wi ? ? of the input sequence w. Therefore, the probability of the partial parse ? given an input sequence w is: P (?|w) = P (?|w) ."}
{"citing_paper_id": "C08-1046", "cited_paper_id": "C04-1002", "citing_paper_abstract": "In Japanese dependency parsing, Kudo?s relative preference-based method (Kudo and Matsumoto, 2005) outperforms both deterministic and probabilistic CKY-based parsing methods. In Kudo?s method, for each dependent word (or chunk) a loglinear model estimates relative preference of all other candidate words (or chunks) for being as its head. This cannot be considered in the deterministic parsing methods. We propose an algorithm based on a tournament model, in which the relative preferences are directly modeled by one-onone games in a step-ladder tournament. In an evaluation experiment with Kyoto Text Corpus Version 4.0, the proposed method outperforms previous approaches, including the relative preference-based method.", "cited_paper_abstract": "We present a novel algorithm for Japanese dependency analysis. The algorithm allows us to analyze dependency structures of a sentence in linear-time while keeping a state-of-the-art accuracy. In this paper, we show a formal description of the algorithm and discuss it theoretically with respect to time complexity. In addition, we evaluate its efficiency and performance empirically against the Kyoto University Corpus. The proposed algorithm with improved models for dependency yields the best accuracy in the previously published results on the Kyoto University Corpus.", "citation": "Standard and additional features are introduced by #REFR.", "context": "Case particle features are the following: All case particles appearing in the candidates? dependent. These features are intended to take into consideration the correlation between the case particles in the dependent of a head. When the head is a verb, it has a similar effect of learning case frame information.[Citation]The case particle feature is newly introduced in this paper. Features corresponding to the already-determined dependency relation are called dynamic features, and the other contextual features are called static features. Standard and additional features are static features, and case particle features are dynamic features."}
{"citing_paper_id": "P06-1105", "cited_paper_id": "C04-1002", "citing_paper_abstract": "In this paper, we present a method that improves Japanese dependency parsing by using large-scale statistical information. It takes into account two kinds of information not considered in previous statistical (machine learning based) parsing methods: information about dependency relations among the case elements of a verb, and information about co-occurrence relations between a verb and its case element. This information can be collected from the results of automatic dependency parsing of large-scale corpora. The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method.", "cited_paper_abstract": "We present a novel algorithm for Japanese dependency analysis. The algorithm allows us to analyze dependency structures of a sentence in linear-time while keeping a state-of-the-art accuracy. In this paper, we show a formal description of the algorithm and discuss it theoretically with respect to time complexity. In addition, we evaluate its efficiency and performance empirically against the Kyoto University Corpus. The proposed algorithm with improved models for dependency yields the best accuracy in the previously published results on the Kyoto University Corpus.", "citation": "We also added our original features and their combinations, with reference to #REFR, but we removed the features that had a frequency of less than 30 in our training data.", "context": "n-best parses We generated the n-best parses by using the ?posterior context model? #OTHEREFR. The features we used were those in #OTHEREFR and their combinations.[Citation]The total number of features is thus 105,608."}
{"citing_paper_id": "P09-2013", "cited_paper_id": "C04-1002", "citing_paper_abstract": "We describe an algorithm for Japanese analysis that does both base phrase chunking and dependency parsing simultaneously in linear-time with a single scan of a sentence. In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus. Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy.", "cited_paper_abstract": "We present a novel algorithm for Japanese dependency analysis. The algorithm allows us to analyze dependency structures of a sentence in linear-time while keeping a state-of-the-art accuracy. In this paper, we show a formal description of the algorithm and discuss it theoretically with respect to time complexity. In addition, we evaluate its efficiency and performance empirically against the Kyoto University Corpus. The proposed algorithm with improved models for dependency yields the best accuracy in the previously published results on the Kyoto University Corpus.", "citation": "In addition, most of algorithms of Japanese dependency parsing, e.g., #OTHEREFR; #REFR, assume the three constraints below. (1) Each bunsetsu has only one head except the rightmost one. (2) Dependency links between bunsetsus go from left to right. (3) Dependency links do not cross one another.", "context": "ID 0 1 2 3 4 Head 4 4 3 4 - Figure 1: Sample sentence (bunsetsu-based) among bunsetsus. A bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words.[Citation]In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied."}
{"citing_paper_id": "I05-3003", "cited_paper_id": "C04-1010", "citing_paper_abstract": "We present a method for improving dependency structure analysis of Chinese. Our bottom-up deterministic analyzer adopt Nivre?s algorithm (Nivre and Scholz, 2004). Support Vector Machines (SVMs) are utilized to determine the word dependency relations. We find that there are two problems in our analyzer and propose two methods to solve them. One problem is that some operations cannot be solved only using local feature. We utilize the global features to solve this. The other problem is that this bottom-up analyzer doesn?t use top-down information. We supply the top-down information by constructing SVMs based root node finder to solve this problem. Experimental evaluation on the Penn Chinese Treebank Corpus shows that the proposed extensions improve the parsing accuracy significantly.", "cited_paper_abstract": "This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).", "citation": "Our bottom-up deterministic analyzer adopt Nivre?s algorithm #REFR.", "context": "We present a method for improving dependency structure analysis of Chinese.[Citation]Support Vector Machines (SVMs) are utilized to determine the word dependency relations. We find that there are two problems in our analyzer and propose two methods to solve them. One problem is that some operations cannot be solved only using local feature."}
{"citing_paper_id": "P06-1054", "cited_paper_id": "C04-1010", "citing_paper_abstract": "We present a novel classifier-based deterministic parser for Chinese constituency parsing. Our parser computes parse trees from bottom up in one pass, and uses classifiers to make shift-reduce decisions. Trained and evaluated on the standard training and test sets, our best model (using stacked classifiers) runs in linear time and has labeled precision and recall above 88% using gold-standard part-of-speech tags, surpassing the best published results. Our SVM parser is 2-13 times faster than state-of-the-art parsers, while producing more accurate results. Our Maxent and DTree parsers run at speeds 40-270 times faster than state-of-the-art parsers, but with 5-6% losses in accuracy.", "cited_paper_abstract": "This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).", "citation": "Deterministic parsing has emerged as an attractive alternative to probabilistic parsing, offering accuracy just below the state-of-the-art in syntactic analysis of English, but running in linear time #OTHEREFR; #REFR.", "context": "Traditional statistical approaches build models which assign probabilities to every possible parse tree for a sentence. Techniques such as dynamic programming, beam-search, and best-first-search are then employed to find the parse tree with the highest probability. The massively ambiguous nature of wide-coverage statistical parsing,coupled with cubic-time (or worse) algorithms makes this approach too slow for many practical applications.[Citation]Encouraging results have also been shown recently by Cheng et al #OTHEREFR in applying deterministic models to Chinese dependency parsing. We present a novel classifier-based deterministic parser for Chinese constituency parsing. In our approach, which is based on the shift-reduce parser for English reported in #OTHEREFR, the parsing task is transformed into a succession of classification tasks."}
{"citing_paper_id": "W07-2217", "cited_paper_id": "C04-1010", "citing_paper_abstract": "This paper investigates new design options for the feature space of a dependency parser. We focus on one of the simplest and most efficient architectures, based on a deterministic shift-reduce algorithm, trained with the perceptron. By adopting second-order feature maps, the primal form of the perceptron produces models with comparable accuracy to more complex architectures, with no need for approximations. Further gains in accuracy are obtained by designing features for parsing extracted from semantic annotations generated by a tagger. We provide experimental evaluations on the Penn Treebank.", "cited_paper_abstract": "This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).", "citation": "#REFR proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear.", "context": "Shift/Reduce actions to use in building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict which action to perform on the tree. Similar deterministic approaches to parsing have been investigated also in the context of constituent parsing #OTHEREFR.[Citation]Attardi #OTHEREFR proposed a variant of the rules that handle non-projective relations while parsing deterministically in a single pass. Shift-reduce algorithms are simple and efficient, yet competitive in terms of accuracy: in the CoNLL-X shared task, for several languages, there was no statistically significant difference between second-order MST parsers and shift-reduce parsers."}
{"citing_paper_id": "C10-2051", "cited_paper_id": "C04-1041", "citing_paper_abstract": "Because English is a low morphology language, current statistical parsers tend to ignore morphology and accept some level of redundancy. This paper investigates how costly such redundancy is for a lexicalised grammar such as CCG. We use morphological analysis to split verb inflectional suffixes into separate tokens, so that they can receive their own lexical categories. We find that this improves accuracy when the splits are based on correct POS tags, but that errors in gold standard or automatically assigned POS tags are costly for the system. This shows that the parser can benefit from morphological analysis, so long as the analysis is correct.", "cited_paper_abstract": "This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms.", "citation": "This is particularly true of the C&C parser, which exploits CCG?s lexicalisation to divide the parsing task between two integrated models #REFR.", "context": "Lexicalised formalisms like CCG #OTHEREFR parser. The performance of these parsers can be partially attributed to their theoretical foundations.[Citation]We have followed this formalism-driven approach by exploiting morphology for English syntactic parsing, using a strategy designed for morphologically rich languages. Combining our technique with hat categories leads to a 20% improvement in efficiency, with a 0.25% loss of accuracy. If the POS tag error problem were addressed, the two strategies combined would improve efficiency by 50%, and improve accuracy by 0.37%."}
{"citing_paper_id": "P14-1112", "cited_paper_id": "C04-1041", "citing_paper_abstract": "We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision. The trained parser produces a full syntactic parse of any sentence, while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser?s predicate vocabulary. We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology. A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach. A syntactic evaluation on CCGbank demonstrates that the parser?s dependency F- score is within 2.5% of state-of-the-art.", "cited_paper_abstract": "This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms.", "citation": "We improve parser performance by performing supertagging #OTHEREFR; #REFR.", "context": "Parsing in practice can be slow because the parser?s lexicalized grammar permits a large number of parses for a sentence.[Citation]We trained a logistic regression classifier to predict the syntactic category of each token in a sentence from features of the surrounding tokens and POS tags. Subsequent parsing is restricted to only consider categories whose probability is within a factor of ? of the highest-scoring category. The parser uses a backoff strategy, first attempting to parse with the supertags from ? = 0.01, backing off to ? = 0.001 if the initial parsing attempt fails."}
{"citing_paper_id": "D12-1024", "cited_paper_id": "C04-1057", "citing_paper_abstract": "We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL) in this paper, which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary. We demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework. The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method, in terms of ROUGE scores. The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function.", "cited_paper_abstract": "Selecting important information while accounting for repetitions is a hard task for both summarization and question answering. We propose a formal model that represents a collection of documents in a two-dimensional space of textual and conceptual units with an associated mapping between these two dimensions. This representation is then used to describe the task of selecting textual units for a summary or answer as a formal optimization task. We provide approximation algorithms and empirically validate the performance of the proposed model when used with two very different sets of features, words and atomic events.", "citation": "Global inference algorithms for the extractive approach have been researched widely in recent years #REFR to consider whether the summary is ?good? as a whole.", "context": "The extractive approach to automatic summarization is a popular and well-known approach in this field, which creates a summary by directly selecting some textual units (e.g., words and sentences) from the original documents, because it is difficult to genuinely evaluate and guarantee the linguistic quality of the produced summary. One of the most well-known extractive approaches is maximal marginal relevance #OTHEREFR. Greedy MMR-style algorithms are widely used; however, they cannot take into account the whole quality of the summary due to their greediness, although a summary should convey all the information in given documents.[Citation]These algorithms formulate the problem as integer linear programming (ILP) to optimize the score: however, as ILP is non-deterministic polynomialtime hard (NP-hard), the time complexity is very large. Consequently, we need some more efficient algorithm for calculations. We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL), which models the process of construction of a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary."}
{"citing_paper_id": "H05-1019", "cited_paper_id": "C04-1064", "citing_paper_abstract": "In order to promote the study of automatic summarization and translation, we need an accurate automatic evaluation method that is close to human evaluation. In this paper, we present an evaluation method that is based on convolution kernels that measure the similarities between texts considering their substructures. We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3). A comparison with conventional techniques shows that our method correlates more closely with human evaluations and is more robust.", "cited_paper_abstract": "In this paper, we describe a method of automatic sentence alignment for building extracts from abstracts in automatic summarization research. Our method is based on two steps. First, we introduce the ?dependency tree path? (DTP). Next, we calculate the similarity between DTPs based on the ESK (Extended String Subsequence Kernel), which considers sequential patterns. By using these procedures, we can derive one-to-many or many-to-one correspondences among sentences. Experiments using different similarity measures show that DTP consistently improves the alignment accuracy and that ESK gives the best performance.", "citation": "Our method is based on the Extended String Subsequence Kernel (ESK) #REFRb) which is a kind of convolution kernel #OTHEREFR.", "context": "To cope with this situation, there is a particular need to establish a high quality automatic evaluation method. Once this is done, we can expect great progress to be made on natural language generation. In this paper, we propose a novel automatic evaluation method for natural language generation technologies.[Citation]ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 #OTHEREFRa). The results of the comparison with ROUGE-N #OTHEREFRb) show that our method correlates more closely with human evaluations and is more robust."}
{"citing_paper_id": "I08-1044", "cited_paper_id": "C04-1081", "citing_paper_abstract": "Chinese named entity recognition (NER) has recently been viewed as a classification or sequence labeling problem, and many approaches have been proposed. However, they tend to address this problem without considering linguistic information in Chinese NEs. We propose a new framework based on probabilistic graphical models with firstorder logic for Chinese NER. First, we use Conditional Random Fields (CRFs), a standard and theoretically well-founded machine learning method based on undirected graphical models as a base system. Second, we introduce various types of domain knowledge into Markov Logic Networks (MLNs), an effective combination of first-order logic and probabilistic graphical models for validation and error correction of entities. Experimental results show that our framework of probabilistic graphical models with first-order logic significantly outperforms the state-of-the-art models for solving this task.", "cited_paper_abstract": "Chinese word segmentation is a difficult, important and widely-studied sequence modeling problem. This paper demonstrates the ability of linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the integration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition. State-of-the-art performance is obtained.", "citation": "CRFs have been successfully applied to a number of real-world tasks, including NP chunking #OTHEREFR, Chinese word segmentation #REFR, information extraction #OTHEREFR, and many others.", "context": "Conditional Random Fields #OTHEREFR are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. CRFs have the great flexibility to encode a wide variety of arbitrary, non-independent features and to straightforwardly combine rich domain knowledge. Furthermore, they are discriminatively trained, and are often more accurate than generative models, even with the same features.[Citation]1In this paper we only focus on PERs, LOCs and ORGs. Since temporal, numerical and monetary phrases can be well identified with rule-based approaches. Recently, CRFs have been shown to perform exceptionally well on Chinese NER shared task on the third SIGHAN Chinese language processing bakeoff #OTHEREFRb,a)."}
{"citing_paper_id": "P06-1048", "cited_paper_id": "C04-1107", "citing_paper_abstract": "Sentence compression is the task of producing a summary at the sentence level. This paper focuses on three aspects of this task which have not received detailed treatment in the literature: training requirements, scalability, and automatic evaluation. We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains (written vs. spoken text). To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used. Finally, we assess whether automatic evaluation measures can be used to determine compression quality.", "cited_paper_abstract": "This paper investigates a novel application of support vector machines (SVMs) for sentence reduction. We also propose a new probabilistic sentence reduction method based on support vector machine learning. Experimental results show that the proposed methods outperform earlier methods in term of sentence reduction performance.", "citation": "It has inspired many discriminative approaches to the compression task #OTHEREFR; #REFR and has been extended to languages other than English #OTHEREFRa).", "context": "In this section we give a brief overview of the algorithms we employed in our comparative study. We focus on two representative methods, Knight and Marcu?s #OTHEREFR word-based model. The decision-tree model operates over parallel corpora and offers an intuitive formulation of sentence compression in terms of tree rewriting.[Citation]We opted for the decisiontree model instead of the also well-known noisychannel model #OTHEREFR. Although both models yield comparable performance, Turner and Charniak #OTHEREFR model was originally developed for Japanese with spoken text in mind, 1The noisy-channel model uses a source model trainedon uncompressed sentences. This means that the most likelycompressed sentence will be identical to the original sen-tence as the likelihood of a constituent deletion is typicallyfar lower than that of leaving it in."}
{"citing_paper_id": "D10-1119", "cited_paper_id": "C04-1180", "citing_paper_abstract": "This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.", "cited_paper_abstract": "This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards widecoverage semantic interpretation, one of the key objectives of the field of NLP.", "citation": "Additionally, #REFR consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.", "context": "Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski & Penn #OTHEREFR developed an approach for modeling child language acquisition.[Citation]"}
{"citing_paper_id": "P10-1111", "cited_paper_id": "C04-1197", "citing_paper_abstract": "For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebankbased (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation.", "cited_paper_abstract": "We present a system for the semantic role labeling task. The system combines a machine learning technique with an inference procedure based on integer linear programming that supports the incorporation of linguistic and structural constraints into the decision process. The system is tested on the data provided in CoNLL- 2004 shared task on semantic role labeling and achieves very competitive results.", "citation": "This basic architecture was introduced by #REFR for the task of semantic role labelling and since then has been applied to different NLP tasks without significant changes.", "context": "Words are lemmatised and part-of-speech tagged with the Stuttgart-Tu?bingen Tag Set #OTHEREFR and contain morphological annotations (Release 2). TiGer uses 25 syntactic categories and a set of 42 function labels to annotate the grammatical function of a phrase. The function labeller consists of two main components, a maximum entropy classifier and an integer linear program.[Citation]In our case, its input is a bare tree 4Although the classifier may, of course, still identify the wrong phrase as subject or object. structure (as obtained by a standard phrase structure parser) and it outputs a tree structure where every node is labelled with the grammatical relation it bears to its mother node. For each possible label and for each node, the classifier assigns a probability that this node is labelled by this label. This results in a complete probability distribution over all labels for each node."}
{"citing_paper_id": "W12-3124", "cited_paper_id": "C08-1005", "citing_paper_abstract": "Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combination via confusion network decoding. Controlled experiments using identical pre-processing, decoding, and weight tuning methods on standard system combination evaluation sets are presented. Translation quality is assessed using case insensitive BLEU scores and bootstrapping is used to establish statistical significance of the score differences. All aligners yield significant BLEU score gains over the best individual system included in the combination. Incremental indirect hidden Markov model and a novel incremental inversion transduction grammar with flexible matching consistently yield the best translation quality, though keeping all things equal, the differences between aligners are relatively small. ?The work reported in this paper was carried out while the authors were at Raytheon BBN Technologies and ?RWTH Aachen University.", "cited_paper_abstract": "The state-of-the-art system combination method for machine translation (MT) is the word-based combination using confusion networks. One of the crucial steps in confusion network decoding is the alignment of different hypotheses to each other when building a network. In this paper, we present new methods to improve alignment of hypotheses using word synonyms and a two-pass alignment strategy. We demonstrate that combination with the new alignment technique yields up to 2.9 BLEU point improvement over the best input system and up to 1.3 BLEU point improvement over a state-of-the-art combination method on two different language pairs.", "citation": "A two-pass alignment algorithm to improve pairwise TER alignments was introduced in #REFR.", "context": "In pairwise alignment, each hypothesis corresponding to a source sentence is aligned independently with the skeleton hypothesis. This set of alignments is consolidated using the skeleton words as anchors to form the confusion network #OTHEREFR. The same word in two hypotheses may be aligned with a different word in the skeleton resulting in repetition in the network.[Citation]In incremental alignment #OTHEREFR, the confusion network is initialized by forming a simple graph with one word per link from the skeleton hypothesis. Each remaining hypothesis is aligned with the partial confusion network, which allows words from all previous hypotheses be considered as matches. The order in which the hypotheses are aligned may influence the alignment quality."}
{"citing_paper_id": "D12-1022", "cited_paper_id": "C08-1018", "citing_paper_abstract": "Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.", "cited_paper_abstract": "In this paper we generalise the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suited to our task and a discriminative tree-totree transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions.", "citation": "Unlike previous applications of STSG to sentence compression #OTHEREFR; #REFR our quasi-synchronous TSG does not attempt to learn the complete translation from source to target sentence; it only loosely links the syntactic structure of the two #OTHEREFR, and is therefore well suited to describing the relationship between documents and their abstracts.", "context": "Our work differs from Gillick et al. #OTHEREFR in three important respects. Firstly, we develop a genuinely abstractive model that is not limited to deletion. Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned automatically from source documents and their summaries.[Citation]Secondly, our content selection component extends to features beyond the bigram horizon, as we learn to identify important concepts based on syntactic and positional information. We also learn which words are unlikely to appear in a summary. Thirdly, unlike Berg-Kirkpatrick et al2011) our model does not try to learn all the parameters (e.g., content, rewrite rules, style) of the summarization problem jointly; although decoupling learning from inference is perhaps less elegant from a modeling perspective, the learning process is more robust and reliable."}
{"citing_paper_id": "D12-1129", "cited_paper_id": "C08-1021", "citing_paper_abstract": "We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.", "cited_paper_abstract": "This paper presents a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources. Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semanticallyrelated concepts is a major step towards the autonomous acquisition of knowledge from raw corpora. In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework.", "citation": "But even if WordNet senses have been enriched with topically-distinctive words and concepts #OTHEREFR; #REFR, manually-developed domain labels #OTHEREFR, the main obstacle of being stuck with an open-ended fine-grained sense inventory remains.", "context": "On the other hand, supervised systems fall short of carrying out high-performance WSD within domains, the main reason being the need for retraining on each new specific knowledge domain. Nonetheless, the knowledge acquisition bottleneck can be relieved by means of domain adaptation #OTHEREFR. However, as mentioned above, most work on domain WSD uses WordNet as a sense inventory.[Citation]Recent results on the SPORTS and FINANCE gold standard dataset #OTHEREFR. In this paper, we take domain WSD to the next level by proposing a new framework based on the minimally-supervised acquisition of large domain sense inventories thanks to which high performance can be attained on virtually any domain using unsupervised algorithms. Glossary acquisition approaches in the literature are mostly focused on pattern-based definition extraction #OTHEREFR starting from an initial terminology, while we jointly bootstrap the lexicon and the definitions for several domains with minimal supervision and without the requirement of domain-specific corpora."}
{"citing_paper_id": "P10-1037", "cited_paper_id": "C08-1046", "citing_paper_abstract": "We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.", "cited_paper_abstract": "In Japanese dependency parsing, Kudo?s relative preference-based method (Kudo and Matsumoto, 2005) outperforms both deterministic and probabilistic CKY-based parsing methods. In Kudo?s method, for each dependent word (or chunk) a loglinear model estimates relative preference of all other candidate words (or chunks) for being as its head. This cannot be considered in the deterministic parsing methods. We propose an algorithm based on a tournament model, in which the relative preferences are directly modeled by one-onone games in a step-ladder tournament. In an evaluation experiment with Kyoto Text Corpus Version 4.0, the proposed method outperforms previous approaches, including the relative preference-based method.", "citation": "There are features that have been commonly used for Japanese dependency parsing among related papers, e.g., #OTHEREFR; #REFR.", "context": "[Citation]We also used the same features here. They are divided into three groups: modifier bunsetsu features, head bunsetsu features, and gap features. A summary of the features is described in Table 1."}
{"citing_paper_id": "C14-1136", "cited_paper_id": "C08-1050", "citing_paper_abstract": "In this paper, we address the role of syntactic parsing for distributional similarity. On the one hand, we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers. On the other hand, we explore whether single unsupervised parsers, or their combination, can contribute to better distributional similarities, or even replace supervised parsing as a preprocessing step for word similarity. We evaluate distributional thesauri against manually created taxonomies both for English and German for five unsupervised parsers. While for English, a supervised parser is the best single parser in this evaluation, we find an unsupervised parser to work best for German. For both languages, we show significant improvements in word similarity when combining features from supervised and unsupervised parsers. To our knowledge, this is the first work where unsupervised parsers are systematically evaluated extrinsically in a semantic task, and the first work to show that unsupervised parsing can complement and even replace supervised parsing, when used as a pre-processing feature.", "cited_paper_abstract": "Almost all automatic semantic role labeling (SRL) systems rely on a preliminary parsing step that derives a syntactic structure from the sentence being analyzed. This makes the choice of syntactic representation an essential design decision. In this paper, we study the influence of syntactic representation on the performance of SRL systems. Specifically, we compare constituent-based and dependencybased representations for SRL of English in the FrameNet paradigm. Contrary to previous claims, our results demonstrate that the systems based on dependencies perform roughly as well as those based on constituents: For the argument classification task, dependencybased systems perform slightly higher on average, while the opposite holds for the argument identification task. This is remarkable because dependency parsers are still in their infancy while constituent parsing is more mature. Furthermore, the results show that dependency-based semantic role classifiers rely less on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data.", "citation": "Other extrinsic evaluations with supervised dependency parsers have been performed in information extraction systems #OTHEREFR or semantic role labeling #REFR.", "context": "They show that the neglectance of these cases has a significant but unjustified negative influence on the evaluation outcomes and propose a new measure, Neutral Edge Direction (NED), which alleviates this problem. Bod #OTHEREFR argues that parser evaluation against a treebank favors supervised approaches and therefore measures the parser quality on the outcome of a syntax based Machine Translation (MT) task where the dependency parsers are evaluated as language models. In Motazedi et al. #OTHEREFR, a single unsupervised parser is evaluated in an extrinsic evaluation for realisation ranking, and does not compare favorably against a supervised parser.[Citation]"}
{"citing_paper_id": "W11-0704", "cited_paper_id": "C08-1056", "citing_paper_abstract": "Microtexts, like SMS messages, Twitter posts, and Facebook status updates, are a popular medium for real-time communication. In this paper, we investigate the writing conventions that different groups of users use to express themselves in microtexts. Our empirical study investigates properties of lexical transformations as observed within Twitter microtexts. The study reveals that different populations of users exhibit different amounts of shortened English terms and different shortening styles. The results reveal valuable insights into how human language technologies can be effectively applied to microtexts.", "cited_paper_abstract": "Electronic written texts used in computermediated interactions (e-mails, blogs, chats, etc) present major deviations from the norm of the language. This paper presents an comparative study of systems aiming at normalizing the orthography of French SMS messages: after discussing the linguistic peculiarities of these messages, and possible approaches to their automatic normalization, we present, evaluate and contrast two systems, one drawing inspiration from the Machine Translation task; the other using techniques that are commonly used in automatic speech recognition devices. Combining both approaches, our best normalization system achieves about 11% Word Error Rate on a test set of about 3000 unseen messages.", "citation": "Normalizing text can traditionally be approached using three well-known NLP metaphors, namely that of spell-checking, machine translation (MT) and automatic speech recognition (ASR) #REFR.", "context": "Although our work is primarily focused on analyzing the lexical variation in language found in online social media, our analysis methodology makes strong use of techniques for normalizing ?noisy text? such as SMS-messages and Twitter messages into standard English.[Citation]In the spell-checking approach, corrections from ?noisy? words to ?clean? words proceed on a wordby-word basis. Choudhury #OTHEREFR improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. However, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to a high degree of accuracy."}
{"citing_paper_id": "D09-1067", "cited_paper_id": "C08-1057", "citing_paper_abstract": "In previous research in automatic verb classification, syntactic features have proved the most useful features, although manual classifications rely heavily on semantic features. We show, in contrast with previous work, that considerable additional improvement can be obtained by using semantic features in automatic classification: verb selectional preferences acquired from corpus data using a fully unsupervised method. We report these promising results using a new framework for verb clustering which incorporates a recent subcategorization acquisition system, rich syntactic-semantic feature sets, and a variation of spectral clustering which performs particularly well in high dimensional feature space.", "cited_paper_abstract": "We conduct large-scale experiments to investigate optimal features for classification of verbs in biomedical texts. We introduce a range of feature sets and associated extraction techniques, and evaluate them thoroughly using a robust method new to the task: cost-based framework for pairwise clustering. Our best results compare favourably with earlier ones. Interestingly, they are obtained with sophisticated feature sets which include lexical and semantic information about selectional preferences of verbs. The latter are acquired automatically from corpus data using a fully unsupervised method.", "citation": "All the other feature sets include information about SCFs which have been widely employed in verb classification, e.g. #OTHEREFR; #REFR.", "context": "Stop words are removed prior to extraction, and the 600 most frequent resulting COs are kept. F2-F3 provide information about lexical preferences of verbs in argument head positions of specific GRs associated with the verb: F2: Prepositional preference (PP): the type and frequency of prepositions in the indirect object relation. F3: Lexical preference (LP): the type and frequency of nouns and prepositions in the subject, object, and indirect object relation.[Citation]F4-F7 include basic SCF information and/or refine it with additional information which has proved useful in previous works: F4: SCFs and relative frequencies with verbs. SCFs abstract over particles and prepositions. F5: F4 with COs (F1)."}
{"citing_paper_id": "C10-2107", "cited_paper_id": "C08-1084", "citing_paper_abstract": "Supervised semantic role labeling (SRL) systems are generally claimed to have accuracies in the range of 80% and higher (Erk and Pado?, 2006). These numbers, though, are the result of highly-restricted evaluations, i.e., typically evaluating on hand-picked lemmas for which training data is available. In this paper we consider performance of such systems when we evaluate at the document level rather than on the lemma level. While it is wellknown that coverage gaps exist in the resources available for training supervised SRL systems, what we have been lacking until now is an understanding of the precise nature of this coverage problem and its impact on the performance of SRL systems. We present a typology of five different types of coverage gaps in FrameNet. We then analyze the impact of the coverage gaps on performance of a supervised semantic role labeling system on full texts, showing an average oracle upper bound of 46.8%.", "cited_paper_abstract": "This paper presents a novel approach to the task of semantic role labelling for event nominalisations, which make up a considerable fraction of predicates in running text, but are underrepresented in terms of training data and difficult to model. We propose to address this situation by data expansion. We construct a model for nominal role labelling solely from verbal training data. The best quality results from salvaging grammatical features where applicable, and generalising over lexical heads otherwise.", "citation": "Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling #OTHEREFR; #REFR to systems which can infer missing word senses #OTHEREFR.", "context": "FrameNet-style semantic role labeling has been shown to, in principle, be beneficial for applications that need to generalise over individual lemmas, such as recognizing textual entailment or question answering. However, studies also found that state-of-the-art FrameNet-style SRL systems perform too poorly to provide any substantial benefit to real applications #OTHEREFR. Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text.[Citation]However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology."}
{"citing_paper_id": "W09-2417", "cited_paper_id": "C08-1084", "citing_paper_abstract": "In this paper, we describe the SemEval-2010 shared task on ?Linking Events and Their Participants in Discourse?. This task is a variant of the classical semantic role labelling task. The novel aspect is that we focus on linking local semantic argument structures across sentence boundaries. Specifically, the task aims at linking locally uninstantiated roles to their coreferents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction.", "cited_paper_abstract": "This paper presents a novel approach to the task of semantic role labelling for event nominalisations, which make up a considerable fraction of predicates in running text, but are underrepresented in terms of training data and difficult to model. We propose to address this situation by data expansion. We construct a model for nominal role labelling solely from verbal training data. The best quality results from salvaging grammatical features where applicable, and generalising over lexical heads otherwise.", "citation": "The likelihood of a potential filler filling the target role can then be modelled as the distance between the meaning vector of the filler and the role in the vector space model (see #REFR for a similar approach for semi-automatic SRL).", "context": "For instance, typical fillers of the CHARGES role of clear might be murder, accusations, allegations, fraud etc. The semantic content of the role could then be represented in a vector space model, using additional unannotated data to build meaning vectors for the attested role fillers. Meaning vectors for potential role fillers in the context of the null instantiation could be built in a similar fashion.[Citation]We envisage that the manually annotated null instantiated data can be used to learn additionally heuristics for the filler resolution task, such as information about the average distance between a null instantiation and its most recent co-referent."}
{"citing_paper_id": "N12-1009", "cited_paper_id": "C08-1087", "citing_paper_abstract": "A citing sentence is one that appears in a scientific article and cites previous work. Citing sentences have been studied and used in many applications. For example, they have been used in scientific paper summarization, automatic survey generation, paraphrase identification, and citation function classification. Citing sentences that cite multiple papers are common in scientific writing. This observation should be taken into consideration when using citing sentences in applications. For instance, when a citing sentence is used in a summary of a scientific paper, only the fragments of the sentence that are relevant to the summarized paper should be included in the summary. In this paper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference. Our methods are: word classification, sequence labeling, and segment classification. Our experiments show that segment classification achieves the best results.", "cited_paper_abstract": "Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others? viewpoint of the target article?s contributions and the study of its citation summary network using a clustering approach.", "citation": "Previous work has studied and used citation sentences in various applications such as: scientific paper summarization #OTHEREFR; #REFR, automatic survey generation #OTHEREFR.", "context": "When a reference appears in a scientific article, it is usually accompanied by a span of text that highlights the important contributions of the cited article. We call a sentence that contains an explicit reference to previous work a citation sentence. For example, sentence #OTHEREFR addressed the issue of language identification for finding Web pages in the languages of interest.[Citation]Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common."}
{"citing_paper_id": "D12-1030", "cited_paper_id": "C08-1094", "citing_paper_abstract": "State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higherorder dependencies. The generalization is at the cost of asymptotic efficiency. To account for this, cube pruning for decoding is utilized (Chiang, 2007). For the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser. Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al2010).", "cited_paper_abstract": "In this paper, we consider classifying word positions by whether or not they can either start or end multi-word constituents. This provides a mechanism for ?closing? chart cells during context-free inference, which is demonstrated to improve efficiency and accuracy when used to constrain the wellknown Charniak parser. Additionally, we present a method for ?closing? a sufficient number of chart cells to ensure quadratic worst-case complexity of context-free inference. Empirical results show that this O(n ) bound can be achieved without impacting parsing accuracy.", "citation": "This includes work on coarse-to-fine parsing #OTHEREFR, chart-cell closing and pruning #REFR, and dynamic beam-width prediction #OTHEREFR.", "context": "Hence, our formulation is more akin to the one pass decoding algorithm of Chiang #OTHEREFR for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking #OTHEREFR as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms.[Citation]Of particular note, Rush and Petrov #OTHEREFR report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider."}
{"citing_paper_id": "P11-2119", "cited_paper_id": "C08-1094", "citing_paper_abstract": "We present a novel pruning method for context-free parsing that increases efficiency by disallowing phrase-level unary productions in CKY chart cells spanning a single word. Our work is orthogonal to recent work on ?closing? chart cells, which has focused on multi-word constituents, leaving span-1 chart cells unpruned. We show that a simple discriminative classifier can learn with high accuracy which span-1 chart cells to close to phrase-level unary productions. Eliminating these unary productions from the search can have a large impact on downstream processing, depending on implementation details of the search. We apply our method to four parsing architectures and demonstrate how it is complementary to the cell-closing paradigm, as well as other pruning methods such as coarse-to-fine, agenda, and beam-search pruning.", "cited_paper_abstract": "In this paper, we consider classifying word positions by whether or not they can either start or end multi-word constituents. This provides a mechanism for ?closing? chart cells during context-free inference, which is demonstrated to improve efficiency and accuracy when used to constrain the wellknown Charniak parser. Additionally, we present a method for ?closing? a sufficient number of chart cells to ensure quadratic worst-case complexity of context-free inference. Empirical results show that this O(n ) bound can be achieved without impacting parsing accuracy.", "citation": "#REFR have recently shown that using a finite-state tagger to close cells within the CKY chart can reduce the worst-case and average-case complexity of context-free parsing, without reducing accuracy.", "context": "Graph-based pruning methods such as best-first and beam-search have both be used within context-free parsers to increase their efficiency. Pipeline systems make use of simpler models to reduce the search space of the full model. For example, the well-known Charniak parser #OTHEREFR uses a simple grammar to prune the search space for a richer model in a second pass.[Citation]In their work, word positions are classified as beginning and/or ending multi-word constituents, and all chart cells not conforming to these constraints can be pruned. Zhang et al #OTHEREFR both extend this approach by classifying chart cells with a finer granularity. Pruning based on constituent span is straightforwardly applicable to all parsing architectures, yet the methods mentioned above only consider spans of length two or greater."}
{"citing_paper_id": "N13-2011", "cited_paper_id": "C08-1103", "citing_paper_abstract": "Review mining and summarization has been a hot topic for the past decade. A lot of effort has been devoted to aspect detection and sentiment analysis under the assumption that every review has the same utility for related tasks. However, reviews are not equally helpful as indicated by user-provided helpfulness assessment associated with the reviews. In this thesis, we propose a novel review summarization framework which summarizes review content under the supervision of automated assessment of review helpfulness. This helpfulness-guided framework can be easily adapted to traditional review summarization tasks, for a wide range of domains.", "cited_paper_abstract": "Within the area of general-purpose finegrained subjectivity analysis, opinion topic identification has, to date, received little attention due to both the difficulty of the task and the lack of appropriately annotated resources. In this paper, we provide an operational definition of opinion topic and present an algorithm for opinion topic identification that, following our new definition, treats the task as a problem in topic coreference resolution. We develop a methodology for the manual annotation of opinion topics and use it to annotate topic information for a portion of an existing general-purpose opinion corpus. In experiments using the corpus, our topic identification approach statistically significantly outperforms several non-trivial baselines according to three evaluation measures.", "citation": "#REFR focused on identifying opinion entities (opinion, source, target) and presenting them in a structured way (templates or diagrams).", "context": "Generally speaking, the goal of text summarization is to retain the most important points of the input text within a shorter length. Either extractively or abstractively, one important task is to determine the informativeness of a text element. In addition to reducing information redundancy, different heuristics were proposed within the context of opinion summarization.[Citation]Lerman et. al #OTHEREFR further considered an effective review summary as representative contrastive opinion pairs. Different from all above, Ganesan et. al #OTHEREFR represented text input as token-based graphs based on the token order in the string. They rank summary candidates by scoring paths after removing redundant information from the graph."}
{"citing_paper_id": "N10-1120", "cited_paper_id": "C08-1106", "citing_paper_abstract": "In this paper, we present a dependency treebased method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features.", "cited_paper_abstract": "Shallow parsing is one of many NLP tasks that can be reduced to a sequence labeling problem. In this paper we show that the latent-dynamics (i.e., hidden substructure of shallow phrases) constitutes a problem in shallow parsing, and we show that modeling this intermediate structure is useful. By analyzing the automatically learned hidden states, we show how the latent conditional model explicitly learn latent-dynamics. We propose in this paper the Best Label Path (BLP) inference algorithm, which is able to produce the most probable label sequence on latent conditional models. It outperforms two existing inference algorithms. With the BLP inference, the LDCRF model significantly outperforms CRF models on word features, and achieves comparable performance of the most successful shallow parsers on the CoNLL data when further using part-ofspeech features.", "citation": "Latent- Dynamic Conditional Random Fields #OTHEREFR; #REFR are probabilistic models with hidden variables for sequential labeling, and belief propagation is used for inference.", "context": "For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use syntactic dependency structures. Conditional random fields with hidden variables have been studied so far for other tasks.[Citation]Out method is similar to the models, but there are several differences. In our method, only one variable which represents the polarity of the whole sentence is observable, and dependency relation among random variables is not a linear chain but a tree structure which is identical to the syntactic dependency."}
{"citing_paper_id": "W09-2504", "cited_paper_id": "C08-1107", "citing_paper_abstract": "WordNet is a useful resource for lexical inference in applications. Inference over predicates, however, often requires a change in argument positions, which is not specified in WordNet. We propose a novel framework for augmenting WordNet-based inferences over predicates with corresponding argument mappings. We further present a concrete implementation of this framework, which yields substantial improvement to WordNet-based inference.", "cited_paper_abstract": "Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rulesentailment rules between templates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.", "citation": "As defined in earlier works, an entailment rule specifies an inference relation between an entailing template and an entailed template, where templates are parse subtrees with argument variables #REFR.", "context": "In our framework we represent argument mappings for inferential relations between predicates through an extension of entailment rules over syntactic representations.[Citation]For example, ?X subj ??? buy obj ??. Y ? ?X subj ??? pay prep?for ??????. Y ?."}
{"citing_paper_id": "C10-1030", "cited_paper_id": "C08-1109", "citing_paper_abstract": "To speed up the process of categorizing learner errors and obtaining data for languages which lack error-annotated data, we describe a linguistically-informed method for generating learner-like morphological errors, focusing on Russian. We outline a procedure to select likely errors, relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context.", "cited_paper_abstract": "In this paper we describe a methodology for detecting preposition errors in the writing of non-native English speakers. Our system performs at 84% precision and close to 19% recall on a large set of student essays. In addition, we address the problem of annotation and evaluation in this domain by showing how current approaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issues that complicate evaluation of error detection systems.", "citation": "Work on detecting grammatical errors in the language of non-native speakers covers a range of errors, but it has largely focused on syntax in a small number of languages #OTHEREFR; #REFR.", "context": "[Citation]In more morphologically-rich languages, learners naturally make many errors in morphology #OTHEREFR. Yet for many languages, there is a major bottleneck in system development: there are not enough error-annotated learner corpora which can be mined to discover the nature of learner errors, let alne enough data to train or evaluate a system. Our perspective is that one can speed up the process of determining the nature of learner errors via semi-automatic means, by generating plausible errors."}
{"citing_paper_id": "P11-1092", "cited_paper_id": "C08-1109", "citing_paper_abstract": "We present a novel approach to grammatical error correction based on Alternating Structure Optimization. As part of our work, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using various feature sets. Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. Our approach also outperforms two commercial grammar checking software packages.", "cited_paper_abstract": "In this paper we describe a methodology for detecting preposition errors in the writing of non-native English speakers. Our system performs at 84% precision and close to 19% recall on a large set of student essays. In addition, we address the problem of annotation and evaluation in this domain by showing how current approaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issues that complicate evaluation of error detection systems.", "citation": "TetreaultChunk The system in #REFR uses a chunker to extract features from a two-word window around the preposition, including lexical and POS ngrams, and the head words from neighboring constituents. .", "context": "4.2.2 Preposition Errors . DeFelice The system in #OTHEREFR for preposition errors uses a similar rich set of syntactic and semantic features as the system for article errors. In our re-implementation, we do not use a subcategorization dictionary, as this resource was not available to us. .[Citation]TetreaultParse The system in #OTHEREFR by adding additional features derived from a constituency and a dependency parse tree. For each of the above feature sets, we add the observed article or preposition as an additional feature when training on learner text."}
{"citing_paper_id": "D10-1053", "cited_paper_id": "C08-1144", "citing_paper_abstract": "We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignment model. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteriors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-totarget and target-to-source alignment models is to build two separate systems and combine their output translation lattices.", "cited_paper_abstract": "Probabilistic synchronous context-free grammar (PSCFG) translation models define weighted transduction rules that represent translation and reordering operations via nonterminal symbols. In this work, we investigate the source of the improvements in translation quality reported when using two PSCFG translation models (hierarchical and syntax-augmented), when extending a state-of-the-art phrasebased baseline that serves as the lexical support for both PSCFG models. We isolate the impact on translation quality for several important design decisions in each model. We perform this comparison on three NIST language translation tasks; Chinese-to-English, Arabic-to-English and Urdu-to-English, each representing unique challenges.", "citation": "For instance #REFR extract rules from n-best lists of alignments for a syntax-augmented hierarchical system.", "context": "Some authors have previously addressed the limitation caused by decoupling word alignment models from grammar extraction.[Citation]Alignment n-best lists are also used in Liu et al #OTHEREFR to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation #OTHEREFR. In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction #OTHEREFR."}
{"citing_paper_id": "C14-1055", "cited_paper_id": "C08-2022", "citing_paper_abstract": "We present a cross-lingual discourse relation analysis based on a parallel corpus with discourse information available only for one language. First, we conduct a corpus study to explore differences in discourse organization between Chinese and English, including differences in information packaging, implicit/explicit discourse expression divergence, and discourse connective ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations, using the parallel corpus instead of discourse annotation in the language of interest. Our resulting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification.", "cited_paper_abstract": "We present a corpus study of local discourse relations based on the Penn Discourse Tree Bank, a large manually annotated corpus of explicitly or implicitly realized relations. We show that while there is a large degree of ambiguity in temporal explicit discourse connectives, overall connectives are mostly unambiguous and allow high-accuracy prediction of discourse relation type. We achieve 93.09% accuracy in classifying the explicit relations and 74.74% accuracy overall. In addition, we show that some pairs of relations occur together in text more often than expected by chance. This finding suggests that global sequence classification of the relations in text can lead to better results, especially for implicit relations.", "citation": "Although most of the English discourse connectives identified in the PDTB are not ambiguous, some of the most frequently used ones are #REFR.", "context": "[Citation]For example, while can signal both TEMPORAL and COMPARISON relations; since, as can signal both TEMPORAL and CONTIN- GENCY relations. Discourse connectives in different languages have different ambiguities; prior work has shown that it is easier to disambiguate the sense of an ambiguous connective when parallel corpora are available #OTHEREFR. The two languages analyzed in Meyer et al. #OTHEREFR, English and French, are closely related European languages; here we investigate such differences in ambiguities between English and Chinese connectives."}
{"citing_paper_id": "C14-1055", "cited_paper_id": "C08-2022", "citing_paper_abstract": "We present a cross-lingual discourse relation analysis based on a parallel corpus with discourse information available only for one language. First, we conduct a corpus study to explore differences in discourse organization between Chinese and English, including differences in information packaging, implicit/explicit discourse expression divergence, and discourse connective ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations, using the parallel corpus instead of discourse annotation in the language of interest. Our resulting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification.", "cited_paper_abstract": "We present a corpus study of local discourse relations based on the Penn Discourse Tree Bank, a large manually annotated corpus of explicitly or implicitly realized relations. We show that while there is a large degree of ambiguity in temporal explicit discourse connectives, overall connectives are mostly unambiguous and allow high-accuracy prediction of discourse relation type. We achieve 93.09% accuracy in classifying the explicit relations and 74.74% accuracy overall. In addition, we show that some pairs of relations occur together in text more often than expected by chance. This finding suggests that global sequence classification of the relations in text can lead to better results, especially for implicit relations.", "citation": "The vast majority of connectives (at least in English) are unambiguous, so using the identity of the connective is a hard-to-beat baseline for sense prediction #REFR.", "context": "The following set of features for each expression we need to classify are extracted solely from the Chinese part of the corpus . The syntactic parse trees were obtained automatically #OTHEREFR. Connective The connective expressions themselves.[Citation]Categories The syntactic category of the expression itself, as well as that of its parents, and its left and right siblings (if any). These features are adapted from Pitler and Nenkova #OTHEREFR. Depth Depth of the expressions?s syntactic category in the parse tree for the sentence."}
{"citing_paper_id": "D13-1094", "cited_paper_id": "C08-2022", "citing_paper_abstract": "We present a classification model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6%. Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. We also present results of an experiment that provides insight into the nature and difficulty of the task.", "cited_paper_abstract": "We present a corpus study of local discourse relations based on the Penn Discourse Tree Bank, a large manually annotated corpus of explicitly or implicitly realized relations. We show that while there is a large degree of ambiguity in temporal explicit discourse connectives, overall connectives are mostly unambiguous and allow high-accuracy prediction of discourse relation type. We achieve 93.09% accuracy in classifying the explicit relations and 74.74% accuracy overall. In addition, we show that some pairs of relations occur together in text more often than expected by chance. This finding suggests that global sequence classification of the relations in text can lead to better results, especially for implicit relations.", "citation": "In their work on implicit relation classification, #REFR identified various dependencies between bigram sequences of explicitlyand implicitly-realized relations of different semantic types.", "context": "Specifically, informal observation of the data suggests that there is a tendency for the second argument of an implicit relation to begin with a longer, contentful noun phrase, rather than a pronoun. Consequently, our model includes a binary feature (FirstA2Pron) indicating whether the first word in the second argument is a pronoun. Discourse-level features The final class of features takes account of the way in which a relation fits into the broader discourse structure in the text.[Citation]These results suggest that the semantic type and the presence of a connective in one relation may be predictive of whether or not the following relation in the text is marked with a connective. Consequently, we include features indicating the semantic type of the relation occurring immediately prior in the text (PrevSemType), and whether this relation was marked implicitly or explicitly (PrevForm). The other discourse-level features take account of the dependencies between the relation in question and its neighboring relations in the text."}
{"citing_paper_id": "W09-0634", "cited_paper_id": "C08-2029", "citing_paper_abstract": "This paper presents a probabilistic model both for generation and understanding of referring expressions. This model introduces the concept of parts of objects, modelling the necessity to deal with the characteristics of separate parts of an object in the referring process. This was ignored or implicit in previous literature. Integrating this concept into a probabilistic formulation, the model captures human characteristics of visual perception and some type of pragmatic implicature in referring expressions. Developing this kind of model is critical to deal with more complex domains in the future. As a first step in our research, we validate the model with the TUNA corpus to show that it includes conventional domain modeling as a subset.", "cited_paper_abstract": "We seek to develop an efficient algorithm selecting attributes that approximates human selection. In contrast to previous work we sought to combine the strengths of cognitive theories and simple learning algorithms. We then developed a new algorithm for attribute selection based on observations from a corpus, which outperformed a simple base algorithm by a significant margin. We then carried out a detailed comparison between our algorithm and Reiter & Dale?s ?Incremental Algorithm?. In terms of achieving a human-like attribute selection, the overall performance of both algorithms is fundamentally equivalent, while differing in the handling of redundancy in selected attributes. We further investigated this phenomenon and draw some conclusions for further improvement of attribute-selection algorithms.", "citation": "The constraint on minimality has, however, been relaxed due to the computational complexity of generation, the perceived naturalness of redundant expressions, and the easiness of understanding them #OTHEREFR; #REFR).", "context": "Generation of referring expressions has been studied for the last two decades. The basic orientation of this research was pursuing an algorithm that generates a minimal description which uniquely identifies a target object from distractors. Thus the research was oriented and limited by two constraints: minimality and uniqueness.[Citation]On the other hand, the other constraint of uniqueness has not been paid much attention to. One major aim of our research is to relax this constraint on uniqueness because of the reason explained below. The fundamental goal of our research is to deal with multipartite objects, which have constituents with different attribute values."}
{"citing_paper_id": "D12-1003", "cited_paper_id": "C10-1003", "citing_paper_abstract": "This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation. In previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. The seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. We propose two types of the graphs: a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. Further, the similarity graph achieved improved performance by clustering synonyms into the same translation.", "cited_paper_abstract": "In cross-language information retrieval it is often important to align words that are similar in meaning in two corpora written in different languages. Previous research shows that using context similarity to align words is helpful when no dictionary entry is available. We suggest a new method which selects a subset of words (pivot words) associated with a query and then matches these words across languages. To detect word associations, we demonstrate that a new Bayesian method for estimating Point-wise Mutual Information provides improved accuracy. In the second step, matching is done in a novel way that calculates the chance of an accidental overlap of pivot words using the hypergeometric distribution. We implemented a wide variety of previously suggested methods. Testing in two conditions, a small comparable corpora pair and a large but unrelated corpora pair, both written in disparate languages, we show that our approach consistently outperforms the other systems.", "citation": "Our experiments showed that the proposed method outperforms conventional contextsimilarity-based methods #OTHEREFR; #REFR, and the similarity graphs improve the performance by clustering synonyms into the same translation.", "context": "We proposed a novel bilingual lexicon extraction method using label propagation for alleviating the limited seed lexicon size problem. The proposed method captures relations with all the seeds including indirect relations by propagating seed information. Moreover, we proposed using similarity graphs in propagation process in addition to cooccurrence graphs.[Citation]We are planning to investigate the following open problems in future work: word sense disambiguation and translation of compound words as described in #OTHEREFR. In addition, indirect relations have also been used in other tasks, such as paraphrase acquisition from bilingual parallel corpora #OTHEREFR. We will utilize their random walk approach or other graph-based techniques such as modified adsorption #OTHEREFR for generating seed distributions."}
{"citing_paper_id": "C14-1132", "cited_paper_id": "C10-1011", "citing_paper_abstract": "In this paper, we investigate the differences between Hungarian sentence parses based on automatically converted and manually annotated dependency trees. We also train constituency parsers on the manually annotated constituency treebank and then convert their output to dependency trees. We argue for the importance of training on gold standard corpora, and we also demonstrate that although the results obtained by training on the constituency treebank and converting the output to dependency format and those obtained by training on the automatically converted dependency treebank are similar in terms of accuracy scores, the typical errors made by different systems differ from each other.", "cited_paper_abstract": "In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, parsing and training times are still relatively long. To determine why, we analyzed the time usage of a dependency parser. We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive perceptron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account the features of negative examples built during the training. This has lead to a higher accuracy. We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm. We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation.", "citation": "We also experimented with training the Bohnet dependency parser #REFR on the manually annotated (gold standard) and the converted (silver standard) treebank.", "context": "[Citation]The Bohnet parser #OTHEREFR is a state-of-the-art graph-based parser, which employs online training with a perceptron. The parser contains a feature function for the first order factor, one for the sibling factor, and one for the grandchildren. From the corpus, 5,892 sentences (130,211 tokens) were used in the training dataset and the remaining 1,480 sentences (32,749 tokens) in the test dataset."}
{"citing_paper_id": "E12-1007", "cited_paper_id": "C10-1011", "citing_paper_abstract": "Hungarian is a stereotype of morphologically rich and non-configurational languages. Here, we introduce results on dependency parsing of Hungarian that employ a 80K, multi-domain, fully manually annotated corpus, the Szeged Dependency Treebank. We show that the results achieved by state-of-the-art data-driven parsers on Hungarian and English (which is at the other end of the configurational-nonconfigurational spectrum) are quite similar to each other in terms of attachment scores. We reveal the reasons for this and present a systematic and comparative linguistically motivated error analysis on both languages. This analysis highlights that addressing the language-specific phenomena is required for a further remarkable error reduction.", "cited_paper_abstract": "In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, parsing and training times are still relatively long. To determine why, we analyzed the time usage of a dependency parser. We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive perceptron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account the features of negative examples built during the training. This has lead to a higher accuracy. We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm. We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation.", "citation": "We employed three state-of-the-art data-driven parsers #OTHEREFR; #REFR, which achieved (un)labeled attachment scores on Hungarian not so different from the corresponding English scores (and even higher on certain domains/subcorpora).", "context": "A large part of the methodology for syntactic parsing has been developed for English. However, parsing non-configurational and less configurational languages requires different techniques. In this study, we present results on Hungarian dependency parsing and we investigate this general issue in the case of English and Hungarian.[Citation]Our investigations show that the feature representation used by the data-driven parsers is so rich that they can ? without any modification ? effectively learn a reasonable model for non-configurational languages as well. We also conducted a systematic and comparative error analysis of the system?s outputs for Hungarian and English. This analysis highlights the challenges of parsing Hungarian and suggests that the further improvement of parsers requires special handling of language-specific phenomena."}
{"citing_paper_id": "D11-1132", "cited_paper_id": "C10-1018", "citing_paper_abstract": "This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Specifically, we detect a new semantic relation by projecting the new relation?s training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.", "cited_paper_abstract": "Relation extraction is the task of recognizing semantic relations among entities. Given a particular sentence supervised approaches to Relation Extraction employed feature or kernel functions which usually have a single sentence in their scope. The overall aim of this paper is to propose methods for using knowledge and resources that are external to the target sentence, as a way to improve relation extraction. We demonstrate this by exploiting background knowledge such as relationships among the target relations, as well as by considering how target relations relate to some existing knowledge resources. Our methods are general and we suggest that some of them could be applied to other NLP tasks.", "citation": "Our work fits in to a class of relation extraction research based on ?distant supervision?, which studies how knowledge and resources external to the target domain can be used to improve relation extraction. #OTHEREFR; #REFR.", "context": "The third challenge is how to use the relation topics for a relation detector. We map relation instances in the new domains to the relation topic space, resulting in a set of new features characterizing the relationship between the relation instances and existing relations. By doing so, background knowledge from the existing relations can be introduced into the new relations, which overcomes the limitations of the existing approaches when the training data is not sufficient.[Citation]One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics. When we test on new instances, we do not need to search against the knowledge base. In addition, our topics also model the indirect relationship between relations."}
{"citing_paper_id": "P13-2012", "cited_paper_id": "C10-1032", "citing_paper_abstract": "We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17% F1.", "cited_paper_abstract": "The integration of facts derived from information extraction systems into existing knowledge bases requires a system to disambiguate entity mentions in the text. This is challenging due to issues such as non-uniform variations in entity names, mention ambiguity, and entities absent from a knowledge base. We present a state of the art system for entity disambiguation that not only addresses these challenges but also scales to knowledge bases with several million entries using very little resources. Further, our approach achieves performance of up to 95% on entities mentioned from newswire and 80% on a public test set that was designed to include challenging queries.", "citation": "Another view of predicate disambiguation seeks to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) #REFR.", "context": "The first work on event coreference dates back to Bagga and Baldwin #OTHEREFR. More recently, this task has been considered by Bejan and Harabagiu #OTHEREFR. As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics.[Citation]The most relevant, and perhaps only, work in this area is that of Roth and Frank #OTHEREFR who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new system for predicate argument alignment. As opposed to Roth and Frank, PARMA is designed as a a trainable platform for the incorporation of the sort of lexical semantic resources used in the related areas of Recognizing Textual Entailment (RTE) and Question Answering (QA)."}
{"citing_paper_id": "N13-1030", "cited_paper_id": "C10-1039", "citing_paper_abstract": "Multi-Sentence Compression (MSC) is the task of generating a short single sentence summary from a cluster of related sentences. This paper presents an N-best reranking method based on keyphrase extraction. Compression candidates generated by a word graph-based MSC approach are reranked according to the number and relevance of keyphrases they contain. Both manual and automatic evaluations were performed using a dataset made of clusters of newswire sentences. Results show that the proposed method significantly improves the informativity of the generated compressions.", "cited_paper_abstract": "We present a novel graph-based summarization framework (Opinosis) that generates concise abstractive summaries of highly redundant opinions. Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. The summaries are readable, reasonably well-formed and are informative enough to convey the major opinions.", "citation": "Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization #OTHEREFR, opinion summarization #REFR and newswire summarization #OTHEREFR.", "context": "The key assumption is that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences (i.e. word ordering). Sentence compressions are obtained by finding commonly used paths in the graph.[Citation]"}
{"citing_paper_id": "D11-1011", "cited_paper_id": "C10-1057", "citing_paper_abstract": "Class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. Yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. We propose two graph-theoretic methods (centrality and regularization), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. We carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. We conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy.", "cited_paper_abstract": "Fact collections are mostly built using semi-supervised relation extraction techniques and wisdom of the crowds methods, rendering them inherently noisy. In this paper, we propose to validate the resulting facts by leveraging global constraints inherent in large fact collections, observing that correct facts will tend to match their arguments with other facts more often than with incorrect ones. We model this intuition as a graph-ranking problem over a fact graph and explore novel random walk algorithms. We present an empirical study, over a large set of facts extracted from a 500 million document webcrawl, validating the model and showing that it improves fact quality over state-of-the-art methods.", "citation": "The proposed methods can be also applied to enhance fact farms #REFR.", "context": "Finally, we have studied the impact of the class-instance and instance-instance graphs for the class-label propagation task. The latter approach has shown to produce much more accurate results. In the future, we want to apply our approach to Web-based taxonomy induction, which according to #OTHEREFR is stifled due to the lacking relations between the instances and the classes, and the classes themselves.[Citation]Acknowledgments We acknowledge the support of DARPA contract number FA8750-09-C-3705 and NSF grant IIS-"}
{"citing_paper_id": "P11-2018", "cited_paper_id": "C10-1059", "citing_paper_abstract": "We investigate systems that identify opinion expressions and assigns polarities to the extracted expressions. In particular, we demonstrate the benefit of integrating opinion extraction and polarity classification into a joint model using features reflecting the global polarity structure. The model is trained using large-margin structured prediction methods. The system is evaluated on the MPQA opinion corpus, where we compare it to the only previously published end-to-end system for opinion expression extraction and polarity classification. The results show an improvement of between 10 and 15 absolute points in F-measure.", "cited_paper_abstract": "We describe the implementation of reranking models for fine-grained opinion analysis ? marking up opinion expressions and extracting opinion holders. The reranking approach makes it possible to model complex relations between multiple opinions in a sentence, allowing us to represent how opinions interact through the syntactic and semantic structure. We carried out evaluations on the MPQA corpus, and the experiments showed significant improvements over a conventional system that only uses local information: for both tasks, our system saw recall boosts of over 10 points.", "citation": "We also used the features we developed in #REFRb) to represent relations between expressions without taking polarity into account.", "context": "When two opinions are directly connected through a link in the semantic structure, we add the role label as a feature. Polarity pair and words along syntactic path. We follow the path between the expressions and add a feature for every word we pass: NEG- ATIVE:+emboldened+NEGATIVE.[Citation]"}
{"citing_paper_id": "D11-1109", "cited_paper_id": "C10-1080", "citing_paper_abstract": "Part-of-speech (POS) is an indispensable feature in dependency parsing. Current research usually models POS tagging and dependency parsing independently. This may suffer from error propagation problem. Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones. To solve this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model. We design several joint models and their corresponding decoding algorithms to incorporate different feature sets. We further present an effective pruning strategy to reduce the search space of candidate POS tags, leading to significant improvement of parsing speed. Experimental results on Chinese Penn Treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5%. Detailed analysis shows that the joint method is able to choose such POS tags that are more helpful and discriminative from parsing viewpoint. This is the fundamental reason of parsing accuracy improvement.", "cited_paper_abstract": "Tree-based translation models, which exploit the linguistic syntax of source language, usually separate decoding into two steps: parsing and translation. Although this separation makes tree-based decoding simple and efficient, its translation performance is usually limited by the number of parse trees offered by parser. Alternatively, we propose to parse and translate jointly by casting tree-based translation as parsing. Given a source-language sentence, our joint decoder produces a parse tree on the source side and a translation on the target side simultaneously. By combining translation and parsing models in a discriminative framework, our approach significantly outperforms a forestbased tree-to-string system by 1.1 absolute BLEU points on the NIST 2005 Chinese-English test set. As a parser, our joint decoder achieves an F1 score of 80.6% on the Penn Chinese Treebank.", "citation": "Recently, many successful joint models have been proposed, such as joint tokenization and POS tagging #OTHEREFR and joint parsing and MT #REFR.", "context": "Recent research on dependency parsing usually overlooks this issue by simply adopting gold POS tags for Chinese data #OTHEREFR. In this paper, we address this issue by jointly optimizing POS tagging and dependency parsing. Joint modeling has been a popular and effective approach to simultaneously solve related tasks.[Citation]Note that the aforementioned ?parsing? all refer to constituent parsing. As far as we know, there are few successful models for jointly solving dependency parsing and other tasks. Being facilitated by Conference on Computational Natural Language Learning #OTHEREFR shared tasks, several joint models of dependency parsing and SRL have been proposed."}
{"citing_paper_id": "P12-1059", "cited_paper_id": "C10-1105", "citing_paper_abstract": "We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entitybearing queries are generated. We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries. We show that modeling user intent significantly improves entity type resolution for head queries over the state of the art, on several metrics, without degradation in tail query performance.", "cited_paper_abstract": "Research in named entity recognition and mention detection has typically involved a fairly small number of semantic classes, which may not be adequate if semantic class information is intended to support natural language applications. Motivated by this observation, we examine the under-studied problem of semantic subtype induction, where the goal is to automatically determine which of a set of 92 fine-grained semantic classes a noun phrase belongs to. We seek to improve the standard supervised approach to this problem using two techniques: hierarchical classification and collective classification. Experimental results demonstrate the effectiveness of these techniques, whether or not they are applied in isolation or in combination with the standard approach.", "citation": "Automatic techniques for finding semantic classes include unsupervised clustering #OTHEREFR, classification #REFR and many others.", "context": "A closely related problem is that of finding the semantic classes of entities.[Citation]These techniques typically leverage large corpora, while projects such as WordNet #OTHEREFR have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage is more common."}
{"citing_paper_id": "P11-2015", "cited_paper_id": "C10-1129", "citing_paper_abstract": "Community-based knowledge forums, such as Wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. Most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in Wikipedia. In this paper, we explore more linguistically motivated approaches to vandalism detection. In particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. Experimental results suggest that (1) statistical models give evidence to unique language styles in vandalism, and that (2) deep syntactic patterns based on probabilistic context free grammars (PCFG) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams.", "cited_paper_abstract": "Discriminating vandalism edits from non-vandalism edits in Wikipedia is a challenging task, as ill-intentioned edits can include a variety of content and be expressed in many different forms and styles. Previous studies are limited to rule-based methods and learning based on lexical features, lacking in linguistic analysis. In this paper, we propose a novel Web-based shallow syntacticsemantic modeling method, which utilizes Web search results as resource and trains topic-specific n-tag and syntactic n-gram language models to detect vandalism. By combining basic task-specific and lexical features, we have achieved high F-measures using logistic boosting and logistic model trees classifiers, surpassing the results reported by major Wikipedia vandalism detection systems.", "citation": "#REFR present the first approach that is linguistically motivated.", "context": "[Citation]Their approach was based on shallow syntactic patterns, while ours explores the use of deep syntactic patterns, and performs a comparative evaluation across different stylometry analysis techniques. It is worthwhile to note that the approach of Wang and McKeown #OTHEREFR is not as practical and scalable as ours in that it requires crawling a substantial number (150) of webpages to detect each vandalism edit. From our pilot study based on 1600 edits (50% of which is vandalism), we found that the topic-specific language models built from web search do not produce stronger result than PCFG based features."}
{"citing_paper_id": "D13-1056", "cited_paper_id": "C10-1131", "citing_paper_abstract": "We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model?s alignment score approaches the state of the art.", "cited_paper_abstract": "A range of Natural Language Processing tasks involve making judgments about the semantic relatedness of a pair of sentences, such as Recognizing Textual Entailment (RTE) and answer selection for Question Answering (QA). A key challenge that these tasks face in common is the lack of explicit alignment annotation between a sentence pair. We capture the alignment by using a novel probabilistic model that models tree-edit operations on dependency parse trees. Unlike previous tree-edit models which require a separate alignment-finding phase and resort to ad-hoc distance metrics, our method treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. We demonstrate the robustness of our model by conducting experiments for RTE and QA, and show that our model performs competitively on both tasks with the same set of general features.", "citation": "Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance #OTHEREFR; #REFR, and min-cut #OTHEREFR.", "context": "[Citation]These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner #OTHEREFR are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths."}
{"citing_paper_id": "W12-2005", "cited_paper_id": "C10-1149", "citing_paper_abstract": "To date, most work in grammatical error correction has focused on targeting specific error types. We present a probe study into whether we can use round-trip translations obtained from Google Translate via 8 different pivot languages for whole-sentence grammatical error correction. We develop a novel alignment algorithm for combining multiple round-trip translations into a lattice using the TERp machine translation metric. We further implement six different methods for extracting whole-sentence corrections from the lattice. Our preliminary experiments yield fairly satisfactory results but leave significant room for improvement. Most importantly, though, they make it clear the methods we propose have strong potential and require further study.", "cited_paper_abstract": "This paper proposes a method that leverages multiple machine translation (MT) engines for paraphrase generation (PG). The method includes two stages. Firstly, we use a multi-pivot approach to acquire a set of candidate paraphrases for a source sentence S. Then, we employ two kinds of techniques, namely the selection-based technique and the decoding-based technique, to produce a best paraphrase T for S using the candidates acquired in the first stage. Experimental results show that: (1) The multi-pivot approach is effective for obtaining plenty of valuable candidate paraphrases. (2) Both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases. Moreover, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach.", "citation": "More recently, #REFR perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases.", "context": "Pang et al #OTHEREFR propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee #OTHEREFR cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora.[Citation]However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in #OTHEREFR."}
{"citing_paper_id": "D11-1113", "cited_paper_id": "C10-2013", "citing_paper_abstract": "This paper develops a framework for syntactic dependency parse correction. Dependencies in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. We use a discriminative linear ranking model to select the best governor from a group of candidates for a dependent, and our model includes a rich feature set that encodes syntactic structure in the input parse tree. The parse correction framework is parser-agnostic, and can correct attachments using either a generic model or specialized models tailored to difficult attachment types like coordination and pp-attachment. Our experiments show that parse correction, combining a generic model with specialized models for difficult attachment types, can successfully improve the quality of predicted parse trees output by several representative state-of-the-art dependency parsers for French.", "cited_paper_abstract": "We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French. The architectures are based on PCFGs with latent variables, graph-based dependency parsing and transition-based dependency parsing, respectively. We also study the in?uence of three types of lexical information: lemmas, morphological features, and word clusters. The results show that all three systems achieve competitive performance, with a best labeled attachment score over 88%. All three parsers bene?t from the use of automatically derived lemmas, while morphological features seem to be less important. Word clusters have a positive effect primarily on the latent variable parser.", "citation": "For MaltParser and MSTParser, we use the best settings from a benchmarking of parsers for French #REFRb), except that we remove unsupervised word clusters as features.", "context": "In this paper, we use the following baseline parsers: MaltParser #OTHEREFR for constituency-based parsing.[Citation]The parsing models are thus trained using features including predicted part-of-speech tags, lemmas and morphological features. For BohnetParser, we trained a new model using these same predicted features. For BerkelyParser, which was included in the benchmarking experiments, we trained a model using the so-called ?desinflection? process that addresses data sparseness due to morphological variation: both at training and parsing time, terminal symbols are word forms in which redundant morphological suffixes are removed, provided the original part-ofspeech ambiguities are kept #OTHEREFRb)."}
{"citing_paper_id": "P13-1155", "cited_paper_id": "C10-2022", "citing_paper_abstract": "We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.", "cited_paper_abstract": "In this paper we look at the problem of cleansing noisy text using a statistical machine translation model. Noisy text is produced in informal communications such as Short Message Service (SMS), Twitter and chat. A typical Statistical Machine Translation system is trained on parallel text comprising noisy and clean sentences. In this paper we propose an unsupervised method for the translation of noisy text to clean text. Our method has two steps. For a given noisy sentence, a weighted list of possible clean tokens for each noisy token are obtained. The clean sentence is then obtained by maximizing the product of the weighted lists and the language model scores.", "citation": "We use a similarity function proposed in #REFR which is based on Longest Common Subsequence Ratio #OTHEREFR.", "context": "[Citation]This cost function is defined as the ratio of LCSR and Edit distance between two strings as follows: SimCost(n,m) = LCSR(n,m)/ED(n,m) (5) LCSR(n,m) = LCS(n,m)/MaxLenght(n,m) (6) We have modified the Edit Distance calculation ED(n,m) to be more adequate for social media text. The edit distance is calculated between the consonant skeleton of the two words; by removing all vowels, we used Editex edit distance as proposed in #OTHEREFR, repetition is reduced to a single letter before calculating the edit distance, and numbers in the middle of words are substituted by their equivalent letters."}
{"citing_paper_id": "P12-1060", "cited_paper_id": "C10-2028", "citing_paper_abstract": "The amount of labeled sentiment data in English is much larger than that in other languages. Such a disproportion arouse interest in cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) using labeled data in the source language (e.g. English). Most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language. This approach suffers from the limited coverage of vocabulary in the machine translation results. In this paper, we propose a generative cross-lingual mixture model (CLMM) to leverage unlabeled bilingual parallel data. By fitting parameters to maximize the likelihood of the bilingual parallel data, the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly. Experiments on multiple data sets show that CLMM is consistently effective in two settings: (1) labeled data in the target language are unavailable; and (2) labeled data in the target language are also available.", "cited_paper_abstract": "Automated identification of diverse sentiment types can be beneficial for many NLP systems such as review summarization and public media analysis. In some of these systems there is an option of assigning a sentiment value to a single sentence or a very short text. In this paper we propose a supervised sentiment classification framework which is based on data from Twitter, a popular microblogging service. By utilizing", "citation": "Since then, sentiment classification has been investigated in various domains and different languages #OTHEREFR; #REFR.", "context": "Early work of sentiment classification focuses on English product reviews or movie reviews #OTHEREFR.[Citation]There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach #OTHEREFR treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons."}
{"citing_paper_id": "D14-1040", "cited_paper_id": "C10-2029", "citing_paper_abstract": "We propose the first probabilistic approach to modeling cross-lingual semantic similarity (CLSS) in context which requires only comparable data. The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts (e.g., crosslingual topics obtained by a multilingual topic model). These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources. Word meaning is represented as a probability distribution over the latent concepts, and a change in meaning is represented as a change in the distribution over these latent concepts. We present new models that modulate the isolated out-ofcontext word representations with contextual knowledge. Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of crosslingual semantic similarity.", "cited_paper_abstract": "Recent work on distributional methods for similarity focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. In this paper we present a method for computing similarity which builds vector representations for words in context by modeling senses as latent variables in a large corpus. We apply this to the Lexical Substitution Task and we show that our model significantly outperforms typical distributional methods.", "citation": "Here, we present only the results obtained withK = 2000 for all language pairs which also yielded the best or near-optimal performance in #REFR.", "context": "The BiLDA model is a straightforward multilingual extension of the standard LDA model #OTHEREFR. For the details regarding the modeling, generative story and training of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. We have used the Gibbs sampling procedure Available at http://people.cs.kuleuven.be/ ?ivan.vulic/software/ #OTHEREFR.[Citation]Other parameters of the model are set to the typical values according to Steyvers and Griffiths #OTHEREFR: ? = 50/K and ? = 0.01. Models in Comparison. We test the performance of our Direct-Fusion, Smoothed-Fusion and Late- Fusion models, and compare their results with the context-insensitive CLSS models described in sect. 2 (No-Context)."}
{"citing_paper_id": "C14-1033", "cited_paper_id": "C10-2032", "citing_paper_abstract": "In this paper we present a readability assessment system for Basque, ErreXail, which is going to be the preprocessing module of a Text Simplification system. To that end we compile two corpora, one of simple texts and another one of complex texts. To analyse those texts, we implement global, lexical, morphological, morpho-syntactic, syntactic and pragmatic features based on other languages and specially considered for Basque. We combine these feature types and we train our classifiers. After testing the classifiers, we detect the features that perform best and the most predictive ones.", "cited_paper_abstract": "Several sets of explanatory variables ? including shallow, language modeling, POS, syntactic, and discourse features ? are compared and evaluated in terms of their impact on predicting the grade level of reading material for primary school students. We find that features based on in-domain language models have the highest predictive power. Entity-density (a discourse feature) and POS-features, in particular nouns, are individually very useful but highly correlated. Average sentence length (a shallow feature) is more useful ? and less expensive to compute ? than individual syntactic features. A judicious combination of features examined here results in a significant improvement over the state of the art.", "citation": "Among other readability assessment whose motivation is TS, #REFR use LIBSVM #OTHEREFR and Logistic Regression from WEKA and 10 fold cross-validation.", "context": "They experiment with feature types as well but they obtain their best results using all the features. Among their highly correlated features they present the incidence of apposition in second place as we do here. We do not have any other feature in common.[Citation]They assess the readability of grade texts and obtain as best results 59.63 % with LIBSVM and 57.59 % with Logistic Regression. Since they assess different grades and use other classifiers it is impossible to compare with our results but we find that we share predictive features. They found out that named entity density and and nouns have predictive power as well."}
{"citing_paper_id": "D12-1040", "cited_paper_id": "C10-2062", "citing_paper_abstract": "?Grounded? language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts. Bo?rschinger et al2011) introduced an approach to grounded language learning based on unsupervised PCFG induction. Their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task. However, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by Chen and Mooney (2011). This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision. Experimental results on the navigation task demonstrates the effectiveness of our approach.", "cited_paper_abstract": "We present a probabilistic generative model for learning semantic parsers from ambiguous supervision. Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations. It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators.", "citation": "A number of approaches #OTHEREFR; #REFR assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence.", "context": "Most work on learning semantic parsers that map natural-language sentences to formal representations of their meaning have relied upon totally supervised training data consisting of NL/MR pairs #OTHEREFR. Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context.[Citation]Many of these approaches #OTHEREFR disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser. Kim and Mooney #OTHEREFR containing both the MR and NL sentence. They train this model on ambiguous data using EM."}
{"citing_paper_id": "W11-1203", "cited_paper_id": "C10-2070", "citing_paper_abstract": "Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic idea is based on the assumption that similar words have similar contexts across languages. The context of a word is often summarized by using the bag-of-words in the sentence, or by using the words which are in a certain dependency position, e.g. the predecessors and successors. These different context positions are then combined into one context vector and compared across languages. However, previous research makes the (implicit) assumption that these different context positions should be weighted as equally important. Furthermore, only the same context positions are compared with each other, for example the successor position in Spanish is compared with the successor position in English. However, this is not necessarily always appropriate for languages like Japanese and English. To overcome these limitations, we suggest to perform a linear transformation of the context vectors, which is defined by a matrix. We define the optimal transformation matrix by using a Bayesian probabilistic model, and show that it is feasible to find an approximate solution using Markov chain Monte Carlo methods. Our experiments demonstrate that our proposed method constantly improves translation accuracy.", "cited_paper_abstract": "We present a new method, based on graph theory, for bilingual lexicon extraction without relying on resources with limited availability like parallel corpora. The graphs we use represent linguistic relations between words such as adjectival modification. We experiment with a number of ways of combining different linguistic relations and present a novel method, multi-edge extraction (MEE), that is both modular and scalable. We evaluate MEE on adjectives, verbs and nouns and show that it is superior to cooccurrence-based extraction (which does not use linguistic analysis). Finally, we publish a reproducible baseline to establish an evaluation benchmark for bilingual lexicon extraction.", "citation": "Using comparable corpora to automatically extend bilingual dictionaries is becoming increasingly popular #OTHEREFR; #REFR.", "context": "[Citation]The general idea is based on the assumption that similar words have similar contexts across languages. The context of a word can be described by the sentence in which it occurs #OTHEREFR. A few previous studies, like #OTHEREFR, suggested to use the predecessor and successors from the dependency-parse tree, instead of a word window."}
{"citing_paper_id": "P14-1040", "cited_paper_id": "C10-2116", "citing_paper_abstract": "We present a simple, data-driven approach to generation from knowledge bases (KB). A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG (Tree Adjoining Grammar); and that it takes into account both syntactic and semantic information. The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data. Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar; and is comparable with a handcrafted symbolic approach.", "cited_paper_abstract": "With OWL (Web Ontology Language) established as a standard for encoding ontologies on the Semantic Web, interest has begun to focus on the task of verbalising OWL code in controlled English (or other natural language). Current approaches to this task assume that axioms in OWL can be mapped to sentences in English. We examine three potential problems with this approach (concerning logical sophistication, information structure, and size), and show that although these could in theory lead to insuperable difficulties, in practice they seldom arise, because ontology developers use OWL in ways that favour a transparent mapping. This result is evidenced by an analysis of patterns from a corpus of over 600,000 axioms in about 200 ontologies.", "citation": "As discussed in #REFR, one important limitation of these approaches is that they assume a simple deterministic mapping between knowledge representation languages and some controlled natural language (CNL).", "context": "With the development of the semantic web and the proliferation of knowledge bases, generation from knowledge bases has attracted increased interest and so called ontology verbalisers have been proposed which support the generation of text from (parts of) knowledge bases. One main strand of work maps each axiom in the knowledge base to a clause. Thus the OWL verbaliser integrated in the Prote?ge? tool #OTHEREFR describes an ontology verbaliser using XML-based generation.[Citation]Specifically, the assumption is that each atomic term (individual, class, property) maps to a word and each axiom maps to a sentence. As a result, the verbalisation of larger ontology parts can produce very unnatural text such as, Every cat is an animal. Every dog is an animal."}
{"citing_paper_id": "P12-1026", "cited_paper_id": "C10-2139", "citing_paper_abstract": "From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.", "cited_paper_abstract": "We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation: word-based models and character-based models. We show that, in spite of similar performance overall, the two models produce different distribution of segmentation errors, in a way that can be explained by theoretical properties of the two models. The analysis is further exploited to improve segmentation accuracy by integrating a word-based segmenter and a character-based segmenter. A Bootstrap Aggregating model is proposed. By letting multiple segmenters vote, our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff.", "citation": "Previous research shows that character-based segmentation models trained on labeled data are reasonably accurate #REFR.", "context": "This data covers all news published by Xinhua News Agency #OTHEREFR, which contains over 473 million characters. 3.1.3 Pre-processing: Word Segmentation Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characters. To find the basic language units, i.e. words, segmentation is a necessary pre-processing step for word clustering.[Citation]Furthermore, as shown in #OTHEREFR, appropriate string knowledge acquired from large-scale unlabeled data can significantly enhance a supervised model, especially for the prediction of out-of-vocabulary (OOV) words. In this paper, we employ such supervised and semisupervised segmenters4 to process raw texts."}
{"citing_paper_id": "P13-2031", "cited_paper_id": "C10-2139", "citing_paper_abstract": "This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the ?segmentation agreements? between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data. The segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature.", "cited_paper_abstract": "We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation: word-based models and character-based models. We show that, in spite of similar performance overall, the two models produce different distribution of segmentation errors, in a way that can be explained by theoretical properties of the two models. The analysis is further exploited to improve segmentation accuracy by integrating a word-based segmenter and a character-based segmenter. A Bootstrap Aggregating model is proposed. By letting multiple segmenters vote, our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff.", "citation": "#REFR carried out a thorough survey that includes theoretical and empirical comparisons from four aspects.", "context": "Character-based and word-based models present different behaviors and each one has its own strengths and weakness.[Citation]Here, two critical properties of the two models supporting the co-regularization in this study are highlighted. Character-based models present better prediction ability for new words, since they lay more emphasis on the internal structure of a word and thereby express more nonlinearity. On the other side, it is easier to define the word-level features in word-based models."}
{"citing_paper_id": "D14-1053", "cited_paper_id": "C10-2167", "citing_paper_abstract": "We propose a semi-supervised bootstrapping algorithm for analyzing China?s foreign relations from the People?s Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, time information is considered in our algorithm through a hierarchical bayesian model to guide the bootstrapping approach. We are hopeful that our approach can facilitate quantitative political analysis conducted by social scientists and politicians.", "cited_paper_abstract": "An important task of opinion mining is to extract people?s opinions on features of an entity. For example, the sentence, ?I love the GPS function of Motorola Droid? expresses a positive opinion on the ?GPS function? of the Motorola phone. ?GPS function? is the feature. This paper focuses on mining features. Double propagation is a state-of-the-art technique for solving the problem. It works well for medium-size corpora. However, for large and small corpora, it can result in low precision and low recall. To deal with these two problems, two improvements based on part-whole and ?no? patterns are introduced to increase the recall. Then feature ranking is applied to the extracted feature candidates to improve the precision of the top-ranked candidates. We rank feature candidates by feature importance which is determined by two factors: feature relevance and feature frequency. The problem is formulated as a bipartite graph and the well-known web page ranking algorithm HITS is used to find important features and rank them high. Experiments on diverse real-life datasets show promising results.", "citation": "Methods that relate to our approach include semi-supervised approaches such as pipeline or propagation algorithms #OTHEREFR; #REFR.", "context": "Kim and Hovy #OTHEREFR identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. #OTHEREFR jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie #OTHEREFR introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions.[Citation]Concretely, Qiu et al. #OTHEREFR proposed a rulebased semi-supervised framework called double propagation for jointly extracting opinion words and targets. Compared to existing bootstrapping approaches, our framework is more general one with less restrictions . In addition, our approach harness global information (e.g. document-level, time-level) to guide the bootstrapping algorithm."}
{"citing_paper_id": "P13-1047", "cited_paper_id": "C10-2172", "citing_paper_abstract": "To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.", "cited_paper_abstract": "Existing works indicate that the absence of explicit discourse connectives makes it difficult to recognize implicit discourse relations. In this paper we attempt to overcome this difficulty for implicit relation recognition by automatically inserting discourse connectives between arguments with the use of a language model. Then we propose two algorithms to leverage the information of these predicted connectives. One is to use these predicted implicit connectives as additional features in a supervised model. The other is to perform implicit relation recognition based only on these predicted connectives. Results on Penn Discourse Treebank 2.0 show that predicted discourse connectives help implicit relation recognition and the first algorithm can achieve an absolute average f-score improvement of 3% over a state of the art baseline system.", "citation": "These features are chosen due to their superior performance in previous work #OTHEREFR and our previous work #REFR.", "context": "For both main task and auxiliary tasks, we adopt the following three feature types.[Citation]Verbs: Following #OTHEREFR, we extract the pairs of verbs from both text spans. The number of verb pairs which have the same highest Levin verb class levels #OTHEREFR is counted as a feature. Besides, the average length of verb phrases in each argument is included as a feature."}
{"citing_paper_id": "E89-1037", "cited_paper_id": "C86-1025", "citing_paper_abstract": "We sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation, as formalized in the LFG notion of codescriptions. The approach is illustrated with examples from English, German and French where the source and the target language sentence show noteworthy differences in linguistic analysis.", "cited_paper_abstract": "The t cansfer components of typ ica l second generation (G2) MT systems do not fully conform to the principles o~ G2 modularity, incorporating extensive target language information while failing to separate translation facts from linguistic theory. The exclusion from transfer of all non-contrastive information \\[eads us to a system design in which the three major components operate in parallel, rather than in seqnence. We also propose that MT systems be designed to allow translators to express their knowledge in natural metalanguage statements. I. Modularity: a Basic Principle of G2 S~stems Modularity is a defining characteristic of second genera ti.on mach ine t rans la t ion systems (hereafter C2 MT). G2 systems are claimed to be based on a mode\\] in which linguistic descriptions are clearly separated from the algorithms and programs that actually produce translations. Moreover, in this mode\\]., the linguistic facts that pertain solely to the source language (SL) are supposed to be clearly separated from the facts that pertain solely to the target language (TL), and from those facts that concern the lexlcai and structural contrasts between TL and S \\ ] , . Such, at Least, are the principles of G2 design, as set fortb for example in Vauquois \\[i \\] . This conception of MT gives rise to systems composed of three distinct and successive phases : a monolingual analysis component, which produces a SL-dependent structural description (SD) of the input text; a transfer component, which maps that SD onto a TL-dependent SD; and a monolingual synthesis component, which transforms that SD into a TL output text. As pointed out by Kay \\[2\\], this classical G2 design offers a number of advantages. Ideally, it should allow the formal\\[ description of a given language to serve the needs of ana lys i s and synthes is indifferently. As well, it should allow a given analysis or synthesis component to be coupled onto other MT systems of simi.lar design to produce translations for other language pairs. And finally, because the a lgor i thms are independent of the particular linguistic descriptions, they too should be reusable in other MT applications. il. Transfer in T_yj~ical G2 Systems Transl!er in G2 systems like TAUM-AVIATION \\[3\\], ARIANE-78 \\[4\\] and METAL \\[511 is essentially a treetransformation system, relating the SDs of two complete translation unlts (generally sentences). Lexieal units (LUs) are not translated in isolation; rather, transfer rules typically test the structural environment of each SL LU and, after inserting the appropriate TL equivalent, may rearrange that structure to accord with contextual\\[ constraints imposed by the TL LU. Details of formalization aside, transfer rules ia these systems will encode facts like the following : (,.a> fP\\s =+ /v:\\ ,' V S l I know savo i r V NP V NP", "citation": "We also achieve modularity of a more basic sort: our correspondence mechanism permits contrastive transfer ules that depend on but do not duplicate the specifications of independently motivated grammars of the source and target languages #REFR.", "context": "Our proposal, as it is set forth below, allows us to state simultaneous correspondences between several levels of source-target representations, and thus is neither interlingual nor transfer-based. We can achieve modularity of linguistic specifications, by not requiring conceptually different kinds of linguistic information to be combined into a single structure. Yet that diverse information is still accessible to determine the set of target strings that adequately translate a source string.[Citation]A GENERAL ARCHITECTURE FOR LINGUISTIC DESCRIPTIONS Our approach uses the equalityand description-based mechanisms of Lexical-Functional Grammar. As introduced by Kaplan and Bresnan #OTHEREFR, lexical-functional grammar assigns to every sentence two levels of syntactic representation, a constituent structure (c-structure) and a functional structure (f-structure). These structures are of different formal types--the c-structure is a phrase-structure t e while the f-structure is a hierarchical finite function--and they characterize different aspects of the information carried by the sentence."}
{"citing_paper_id": "W98-0508", "cited_paper_id": "C86-1046", "citing_paper_abstract": "In this paper we demonstrate that it is possible to parse dependency structures deterministically in linear time using syntactic heuristic choices. We in ist prove theoretically that deterministic, linear parsing of dependency structures i possible under certain conditions. We then discuss a fully implemented parser and argue that those conditions hold for at least one natural language. Empirical data demonstrates that the parsing time is indeed linear. The present quality of the parser in terms of finding the right dependency structure for sentences i about 85%.", "cited_paper_abstract": "This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars. The principles of Dependency Unification Grammar (DUG) are discussed. The computer language DRL (Dependency Representation Language) is introduced J.n which DUGs can be formulated. A unification-based parsing procedure is part of the formalism. PLAIN is implemented at the universities of Heidelberg, Bonn, Flensburg, Kiel, Zurich and Cambridge U.K.", "citation": "A dependency theory of syntactic structure indicates yntactic relations directly between the words of a sentence #OTHEREFR, #REFR.", "context": "If a parsing algorithm were able to make confidently only the right local structural choices for a sentence, it would deterministically produce only a single, correct ree. The benefits would be obvious: there would be no search for the right tree in a forest, and the processing time could be benign. However, to our best knowledge, no one has yet been able to produce a deterministic parser for a constituent analysis of sentences.[Citation]We have studied the parsing of dependency structures over several years #OTHEREFR. In this paper we discuss the final version of our fully implementod dependency parser and show that h is possible to design a heuristic deterministic dependency parser that parses sentences in linear time. The parser chooses heuristically only one direct governor among alternatives for each word in a sentence."}
{"citing_paper_id": "P07-1013", "cited_paper_id": "C86-1063", "citing_paper_abstract": "Grapheme-to-phoneme conversion (g2p) is a core component of any text-to-speech system. We show that adding simple syllabification and stress assignment constraints, namely ?one nucleus per syllable? and ?one main stress per word?, to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy. Secondly, we assessed morphological preprocessing for g2p conversion. While morphological information has been incorporated in some past systems, its contribution has never been quantitatively assessed for German. We compare the relevance of morphological preprocessing with respect to the morphological segmentation method, training set size, the g2p conversion algorithm, and two languages, English and German.", "cited_paper_abstract": "A cent ra l p rob lem in speech synthes is w i th unres t r i c ted vocabu lary is the automat ic der ivat ion of cor rec t p ronunc ia t ion from the graphemic form of a text . The so f tware module GRAPHON was deve loped to per fo rm th i s convers ion for German and is cur rent ly be ing extended by a morpho log ica l ana lys i s component . Th is ana lys i s is based on a morph lexicon and a sot of ru le~ and s t ruc tura l descr ip t ions for German wordfo rms. I t p rov ides each text input i tem wi th an ind iv idua l character i za t ion such that the phonological~ syntact i c , and prosod ic components may operate upon it. Th is sys temat ic approach tht~s serves to minimize the number of wrong t ranscr ip t ions and a t the same time lays the foundat ion for the generat ion of s t ress and in tonat ion pat terns , y ie ld ing more inte l l ig ib le~ natura lsound ing , and genera l ly acceptab le synthet i c speech.", "citation": "Vowel length and quality has been argued to also depend on morphological structure #REFR.", "context": "In German, information about morphological boundaries is needed to correctly insert glottal stops [P] in complex words, to determine irregular pronunciation of affixes (v is pronounced [v] in vertikal but [f] in ver+ticker+n, and the suffix syllable heit is not stressed although superheavy and word final) and to disambiguate letters (e.g. e is always pronounced /@/ when occurring in inflectional suffixes).[Citation]Furthermore, morphological boundaries overrun default syllabification rules, such as the maximum onset principle. Applying default syllabification to the word ?Sternaniso?l? would result in a syllabification into Ster-na-ni-so?l (and subsequent phonemization to something like /StE?\"na:niz?:l/) instead of Stern-a-nis-o?l (/\"StE?nPani:sP?:l/). Syllabification in turn affects phonemization since voiced fricatives and stops are devoiced in syllable-final position."}
{"citing_paper_id": "W00-1107", "cited_paper_id": "C88-1065", "citing_paper_abstract": "This paper argues that a finite-state language model with a ternary expression representation is currently the most practical and suitable bridge between natural language processing and information retrieval. Despite the theoretical computational inadequacies of finitestate grammars, they are very cost effective (in time and space requirements) and adequate for practical purposes. The ternary expressions that we use are not only linguistically-motivated, but also amenable to rapid large-scale indexing. REXTOR (Relations EXtracTOR) is an implementation f this model; in one uniform framework, the system provides two separate grammars for extracting arbitrary patterns of text and building ternary expressions from them. These content representational structures serve as the input to our ternary expressions indexer. This approach to natural language information retrieval promises to significantly raise the performance of current systems.", "cited_paper_abstract": "This paper presents the lexical component of the STAttT Question Answering system developed at the MIT Artificial Intelligence Laboratory. START is able to interpret correctly a wide range of semantic relationships associated with alternate xpressions of the arguments of verbs. The design of the system takes advantage of the results of recent linguistic research into the structure of the lexicon, allowing START to attain a broader range of coverage than many existing systems while maintaining modular organization.", "citation": "In order to solve this problem, START deploys \"S-rules\" #REFR, which are reversible syntactic/semantic transformational rules that render explicit he relationship between alternate realizations of the same meaning.", "context": "Although it is certainly possible to manually encode such semantic knowledge as extraction and relation rules, this solution is far from elegant. A potential solution to this semantic variations problem is to borrow the solution employed by START. A ternary expression representation of natural language mimics its syntactic organization, and hence sentences that differ in surface form but are close in meaning will not map into the same structure.[Citation]For example, a buy expression is semantically equivalent to a sell expression, except he subject and indirect objects are exchanged. Because many verbs can undergo the same alternations, they can in fact be grouped into verb classes, and hence governed by the same S-rules. Thus, S-rules can be viewed as metarules applied over ternary expressions."}
{"citing_paper_id": "P98-2157", "cited_paper_id": "C88-1075", "citing_paper_abstract": "Language models for speech recognition typically use a probability model of the form Pr(an\\[al,a2,...,an-i). Stochastic grammars, on the other hand, are typically used to assign structure to utterances, A language model of the above form is constructed from such grammars by computing the prefix probability ~we~* Pr(al.- .artw), where w represents all possible terminations of the prefix al...an. The main result in this paper is an algorithm to compute such prefix probabilities given a stochastic Tree Adjoining Grammar (TAG). The algorithm achieves the required computation in O(n 6) time. The probability of subderivations that do not derive any words in the prefix, but contribute structurally to its derivation, are precomputed to achieve termination. This algorithm enables existing corpus-based stimation techniques for stochastic TAGs to be used for language modelling.", "cited_paper_abstract": "An efficient context-free parsing algorithm ispresented that can parse sentences with unknown parts of unknown length. It pa'oduees in finite form all possible parses (often infinite in number) that could account for the missing parts. The algorithm is a variation oa the construction due to Earley. ltowever, its presentation is such that it can readily be adapted to any chart parsing schema (topdown, bottom-up, etc...).", "citation": "In specifying the equations, we exploit techniques used in the parsing of incomplete input #REFR.", "context": "Rt* Rtl e~sp ine i f~f2 n i flff/~2 n E C Figure 2: Wrapping of auxiliary trees when computing the prefix probability To derive a method for the computation of prefix probabilities, we give some simple recursive equations. Each equation decomposes an item into other items in all possible ways, in the sense that it expresses the probability of that item as a function of the probabilities of items associated with equal or smaller portions of the input.[Citation]This allows us to compute the prefix probability as a by-product of computing the inside probability. In order to avoid the problem of nontermination outlined above, we transform our equations to remove infinite recursion, while preserving the correctness of the probability computation. The transformation of the equations is explained as follows."}
{"citing_paper_id": "P05-2024", "cited_paper_id": "C88-2121", "citing_paper_abstract": "This paper reports the corpus-oriented development of a wide-coverage Japanese HPSG parser. We first created an HPSG treebank from the EDR corpus by using heuristic conversion rules, and then extracted lexical entries from the treebank. The grammar developed using this method attained wide coverage that could hardly be obtained by conventional manual development. We also trained a statistical parser for the grammar on the treebank, and evaluated the parser in terms of the accuracy of semantic-role identification and dependency analysis.", "cited_paper_abstract": "In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988). In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure. We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not givo the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely. We then show how a 'lexicalized' grammar naturally follows from the extended omain of locality of TAGs and present some of the linguistic advantages ofour approach. A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect o this set. The strategy is independent of the linguistic theory adopted and of the underlying rammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search. We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach. *This work is partially supported by ARO grant DAA29-84-9- 007, DARPA grant N0014-85-K0018, NSF grants MCS-82-191169 and DGR-84-10413. The second author is also partially supported by J.W. Zellldja grant. The authors would llke to thank Mitch Marcus for his helpful conunents about this work. Thanks are also due to Ellen Hays. **Visiting from University of Paris VII. 1 'Lexicalization' of grammar formalisms Most of the current linguistics theories tend to give lexical accounts of several phenomena that used to be considered purely syntactic. The information put in the lexicon is therefore increased and complexified (e.g. lexical rules in LFG, used also by HPSG, or Gross 1984 is lexicongrammar). But the question of what it means to 'lexicalize' a grammar is seldom addressed. The possible consequences of this question for parsing are not fully investigated. We present how to 'lexicalize' grammars uch as CFGs in a radical way, while possibly keeping the rules in their full generality. If one assumes that the input sentence is finite and that it cannot be syntactically infinitely ambiguous, the 'lexicalization' simplifies the task of a parser. We say that a grammar formalism is 'lexicalized' if it consists of: ? a finite set of structures to be associated with lexical items, which usually will be heads of these structures, ? an operation or operations for composing the structures. 1 The finite set of structures define the domain of locality over which constraints are specified and these are local with respect o their lexical heads. Not every grammar formalism in a given form is in a 'lexicalized' form. For example, a CFG, in general, will not be in a 'lexicalized' form. However, by extending its domain of locality, it can be 'lexicalized'. We require that the 'lexicalized' grammar produces not only the same language as the original grammar, but also the same structures (or tree set). We propose to study the conditions under which such a 'lexicalization' is possible for CFGs and TAGs. The domain of locality of a CFG can be extended by using a tree rewriting system that only uses substitution. We state the conditions under which CFGs can be 'lexlcalized' without changing the structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We then", "citation": "Head-Driven Phrase Structure Grammar (HPSG) is classified into lexicalized grammars #REFR.", "context": "[Citation]It attempts to model linguistic phenomena by interactions between a small number of grammar rules and a large number of lexical entries. Figure 1 shows an example of an HPSG derivation of a Japanese sentence ?kare ga shinda,? which means, ?He died.. In HPSG, linguistic entities such as words and phrases are represented by typed feature structures called signs, and the grammaticality of a sentence is verified by applying grammar rules to a sequence of signs."}
{"citing_paper_id": "P96-1027", "cited_paper_id": "C88-2128", "citing_paper_abstract": "Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases.", "cited_paper_abstract": "The use of a single grammar for both parsing and generation is an idea with a certain elegance, the desirability of which several researchers have noted. In this paper, we discuss a more radical possibility: not only can a single grammar be used by different processes engaged in various \"directions\" of processing, but one and the same language-processing architecture can be used for processing the grammar in the various modes. In particular, parsing and generation can be viewed as two processes engaged in by a single parameterized theorem pr6ver for the logical interpretation f the formalism. We discuss our current implementation f such an architecture, which is parameterized in such a way that it can be used for either purpose with grammars written in the PATR formalism. Furthermore, the architecture allows fine tuning to reflect different processing strategies, including parsing models intended to mimic psycholinguistic phenomena. This tuning allows the parsing system to operate within the same realm of efficiency as previous architectures for parsing alone, but with much greater flexibility for engaging in other processing regimes.", "citation": "In this respect, it differs from the proposal of #REFR which starts with all word edges leaving and entering asingle vertex.", "context": "the beginning of this paper, our generator does not involve string positions centrally in the chart representation.[Citation]But there is essentially no information in such a representation. Neither the chart nor any other special data structure is required to capture the fact that a new phrase may be constructible out of any given pair, and in either order, if they meet certain syntactic and semantic riteria."}
{"citing_paper_id": "P01-1004", "cited_paper_id": "C90-3044", "citing_paper_abstract": "In this paper, we compare the relative effects of segment order, segmentation and segment contiguity on the retrieval performance of a translation memory system. We take a selection of both bag-of-words and segment order-sensitive string comparison methods, and run each over both characterand word-segmented data, in combination with a range of local segment contiguity models (in the form of N-grams). Over two distinct datasets, we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N- gram models. Further, in their optimum configuration, bag-of-words methods are shown to be equivalent to segment ordersensitive methods in terms of retrieval accuracy, but much faster. We also provide evidence that our findings are scalable.", "cited_paper_abstract": "An essential problem of example-based translation is how to utilize more than one translation example for translating one source sentence. This 1)aper proposes a method to solve this problem. We introduce tile representation, called .matching e,,z:pressio~z, which tel)resents the combination of fragments of translation examples. The translation process consists of three steps: (.1) Make the source matching expression from lhe source sentence. (2) TransDr the source matching expression into the target matching expression. (3) Construct the target sentence from the target matching expression. This mechanism generates some candidates of translation. To select, the best translation out of them, we define the score of a translation.", "citation": "Typically in example-based machine translation, either a single TRec is retrieved from the TM based on a match with the overall L1 input, or the input is partitioned into coherent segments, and individual translations retrieved for each #REFR; this is the first step toward generating a customised translation for the input.", "context": "Translation memories #OTHEREFR. Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input.[Citation]With stand-alone TM systems, on the other hand, the system selects an arbitrary number of translation candidates falling within a certain empirical corridor of similarity with the overall input string, and simply outputs these for manual manipulation by the user in fashioning the final translation. A key assumption surrounding the bulk of past TR research has been that the greater the match stringency/linguistic awareness of the retrieval mechanism, the greater the final retrieval accuracy will become. Naturally, any appreciation in retrieval complexity comes at a price in terms of computational overhead."}
{"citing_paper_id": "W91-0105", "cited_paper_id": "C90-3052", "citing_paper_abstract": "A consequent use of reversible grammars within natural language generation systems has strong implications for the separation into strategic and tactical components. A central goal of this paper is to make plausible that a uniform architecture for grammatical Processing will serve as a basis to achieve more flexible and efficient generation systems.", "cited_paper_abstract": "We introduce TFS, a computer formalism in the class of logic ibrmaiisms which integrates a powerful type system. Its basic data structures are typed feature structures. The type system encourages an objectoriented approach to linguistic description by providing a multiple inheritance mechanism and an inference mechanism which allows the specitication of relations between levels o\\[ linguistic description defined as classes of objects. We illustrate this alcproach starting from a very simple DCG, and show how to make use of the typing system to enforce general constraints and modularize linguistic descriptions, and how further abstraction leads to a tlPSG-Iike grammar.", "citation": "Furthermore there are also approaches that assume that it is possible to use the same algorithm for processing the grammar in both directions #OTHEREFR, #REFR).", "context": "From a formal point of view the main interest in obtaining non-directional grammars is the specification of the relationship between strings and logical forms. 1 According to van Noord #OTHEREFR, a grammar is reversible if the parsing and generation problem is computable and the relation between strings and logical forms is symmetric. In this case parsing and generation are viewed as mutually inverse processes.[Citation]A great advantage of a uniform process is that a discourse and task independent module for grammatical processing is available. This means that during performing both tasks the same grammatical power is potentially disposable (regardless of the actual language use). Nevertheless, in most of the areal' generation systems where all aspects of the generation process of natural language utterances are considered, grammars are used that are especially designed for generation purposes #OTHEREFR). ~ The purpose of this paper is to show that the use of a uniform architecture for grammatical processing has important influences for the whole generation task."}
{"citing_paper_id": "P91-1020", "cited_paper_id": "C90-3053", "citing_paper_abstract": "Machine translation oflocative prepositions is not straightforward, even between closely related languages. This paper discusses a system of translation of locative prepositions between English and French. The system is based on the premises that English and French do not always conceptualize objects in the same way, and that this accounts for the major differences in the ways that locative prepositions are used in these languages. This paper introduces knowledge representations of conceptualizations of objects, and a method for translating prepositions based on these conceptual representations.", "cited_paper_abstract": "This paper deals with the automatic translatiou of prepositions, which are highly polysemous. Moreover, the same real situation is o f ten expressed by d i f fe rent prepos i t ions in d i f fe rent languages. We proceed f rom the hypothesis that di f ferent usage patterns are due to di f ferent conceptualizations of the same real s i tuat ion . Fo l low ing cogn i t ive pr inc ip les o f spat ia l conceptual izat ion, we design, a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can d i f fe rent ia te subt le d i s t inc t ions between spat ia l l y s ignif icant configurations.", "citation": "2.2.4 Other work Independently, #REFR took an. approach sin~lar to ours to the problem of translating locative prepositions.", "context": "Our system works as follows: given the source-language s ntence, its objective meaning (i.e., its language-independent meaning) is derived. This is done by first using the ideal meanings of the source-language preposition to find the conceptualization that applies to the reference object, and then deriving the objective meaning of the sentence from this conceptualization. (Because each conceptualization f an object used as a reference object corresponds to some objective meaning, this last step is easily performed.) Given the objective meaning of the sentence, the conceptualization f the reference object that should be used in the target language is then found. Finally, using the list of ideal meanings of the target.language prepositions together with the target-language conceptualization, the system derives the preposition to be used in the target-language sentence.[Citation]She worked on translation between English and German rather than English~and French. This supports our hypothesis that the theory we use can be extended to pairs of languages other than English and French. In addition to the types of expressions our system translates, her system translates entences with verbs other than 'to be'."}
{"citing_paper_id": "C90-3031", "cited_paper_id": "C90-3063", "citing_paper_abstract": "Our goal is to explore methods for combining structured but incomplete information from dictionaries with the unstructured but more complete information available in corpora for the creation of a bilingual lexical data base. This paper concentrates on the class of action verbs of movement, and builds on earlier work on lexical correspondences between languages and specific to this verb class. The languages we explore here are English and French. We first examine the way prototypical verbs of movement are translated in the Collins- Robert (Collins 1978, henceforth CR) bilingual dictionary. We then analyze the behavior of some of these verbs in a large bilingual corpus. We take advantage of the results of linguistic research on verb types (e.g. Levin, to appear) coupled with data from machine readable dictionaries to motivate corpus-based text analysis for the purpose of estabfishing lexical correspondences with the full range of associated translations and then attach frequencies to translations.", "cited_paper_abstract": "Manual acquisition of semantic onstraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect, semantic onstraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scherne was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun \"it\" in sentences that were randomly selected from the corpus. Ttle results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic onstraints and thus provide a basis {'or a useful disambiguat.ion tool.", "citation": "On more syntactic note, #REFR use statistical methods over linguistically parsed text #OTHEREFR to resolve anaphorie reference.", "context": "Church and ttanks 1989 and Church et al 1990 develop a battery of statistical methods to induce linguistic regularities. They identify coocurrence relations by computing statistics (e.g. by use of mutual information, t-score) over millions of words of text. Their approach is focussed on monolingual rather than bilingual corpus analysis, and constitutes a significant contribution to lexical research.[Citation]In the arena of automatic bilingual lexicon construction, Catizone el: al. 1989 take two corresponding texts (English and German) and develop aigoritluns to deternffne l xical alignments by using statistical methods over texts combined with the optional support of an MRD. In contrast, Sadler 1989 proposes parsing aligned corpora into dependency trees, which form the structures upon which lexieal correspondences are suggested to the user."}
{"citing_paper_id": "S12-1002", "cited_paper_id": "C90-3063", "citing_paper_abstract": "We present a novel adaptive clustering model for coreference resolution in which the expert rules of a state of the art deterministic system are used as features over pairs of clusters. A significant advantage of the new approach is that the expert rules can be easily augmented with new semantic features. We demonstrate this advantage by incorporating semantic compatibility features for neutral pronouns computed from web n-gram statistics. Experimental results show that the combination of the new features with the expert rules in the adaptive clustering approach results in an overall performance improvement, and over 5% improvement in F1 measure for the target pronouns when evaluated on the ACE 2004 newswire corpus.", "cited_paper_abstract": "Manual acquisition of semantic onstraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect, semantic onstraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scherne was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun \"it\" in sentences that were randomly selected from the corpus. Ttle results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic onstraints and thus provide a basis {'or a useful disambiguat.ion tool.", "citation": "One of the earliest methods for using predicate-argument frequencies in pronoun resolution is that of #REFR.", "context": "This allows clustering to start in any section of the document where coreference decisions are easier to make, and thus create accurate clusters earlier in the process. The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them #OTHEREFR. The focus in these studies has been on the semantic similarity between a mention and a candidate antecedent, or the parallelism between the semantic role structures in which the two appear.[Citation]Closer to our use of semantic compatibility features for pronouns are the approaches of Kehler et al #OTHEREFR. The last work showed that pronoun resolution can be improved by incorporating semantic compatibility features derived from search engine statistics in the twin-candidate model. In our approach, we use web-based language models to compute semantic compatibility features for neutral pronouns and show that they can improve performance over a state-of-the-art coreference resolution system."}
{"citing_paper_id": "P99-1060", "cited_paper_id": "C92-1024", "citing_paper_abstract": "We present a new chart parsing method for Lambek grammars, inspired by a method for D- Tree grammar parsing. The formulae of a Lambek sequent are firstly converted into rules of an indexed grammar formalism, which are used in an Earley-style predictive chart algorithm. The method is non-polynomial, but performs well for practical purposes - - much better than previous chart methods for Lambek grammars.", "cited_paper_abstract": "This paper I describes a method for chart parsing Lambek grammars. The method is of particular interest in two regards. Firstly, it allows efficient processing of grammars which use necessity operators, mr extension proposed for handling locality phenomena. Secondly, the method is easily adapted to allow incremental proceasing of Lambek grammars, a possibility that has hitherto been unavailable.", "citation": "The method of #REFR avoids this complicated book-keeping, and also rules out some useless ubderivations allowed by Khnig is method, but does so at the cost of computing a representation of all the possible category sequences that might be tested in an exhaustive sequent proof search.", "context": "1 The previous chart methods for the Lambek calculus deal with this problem in different ways. The method of K6nig #OTHEREFR places hypotheticals on separate 'minicharts' which can attach into other (mini)charts where combinations are 1In effect, hypotheticals belong on additional suborderings, which can connect into the main ordering of the chart at various positions, generating a branching, multi-dimensional ordering scheme. possible. The method requires rather complicated book-keeping.[Citation]Neither of these methods exhibits performance that would be satisfactory for practical use. 2"}
{"citing_paper_id": "P96-1023", "cited_paper_id": "C92-2066", "citing_paper_abstract": "We present a language model consisting of a collection of costed bidirectional finite state automata ssociated with the head words of phrases. The model is suitable for incremental pplication of lexical associations in a dynamic programming search for optimal dependency tree derivations. We also present a model and algorithm for machine translation involving optimal \"tiling\" of a dependency tree with entries of a costed bilingual exicon. Experimental results are reported comparing methods for assigning cost functions to these models. We conclude with a discussion of the adequacy of annotated linguistic strings as representations formachine translation.", "cited_paper_abstract": "The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexieally sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic ontext-free grammars). Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outsidelike iterative algorithm for estimating the parameters of a SLTAG given a training corpus. Finally, we should how SLTAG enables to define a lexicalized version of stochastic ontext-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic ontext-free grammars.", "citation": "The model is intended to combine the lexical sensitivity of N-gram models #OTHEREFR without he computational overhead of statistical lexicalized treeadjoining rammars #REFR.", "context": "A secondary motivation is to test the extent o which a non-trivial language processing task can be carried out without complex semantic representations. In Section 2 we present reversible mono-lingual models consisting of collections of simple automata associated with the heads of phrases. These head automata re applied by an algorithm with admissible incremental pruning based on semantic association costs, providing apractical solution to the problem of combinatoric disambiguation #OTHEREFR.[Citation]For translation, we use a model for mapping dependency graphs written by the source language head automata. This model is coded entirely as a bilingual exicon, with associated cost parameters. The transfer algorithm described in Section 4 searches for the lowest cost 'tiling' of the target dependency graph with entries from the bilingual lexicon."}
{"citing_paper_id": "W97-0313", "cited_paper_id": "C92-2070", "citing_paper_abstract": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon.", "cited_paper_abstract": "This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Roget is Thesaurus categories. Roget is categories serve as approximations of conceptual classes. The categories li ted for a word in Roger is index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful evel of sense disambiguatiou. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework. Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unresUicted monolingual text without human intervention. Applied to the 10 million word Grolier is Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature.", "citation": "Note that our context window is much narrower than those used by other researchers #REFR.", "context": ". The context windows do not cut across sentence boundaries.[Citation]We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category. Given the context windows for a category, we compute a category score for each word, which is essentially the conditional probability that the word appears in a category context. The category score of a word W for category C is defined as: ?corefW ?7~ - /reg. o/ w in O is context windows v / freq. o\\] W in corpus . ."}
{"citing_paper_id": "C10-2094", "cited_paper_id": "C92-2082", "citing_paper_abstract": "This paper investigates the new problem of automatic sense induction for instance names using automatically extracted attribute sets. Several clustering strategies and data sources are described and evaluated. We also discuss the drawbacks of the evaluation metrics commonly used in similar clustering tasks. The results show improvements in most metrics with respect to the baselines, especially for polysemous instances.", "cited_paper_abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.", "citation": "The procedure uses a collection of Web documents and applies some IsA extraction patterns selected from #REFR.", "context": "To ensure that the user did not change intent during the session, we also require the queries from which we extract phrases to contain the instance of interest. The pseudocode of the procedure is shown in Figure 1. Class labels: As described in #OTHEREFR, we collect for each instance (e.g., turkey), a ranked list of class labels (e.g., country, location, poultry, food).[Citation]Using the (instance, ranked-attributes) and the (instance, ranked-class labels) lists, it is possible to aggregate the two datasets to obtain, for each attribute, the class labels that are most strongly associated to it (Figure 2)."}
{"citing_paper_id": "D11-1076", "cited_paper_id": "C92-2082", "citing_paper_abstract": "This paper proposes a semi-supervised relation acquisition method that does not rely on extraction patterns (e.g. ?X causes Y? for causal relations) but instead learns a combination of indirect evidence for the target relation ? semantic word classes and partial patterns. This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large JapaneseWeb corpus? in extreme cases, patterns that occur only once in the entire corpus. Such patterns are beyond the reach of current pattern based methods. We show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance.", "cited_paper_abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.", "citation": "Pattern based relation acquisition methods rely on lexico-syntactic patterns #REFR for extracting relation instances.", "context": "[Citation]These are templates of natural language expressions such as ?X causes Y ? that signal an instance of some semantic relation (i.e., causality). Pattern based methods #OTHEREFR learn many . This work was done when all authors were at the National Institute of Information and Communications Technology. such patterns to extract new instances (word pairs) from the corpus."}
{"citing_paper_id": "P07-1112", "cited_paper_id": "C92-2082", "citing_paper_abstract": "This paper presents an approach for the automatic acquisition of qualia structures for nouns from the Web and thus opens the possibility to explore the impact of qualia structures for natural language processing at a larger scale. The approach builds on earlier work based on the idea of matching specific lexico-syntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. In our approach, the qualia elements are actually ranked for each qualia role with respect to some measure. The specific contribution of the paper lies in the extensive analysis and quantitative comparison of different measures for ranking the qualia elements. Further, for the first time, we present a quantitative evaluation of such an approach for learning qualia structures with respect to a handcrafted gold standard.", "cited_paper_abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.", "citation": "Instead of matching Hearst-style patterns #REFR in a large text collection, some researchers have recently turned to the Web to match these patterns such as in #OTHEREFR.", "context": "[Citation]Our approach goes further in that it not only learns typing, superconcept or instance-of relations, but also Constitutive, Telic and Agentive relations. Figure 2: Average ratings for each qualia role There also exist approaches specifically aiming at learning qualia elements from corpora based on machine learning techniques. Claveau et al #OTHEREFR for example use Inductive Logic Programming to learn if a given verb is a qualia element or not."}
{"citing_paper_id": "P12-1087", "cited_paper_id": "C92-2082", "citing_paper_abstract": "Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.", "cited_paper_abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.", "citation": "The idea of using entity-level structured data (e.g., facts in a database) to generate mention-level training data (e.g., in English text) is a classic one: researchers have used variants of this idea to extract entities of a certain type from webpages #REFR.", "context": "[Citation]More closely related to relation extraction is the work of Lin and Patel #OTHEREFR that uses dependency paths to find answers that express the same relation as in a question. Since Mintz et al #OTHEREFR coined the name ?distant supervision,? there has been growing interest in this technique. For example, distant supervision has been used for the TAC-KBP slot-filling tasks #OTHEREFRb)."}
{"citing_paper_id": "W03-0415", "cited_paper_id": "C92-2082", "citing_paper_abstract": "In this paper we demonstrate methods of improving both the recall and the precision of automatic methods for extraction of hyponymy (IS A) relations from free text. By applying latent semantic analysis (LSA) to filter extracted hyponymy relations we reduce the rate of error of our initial pattern-based hyponymy extraction by 30%, achieving precision of 58%. Applying a graph-based model of noun-noun similarity learned automatically from coordination patterns to previously extracted correct hyponymy relations, we achieve roughly a fivefold increase in the number of correct hyponymy relations extracted.", "cited_paper_abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.", "citation": "The first major attempt to extract hyponyms from text was that of #REFR, described in more detail in #OTHEREFR, who extracted relationships from the text of Grolier?s Encyclopedia.", "context": "[Citation]The method is illustrated by the following example. The sentence excerpt Even then, we would trail behind other European Community members, such as Germany, France and Italy. . . (BNC)2 indicates that Germany, France, and Italy are all European Community members. More generally, phrases of the form x such as y1 (y2, . . . , and/or yn) frequently indicate that the yi are all hyponyms of the hypernym x."}
{"citing_paper_id": "W04-1807", "cited_paper_id": "C92-2082", "citing_paper_abstract": "Terminology structuring aims to elicit semantic relations between the terms of a domain. We propose here to exploit definitions found in corpora to obtain such semantic relations. Definition typologies show that definitions can be introduced by different semantic relations, some of these relations being likely to structure terminologies. Our aim is therefore to mine ?defining expressions? in domainspecific corpora, and to detect the semantic relations they involve between their main terms. We use lexico-syntactic markers and patterns to detect at the same time both a definition and its main semantic relation. 46 markers and 74 patterns have been designed and tuned on a first corpus in the field of anthropology. We report on their evaluation on a second corpus in the field of dietetics, where they obtained 4% to 36% recall and from 61 to 66% precision, and discuss the relative accuracy of different subclasses of markers for this task.", "cited_paper_abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.", "citation": "Pearson #OTHEREFR have followed the methodology described by #REFR, up to now mainly applied to discover hyponymous terms.", "context": "They have used respectively ?contextual exploration?, lexicosyntactic patterns and linguistic analysis and rules. The former one extracts defining statements on the basis of the match of linguistic clues, when they are relayed in the sentence by some linguistic rules. These rules are developped by the author, withing the schema defined in the ?contextual exploration? methodology #OTHEREFR.[Citation]It consists in describing the lexico-syntactic context of an occurrence of a pair of terms known to share a semantic relation. Modelling the context in which they occur provides a ?pattern? to apply to the corpus, in order to extract other pairs of terms connected by the same relation. Pearson and Rebeyrolle have modelled lexico-syntactic contexts around lexical clues interpreted as ?definition markers?."}
{"citing_paper_id": "W05-1006", "cited_paper_id": "C92-2082", "citing_paper_abstract": "This paper describes a technique for extracting idioms from text. The technique works by finding patterns such as ?thrills and spills?, whose reversals (such as ?spills and thrills?) are never encountered. This method collects not only idioms, but also many phrases that exhibit a strong tendency to occur in one particular order, due apparently to underlying semantic issues. These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects.", "cited_paper_abstract": "We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur iYequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest hat other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built hesaurus. Extensions and applications to areas uch as information retrieval are suggested.", "citation": "Lexicosyntactic patterns were pioneered by Marti #REFR in the early 1990?s, to enable the addition of new information to lexical resources such as WordNet #OTHEREFR.", "context": "This section describes previous work in extracting information from text, and inferring semantic or idiomatic properties of words from the information so derived. The main technique used in this paper to extract groups of words that are semantically or idiomatically related is a form of lexicosyntactic pattern recognition.[Citation]The main insight of this sort of work is that certain regular patterns in word-usage can reflect underlying semantic relationships. For example, the phrase ?France, Germany, Italy, and other European countries? suggests that France, Germany and Italy are part of the class of European countries. Such hierarchical examples are quite sparse, and greater coverage was later attained by Riloff and Shepherd #OTHEREFR in extracting relations not of hierarchy but of similarity, by finding conjunctions or co-ordinations such as ?cloves, cinammon, and nutmeg? and ?cars and trucks.."}
{"citing_paper_id": "N06-1045", "cited_paper_id": "C92-3126", "citing_paper_abstract": "Ranked lists of output trees from syntactic statistical NLP applications frequently contain multiple repeated entries. This redundancy leads to misrepresentation of tree weight and reduced information for debugging and tuning purposes. It is chiefly due to nondeterminism in the weighted automata that produce the results. We introduce an algorithm that determinizes such automata while preserving proper weights, returning the sum of the weight of all multiply derived trees. We also demonstrate our algorithm?s effectiveness on two large-scale tasks.", "cited_paper_abstract": "1)ata Oriented Parsing (IX)P) is a model where no abstract rules, but language xt~riences in the ti3ru~ of all ,'malyzed COlpUS, constitute the basis for langnage processing. Analyzing a new input means that the system attempts to find tile most probable way to reconstruct the input out of frugments that alr\"c~y exist ill the corpus. Disambiguation occurs as a side-effect. DOP can be implemented by using colivelllional parsing strategies. In~oducfion This paper tommlizes the model for natural Imlgnage introduced m \\[Sclm 199o\\]. Since that article is written in Dutch, we will translate Some parts of it more or less literally in this introduction. According to Scba, the current radition of language processing systems is based on linguistically motivated competence models of natural Imlguages. \"llte problems that these systems lull iato, suggest file necessity of a more perfommnce oriented model of language processing, that takes into account the statistical properties of real language use. qllerefore Scha proposes a system ritat makes use of an annotated corpus. AnMyzing a new input means that the system attempts to find the most probable way to reconstruct the input out of fragments that already exist in the corpus. The problems with competence grammars that are mentioned in Scha is aiticle, include the explosion of ambiguities, the fact tilat Itunmn judgemeats on grammaticality are not stable, that competence granunars do not account for language h~alge, alld that no existing rule-based grammar gives a descriptively 'adequate characterization of an actual language. According to Scha, tile deveh,pment of a fornml gnatunar fur natural latlguage gets more difficult ,as tire grammar gets larger. When the number of phenotnena one has already takea into account gets larger, the number of iareractions that must be considered when ,me tries to introduce all account of a new pllenomenon grows accordingly. As to tile problem of ,'mtbiguity, it has turned out that as soon as a formal gratmnar clmracterizes a non-trivial part of a natural anguage, almost every input sentence of reasonable length gets ml re\\]manageably large number of different structural analyses (and * The author wishes to thank his colleagues at the Department of Computational Linguistics of the Ilaiversity of Amsterdam for many fruitful discussions, and, in particular, Remko Scha, Martin van den Berg, Kwee Tjoe l,iong and Frodenk Somsen for valuable comments on earlier w~'rsions of this paper. semantical interpretations). I \"lids is problenmtic since most of these interpretations ~re not perceived as lVossible by a hunmn language user, while there are no systematic reasons 111 exclude tileln on syutactic or sematltic grounds. Often it is just a ntatter of relative implausibility: tile only reason why a certain iarerpmtarion of a sentence is not perceived, is that aanther interprctatilm is much more plausible. Competence and Performance 'tale lhnriations of the current language procossing systerus are not suprising: riley are the direct consequence of rile fact that these systems implement Chart\\]sky is notion of a coutpetence grmnmar. The formal grilnuuars that constitute the subject-nmtter of theoretieal linguistics, aim at characterizing the clnnpetencc of tile langnage user. But the preferences language users have m the case of ambiguous entences, are paradigm instances of perfonatmce phenomena. In order to build effective lauguage processing systems we nmst intplement performanec-grammars, rather than competence gratumars, qlaese performance granmuus houM not only contain information on the structural possibilities of file general I~mgnage system, but also on details of actual language use in a language conmmnity, and of tile language experiences of an individual, which cause this individual to have certain expectations on what kinds of uUerances are going to occur, and what slractures and interpretations these utterances are going to have. Therc is all alternative linguistic tradition tluat has always focused on the concrete details of actual language use: file statistical tradition. In this approach, syntactic structure is usually ignored; only isuperficial' stalistical properties of a large coqms are described: file probability that a certain word is followed by a certain other word, the probability that a certain sequence of two words is followed by a ce~ml word, etc. (Markovcludns, see e.g. \\[Bahl 1983\\]). This approach bus perforumd succesfully ill certain practical tasks, such ,as selecting the most probable sentence from the outputs of a speech recognition coruptment. It will be clear that this approach is not suitable for mmly other tasks, because no uotion of syntactic structme is used. Aud there are statistical dependencies within the sentences of a corpus, that cam extend over all arbitrarily long sequence of words; this is ignored by file Markov-approach. The challenge is now to develop a theory of language processlag that does justice to tile statistieM ,as well as to tile structural aspects of langange.", "citation": "It occurs because many systems, such as the ones proposed by #REFR, #OTHEREFR represent their result space in terms of weighted partial results of various sizes that may be assembled in multiple ways.", "context": "Figure 1 shows the best 10 English translation parse trees obtained from a syntax-based translation system based on #OTHEREFR. Notice that the same tree occurs multiple times in this list. This repetition is quite characteristic of the output of ranked lists.[Citation]There is in general more than one way to assemble the partial results to derive the same complete result. Thus, the -best list of results is really an -best list of derivations. When list-based tasks, such as the ones mentioned above, take as input the top results for some constant , the effect of repetition on these tasks is deleterious."}
{"citing_paper_id": "P97-1004", "cited_paper_id": "C92-4177", "citing_paper_abstract": "A system for the automatic production of controlled index terms is presented using linguistically-motivated techniques. This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unificationbased shallow-level parser using transformational rules over syntactic patterns. The contribution of this research is the successful combination of parsing over a seed term list coupled with derivational morphology to achieve greater coverage of multi-word terms for indexing and retrieval. Final results are evaluated for precision and recall, and implications for indexing and retrieval are discussed.", "cited_paper_abstract": "L'acqnisition automatique de connaissance lexicale h partir de larges corpus s'est essentiellement oceup& des phfinom~nes de co-occurrence, aux dfipens des traits lexicaux inh~rents. Nous prfisentons ici une m&hodologie qui permet d'obtenir l'information sfimantique sur l'aspect du verbe en analysa~t automatiquement un corpus et en appliquant des tests linguistiques h l'aide d'une stifle d'outils d'anrdyse structura\\]e. Lorsque ces deux t~:hes sont accomplies, nous proposons une rfipresentation de l'aspect du verbe qni associe une valeur de mesure pour les difffirents types d'fiv~nements. Les mesures zefl~tent l'usage typique du verbe, et par consfiquent une mesure de rfisistance ou de non-r~sistance h la coercion dans le eontexte de la phrase. Les rfisultats que nous rapportons ici ont fitfi obtenus de deux mani~res: en extrayant l'information n&essrSre h partir du corpus &iquetfi de Francis and Ku~era (1982), et en fais~nt tourner un analyseur syntaxlque (MeCord 1980, 1990) sur le corpus du Reader is Digest afin d'extraire une information plus pr&ise sur l'usage du verbe dans le texte. Notre travail iUustre deux aspects:", "citation": "NLP techniques have been applied to extraction of information from corpora for tasks such as free indexing #OTHEREFR, and event structure of verbs #REFR.", "context": "[Citation]Although useful, these approaches suffer from two weaknesses which we address. First is the issue of filtering term lists; this has been dealt with by constraints on processing and by post-processing overgenerated lists. Second is the problem of difficulties in identifying related terms across parts of speech."}
{"citing_paper_id": "W98-0702", "cited_paper_id": "C92-4177", "citing_paper_abstract": "In this paper, I demonstrate that verbs can be disambiguated according to aspect by rules that exam? hue the WordNet category of the direct object. First, when evaluated over a corpus of medical reports, I show that WordNet categories correlate with aspectual class. Then, I develop a rule for distinguishing between stative and event occurrences of have by the WordNet category of the direct object. This rule, which is motivated by both linguistic and statistical analysis, is evaluated over an unrestricted set of nouns. I also show that WordNet categories improve a system that performs aspectual classification with linguistically-based numerical indicators.", "cited_paper_abstract": "L'acqnisition automatique de connaissance lexicale h partir de larges corpus s'est essentiellement oceup& des phfinom~nes de co-occurrence, aux dfipens des traits lexicaux inh~rents. Nous prfisentons ici une m&hodologie qui permet d'obtenir l'information sfimantique sur l'aspect du verbe en analysa~t automatiquement un corpus et en appliquant des tests linguistiques h l'aide d'une stifle d'outils d'anrdyse structura\\]e. Lorsque ces deux t~:hes sont accomplies, nous proposons une rfipresentation de l'aspect du verbe qni associe une valeur de mesure pour les difffirents types d'fiv~nements. Les mesures zefl~tent l'usage typique du verbe, et par consfiquent une mesure de rfisistance ou de non-r~sistance h la coercion dans le eontexte de la phrase. Les rfisultats que nous rapportons ici ont fitfi obtenus de deux mani~res: en extrayant l'information n&essrSre h partir du corpus &iquetfi de Francis and Ku~era (1982), et en fais~nt tourner un analyseur syntaxlque (MeCord 1980, 1990) sur le corpus du Reader is Digest afin d'extraire une information plus pr&ise sur l'usage du verbe dans le texte. Notre travail iUustre deux aspects:", "citation": "For example, a verb that appears more frequently in the progressive is more likely to describe an event than a state #REFR.", "context": "Therefore, it is necessary to produce a specialized lexicon for each domain. One statistical approach is to measure linguistic indicators over a corpus #OTHEREFR. These indicators measure how frequently each verb appears with markers uch as those in Table 1.[Citation]However, this approach attempts to classif T verbs independent of their context. Incorporating additional constituents of a clause could alleviate this problem. For example, indicators could be measured over verb-object pairs."}
{"citing_paper_id": "P99-1036", "cited_paper_id": "C94-1032", "citing_paper_abstract": "We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word. The point is quite simple: different character sets should be treated ifferently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented.", "cited_paper_abstract": "We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient wo-pa~qs N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. q'he proposed Japanese morphological nalyzer achieved 95. l% recall and 94.6% precision for open text when it was trained and tested on the ATI'. Corpus.", "citation": "In Japanese, around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedures #REFR.", "context": "[Citation]About the same accuracy is reported in Chinese by statistical methods #OTHEREFR. But there has been relatively little improvement in recent years because most of the remaining errors are due to unknown words. There are two approaches to solve this problem: to increase the coverage of the dictionary #OTHEREFR."}
{"citing_paper_id": "P12-1044", "cited_paper_id": "C94-1042", "citing_paper_abstract": "We present a novel approach for building verb subcategorization lexicons using a simple graphical model. In contrast to previous methods, we show how the model can be trained without parsed input or a predefined subcategorization frame inventory. Our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. This quantitative evaluation is complemented by a qualitative discussion of verbs and their frames. We discuss the advantages of graphical models for this task, in particular the ease of integrating semantic information about verbs and arguments in a principled fashion. We conclude with future work to augment the approach.", "cited_paper_abstract": "We des((tile tile design of Comlex Syntax, a co,nputational lexicon providing detailed syntactic iuformation ff)r approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.", "citation": "Large, manually-constructed SCF lexicons mostly target general language #OTHEREFR; #REFR.", "context": "A comprehensive lexicon would also include semantic information about selectional preferences (or restrictions) on argument heads of verbs, diathesis alternations (i.e. semantically-motivated alternations between pairs of SCFs) and a mapping from surface frames to the underlying predicate-argument structure. Information about verb subcategorization is useful for tasks like information extraction #OTHEREFR. In general, tasks that depend on predicate-argument structure can benefit from a high-quality SCF lexicon #OTHEREFR.[Citation]However, in many domains verbs exhibit different syntactic behavior #OTHEREFR. For example, the verb ?develop? has specific usages in newswire, biomedicine and engineering that dramatically change its probability distribution over SCFs. In a few domains like biomedicine, the need for focused SCF lexicons has led to manually-built resources #OTHEREFR."}
{"citing_paper_id": "P99-1051", "cited_paper_id": "C94-1042", "citing_paper_abstract": "This paper examines the extent to which verb diathesis alternations are empirically attested in corpus data. We automatically acquire alternating verbs from large balanced corpora by using partialparsing methods and taxonomic information, and discuss how corpus data can be used to quantify linguistic generalizations. We estimate the productivity of an alternation and the typicality of its members using type and token frequencies.", "cited_paper_abstract": "We des((tile tile design of Comlex Syntax, a co,nputational lexicon providing detailed syntactic iuformation ff)r approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.", "citation": "The threshold values varied from frame to flame but not from verb to verb and were determined by taking into account for each frame its overall frame frequency which was estimated from the COMLEX subcategorization dictionary (6,000 verbs) #REFR.", "context": "Erroneous frames can be the result of tagging errors, parsing mistakes, or errors introduced by the heuristics and procedures we used to guess syntactic structure. We discarded verbs for which we had very little evidence (frame frequency = 1) and applied a relative frequency cutoff: the verb is acquired frame frequency was compared against its overall frequency in the BNC. Verbs whose relative frame frequency was lower than an empirically established threshold were discarded.[Citation]This meant hat the threshold was higher for less frequent frames (e.g., the double object frame for which only 79 verbs are listed in COMLEX). We also experimented with a method suggested by Brent #OTHEREFR which applies the binomial test on frame frequency data. Both methods yielded comparable r sults."}
{"citing_paper_id": "W06-2605", "cited_paper_id": "C94-1042", "citing_paper_abstract": "We report on our work to build a discourse parser (SemDP) that uses semantic features of sentences. We use an Inductive Logic Programming (ILP) System to exploit rich verb semantics of clauses to induce rules for discourse parsing. We demonstrate that ILP can be used to learn from highly structured natural language data and that the performance of a discourse parsing model that only uses semantic information is comparable to that of the state of the art syntactic discourse parsers.", "cited_paper_abstract": "We des((tile tile design of Comlex Syntax, a co,nputational lexicon providing detailed syntactic iuformation ff)r approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled.", "citation": "The lexicon is based on COMLEX #REFR.", "context": "Nouns that share the same bundle of basic types are grouped in the same Systematic Polysemous Class (SPC). The resulting 126 SPCs cover about 40,000 nouns. We modified and augmented LCFLEX?s existing lexicon to incorporate VerbNet and CoreLex.[Citation]Verb and noun entries in the lexicon contain a link to a semantic type defined in the ontology. VerbNet classes (including subclasses and frames) and CoreLex SPCs are realized as types in the ontology. The deep syntactic roles are mapped to the thematic roles, which are defined as variables in the ontology types."}
{"citing_paper_id": "N03-3004", "cited_paper_id": "C94-1049", "citing_paper_abstract": "This paper presents an unsupervised method for discriminating among the senses of a given target word based on the context in which it occurs. Instances of a word that occur in similar contexts are grouped together via McQuitty?s Similarity Analysis, an agglomerative clustering algorithm. The context in which a target word occurs is represented by surface lexical features such as unigrams, bigrams, and second order co-occurrences. This paper summarizes our approach, and describes the results of a preliminary evaluation we have carried out using data from the SENSEVAL-2 English lexical sample and the line corpus.", "cited_paper_abstract": "A comparison W~LS made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the interword distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors frorn the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English l)ictionary (60K head words + 1.6M definition words), llowever, other experimen-- tal results suggest hat distance vectors contain some different semantic information from co-occurrence vectors.", "citation": "This idea is based loosely on work by #REFR, who compare word co?occurrence vectors derived from large corpora of text with co?occurrence vectors based on the definitions or glosses of words in a machine readable dictionary.", "context": "While there are certainly applications for unlabeled sense clusters, having some indication of the sense of the cluster would bring discrimination and disambiguation closer together. We will treat glosses as found in a dictionary as vectors that we project into the same space that is populated by instances as we have already described. A cluster could be assigned the sense of the gloss whose vector it was most closely located to.[Citation]A co?occurrence vector indicates how often words are used with each other in a large corpora or in dictionary definitions. These vectors can be projected into a high dimensional space and used to measure the distance between concepts or words. Niwa and Nitta show that while the co?occurrence data from a dictionary has different characteristics that a co?occurrence vector derived from a corpus, both provide useful information about how to categorize a word based on its meaning."}
{"citing_paper_id": "P06-1112", "cited_paper_id": "C94-1079", "citing_paper_abstract": "In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction. Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question. Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure. The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training. Experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20% in MRR.", "cited_paper_abstract": "We present an efI\\]cient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It conrains a lexicon with over 90,000 entries, constructed automatically b applying a set of extraction and conversion rules to entries from machine readable dictionaries.", "citation": "We parse questions and candidate sentences with MiniPar #REFR, a fast and robust parser for grammatical dependency relations.", "context": "[Citation]Then, we extract relation paths from dependency trees. Dependency relation path is defined as a structure P =< N1, R,N2 > where, N1, N2 are two phrases and R is a relation sequence R =< r1, ..., ri > in which ri is one of the predefined dependency relations. Totally, there are 42 relations defined in MiniPar."}
{"citing_paper_id": "W96-0205", "cited_paper_id": "C94-1101", "citing_paper_abstract": "We present a novel new word extraction method from Japanese texts based on expected word frequencies. First, we compute expected word frequencies from Japanese texts using a robust stochastic N-best word segmenter. We then extract new words by filtering out erroneous word hypotheses whose expected word frequencies are lower than the predefined threshold. The method is derived from an approximation of the generalized version of the Forward-Backward algorithm. When the Japanese word segmenter is trained on a 4.7 million word segmented corpus and tested on 1000 sentences whose out-of-vocabulary ate is 2.1%, the accuracy of the new word extraction method is 43.7% recall and 52.3% precision.", "cited_paper_abstract": "In the process of establish in g the it, form ation theory, C. F,. Shannon prol)ose.d the Markov I)ro(:ess as a good model to characterize ~t natural la.nguage. The core or this ide.a is t;o cah:ula.te the \\[ are(lU('Iides of strings compose(l of 'n characters ('n-grams), but this statistical analysis of large text. (lata a.,id for a large n lilts l lever be(HI carried ()tit })eca./ise of the memory l imitat ion of (:omputer and the shortage of text data. Taking advantage of the recent powerful computers we developed a. new aJgorithm of n-grams of large text data for arbitr~try hu'ge 'n a,nd (:alculated successl'ully, within ,'ela, t iv(. ly short thlle~ n-grams of some Japa,nese text (la, t~t containing between two an(l thirty million chara,(:ters. From this exl)eriment it 1)ecame (:loa,r t\\]l&t the automatic extraction or detern,i,tation of words, (:oml)ound words and (;ol\\]ocations i possible by mutually comparing n-gram statistics for dill'etch t values of lt. category : topical pa,per~ quantitative linguisth:s, large text corpora, text t)rocesshlg", "citation": "For Japanese, #REFR proposed a method of computing an arbitrary length character N-gram, and showed that the character N-gram statistics obtained from a large corpus includes information useful for word extraction.", "context": "They combined Viterbi reestimation using the word unigram model with a post filter called the \"Two-Class Classifier\", which is a linear discrimination function to decide whether the string is actually a word or not based on features derived from the character N-gram in a large unsegmented corpus. The system is performance is compared with a word list derived from two online Chinese dictionaries #OTHEREFR words). Tile reported recall and precision values were 56.88% and 77.37% for two character words, and 6.12% and 85.97% for three character words, respectively.[Citation]However, they did not report any evaluation of their word extraction method. #OTHEREFR proposed a very naive probabilistic word segmentation method for Japanese, based on character type information and hiragana bigram frequencies. They claimed 98% word segmentation accuracy, while we clMrn 94.7%. However, their evaluation method is very optimistic, and completely different from ours."}
{"citing_paper_id": "W96-0305", "cited_paper_id": "C94-2125", "citing_paper_abstract": "This paper describes a heuristic algorithm capable of automatically assigning a label to each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a computational-semantic lexicon for treatment of lexical ambiguity. Including these labels in the MRD-based lexical database offers several positive ffects. The labels can be used as a coarser sense division so unnecessarily fine sense distinction can be avoided in word sense disambiguation (WSD).The algorithm is based primarily on simple word matching between an MRD definition sentence and word lists of an LLOCE topic. We also describe an implementation f the algorithm for labeling definition sentences in Longman Dictionary of Contemporary English (LDOCE). For this purpose the topics and sets of related words in Longman Lexicon of Contemporary English (LLOCE) are used in this work. Quantitative r sults for a 12-word test set are reported. Our discussion entails how the availability of these labels provides the means for treating such problems as: acquisition of a lexicon capable of providing broad coverage, systematic word sense shifts, lexical underspecification, and acquisition of zero-derivatives.", "cited_paper_abstract": "This paper describes an algorithm for automatically interpreting noun sequences in unrestricted text. This system uses broadcoverage semantic information which has been acquired automatically by analyzing the definitions ira an on-line dictionary. Previously, computational studies of noun sequences made use of hand-coded semantic information, and they applied the analysis rules sequentially. In contrast, the task of analyzing noun sequences in unrestricted text strongly favors an algorithm according to which the rules are applied in parallel and the best interpretation is determined by weights associated with rule applications.", "citation": "Moreover, those relations have been shown to be very effective knowledge sources for WSD #OTHEREFR and interpretation f noun sequences #REFR.", "context": "The scheme that lexicographers u ed in generating the definitions above is similar to the DEFINITION scheme described in McKeown #OTHEREFR. A DEFINITION scheme begins with a genus term (that is, conceptual parent or ancestor of the sense), followed by the so-called ifferentia that consists of words: semanficaUy related to the sense to provide specifics about he sense. Those relations between the sense and its defining words are reflected in semantic dusters that are termed categorical, functional, and situational clusters in McRoy #OTHEREFR.[Citation]For instance, land, earth, mass, slope, and sand are the genus terms that are categorically related to bank. On the other hand, words in the differentia such as river, lake.field, garden, l~end, road. race-track, and harbour are Situationally related to bank through the Location relation. Other keywords uch as rOOd, and race-tra?\\[~ are related functionally to bank through the PartOfrelation."}
{"citing_paper_id": "W97-1510", "cited_paper_id": "C94-2135", "citing_paper_abstract": "In this paper we describe EFLUF - an implementation of FLUF. The idea with this environment is to achieve a base for experimenting with unification grammars. In this environment we want to allow the user to affect as many features as possible of the formalism, thus being able to test and compare various constructions proposed for unification-based formalisms. The paper exemplifies the main features of EFLUF and shows how these can be used for defining a grammar. The most interesting features of EFLUF are the various possibilities to affect the behavior of the system. The user can define new constructions and how they unify, define new syntax for his constructions and use external unification modules. The paper also gives a discussion on how a system like EFLUF would work for a larger application and suggests ome additional features and restrictions that would be needed for this.", "cited_paper_abstract": "We argue that flexibility is an important property for unification-based formalisms. By flexibility we mean the ability for the user to modify and extend the formalism according to the needs of his problem. The paper discusses some properties necessary to achieve a flexible formalism and presents the FLUF formalism as a realization of these ideas.", "citation": "In this paper we will describe FLUF (FLexible Unification Formalism) #REFR, #OTHEREFR and its implementation EFLUF.", "context": "Even more recent is the work on GATE #OTHEREFR which allows the user to combine different modules in a simple way. GATE differs from the systems mentioned above since it is an environment hat enables to combine various kinds of modules into a system. This means that a particular submodule in a system built with GATE can be unification-based or of any other kind but GATE in itself does not make any prerequisites on the type of the module.[Citation]FLUF differs from other unification-based formalisms in that its aim is to provide a general environment for experimenting with unification-based formalisms. This means that the basic FLUF formalism does only cover very basic concepts used for unification, such as terms, inheritance and a possibility to define new constructions. The user can then tailor FLUF to his current needs by making definitions or importing external modules."}
{"citing_paper_id": "C00-2117", "cited_paper_id": "C94-2174", "citing_paper_abstract": "In this paper we present a method for detecting the text genre quickly and easily following an approach originally proposed in authorship attribution studies which uses as style markers the frequencies of occurrence of the most frequent words in a training corpus (Burrows, 1992). In contrast to this approach we use the frequencies of occurrence of the most frequent words of the entire written language. Using as testing ground a part of the Wall Street Journal corpus, we show that the most frequent words of the British National Corpus, representing the most frequent words of the written English language, are more reliable discriminators oftext genre in comparison to the most frequent words of the training corpus. Moreover, the fi'equencies of occurrence of the most common punctuation marks play an important role in terms of accurate text categorization aswell as when dealing with training data of limited size.", "cited_paper_abstract": "A siml)le method for (:~d, egorizing texts into pre-deturmincd text gem:e c;ttcgorics using tit(: st;tti.',t, icM sl.+utd+u would tcch nique of discriminatH, amdysis is demonstrated wil.h application to the Brown(:orpus. I)is(:rimina.ut analysis makes it possibh~ tl,qC it, la,rge l l l l l l lber of l).~Xl'a-Ill(:l,(:rs Lh;tL llHl,y 1)(! SI)(1 cific for a. certain corpus or inlormation stream, and combine I.henl into ~t small tmmber ol + functions, wiLh t.he pa.ram(:i(:rs weighted oil basis of how usehd they ;u:e for discritniml.t ing text genres. An a.ppli(:~tl.ion to inforuta.tiott retrieval is discussed. Text Types Thor(; are. different types of l;exL '\\['exl.s \"al)oui,\" l.he sa.me th ing m~ty be in differing geurcs, of difl'(~rem. I y I)eS, ;rod of v;trying quality. Texts vary along st'.ver;d param. el.ers, a.ll relc'wull, for l,he gcuera.l inlortlu~tiol~ rel, ri(wal problem of real.thing rea(lcr needs m.I texts. (liven this variat ion, in a text retrieval eonl.ext, the l)rol)lems arc (i) i (Mttifying ;cures, and (ii) choosing criteria t,o ch,s-- ter texts of the smnc gem:e, wit, h l)redictal>le l>recision aml rcca.ll. This should uot he eonfused with t, he issue of idenl.ifying topics, m,d choosiug criW+ria that. diserinlinatc on(: topic from auother. All.hough u(>t orthogonal to gem'(', del)endent; wu+iat, ion, the wu'iat, ioll i, hat, rela, l,es dirc(-t.ly to (:onW.uI; and topic is Moug or, her (litu<'.usions. Na.l,ura.lly, there is (;o-va.riancc.. 'I'exl.s al)oul. (:(+rl.aitl topics ula,y only occur iu (:(;rt;ailt g(!tll'(!s, alt(\\] {.exl.s ill eertaiu ge.nres may only t.rea.t c(q'l.ain topics; mosl. l.ol)- ics do, however, occur iu several ;cures, which is what inl;erests us here. Douglas I~il)et: has sl, udied l;exl, variat.ion along scv eral l )aranmtcrs, and found that t,cxt.s can I)(,, cousidcrcd to wvry along live ditnensious. In his st, udy, he clush'.rs \\[~ai.ures according t.o eowu'iauce, t.o find tmderlyiug di mens ions (198!)). We wish to liud a method for idenl.ifvin ; easily eomput.al)h; I)\\[tl:al,|et.cH is t.hat ra.l>idly classify previously IlllS(?(~ll texts in gell(':r~ql classes and along a smal l set smalh~r 1,tmn I~,il>er is \\[ivl'. of dimm,siot,s, s,,ch that l.hcy can bc cxplai,,(~d in i,,t,tit.iwdy siml)le terms to l.hc ,,so,\" of a.n informal.ion rel.riewd ~Hq)liea-- tion. ()m: a im is 1,o t;~ke ~ set of texts that. has b(:ei, select, ed I)y sotne sort of crude semant ic analysis uch as is typica.lly performexl I>y an iufornmtion rel, ri(!vM sysl, em and I)art.il.ion il, flu'lher I)y genre or (.cxl. t;yl)e , aud Experiment I I\",xperiment 2 l'~xperiment 3 . . . . . . . . . . . . . ( l\\]~'?w~Lc at e g' ?zies) .. \\[. Informa.tive 1. I)ress A. Press: report;tge B. Press: editoriaJ (L Press: reviews", "citation": "On the other hand, some computational models for detecting automatically the text genre have recently been available #REFR.", "context": "Apart from the propositional content &the text, stylistic aspects can also be used as classificatory means. Biber studied the stylistic differences between written and spoken language #OTHEREFR and presented a model for interpreting the functions of various linguistic features. Unfortunately, his model can not be easily realized using existing natural anguage processing tools.[Citation]Kessler gives an excellent summarization f the potential applications of a text genre detector. In particular, part-of-speech tagging, parsing accuracy and word-sense disambiguation could be considerably enhanced by taking genre into account since certain grammatical constructions or word senses are closely related to specific genres. Moreover, in information retrieval the search results could be sorted according to the genre as well."}
{"citing_paper_id": "P98-2163", "cited_paper_id": "C94-2192", "citing_paper_abstract": "This paper describes a method for recognizing coherence relations between clauses which are linked by te in Japanese - - a translational equivalent of English and. We consider that the coherence relations are categories each of which has a prototype structure as well as the relationships among them. By utilizing this organization of the relations, we can infer an appropriate relation from the semantic structures of the clauses between which that relation holds. We carried out an experiment and obtained the correct recognition ratio of 82% for the 280 sentences.", "cited_paper_abstract": "In R, hetorical Strue(;ure Theory (RST) the detinitions of some rela|,iotts are rather w~gue because they are given on a pragmal, ic 1)asis. 'Fhis pal)er presents an- <>\\[her way of seeing the. relations which leads to a more precise specification of the relations. 'l'\\]m re.la: \\[ions are associated with constrailg;S on the semantic relationships between tile proposit, iomd contents of two clauses, t,heir Modality and Tense/Asl>eet.", "citation": "However, the definitions are rather vague and they are often recognized to be underspecified #OTHEREFR; #REFR.", "context": "One of the basic requirements for understanding discourse is recognizing how each clause coheres with its predecessor. Our linguistic and pragmatic ompetence nables us to read in conceivable relations even when two clauses are copresent without any overt cues, i.e., in parataxis. There has been a variety of definitions for coherence relations #OTHEREFR for a survey).[Citation]This paper attempts to explicate how such coherence relations arise between segments of discourse. We focus on re-linkage in Japanese - - a translational equivalent of English and-linkage, since mere parataxis ranges over too widely to capture the underlying principles on the coherence relations. We consider that coherence relations are categories each of which has its prototypical instances and marginal ones."}
{"citing_paper_id": "P06-1006", "cited_paper_id": "C96-1021", "citing_paper_abstract": "Syntactic knowledge is important for pronoun resolution. Traditionally, the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically. In the paper, we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution. Specifically, we utilize the parse trees directly as a structured feature and apply kernel functions to this feature, as well as other normal features, to learn the resolution classifier. In this way, our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features. The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task.", "cited_paper_abstract": "We present an algorithm for anaphora resolutkm which is a modified and extended version of that developed by (Lappin and Leass,/994). In contrast to that work, our algorithm does not require in-depth, full, syn.. tactic parsing of text. Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from tile output of a part of speech tagge~; enriched only with annotations of grammatica\\] functkm of lexical items in the input text stream. Evaluation of the results of our in-tplementation demonstrates that accurate anaphora resolution can be realized within natural anguage processing fl'ameworks which do not--~,)r cannotemploy robust and rcqiable parsing components.", "citation": "These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules #OTHEREFR; #REFR, or using machine-learning methods #OTHEREFR.", "context": "For a practical pronoun resolution system, the syntactic knowledge usually comes from the parse trees of the text. The issue that arises is how to effectively incorporate the syntactic information embedded in the parse trees to help resolution. One common solution seen in previous work is to define a set of features that represent particular syntactic knowledge, such as the grammatical role of the antecedent candidates, the governing relations between the candidate and the pronoun, and so on.[Citation]However, such a solution has its limitation. The syntactic features have to be selected and defined manually, usually by linguistic intuition. Unfortunately, what kinds of syntactic information are effective for pronoun resolution still remains an open question in this research community."}
{"citing_paper_id": "A97-1010", "cited_paper_id": "C96-1039", "citing_paper_abstract": "Repair processing plays an important role in spoken language processing systems. This paper proposes a method for correcting Chinese repetition repairs and demonstrates the effects of repair processing in Chinese homophone disambiguation. The experimental results show that the precision rate of 93.87% and the recall rate of 90?65% can be achieved for the repair processing. At the same time, 50% of errors in the repairing segments can be reduced for Chinese homophone disambiguation.", "cited_paper_abstract": "Various strategies are proposed to identify and classify three types of proper nouns in Chinese texts. Clues from character, sentence and paragraph levels are employed to resolve Chinese personal names. Character, Syllable and Frequency Conditions are presented to treat transliterated personal names, To deal with organization ames, keywords, prefix, word association and parts-of-speech are applied. For fair evaluation, large scale test data are selected from six sections of a newspaper. The precision and the recall for these three types are (88.04%, 92.56%), (50.62%, 71.93%) and (61.79%, 54.50%), respectively. When the former two types are regarded as a category, the performance becomes (81.46%, 91.22%). Compared with other approaches, our approach as better performance and our classification is automatic.", "citation": "In other words, it is necessary to segment Chinese sentence before tagging and parsing #OTHEREFR; #REFR.", "context": "In other words, they do not determine which words are undesired. These approaches cannot be adopted to deal with Chinese speech repairs for the following reasons. First, a Chinese sentence is composed of a string of characters without any word boundaries.[Citation]Repairs make segmentation and text-first approach more difficult. Second, Chinese repairs may not always have an editing terms between a repaired segment and a repairing segment. In other words, editing terms do not have much effect in Chinese repair processing."}
{"citing_paper_id": "P98-1056", "cited_paper_id": "C96-1054", "citing_paper_abstract": "We present wo approaches for syntactic and semantic transfer based on LFG f-structures and compare the results with existing co-description and restriction operator based approaches, focusing on aspects of ambiguity preserving transfer, complex cases of syntactic structural mismatches as well as on modularity and reusability. The two transfer approaches are interfaced with an existing, implemented transfer component (Verbmobi1), by translating f-structures into a term language, and by interfacing fstructure representations with an existing semantic based transfer approach, respectively.", "cited_paper_abstract": "This article presents a new semanticbased transfer approach developed and applied within the Verbmobil Machine Translation project. We give an overview of the declarative transfer formalism together with its procedural realization. Our approach is discussed and compared with several other approaches from the MT literature. The results presented in this article have been implemented and integrated into the Verbmobil system.", "citation": "We first show how the underlying machinery of the semantic-based transfer approach developed in #REFRb) can be ported to syntactic f-structure representations.", "context": "Transfer on packed representations is considered in #OTHEREFR. In the present paper we consider alternative approaches to transfer on underspecifiedsyntactic or semanticrepresentations, focusing on issues of modularity, reusability and practicality, interfacing existing implemented approaches in a flexible way. At the same time, the proposals readdress the issue of what is an appropriate level of representation fortranslation, in view of the known problems engendered by structural mismatches and semantic ambiguity.[Citation]Second, we show how the underspecified semantic interpretation approach developed in Genabith and Crouch #OTHEREFR can be exploited to interface f-structure representations directly with the named semantic-based transfer approach. Third, we compare the two approaches with each other, and with co-description and restriction operator based approaches."}
{"citing_paper_id": "P08-1057", "cited_paper_id": "C96-1055", "citing_paper_abstract": "This paper presents an innovative, complex approach to semantic verb classification that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semantic classes is trained by a combination of the EM algorithm and the MDL principle, providing soft clusters with two dimensions (verb senses and subcategorisation frames with selectional preferences) as a result. A language-model-based evaluation shows that after 10 training iterations the verb class model results are above the baseline results.", "cited_paper_abstract": "This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic lassification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate hat a 15-fold improvement can be achieved in deriving semantic information from syntactic ues if we first divide the syntactic cues into distinct groupings that correlate with different word senses. Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources.", "citation": "Up to now, such classifications have been used in applications such as word sense disambiguation #REFR, machine translation #OTHEREFR.", "context": "In recent years, the computational linguistics community has developed an impressive number of semantic verb classifications, i.e., classifications that generalise over verbs according to their semantic properties. Intuitive examples of such classifications are the MOTION WITH A VEHICLE class, including verbs such as drive, fly, row, etc., or the BREAK A SOLID SURFACE WITH AN INSTRUMENT class, including verbs such as break, crush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language.[Citation]Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown #OTHEREFR used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verbs. Merlo and Stevenson #OTHEREFR presented an automatic classification of three types of English intransitive verbs, based on argument structure and heuristics to thematic relations."}
{"citing_paper_id": "P99-1051", "cited_paper_id": "C96-1055", "citing_paper_abstract": "This paper examines the extent to which verb diathesis alternations are empirically attested in corpus data. We automatically acquire alternating verbs from large balanced corpora by using partialparsing methods and taxonomic information, and discuss how corpus data can be used to quantify linguistic generalizations. We estimate the productivity of an alternation and the typicality of its members using type and token frequencies.", "cited_paper_abstract": "This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic lassification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate hat a 15-fold improvement can be achieved in deriving semantic information from syntactic ues if we first divide the syntactic cues into distinct groupings that correlate with different word senses. Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources.", "citation": "Levin is study on diathesis alternations has influenced recent work on word sense disambiguation #REFR, machine translation #OTHEREFR.", "context": "The benefactive alternation (cf. (2)) is structurally similar to the dative, the difference being that it involves the preposition for rather than to. Levin #OTHEREFR for a similar proposal). Thus one would expect verbs that undergo the same alternations to form a semantically coherent class.[Citation]The objective of this paper is to investigate the extent to which diathesis alternations are empirically attested in corpus data. Using the dative and benefactive alternations as a test case we attempt o determine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame preferences and (c) what the representative members of an alternation are. In section 2 we describe and evaluate the set of automatic methods we used to acquire verbs undergoing the dative and benefactive alternations."}
{"citing_paper_id": "D07-1101", "cited_paper_id": "C96-1058", "citing_paper_abstract": "We present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.", "cited_paper_abstract": "Alter presenting a novel O(n a) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical atfinity mode\\] where words struggle to modify each other, (b) a sense tagging model where words tluctuate randomly in their selectional preferences, and (e) a. generative model where the speaker fleshes ()tit each word is syntactic and concep{.ual structure without regard to the implications :for the hearer. W(! also give preliminary empirical results from evaluating the three models' p;Lrsing performance on annotated Wall Street Journal trMning text (derived fi'om the Penn Treebank). in these results, the generative model performs significantly better than the others, and does about equally well at assigning pa.rtof-speech tags.", "citation": "Specifically, these approaches considered sibling relations of the modifier token #REFR.", "context": "However, richer representations translate into higher complexity of the inference algorithms associated with the model. In dependency parsing, the basic first-order model is defined by a decomposition of a tree into headmodifier dependencies. Previous work extended this basic model to include second-order relations?i.e. dependencies that are adjacent to the main dependency of the factor.[Citation]In this paper we extend the parsing model with other types of second-order relations. In particular, we incorporate relations between the head and modifier tokens and the children of the modifier. One paradigmatic case where the relations we consider are relevant is PP-attachment."}
{"citing_paper_id": "D11-1137", "cited_paper_id": "C96-1058", "citing_paper_abstract": "We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner?s generative model. In our framework, we define two kinds of generative model for reranking. One is learned from training data offline and the other from a forest generated by a baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches.", "cited_paper_abstract": "Alter presenting a novel O(n a) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical atfinity mode\\] where words struggle to modify each other, (b) a sense tagging model where words tluctuate randomly in their selectional preferences, and (e) a. generative model where the speaker fleshes ()tit each word is syntactic and concep{.ual structure without regard to the implications :for the hearer. W(! also give preliminary empirical results from evaluating the three models' p;Lrsing performance on annotated Wall Street Journal trMning text (derived fi'om the Penn Treebank). in these results, the generative model performs significantly better than the others, and does about equally well at assigning pa.rtof-speech tags.", "citation": "To reduce the data sparseness problem, we use the back-off strategy proposed in #REFRa).", "context": "We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factor ? = 1.0. We also train a generative reranking model from the training data.[Citation]Parameters ? are trained using MERT #OTHEREFR and for each sentence in the development data, 300-best dependency trees are extracted from its forest. Our variational reranking does not need much time to train the model because the training is performed over not the training data #OTHEREFR sentences)4. After MERT was performed until the convergence, the variational reranking finally achieved a 94.5 accuracy score on development data."}
{"citing_paper_id": "W07-2220", "cited_paper_id": "C96-1058", "citing_paper_abstract": "The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, I summarize the main findings from the 2007 shared task and try to identify major challenges for the parsing community based on these findings.", "cited_paper_abstract": "Alter presenting a novel O(n a) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical atfinity mode\\] where words struggle to modify each other, (b) a sense tagging model where words tluctuate randomly in their selectional preferences, and (e) a. generative model where the speaker fleshes ()tit each word is syntactic and concep{.ual structure without regard to the implications :for the hearer. W(! also give preliminary empirical results from evaluating the three models' p;Lrsing performance on annotated Wall Street Journal trMning text (derived fi'om the Penn Treebank). in these results, the generative model performs significantly better than the others, and does about equally well at assigning pa.rtof-speech tags.", "citation": "The optimal parse can be found using a spanning tree algorithm #REFR. .", "context": "In total, test runs were submitted for twenty-three systems in the multilingual track, and ten systems in the domain adaptation track (six of which also participated in the multilingual track). The majority of these systems used models belonging to one of the two dominant approaches in data-driven dependency parsing in recent years #OTHEREFR: . In graph-based models, every possible dependency graph for a given input sentence is given a score that decomposes into scores for the arcs of the graph.[Citation]In transition-based models, dependency graphs are modeled by sequences of parsing actions (or transitions) for building them. The search for an optimal parse is often deterministic and guided by classifiers #OTHEREFR. The majority of graph-based parsers in the shared task were based on what McDonald and Pereira #OTHEREFR."}
{"citing_paper_id": "W13-2412", "cited_paper_id": "C96-1079", "citing_paper_abstract": "The task of Named Entity Recognition (NER) is to identify in text predefined units of information such as person names, organizations and locations. In this work, we address the problem of NER in Estonian using supervised learning approach. We explore common issues related to building a NER system such as the usage of language-agnostic and languagespecific features, the representation of named entity tags, the required corpus size and the need for linguistic tools. For system training and evaluation purposes, we create a gold standard NER corpus. On this corpus, our CRF-based system achieves an overall F1-score of 87%.", "cited_paper_abstract": "We have recently completed the sixth in a series of \"Message Understanding Conferences\" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different asks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.", "citation": "The concept of NER originated in the 1990s in the course of the Message Understanding Conferences #REFR, and since then there has been a steady increase in research boosted by evaluation programs such as CoNLL #OTHEREFR.", "context": "To train and evaluate our system, we have created a gold standard NER corpus of Estonian news stories, in which we manually annotated occurrences of locations, persons and organizations. Our system, based on Conditional Random Fields, achieves an overall cross-validation F1-score of 87%, which is compatible with results reported for similar languages. Related work.[Citation]The earliest works mainly involved using hand-crafted linguistic rules #OTHEREFR. Rule-based systems typically achieve high precision, but suffer low coverage, are laborious to build and and not easily portable to new text domains #OTHEREFR. The current dominant approach for addressing NER problem is supervised machine learning #OTHEREFR."}
{"citing_paper_id": "P97-1061", "cited_paper_id": "C96-1089", "citing_paper_abstract": "In this paper, we describe a method for automatically retrieving collocations from large text corpora. This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations. Through the method, various range of collocations, especially domain specific collocations, are retrieved. The method is practical because it uses plain texts without any information dependent on a language such as lexical knowledge and parts of speech.", "cited_paper_abstract": "This paper I)roposes ;t new tnethod for learning bilingual colloca, tions from sentence-aligned paralM corpora. Our method COml)ris( is two steps: (1) extracting llseftll word chunks (n-grmns) by word-level sorting and (2) constructing bilingua,l ('ollocations t)y combining the word-(;hunl(s a(-quired iu stag(' (1). We apply the method to a very ('hallenging text l)~tir: a stock market 1)ullet;in in Japanese and il;s abstract in En-- glish. I)om;tin sl)ecific collocations are well captured ewm if they were not conta.ined in the dictionaric is of economic tel?IllS.", "citation": "There has been a growing interest in corpus-based approaches which retrieve collocations from large corpora #OTHEREFR, #REFR.", "context": "The features of collocations are defined as follows: ? collocations are recurrent ? collocations consist of one or several lexical units ? order of units are rigid in a collocation. For language processing such as machine translation, a knowledge of domain specific collocations is indispensable because what collocations mean are different from their literal meaning and the usage and meaning of a collocation is totally dependent on each domain. In addition, new collocations are produced one after another and most of them are technical jargons.[Citation]Although these approaches achieved good results for the task considered, most of them aim to extract fixed collocations, mainly noun phrases, and require the information which is dependent on each language such as dictionaries and parts of speech. From a practical point of view, however, a more robust and flexible approach is desirable. We propose a method to retrieve interrupted and uninterrupted collocations by the frequencies of co-occurrences and word order constraints from a monolingual corpus."}
{"citing_paper_id": "P98-1110", "cited_paper_id": "C96-2098", "citing_paper_abstract": "A term-list is a list of content words that characterize a consistent ext or a concept. This paper presents a new method for translating a term-list by using a corpus in the target language. The method first retrieves alternative translations for each input word from a bilingual dictionary. It then determines the most 'coherent' combination of alternative translations, where the coherence of a set of words is defined as the proximity among multi-dimensional vectors produced from the words on the basis of co-occurrence statistics. The method was applied to term-lists extracted from newspaper articles and achieved 81% translation accuracy for ambiguous words (i.e., words with multiple translations).", "cited_paper_abstract": "A method for extracting lexical translations from non-aligned corpora is proposed to cope with the unavailability of large aligned corpus. The assumption that \"translations of two co-occurring words in a source language also co-occur in the target language\" is adopted and represented in the stochastic matrix formulation. The translation matrix provides the co-occurring information translated from the source into the target. This translated co-occurring information should resemble that of the original in the target when the ambiguity of the translational relation is resolved. An algorithm to obtain the best translation matrix is introduced. Some experiments were performed to evaluate the effectiveness of the ambiguity resolution and the refinement of the dictionary.", "citation": "#REFR also proposed a method for choosing translations that solely relies on co-occurrence statistics in the target language.", "context": "Another difference, which also relates to the data sparseness problem, is that their method uses \"row\" co-occurrence statistics, whereas ours uses statistics converted with SVD. The converted matrix has the advantage that it represents the co-occurrence r lationship between two words that share similar contexts but do not co-occur in the same text s. SVD conversion may, however, weaken co-occurrence rlations which actually exist in the corpus.[Citation]The main difference with our approach lies in the plausibility measure of a translation candidate. Instead of using a \"coherence score\", their method employs proximity, or inverse distance, between the two cooccurrence matrices: one from the corpus (in the target language) and the other from the translation candidate. The distance measure of two matrices given in the paper is the sum of the absolute distance of each corresponding element."}
{"citing_paper_id": "W96-0205", "cited_paper_id": "C96-2136", "citing_paper_abstract": "We present a novel new word extraction method from Japanese texts based on expected word frequencies. First, we compute expected word frequencies from Japanese texts using a robust stochastic N-best word segmenter. We then extract new words by filtering out erroneous word hypotheses whose expected word frequencies are lower than the predefined threshold. The method is derived from an approximation of the generalized version of the Forward-Backward algorithm. When the Japanese word segmenter is trained on a 4.7 million word segmented corpus and tested on 1000 sentences whose out-of-vocabulary ate is 2.1%, the accuracy of the new word extraction method is 43.7% recall and 52.3% precision.", "cited_paper_abstract": "We present a novel spelling correction method \\['or those languages that have no delimiter between words, such ~rs ,lap;mese, (.',hinese, ,~nd ThM. It consists of an al)proximate word matching method and an N-best word seg mental|on Mgorithm using a statistical la.nguage model. For OCR errors, the proposed word-based correction method outperf.ornrs the conventional charactm'- b`ased correction method. When the bmselme character ecognition accuracy is 90%, it achieves 96.0% character recognition accuracy and 96.3% word segmentation accuracy, while the cilaracter recognition accuracy of cilaracterb,ased correction is", "citation": "The application of the word segmenter is described elsewhere #REFR.", "context": "Our goal is to provide a method to automatically extract new words from Japanese texts. This nmthod should adapt the dictionary of the word segmenter to new domains and applications. It should also maintain the dictionary by collecting new words in the target domain.[Citation]The approach we take is as follows: First, we design a statistical language model that can assign a reasonable word probability to an arbitrary substring in the input sentence, whether or not it is truly a word. Second, we devised a method to obtain the expected word N-gram count in the target texts, using an N-best word segmentation algorithm #OTHEREFR. Finally, we extract new words by filtering out spurious word hypotheses whose expected word frequencies are lower than the threshold."}
{"citing_paper_id": "D14-1018", "cited_paper_id": "C96-2141", "citing_paper_abstract": "Distinct properties of translated text have been the subject of research in linguistics for many year (Baker, 1993). In recent years computational methods have been developed to empirically verify the linguistic theories about translated text (Baroni and Bernardini, 2006). While many characteristics of translated text are more apparent in comparison to the original text, most of the prior research has focused on monolingual features of translated and original text. The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level, rather than using monolingual statistics at the document level. We show that these bilingual features outperform the monolingual features used in prior work (Kurokawa et al., 2009) for the task of classifying translation direction.", "cited_paper_abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "citation": "The HMM alignment model #REFR trained on A character n-gram language model is used to detect the language of source and target side text and filter them out if they do not match their annotated language.", "context": "We used a language filter , deduplication filter and length ratio filter to clean the data. After filtering we were left with 1,890,603 English-French sentence pairs and 640,117 French-English sentence pairs. The Stanford POS tagger #OTHEREFR was used to tag the English and the French sides of the corpus.[Citation]Duplicate sentences pairs are filtered out. WMT data was used to word-align the Hansard corpus while replacing words with their corresponding POS tags. Due to differences in word breaking between the POS tagger tool and our word alignment tool there were some mismatches."}
{"citing_paper_id": "P06-1067", "cited_paper_id": "C96-2141", "citing_paper_abstract": "In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation. We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations. We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used. We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.", "cited_paper_abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "citation": "Any aligner such as #OTHEREFR or #REFR can be used to obtain word alignments.", "context": "Do we tend to translate words that precede it or succeed it. Which word position to translate next. Our distortion parameters are directly estimated from word alignments by simple counting over alignment links in the training data.[Citation]For the results reported in this paper word alignments were obtained using a maximum-posterior word aligner4 described in #OTHEREFR. We will illustrate the components of our model with a partial word alignment. Let us assume that our source sentence5 is (f10, f250, f300)6, and our target sentence is (e410, e20), and their word alignment is a = ((f10, e410), (f300, e20))."}
{"citing_paper_id": "P10-2025", "cited_paper_id": "C96-2141", "citing_paper_abstract": "We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.", "cited_paper_abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "citation": "Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner #REFR.", "context": "Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance.[Citation]One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language #OTHEREFR."}
{"citing_paper_id": "P12-1018", "cited_paper_id": "C96-2141", "citing_paper_abstract": "In this paper, we demonstrate that accurate machine translation is possible without the concept of ?words,? treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.", "cited_paper_abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "citation": "The most well-known and widely-used models for bitext alignment are for one-to-many alignment, including the IBM models #OTHEREFR and HMM alignment model #REFR.", "context": "[Citation]These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P (eI1|fJ1 ,aK1 ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v."}
{"citing_paper_id": "W05-0814", "cited_paper_id": "C96-2141", "citing_paper_abstract": "We discuss results on the shared task of Romanian-English word alignment. The baseline technique is that of symmetrizing two word alignments automatically generated using IBM Model 4. A simple vocabulary reduction technique results in an improvement in performance. We also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate.", "cited_paper_abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "citation": "The starting point is the final alignment generated using GIZA++?s implementation of IBM Model 1 and the Aachen HMM model #REFR.", "context": "We experimented with a statistical model for inducing a stemmer cross-lingually, but found that the best performance was obtained by simply lowercasing both the English and Romanian text and removing all but the first four characters of each word. We also tried a new model and a new training criterion based on alternating the maximization of likelihood and minimization of the alignment error rate. For these experiments, we have implemented an alignment package for IBM Model 4 using a hillclimbing search and Viterbi training as described in #OTHEREFR, and extended this to use new submodels.[Citation]Paper organization: Section 2 is on the baseline, Section 3 discusses vocabulary reduction, Section 4 introduces our new model and training method, Section 5 describes experiments, Section 6 concludes. We use the following notation: e refers to an English sentence composed of English words labeled ei. f refers to a Romanian sentence composed of Romanian words labeled fj . a is an alignment of e to f . We use the term ?Viterbi alignment? to denote the most probable alignment we can find, rather than the true Viterbi alignment."}
{"citing_paper_id": "W06-2402", "cited_paper_id": "C96-2141", "citing_paper_abstract": "This paper studies a strategy for identifying and using multi-word expressions in Statistical Machine Translation. The performance of the proposed strategy for various types of multi-word expressions (like nouns or verbs) is evaluated in terms of alignment quality as well as translation accuracy. Evaluations are performed by using real-life data, namely the European Parliament corpus. Results from translation tasks from English-to-Spanish and from Spanish-to-English are presented and discussed.", "cited_paper_abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "citation": "Despite the change from a word-based to a phrase-based translation approach, word to word approaches for inferring alignment models from bilingual data #REFR continue to be widely used.", "context": "Statistical machine translation #OTHEREFR. Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two issues: first, word-based translation models have been replaced by phrasebased translation models #OTHEREFR. Nevertheless, it is interesting to call the attention about one important fact.[Citation]On the other hand, from observing bilingual data sets, it becomes evident that in some cases it is just impossible to perform a word to word alignment between two phrases that are translations of each other. For example, certain combination of words might convey a meaning which is somehow independent from the words it contains. This is the case of bilingual pairs such as ?fire engine? and ?camio?n de bomberos?."}
{"citing_paper_id": "P97-1031", "cited_paper_id": "C96-2148", "citing_paper_abstract": "We present an algorithm that automatically learns context constraints using statistical decision trees. We then use the acquired constraints in a flexible POS tagger. The tagger is able to use information of any degree: n-grams, automatically learned context constraints, linguistically motivated manually written constraints, etc. The sources and kinds of constraints are unrestricted, and the language model can be easily extended, improving the results. The tagger has been tested and evaluated on the WSJ corpus.", "cited_paper_abstract": "Relaxation labelling is an optimization technique used in many fields to solve constraint satisfael,ion problems. The algorithm finds a combination of values for a set of variables such that satisfies -to the maximum possible degreea set of given constraints. This paper describes some experiments performed applying it to POS tagging, and the results obtained, it also ponders the possibility of applying it to Word Sense Disambiguation.", "citation": "We describe a POS tagger based on the work described in #REFR, that is able to use bi/tr igram information, automatically learned context constraints and linguistically motivated manually written constraints.", "context": "The acquisition methods range from supervised-inductivelearning-from-example a gorithms #OTHEREFR-C03-02 I When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised. Aha et al, 1991) to genetic algorithm strategies #OTHEREFR. We present in this paper a hybrid approach that puts together both trends in automatic approach and the linguistic approach.[Citation]The sources and kinds of constraints are unrestricted, and the language model can be easily extended. The structure of the tagger is presented in figure 1. Language Model ."}
{"citing_paper_id": "W10-2914", "cited_paper_id": "C96-2162", "citing_paper_abstract": "Sarcasm is a form of speech act in which the speakers convey their message in an implicit way. The inherently ambiguous nature of sarcasm sometimes makes it hard even for humans to decide whether an utterance is sarcastic or not. Recognition of sarcasm can benefit many sentiment analysis NLP applications, such as review summarization, dialogue systems and review ranking systems. In this paper we experiment with semisupervised sarcasm identification on two very different data sets: a collection of 5.9 million tweets collected from Twitter, and a collection of 66000 product reviews from Amazon. Using the Mechanical Turk we created a gold standard sample in which each sentence was tagged by 3 annotators, obtaining F-scores of 0.78 on the product reviews dataset and 0.83 on the Twitter dataset. We discuss the differences between the datasets and how the algorithm uses them (e.g., for the Amazon dataset the algorithm makes use of structured information). We also discuss the utility of Twitter #sarcasm hashtags for the task.", "cited_paper_abstract": "This paper presents a unified theory of verbal irony tbr developing a computational model of irony. The theory claims that an ironic utterance implicitly communicates the fact that its utterance situation is surrounded by ironic environment which has three properties, but hearers can assume an utterance to be ironic even when they recognize that it implicitly communicates only two of the three properties. Implicit communication of three properties is accomplished in such a way that an utterance alludes to the speaker is expectation, violates pragmatic principles, and implies the speaker is emotional attitude. This paper also describes a method for computationally formalizing ironic environment and its implicit communication using situation theory with action theory.", "citation": "#REFR introduces the implicit display theory, a cognitive computational framework that models the ironic environment.", "context": "Tsur et al #OTHEREFR propose a semi supervised framework for recognition of sarcasm. The proposed algorithm utilizes some features specific to (Amazon) product reviews. This paper continues this line, proposing SASI a robust algorithm that successfully captures sarcastic sentences in other, radically different, domains such as twitter.[Citation]The complex axiomatic system depends heavily on complex formalism representing world knowledge. While comprehensive, it is currently impractical to implement on a large scale or for an open domain. Mihalcea and Strapparava #OTHEREFR present a system that identifies humorous one-liners."}
{"citing_paper_id": "C10-1152", "cited_paper_id": "C96-2183", "citing_paper_abstract": "In this paper, we consider sentence simplification as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reordering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems.", "cited_paper_abstract": "Lottg alld eolni)licated seltteltces prov(: to b(: a. stumbling block for current systems relying on N\\[, input. These systenls stand to gaill frolil ntethods that syntacti<:aHy simplily su<:h sentences. '\\]b simplify a sen= tence, we nee<t an idea of tit(.\" structure of the sentence, to identify the <:omponents o be separated out. Obviously a parser couhl be used to obtain the complete structure of the sentence. \\]\\[owever, hill parsing is slow a+nd i)rone to fa.ilure, especially on <:omph!x sentences. In this l)aper, we consider two alternatives to fu\\]l parsing which could be use<l for simplification. The tirst al)l)roach uses a Finite State Grammar (FSG) to prodn<:e noun and verb groups while the second uses a Superta.gging model to i)roduce dependency linkages. We discuss the impact of these two input representations on the simplification pro(:ess.", "citation": "The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks #REFR.", "context": "This helps humans read texts more easily and faster. Reading assistance is thus an important application of sentence simplification, especially for people with reading disabilities #OTHEREFR. Not only human readers but also NLP applications can benefit from sentence simplification.[Citation]Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization #OTHEREFR, . This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No."}
{"citing_paper_id": "W96-0103", "cited_paper_id": "C96-2212", "citing_paper_abstract": "This paper describes a data-driven method for hierarchical clustering of words and clustering of multiword compounds. A large vocabulary of English words (70,000 words) is clustered bottom-up, with respect o corpora ranging in size from 5 million to 50 million words, using mutual information as an objective function. The resulting hierarchical clusters of words are then naturally transformed to a bit-string representation f (i.e. word bits for) all the words in the vocabulary. Evaluation of the word bits is carried out through the measurement of the error rate of the ATR Decision-Tree Part-Of-Speech Tagger. The same clustering technique is then applied to the classification of multiword compounds. In order to avoid the explosion of the number of compounds to be handled, compounds in a small subclass are bundled and treated as a single compound. Another merit of this approach is that we can avoid the data sparseness problem which is ubiquitous in corpus statistics. The quality of one of the obtained compound classes is examined and compared to a conventional approach.", "cited_paper_abstract": "This plq)er (lescril)es a (hit i~-(triven nlet, hod for hiera, rchicM chlstering of words ill whicii a, la, rge vo(:aJ)ul~ry of I,;ii. glis\\]'l words is (:histered botl;oln--uf) >with resl)e(:t 1,o (:orpor;~ ranghig in size fi'otn 5 to 50 nlillion wor(ts, using a greedy al gorithm that I;ries I,o nliniluize i~veri~ge lOS8 Of liCllltllal iriforuu:l,l, ion of a, djax:ent classes. The resulting hierar('.hi('al (:illS- tiers of woMs are then tumirMly 1,ransrorlned to a bit-string representld, ion of (i.e. word bits for) all the words ill the vocabulary, Introducing wor(l bits hito i.he ATI{ I)ecision-Tree DOS Tagger is shown to signific~mt,ly reduce l, he ti~gging error rld;e. PortM)ility of word t)il.s h:om Olle (tonlMn to i~Hotilel: iS ~tlSO diss(:ussed.", "citation": "Our word bits construction algorithm #REFR is a modification and an extension of the mutual information #OTHEREFR.", "context": "In this paper we adopt the merging approach and propose an improved method of constructing hierarchical clustering. An attempt is also made to combine the two types of clustering and some results will be shown. The combination is realized by the construction of clusters using the merging method followed by the reshuffling of words from class to class.[Citation]The reader is referred to #OTHEREFR for details of MI clustering, but we will first briefly summarize the MI clustering and then describe our hierarchical clustering algorithm."}
{"citing_paper_id": "Q13-1010", "cited_paper_id": "C96-2215", "citing_paper_abstract": "In this paper, we present the first incremental parser for Tree Substitution Grammar (TSG). A TSG allows arbitrarily large syntactic fragments to be combined into complete trees; we show how constraints (including lexicalization) can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. In addition to whole-sentence F-score, we also evaluate the partial trees that the parser constructs for sentence prefixes; partial trees play an important role in incremental interpretation, language modeling, and psycholinguistics. Unlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word.", "cited_paper_abstract": "This paper studies the computational complexity of disambiguation under probabilistic tree-grammars a in (Bod, 1992; Schabes and Waters, 1993). It presents a proof that the following problems are NP-hard: computing the Most Probable Parse from a sentence or from a word-graph, and computing the Most Probable Sentence (MPS) from a wordgraph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free Grammars (SCFGs).", "citation": "Unfortunately the number of possible derivations grows exponentially with the length of the sentence, and computing the exact MPP is NP-hard #REFR.", "context": "According to Equation (5), if we want to compute the MPP we need to retrieve all possible derivations of the current sentence, sum up the probabilities of those generating the same tree, and returning the tree with max marginal probability.[Citation]In our implementation, we approximate the MPP by performing this marginalization over the Viterbi-best derivations obtained from all stop states in the chart."}
{"citing_paper_id": "W10-3813", "cited_paper_id": "D07-1006", "citing_paper_abstract": "In this paper, we extend the HMM wordto-phrase alignment model with syntactic dependency constraints. The syntactic dependencies between multiple words in one language are introduced into the model in a bid to produce coherent alignments. Our experimental results on a variety of Chinese?English data show that our syntactically constrained model can lead to as much as a 3.24% relative improvement in BLEU score over current HMM word-to-phrase alignment models on a Phrase-Based Statistical Machine Translation system when the training data is small, and a comparable performance compared to IBM model 4 on a Hiero-style system with larger training data. An intrinsic alignment quality evaluation shows that our alignment model with dependency constraints leads to improvements in both precision (by 1.74% relative) and recall (by 1.75% relative) over the model without dependency information.", "cited_paper_abstract": "Word alignment is the problem of annotating parallel text with translational correspondence. Previous generative word alignment models have made structural assumptions such as the 1-to-1, 1-to-N, or phrase-based consecutive word assumptions, while previous discriminative models have either made such an assumption directly or used features derived from a generative model making one of these assumptions. We present a new generative alignment model which avoids these structural limitations, and show that it is effective when trained using both unsupervised and semi-supervised training methods.", "citation": "We also plan to compare our model with other alignment models, e.g. #REFRa), and test this approach on more data and on different language pairs and translation directions.", "context": "Secondly, the syntactic coherence model itself is very simple, in that it only covers the syntactic dependency between the first and last word in a phrase. Accordingly, we intend to extend this model to cover more sophisticated syntactic relations within the phrase. Furthermore, given that we can construct different MT systems using different word alignments, multiple system combination can be conducted to avail of the advantages of different systems.[Citation]Acknowledgements This research is supported by the Science Foundation Ireland #OTHEREFR as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University. Part of the work was carried out at Cambridge University Engineering Department with Dr. William Byrne."}
{"citing_paper_id": "P13-1141", "cited_paper_id": "D07-1007", "citing_paper_abstract": "Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.", "cited_paper_abstract": "We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese- English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task? and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. ?This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants", "citation": "We use the same set of features as in #REFR.", "context": "Towards this end, first, we pose the problem as a phrase sense disambiguation (PSD) problem over the known sense inventory. Given a source word in a context, we train a classifier to predict the most likely target translation. The ground truth labels (target translation for a given source word) for this classifier are generated from the phrase table of the old domain data.[Citation]Second, given a source word s, we use this classifier to compute the probability distribution of target translations (p(t|s)). Subsequently, we use this probability distribution to define new features for the SENSESPOTTING task. The idea is that, if a word is used in one of the known senses then its context must have been seen previously and hence we hope that the PSD classifier outputs a spiky distribution."}
{"citing_paper_id": "W11-2203", "cited_paper_id": "D07-1007", "citing_paper_abstract": "Cross-Lingual Lexical Substitution (CLLS) is the task that aims at providing for a target word in context, several alternative substitute words in another language. The proposed sets of translations may come from external resources or be extracted from textual data. In this paper, we apply for the first time an unsupervised cross-lingual WSD method to this task. The method exploits the results of a cross-lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity. We evaluate the impact of using clustering information for CLLS by applying the WSD method to the SemEval-2010 CLLS data set. Our system performs better on the ?out-of-ten? measure than the systems that participated in the SemEval task, and is ranked medium on the other measures. We analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting.", "cited_paper_abstract": "We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese- English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task? and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. ?This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants", "citation": "It has thus been widely adopted in works on multilingual WSD and WSD in MT, where senses are derived from parallel data #OTHEREFR; #REFR.", "context": "In this setting, the senses of words in one language are identified by their translations in another language, usually found in a parallel corpus #OTHEREFR. This empirical approach to sense induction offers a standard criterion for sense delimitation and, consequently, dissociates WSD from semantic theories and predefined semantic inventories. Moreover, by establishing semantic distinctions pertinent for translation between the implicated languages, it allows to tune sense induction to the needs of multilingual applications.[Citation]By linking WSD and its evaluation to translation, this hypothesis also offers a solution to the problem of non-conformity of monolingual WSD methods in this setting. Nevertheless, the assumption of biunivocal (?oneto-one?) correspondences between senses and translations is rather simplistic. One word sense may be translated by different synonymous words in another language, whose relatedness should be considered during sense induction."}
{"citing_paper_id": "W13-3522", "cited_paper_id": "D07-1007", "citing_paper_abstract": "The use of pivot languages and wordalignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases. However, inherent ambiguities in the pivot language(s) can lead to inadequate paraphrases. We propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language, i.e., the language to be paraphrased. Text in the input language is annotated with ?senses? in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment. This approach shows 62% relative improvement over previous work in generating paraphrases that are judged both more accurate and more fluent.", "cited_paper_abstract": "We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese- English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task? and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. ?This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants", "citation": "These translations can be easily captured given the availability of bilingual parallel data and robust automatic word-alignment techniques #REFR. ", "context": "Table 1: Size of the bilingual parallel corpora in millions of sentence pairs the pivot phrases that will lead to adequate paraphrases. In our approach a sense tag consists in a phrase in a foreign language, that is, a valid translation of the input phrase in a language of interest, here referred to as target language. Treating the target language vocabulary as a sense repository is a good strategy from both theoretical and practical perspectives: it has been shown that monolingual sense distinctions can be effectively captured by translations into second languages, especially as language family distance increases #OTHEREFR.[Citation]Figure 1 illustrates the proposed model to produce sense tagged paraphrases. We start the process at e1 and we need to make sure that the pivot phrases f . F align back to the input language, producing the paraphrase e2, and to the target language, producing the sense tag q."}
{"citing_paper_id": "E14-1021", "cited_paper_id": "D07-1008", "citing_paper_abstract": "Paraphrase evaluation is typically done either manually or through indirect, taskbased evaluation. We introduce an intrinsic evaluation PARADIGM which measures the goodness of paraphrase collections that are represented using synchronous grammars. We formulate two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus. The first measure calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in the corpus. The second measure enumerates paraphrase rules from the monolingual parallel corpus and calculates the overlap between this reference paraphrase collection and the paraphrase resource being evaluated. We demonstrate the use of these evaluation metrics on paraphrase collections derived from three different data types: multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We show that PARADIGM correlates with human judgments more strongly than BLEU on a task-based evaluation of paraphrase quality.", "cited_paper_abstract": "This paper presents a tree-to-tree transduction method for text rewriting. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model.", "citation": "To validate PARADIGM, we calculated its correlation with human judgments of paraphrase quality on the sentence compression text-to-text generation task, which has been used to evaluate paraphrase grammars in previous research #REFR.", "context": "[Citation]We created sentence compression systems for five of the paraphrase grammars described in Section 5.1. We followed the methodology outlined by Ganitkevitch et al. #OTHEREFR and did the following: . Each paraphrase grammar was augmented with an appropriate set of rule-level features that capture information pertinent to the task."}
{"citing_paper_id": "C10-2013", "cited_paper_id": "D07-1013", "citing_paper_abstract": "We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French. The architectures are based on PCFGs with latent variables, graph-based dependency parsing and transition-based dependency parsing, respectively. We also study the in?uence of three types of lexical information: lemmas, morphological features, and word clusters. The results show that all three systems achieve competitive performance, with a best labeled attachment score over 88%. All three parsers bene?t from the use of automatically derived lemmas, while morphological features seem to be less important. Word clusters have a positive effect primarily on the latent variable parser.", "cited_paper_abstract": "We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development.", "citation": "The two dependency parsers use radically different parsing approaches but have achieved very similar performance for a wide range of languages #REFR.", "context": "The evaluation showed that the Berkeley parser had signi?cantly better performance for French than the other parsers, whether measured using a parseval-style labeled bracketing F-score or a CoNLL-style unlabeled attachment score. Contrary to most of the other parsers in that study, the Berkeley parser has the advantage of a strict separation of parsing model and linguistic constraints: linguistic information is encoded in the treebank only, except for a language-dependent suf?x list used for handling unknown words. In this study, we compare the Berkeley parser to MSTParser and MaltParser, which have the same separation of parsing model and linguistic representation, but which are trained directly on labeled dependency trees.[Citation]We describe below the three architectures in more detail.2"}
{"citing_paper_id": "D08-1065", "cited_paper_id": "D07-1014", "citing_paper_abstract": "We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al, 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.", "cited_paper_abstract": "A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence. We exploit the Matrix Tree Theorem (Tutte, 1984) to derive an algorithm that efficiently sums the scores of all nonprojective trees in a sentence, permitting the definition of a conditional log-linear model over trees. While discriminative methods, such as those presented in McDonald et al (2005b), obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization, ?summing trees? permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training.", "citation": "The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition #OTHEREFR; #REFR.", "context": "In contrast, these systems are evaluated using metrics based on string-edit distance #OTHEREFR), or precision/recall relative to human annotations. Minimum Bayes-Risk #OTHEREFR aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion.[Citation]In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne #OTHEREFR gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU."}
{"citing_paper_id": "D11-1114", "cited_paper_id": "D07-1014", "citing_paper_abstract": "We describe a generative model for nonprojective dependency parsing based on a simplified version of a transition system that has recently appeared in the literature. We then develop a dynamic programming parsing algorithm for our model, and derive an insideoutside algorithm that can be used for unsupervised learning of non-projective dependency trees.", "cited_paper_abstract": "A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence. We exploit the Matrix Tree Theorem (Tutte, 1984) to derive an algorithm that efficiently sums the scores of all nonprojective trees in a sentence, permitting the definition of a conditional log-linear model over trees. While discriminative methods, such as those presented in McDonald et al (2005b), obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization, ?summing trees? permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training.", "citation": "Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph #OTHEREFR; #REFR.", "context": "Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order.[Citation]An alternative to this approach is to use transitionbased parsing #OTHEREFR, where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework #OTHEREFR."}
{"citing_paper_id": "D11-1005", "cited_paper_id": "D07-1022", "citing_paper_abstract": "We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing.", "cited_paper_abstract": "In morphologically rich languages, should morphological and syntactic disambiguation be treated sequentially or as a single problem. We describe several efficient, probabilisticallyinterpretable ways to apply joint inference to morphological and syntactic disambiguation using lattice parsing. Joint inference is shown to compare favorably to pipeline parsing methods across a variety of component models. State-of-the-art performance on Hebrew Treebank parsing is demonstrated using the new method. The benefits of joint inference are modest with the current component models, but appear to increase as components themselves improve.", "citation": "This idea was explored for joint inference by #REFR.", "context": "training data for the dependency parser. We consider two variants: the most probable assignment of tags to words (denoted ?Pipeline?), and the posterior distribution over tags for each word, represented as a weighted ?sausage? lattice (denoted ?Joint?).[Citation]"}
{"citing_paper_id": "D13-1033", "cited_paper_id": "D07-1022", "citing_paper_abstract": "Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.", "cited_paper_abstract": "In morphologically rich languages, should morphological and syntactic disambiguation be treated sequentially or as a single problem. We describe several efficient, probabilisticallyinterpretable ways to apply joint inference to morphological and syntactic disambiguation using lattice parsing. Joint inference is shown to compare favorably to pipeline parsing methods across a variety of component models. State-of-the-art performance on Hebrew Treebank parsing is demonstrated using the new method. The benefits of joint inference are modest with the current component models, but appear to increase as components themselves improve.", "citation": "For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax #REFR.", "context": "In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well.[Citation]"}
{"citing_paper_id": "N10-1083", "cited_paper_id": "D07-1031", "citing_paper_abstract": "We show how features can easily be added to standard generative models for unsupervised learning, without requiring complex new training methods. In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits. The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discriminative training of logistic regression models. We apply this technique to part-of-speech induction, grammar induction, word alignment, and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task. These feature-enhanced models each outperform their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.", "cited_paper_abstract": "This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers. We find that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed. This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought.", "citation": "It is substantially simpler than the non-parametric Bayesian models proposed by #REFR, which require sampling procedures to perform inference and achieve an F1 of 87 #OTHEREFR.", "context": "To our knowledge our system achieves the best performance to date on the Bernstein-Ratner corpus, with an F1 of 88.0.[Citation]Similar to our other results, the direct gradient approach outperforms EM for feature-enhanced models, and both approaches outperform the baseline, which achieves an F1 of 76.9."}
{"citing_paper_id": "W12-1909", "cited_paper_id": "D07-1031", "citing_paper_abstract": "This paper presents the results of the PASCAL Challenge on Grammar Induction, a competition in which competitors sought to predict part-of-speech and dependency syntax from text. Although many previous competitions have featured dependency grammars or partsof-speech, these were invariably framed as supervised learning and/or domain adaption. This is the first challenge to evaluate unsupervised induction systems, a sub-field of syntax which is rapidly becoming very popular. Our challenge made use of a 10 different treebanks annotated in a range of different linguistic formalisms and covering 9 languages. We provide an overview of the approaches taken by the participants, and evaluate their results on each dataset using a range of different evaluation metrics.", "cited_paper_abstract": "This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers. We find that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed. This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought.", "citation": "Another metric that was used is Variation of information #OTHEREFR, which is based the conditional entropy of between the two different clusterings #REFR.", "context": "This metric assigns each word cluster to its most common tag, and then measures the proportion of correctly tagged words. The second metric is the one-to-one mapping #OTHEREFR. Word clusters are assigned greedily to tags, and in the event of there being more word classes than tags, some word classes will be left unassigned.[Citation]Lastly, we use the V-measure #OTHEREFR, which is another entropy-based measure, but defined in terms of a F score to balance precision and recall terms (we use equal weighting of the two factors). Please see Christodoulopoulos et al #OTHEREFR for further details about these metrics.3 For these metrics, a higher score is better, with the exception of VI. For all these metrics, the induced tags are evaluated against the universal pos tags, as this means there are a consistent number of tags across the languages."}
{"citing_paper_id": "C08-1054", "cited_paper_id": "D07-1032", "citing_paper_abstract": "The use of similarities has been one of the main approaches to resolve the ambiguities of coordinate structures. In this paper, we present an alternative method for coordination disambiguation, which does not use similarities. Our hypothesis is that coordinate structures are supported by surrounding dependency relations, and that such dependency relations rather yield similarity between conjuncts, which humans feel. Based on this hypothesis, we built a Japanese fully-lexicalized generative parser that includes coordination disambiguation. Experimental results on web sentences indicated the effectiveness of our approach, and endorsed our hypothesis.", "cited_paper_abstract": "This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics. We integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames. This approach simultaneously addresses two tasks of coordination disambiguation: the detection of coordinate conjunctions and the scope disambiguation of coordinate structures. Experimental results on web sentences indicate the effectiveness of our approach.", "citation": "#REFR integrated this method into a generative parsing model.", "context": "Kurohashi and Nagao proposed a Japanese parsing method that included coordinate structure detection #OTHEREFR. Their method first detects coordinate structures in a sentence, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures. Their method correctly analyzed 97 out of 150 Japanese sentences.[Citation]Shimbo and Hara #OTHEREFR, using a discriminative learning model. A number of machine learning-based approaches to Japanese parsing have been developed. Among them, the best parsers are the SVM-based dependency analyzers #OTHEREFR."}
{"citing_paper_id": "E09-1094", "cited_paper_id": "D07-1042", "citing_paper_abstract": "This paper presents a new, exemplar-based model of thematic fit. In contrast to previous models, it does not approximate thematic fit as argument plausibility or ?fit with verb selectional preferences?, but directly as semantic role plausibility for a verb-argument pair, through similaritybased generalization from previously seen verb-argument pairs. This makes the model very robust for data sparsity. We argue that the model is easily extensible to a model of semantic role ambiguity resolution during online sentence comprehension. The model is evaluated on human semantic role plausibility judgments. Its predictions correlate significantly with the human judgments. It rivals two state-of-theart models of thematic fit and exceeds their performance on previously unseen or lowfrequency items.", "cited_paper_abstract": "In this paper, we consider the computational modelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics. By extending a recent model, we obtain a completely corpus-driven model for this task which achieves significant correlations with human judgements. It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone.", "citation": "The model of #REFR does not use semantically annotated resources, but approximates the agent and patient relations with the syntactic subject and object relations, respectively.", "context": "[Citation]The plausibility of a verb-role-argument triple (v, r, a) is found by calculating the weighted mean semantic similarity of the argument headword a to all headwords that have previously been seen together with the verb-role pair (v, r), as shown in Equation 2. The prediction is that high semantic similarity of a target headword a to seen headwords for a given (v, r) tuple corresponds to high thematic fit of the (v, r, a) tuple, while low similarity implies low thematic fit. Plausibilityv,r,a = ? a??Seenr(v) w(a?)? sim(a, a?) |Seenr(v)| (2) w(a?) is the weighting factor."}
{"citing_paper_id": "W09-0205", "cited_paper_id": "D07-1042", "citing_paper_abstract": "We introduce a way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring.", "cited_paper_abstract": "In this paper, we consider the computational modelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics. By extending a recent model, we obtain a completely corpus-driven model for this task which achieves significant correlations with human judgements. It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone.", "citation": "Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods #OTHEREFR; #REFR.", "context": "[Citation]This task is of particular interest to us as an example of a broader class of linguistic problems that involve productive constraints on composition. As has been stressed at least since Chomsky?s early work #OTHEREFR, no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. In the domain of selectional restrictions this is particularly obvious: we would not say that an algorithm learned the constraints on the possible objects/patients of eating simply by producing the list of all the attested objects of this verb in a very large corpus; the interesting issue is whether the algorithm can detect if an unseen object is or is not a plausible ?eatee?, like humans do without problems."}
{"citing_paper_id": "W09-0210", "cited_paper_id": "D07-1043", "citing_paper_abstract": "In this work, we apply Dirichlet Process Mixture Models (DPMMs) to a learning task in natural language processing (NLP): lexical-semantic verb clustering. We thoroughly evaluate a method of guiding DP- MMs towards a particular clustering solution using pairwise constraints. The quantitative and qualitative evaluation performed highlights the benefits of both standard and constrained DPMMs compared to previously used approaches. In addition, it sheds light on the use of evaluation measures and their practical application.", "cited_paper_abstract": "We present V-measure, an external entropybased cluster evaluation measure. V- measure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the ?problem of matching?, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.", "citation": "However, #REFR pointed out that F-measure assumes (the missing) mapping between cl and kj .", "context": "The standard approach to evaluate the quality of the clusters is to use an external gold standard in which the instances are partitioned into a set of classes C = {cl|l = 1, ..., |C|}. Given this, the goal is to find a partitioning of the instances K that is as close as possible to the gold standard C. Most work on verb clustering has used the F- measure or the Rand Index #OTHEREFR for evaluation, which rely on counting pairwise links between instances.[Citation]In practice, RI values concentrate in a small interval near 100% #OTHEREFR. Rosenberg & Hirschberg #OTHEREFR proposed an information-theoretic metric: V-measure. V- measure is the harmonic mean of homogeneity and completeness which evaluate the quality of the clustering in a complementary way."}
{"citing_paper_id": "P10-1153", "cited_paper_id": "D07-1049", "citing_paper_abstract": "Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector techniques.", "cited_paper_abstract": "A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements fall significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we present a general framework for deriving smoothed language model probabilities from BFs. We investigate how a BF containing n-gram statistics can be used as a direct replacement for a conventional n-gram model. Recent work has demonstrated that corpus statistics can be stored efficiently within a BF, here we consider how smoothed language model probabilities can be derived efficiently from this randomised representation. Our proposal takes advantage of the one-sided error guarantees of the BF and simple inequalities that hold between related n-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities. We use these models as replacements for a conventional language model in machine translation experiments.", "citation": "The approach of the present paper borrows from recent statistical machine translation research, which addresses the problem of efficiently representing large-scale language models using a mathematical construction called a Bloom filter #REFR.", "context": "Again, it has been so since the very beginning: A??t-Kaci et al #OTHEREFR devoted several pages to a discussion of how to ?modularize? type codes, which typically achieves a smaller code in exchange for a larger-time operation than bitwise AND as the implementation of type unification. However, in this and later work on the subject #OTHEREFR), one constant has been that we know our unification has failed when the implementation returns the zero vector. Zero preservation #OTHEREFR, i.e., detecting a type unification failure, is just as important as obtaining the right answer quickly when it succeeds.[Citation]The approach is best combined with modularization in order to further reduce the size of the codes, but its novelty lies in the observation that counting the number of one bits in an integer is implemented in the basic instruction sets of many CPUs. The question then arises whether smaller codes would be obtained by relaxing zero preservation so that any resulting vector with at most ? bits is interpreted as failure, with ? ? 1. Penn #OTHEREFR generalized join-preserving encodings of partial orders to the case where more than one code can be used to represent the same object, but the focus there was on codes arising from successful unifications; there was still only one representative for failure."}
{"citing_paper_id": "D09-1124", "cited_paper_id": "D07-1060", "citing_paper_abstract": "In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness.", "cited_paper_abstract": "We present the idea of estimating semantic distance in one, possibly resource-poor, language using a knowledge source in another, possibly resource-rich, language. We do so by creating cross-lingual distributional profiles of concepts, using a bilingual lexicon and a bootstrapping algorithm, but without the use of any sense-annotated data or word-aligned corpora. The cross-lingual measures of semantic distance are evaluated on two tasks: (1) estimating semantic distance between words and ranking the word pairs according to semantic distance, and (2) solving Reader?s Digest ?Word Power? problems. In task (1), cross-lingual measures are superior to conventional monolingual measures based on a wordnet. In task (2), cross-lingual measures are able to solve more problems correctly, and despite scores being affected by many tied answers, their overall performance is again better than the best monolingual measures.", "citation": "Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval #OTHEREFR; #REFR.", "context": "Given the accelerated growth of the number of multilingual documents on the Web and elsewhere, the need for effective multilingual and cross-lingual text processing techniques is becoming increasingly important. In this paper, we address the task of cross-lingual semantic relatedness, and introduce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages.[Citation]The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows."}
{"citing_paper_id": "P10-2068", "cited_paper_id": "D07-1061", "citing_paper_abstract": "We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (?synonyms of crash,? ?types of car,? etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.", "cited_paper_abstract": "Many systems for tasks such as question answering, multi-document summarization, and information retrieval need robust numerical measures of lexical relatedness. Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph. By contrast, we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph. Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics. We treat the graph as a Markov chain and compute a word-specific stationary distribution via a generalized PageRank algorithm. Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions. In our experiments, the resulting relatedness measure is the WordNet-based measure most highly correlated with human similarity judgments by rank ordering at ? = .90.", "citation": "#REFR use random walks over WordNet, incorporating information such as meronymy and dictionary glosses.", "context": "Suppose we want actors who appeared in Star Wars. If we only know that Harrison Ford andMark Hamill are actors, we have little to go on. There has been a large amount of work on other sources of word-similarity.[Citation]Snow et al #OTHEREFR extract hypernyms from free text. Wang and Cohen #OTHEREFR examine query logs. We expect that adding these types of data would significantly improve our system."}
{"citing_paper_id": "P13-1132", "cited_paper_id": "D07-1061", "citing_paper_abstract": "Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening.", "cited_paper_abstract": "Many systems for tasks such as question answering, multi-document summarization, and information retrieval need robust numerical measures of lexical relatedness. Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph. By contrast, we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph. Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics. We treat the graph as a Markov chain and compute a word-specific stationary distribution via a generalized PageRank algorithm. Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions. In our experiments, the resulting relatedness measure is the WordNet-based measure most highly correlated with human similarity judgments by rank ordering at ? = .90.", "citation": "Prior work has demonstrated that multinomials generated from random walks over Word- Net can be successfully applied to linguistic tasks such as word similarity #REFR, paraphrase recognition, textual entailment #OTHEREFR.", "context": "The WordNet ontology provides a rich network structure of semantic relatedness, connecting senses directly with their hypernyms, and providing information on semantically similar senses by virtue of their nearby locality in the network. Given a particular node (sense) in the network, repeated random walks beginning at that node will produce a frequency distribution over the nodes in the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds.[Citation]Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank #OTHEREFR."}
{"citing_paper_id": "P09-1109", "cited_paper_id": "D07-1064", "citing_paper_abstract": "We propose a hybrid approach to coordinate structure analysis that combines a simple grammar to ensure consistent global structure of coordinations in a sentence, and features based on sequence alignment to capture local symmetry of conjuncts. The weight of the alignmentbased features, which in turn determines the score of coordinate structures, is optimized by perceptron training on a given corpus. A bottom-up chart parsing algorithm efficiently finds the best scoring structure, taking both nested or nonoverlapping flat coordinations into account. We demonstrate that our approach outperforms existing parsers in coordination scope detection on the Genia corpus.", "cited_paper_abstract": "We propose a sequence-alignment based method for detecting and disambiguating coordinate conjunctions. In this method, averaged perceptron learning is used to adapt the substitution matrix to the training data drawn from the target language and domain. To reduce the cost of training data construction, our method accepts training examples in which complete word-by-word alignment labels are missing, but instead only the boundaries of coordinated conjuncts are marked. We report promising empirical results in detecting and disambiguating coordinated noun phrases in the GENIA corpus, despite a relatively small number of training examples and minimal features are employed.", "citation": "The method proposed in this paper improves upon our previous work #REFR which also takes a sentence as input but is restricted to flat coordinations.", "context": "The feature weights are optimized with a perceptron algorithm on a training corpus annotated with the scopes of conjuncts. The reason we build a tree of coordinations is to cope with nested coordinations, which are in fact quite common. In Genia Treebank Beta, for example, about 1/3 of the whole coordinations are nested.[Citation]Our new method, on the other hand, can successfully output the correct nested structure of Figure 1(b)."}
{"citing_paper_id": "W11-0105", "cited_paper_id": "D07-1071", "citing_paper_abstract": "We present a system to translate natural language sentences to formulas in a formal or a knowledge representation language. Our system uses two inverse ?-calculus operators and using them can take as input the semantic representation of some words, phrases and sentences and from that derive the semantic representation of other words and phrases. Our inverse ? operator works on many formal languages including first order logic, database query languages and answer set programming. Our system uses a syntactic combinatorial categorial parser to parse natural language sentences and also to construct the semantic meaning of the sentences as directed by their parsing. The same parser is used for both. In addition to the inverse ?-calculus operators, our system uses a notion of generalization to learn semantic representation of words from the semantic representation of other words that are of the same category. Together with this, we use an existing statistical learning approach to assign weights to deal with multiple meanings of words. Our system produces improved results on standard corpora on natural language interfaces for robot command and control and database queries.", "cited_paper_abstract": "We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items? with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).", "citation": "We adopt the learning model given by #REFR and use it to assign weights to the semantic representations of words.", "context": "[Citation]Since a word can have multiple possible syntactic and semantic representations assigned to it, such as John may be represented as John as well as ?x.x@John, we use the probabilistic model to assign weights to these representations. The main differences between our algorithm and the one given by Zettlemoyer and Collins #OTHEREFR are the way in which new semantic representations are obtained. While Zettlemoyer and Collins #OTHEREFR uses a predefined table to obtain these, we obtain the new semantic representations by using inverse ? operators and generalization."}
{"citing_paper_id": "W12-4005", "cited_paper_id": "D07-1073", "citing_paper_abstract": "Key to named entity recognition, the manual gazetteering of entity lists is a costly, errorprone process that often yields results that are incomplete and suffer from sampling bias. Exploiting current sources of structured information, we propose a novel method for extending minimal seed lists into complete gazetteers. Like previous approaches, we value WIKIPEDIA as a huge, well-curated, and relatively unbiased source of entities. However, in contrast to previous work, we exploit not only its content, but also its structure, as exposed in DBPEDIA. We extend gazetteers through Wikipedia categories, carefully limiting the impact of noisy categorizations. The resulting gazetteers easily outperform previous approaches on named entity recognition.", "cited_paper_abstract": "We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part. These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER.", "citation": "The approach presented in #REFR relies solely on WIKIPEDIA, producing gazetteers without explicitly named concepts, arguing that consistent but anonymous labels are still useful.", "context": "One of the first knowledgedriven methods #OTHEREFR employed WORDNET to identify trigger words and candidate gazetteer terms with its word-class and -instance relations. As WORDNET covers domain specific vocabularies only to a limited extent, this approach is also limited in its general applicability. In #OTHEREFR, gazetteers are built from the noun phrases in the first sentences of WIKIPEDIA articles by mapping these phrases to WORDNET and adding further terms found along the hypernymy relations.[Citation]Most closely related to our own work, the authors of #OTHEREFR build an approach solely on WIKIPEDIA which does not only exploit the article text but also analyzes the structural elements of WIKIPEDIA:"}
{"citing_paper_id": "C08-1145", "cited_paper_id": "D07-1077", "citing_paper_abstract": "One style of Multi-Engine Machine Translation architecture involves choosing the best of a set of outputs from different systems. Choosing the best translation from an arbitrary set, even in the presence of human references, is a difficult problem; it may prove better to look at mechanisms for making such choices in more restricted contexts. In this paper we take a classificationbased approach to choosing between candidates from syntactically informed translations. The idea is that using multiple parsers as part of a classifier could help detect syntactic problems in this context that lead to bad translations; these problems could be detected on either the source side?perhaps sentences with difficult or incorrect parses could lead to bad translations?or on the target side?perhaps the output quality could be measured in a more syntactically informed way, looking for syntactic abnormalities. We show that there is no evidence that the source side information is useful. However, a target-side classifier, when used to identify particularly bad translation candidates, can lead to significant improvements in Bleu score. Improvements are even greater when combined with existing language and alignment model approaches. c ? 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved.", "cited_paper_abstract": "Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al, 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules.", "citation": "As to the particular system, in this paper we look at a specific type of MT, the output of systems that use syntactic reordering as preprocessing #OTHEREFR; #REFR.", "context": "The first part of the main idea of this paper is that there are two ways in which problematic translations might be detected. One is on the source side: perhaps sentences with difficult or incorrect parses could lead to bad use of syntax and hence bad translations, and this could be detected by a classifier. The other is that on the target side, perhaps the output quality could be measured in a more syntactically informed way, looking for syntactic abnormalities.[Citation]In these systems, the source language is reordered to mirror the syntax of the target language in certain respects, leading to an improvement in the aggregate quality of the output over the baseline, although it is not always the case that each individual sentence in the reordered version is better. This could then be framed as an MEMT, where the reordered candidate is considered the default one, backing off to the baseline where the reordered one is worse, based on the decision of a classifier. Given the ?unnatural? order of the preprocessed source side, there is reason to expect that bad or unsuccessful reordered translations might be detectable."}
{"citing_paper_id": "P11-1011", "cited_paper_id": "D07-1086", "citing_paper_abstract": "Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.", "cited_paper_abstract": "Query segmentation is the process of taking a user?s search-engine query and dividing the tokens into individual phrases or semantic units. Identification of these query segments can potentially improve both document-retrieval precision, by first returning pages which contain the exact query segments, and document-retrieval recall, by allowing query expansion or substitution via the segmented units. We train and evaluate a machine-learned query segmentation system that achieves 86% segmentationdecision accuracy on a gold standard set of segmented noun phrase queries, well above recently published approaches. Key enablers of this high performance are features derived from previous natural language processing work in noun compound bracketing. For example, token association features beyond simple N-gram counts provide powerful indicators of segmentation.", "citation": "While the simple annotation described in Figure 1 can be done with a very high accuracy for standard document corpora, both previous work #OTHEREFR; #REFR and the experimental results in this paper indicate that it is challenging to perform well on queries.", "context": "To demonstrate a possible implementation of linguistic annotation for search queries, Figure 1 presents a simple mark-up scheme, exemplified using three web search queries #OTHEREFR kentucky derby, (b) kindred where would i be, and (c) shih tzu health problems. In this scheme, each query is markedup using three annotations: capitalization, POS tags, and segmentation indicators. Note that all the query terms are non-capitalized, and no punctuation is provided by the user, which complicates the query annotation process.[Citation]The queries in Figure 1 illustrate this point. Query (a) in Figure 1 is a wh-question, and it contains a capitalized concept (?Kentucky Derby?), a single verb, and four segments. Query (b) is a combination of an artist name and a song title and should be interpreted as Kindred ? ?Where Would I Be?."}
{"citing_paper_id": "N09-1058", "cited_paper_id": "D07-1090", "citing_paper_abstract": "In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems. We present an efficient low-memory method for constructing high-order approximate n-gram frequency counts. The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint. We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine. Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods.", "cited_paper_abstract": "This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.", "citation": "In NLP community, it has been shown that having more data results in better performance #OTHEREFR; #REFR.", "context": "Our method easily scales to billion-word monolingual corpora on conventional (8GB) desktop machines. We have demonstrated that approximate ngram features could be used as a direct replacement for conventional higher order LMs in SMT with significant reductions in memory usage. In future, we will be looking into building streaming skip ngrams, and other variants (like cluster n-grams).[Citation]At web scale, we have terabytes of data and that can capture broader knowledge. Streaming algorithm paradigm provides a memory and space-efficient platform to deal with terabytes of data. We hope that other NLP applications (where we need to compute relative frequencies) like noun-clustering, constructing syntactic rules for SMT, finding analogies, and others can also benefit from streaming methods."}
{"citing_paper_id": "N12-1079", "cited_paper_id": "D07-1090", "citing_paper_abstract": "It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data. To obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case).", "cited_paper_abstract": "This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.", "citation": "It has been repeatedly shown that ?throwing more data at the problem? is effective in increasing SMT output quality, both for translation modeling #OTHEREFR and for language modeling #REFR.", "context": "[Citation]In this paper, we bring together two related research threads to gather parallel sentences for improved translation modeling: cross-lingual pairwise similarity to mine comparable documents and classification to identify sentence pairs that are mutual translations. Unlike most previous work, which sidesteps the computationally-intensive task of pairwise comparisons to mine comparable documents and instead relies on heuristics, we tackle the challenge head on. This paper describes a fully open-source, scalable MapReduce-based processing pipeline that is able to automatically extract large quantities of parallel sentences."}
{"citing_paper_id": "P09-2086", "cited_paper_id": "D07-1090", "citing_paper_abstract": "Efficient processing of tera-scale text data is an important research topic. This paper proposes lossless compression of N - gram language models based on LOUDS, a succinct data structure. LOUDS succinctly represents a trie with M nodes as a 2M + 1 bit string. We compress it further for the N -gram language model structure. We also use ?variable length coding? and ?block-wise compression? to compress values associated with nodes. Experimental results for three large-scale N -gram compression tasks achieved a significant compression rate without any loss.", "cited_paper_abstract": "This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.", "citation": "To support distributed computation #REFR, we further split the N -gram data into ?shards? by hash values of the first bigram.", "context": "The starting positions of different orders are memorized to allow access to arbitrary orders. To store N -gram counts, we use three tables for word ids, counts and pointers. We share the same tables for word ids and pointers with additional probability and back-off coefficient tables.[Citation]Unigram data are shared across shards for efficiency."}
{"citing_paper_id": "W08-0310", "cited_paper_id": "D07-1091", "citing_paper_abstract": "This paper describes our statistical machine translation systems based on the Moses toolkit for the WMT08 shared task. We address the Europarl and News conditions for the following language pairs: English with French, German and Spanish. For Europarl, n-best rescoring is performed using an enhanced n-gram or a neuronal language model; for the News condition, language models incorporate extra training data. We also report unconvincing results of experiments with factored models.", "cited_paper_abstract": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ? may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.", "citation": "Therefore, including a model based on surface forms, as suggested #REFR, is also necessary.", "context": "On the news test set of 2008, this system obtains a BLEU score of 20.2, which is worse than our ?standard? system (20.9). A similar experiment on the Europarl task proved equally unsuccessful. Using only models which ignore the surface form of input words yields a poor system.[Citation]This indeed improved (+1.6 BLEU for Europarl) over using one single decoding path, but not enough to match our baseline system performance. These results may be explained by the use of automatic tools (POS tagger and lemmatizer) that are not entirely error free, and also, to a lesser extend, by the noise in the test data. We also think that more effort has to be put into the generation step."}
{"citing_paper_id": "W11-2138", "cited_paper_id": "D07-1091", "citing_paper_abstract": "We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation. This method called ?reverse self-training? improves the decoder?s ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting. We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words. We also provide a description of the systems we submitted to WMT11 Shared Task.", "cited_paper_abstract": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ? may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.", "citation": "For that purpose we use a factored translation model #REFR with two alternative decoding paths: form?form and back-off?form.", "context": "Figure 1 illustrates the core of the method. Using available parallel data, we first train an MT system to translate from the target to the source language. Since we want to gather new word forms from the monolingual data, this reverse model needs the ability to translate them.[Citation]We experimented with several options for the back-off (simple stemming by truncation or full lemmatization), see Section 4.4. The decoder can thus use a less sparse representation of words if their exact forms are not available in the parallel data. We use this reverse model to translate (much larger) target-side monolingual data into the source language."}
{"citing_paper_id": "W13-2212", "cited_paper_id": "D07-1091", "citing_paper_abstract": "We validated various novel and recently proposed methods for statistical machine translation on 10 language pairs, using large data resources. We saw gains from optimizing parameters, training with sparse features, the operation sequence model, and domain adaptation techniques. We also report on utilizing a huge language model trained on 126 billion tokens. The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs. Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign. We would like to stress especially two contributions: the use of the new operation sequence model (Section 3) within Moses, and ? in a separate unconstraint track submission ? the use of a huge language model trained on 126 billion tokens with a new training tool (Section 4).", "cited_paper_abstract": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ? may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.", "citation": "German?English with syntactic prereordering #OTHEREFR and use of factored representation for a POS target sequence model #REFR .", "context": "We start with systems #OTHEREFR. The notable features of these systems are: . Moses phrase-based models with mostly default settings ? training on all available parallel data, including the large UN parallel data, the French- English 109 parallel data and the LDC Gigaword data ? very large tuning set consisting of the test sets from 2008-2010, with a total of 7,567 sentences per language .[Citation]English?German with morphological target sequence model Note that while our final 2012 systems included subsampling of training data with modified Moore-Lewis filtering #OTHEREFR, we did not use such filtering at the starting point of our development. We will report on such filtering in Section 2. Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 training data at a later stage."}
{"citing_paper_id": "W14-3309", "cited_paper_id": "D07-1091", "citing_paper_abstract": "This paper describes the University of Edinburgh?s (UEDIN) phrase-based submissions to the translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in Russian- English and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus.", "cited_paper_abstract": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ? may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.", "citation": "We used POS and morphological tags as additional factors in phrase translation models #REFR for German- English language pairs.", "context": "We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM #OTHEREFR.[Citation]We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering #OTHEREFR for German-to-English systems. We used trivia tokenizer for tokenizing Hindi."}
{"citing_paper_id": "W14-3320", "cited_paper_id": "D07-1091", "citing_paper_abstract": "This paper describes the system jointly developed by members of the Departament de Llenguatges i Sistemes Inform`atics at Universitat d?Alacant and the Prompsit Language Engineering company for the shared translation task of the 2014 Workshop on Statistical Machine Translation. We present a phrase-based statistical machine translation system whose phrase table is enriched with information obtained from dictionaries and shallowtransfer rules like those used in rule-based machine translation. The novelty of our approach lies in the fact that the transfer rules used were not written by humans, but automatically inferred from a parallel corpus.", "cited_paper_abstract": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ? may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.", "citation": "It can be therefore seen as a novel method to add morphological information to SMT, as factored translation models do #REFR.", "context": "Linguistic data from RBMT systems have already been used to enrich SMT systems #OTHEREFRa). We have already proved No other system was found statistically significantly better using the sign test at p ? 0.10. that using hand-written rules and dictionaries from RBMT yields better results than using only dictionaries #OTHEREFRa). However, in the approach we present in this paper, rules are automatically inferred from a parallel corpus after converting it into the intermediate representation used by the Apertium RBMT platform (see section 3.3).[Citation]Unlike factored models, we do not estimate independent statistical models for the translation of the different factors (lemmas, lexical categories, morphological inflection attributes, etc.) and for the generation of the final surface forms. Instead, we first infer a set of rules that deal with the grammatical divergences between the languages involved by performing operations such as reorderings, gender and number agreements, etc. Afterwards, we add synthetic phrase pairs generated from these rules and the Apertium dictionaries to the data from which the well-known, classical PBSMT models #OTHEREFR are estimated."}
{"citing_paper_id": "D07-1102", "cited_paper_id": "D07-1096", "citing_paper_abstract": "We present our system used in the CoNLL 2007 shared task on multilingual parsing. The system is composed of three components: a k-best maximum spanning tree (MST) parser, a tree labeler, and a reranker that orders the k-best labeled trees. We present two techniques for training the MST parser: tree-normalized and graphnormalized conditional training. The treebased reranking model allows us to explicitly model global syntactic phenomena. We describe the reranker features which include non-projective edge attributes. We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system.", "cited_paper_abstract": "The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.", "citation": "#REFR shows that the oracle parsing accuracy of a k-best edge-factored MST parser is considerably higher than the one-best score of the same parser, even when k is small.", "context": "[Citation]We have verified that this is true for the CoNLL shared-task data by evaluating the oracle rates on a randomly sampled development set for each language. In order to select optimal model parameters for the MST parser, the labeler, and reranker, we sampled approximately 200 sentences from each training set to use as a development test set. Training the reranker requires a jackknife n-fold training procedure where n?1 partitions are used to train a model that parses the remaining partition."}
{"citing_paper_id": "W07-0604", "cited_paper_id": "D07-1096", "citing_paper_abstract": "Corpora of child language are essential for psycholinguistic research. Linguistic annotation of the corpora provides researchers with better means for exploring the development of grammatical constructions and their usage. We describe an ongoing project that aims to annotate the English section of the CHILDES database with grammatical relations in the form of labeled dependency structures. To date, we have produced a corpus of over 65,000 words with manually curated gold-standard grammatical relation annotations. Using this corpus, we have developed a highly accurate data-driven parser for English CHILDES data. The parser and the manually annotated data are freely available for research purposes.", "cited_paper_abstract": "The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.", "citation": "This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing #OTHEREFR; #REFR3.", "context": "Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie #OTHEREFR. Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly.[Citation]Sagae and Tsujii #OTHEREFR present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm #OTHEREFR, keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure)."}
{"citing_paper_id": "W14-1613", "cited_paper_id": "D07-1096", "citing_paper_abstract": "This paper proposes to learn languageindependent word representations to address cross-lingual dependency parsing, which aims to predict the dependency parsing trees for sentences in the target language by training a dependency parser with labeled sentences from a source language. We first combine all sentences from both languages to induce real-valued distributed representation of words under a deep neural network architecture, which is expected to capture semantic similarities of words not only within the same language but also across different languages. We then use the induced interlingual word representation as augmenting features to train a delexicalized dependency parser on labeled sentences in the source language and apply it to the target sentences. To investigate the effectiveness of the proposed technique, extensive experiments are conducted on cross-lingual dependency parsing tasks with nine different languages. The experimental results demonstrate the superior cross-lingual generalizability of the word representation induced by the proposed approach, comparing to alternative comparison methods.", "cited_paper_abstract": "The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.", "citation": "MSTParser is widely used in the literature for dependency parsing tasks and has demonstrated good empirical results in the CoNLL shared tasks on multilingual dependency parsing #OTHEREFR; #REFR.", "context": "We used the MSTParser #OTHEREFRb) as the basic dependency parsing model. MSTParser uses spanning tree algorithms to seek for the candidate dependency trees and employs an online large margin training optimization algorithm.[Citation]For this dependency parsing model, there are a few parameters to be set: the number of maximum iterations for the perceptron training, and the number of best-k dependency tree candidates. We set the number of iterations to be 10 and only considered the best-1 dependency tree candidate. For the proposed cross-lingual dependency parsing approach, we used both the delexi-"}
{"citing_paper_id": "P11-2125", "cited_paper_id": "D07-1097", "citing_paper_abstract": "We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST- Parser framework (McDonald et al, 2005). We also provide an ensemble method for combining diverse cluster-based models. The two contributions together significantly improves unlabeled dependency accuracy from 90.82% to 92.13%.", "cited_paper_abstract": "We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing. The first stage consists in tuning a single-parser system for each language by optimizing parameters of the parsing algorithm, the feature model, and the learning algorithm. The second stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the optimal parameters settings for each language. When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score.", "citation": "Several ensemble models have been proposed for dependency parsing #OTHEREFR; #REFR.", "context": "[Citation]Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths."}
{"citing_paper_id": "W13-4908", "cited_paper_id": "D07-1101", "citing_paper_abstract": "This paper presents a dependency parsing system, presented as BASQUE TEAM at the SPMRL?2013 Shared Task, based on the analysis of each morphological feature of the languages. Once the specific relevance of each morphological feature is calculated, this system uses the most significant of them to create a series of analyzers using two freely available and state of the art dependency parsers, MaltParser and Mate. Finally, the system will combine previously achieved parses using a voting approach.", "cited_paper_abstract": "We present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.", "citation": "The Mate parser #OTHEREFR is a development of the algorithms described in #REFR.", "context": "To fine-tune Maltparser we have used MaltOptimizer #OTHEREFRb). This tool is an interactive system that first performs an analysis of the training set in order to select a suitable starting point for optimization and then guides the user through the optimization of parsing algorithm, feature model, and learning algorithm. Empirical evaluation on data from the CoNLL 2006 and 2007 shared tasks on dependency parsing shows that MaltOptimizer consistently improves over the baseline of default settings and sometimes even surpasses the result of manual optimization.[Citation]It basically adopts the second order maximum spanning tree dependency parsing algorithm. In particular, this parser exploits a hash kernel, a new parallel parsing and feature extraction algorithm that improves accuracy as well as parsing speed #OTHEREFR."}
{"citing_paper_id": "E14-1002", "cited_paper_id": "D07-1104", "citing_paper_abstract": "We present a novel Undirected Machine Translation model of Hierarchical MT that is not constrained to the standard bottomup inference order. Removing the ordering constraint makes it possible to condition on top-down structure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art.", "cited_paper_abstract": "A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrasebased translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.", "citation": "The hierarchical translation grammar was extracted using the Joshua toolkit #OTHEREFR; #REFR. ", "context": "We experiment on the NIST Chinese-English parallel corpus. The training corpus contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. The test set contains 919 sentence pairs.[Citation]Table 1 reports the decoding time measures. HMT with beam1 is the fastest possible configuration for HMT, but it is 71.59% slower than UMT. This is because HMT b1 constructs O(n2) subtrees, many of which end up not being used in the final result, whereas UMT only constructs the rule instantiations that are required."}
{"citing_paper_id": "W09-0424", "cited_paper_id": "D07-1104", "citing_paper_abstract": "We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task.", "cited_paper_abstract": "A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrasebased translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.", "citation": "The toolkit also implements suffixarray grammar extraction #OTHEREFR; #REFR and minimum error rate training #OTHEREFR.", "context": "We have described a scalable toolkit for parsingbased machine translation. It is written in Java and implements all the essential algorithms described in Chiang #OTHEREFRb): chart-parsing, n-gram language model integration, beamand cube-pruning, and k-best extraction.[Citation]Additionally, parallel and distributed computing techniques are exploited to make it scalable. The decoder achieves state of the art translation performance. Acknowledgments This research was supported in part by the Defense Advanced Research Projects Agency?s GALE program under Contract No."}
{"citing_paper_id": "P13-1029", "cited_paper_id": "D07-1112", "citing_paper_abstract": "In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing transfer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars . CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1", "cited_paper_abstract": "We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation. Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks. Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline.", "citation": "Transfer learning in parsing has been applied in different contexts, such as multilingual learning #OTHEREFR; #REFR, and crossformalism transfer #OTHEREFR.", "context": "Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations.[Citation]There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may specify how to convert traces and functional tags in Penn Treebank to the f-structure in LFG #OTHEREFR."}
{"citing_paper_id": "P12-1042", "cited_paper_id": "D07-1114", "citing_paper_abstract": "The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results.", "cited_paper_abstract": "The technology of opinion extraction allows users to retrieve and analyze people?s opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues. Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks.", "citation": "Most of the work have been done in the context of product reviews mining #OTHEREFRb; #REFR.", "context": "Several methods have been proposed to identify the target of an opinion expression.[Citation]In this context, opinion targets usually refer to product features #OTHEREFR). In the work of Hu and Liu #OTHEREFRb), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants."}
{"citing_paper_id": "W13-2404", "cited_paper_id": "D07-1114", "citing_paper_abstract": "Aspect-oriented opinion mining aims to identify product aspects (features of products) about which opinion has been expressed in the text. We present an approach for aspect-oriented opinion mining from user reviews in Croatian. We propose methods for acquiring a domain-specific opinion lexicon, linking opinion clues to product aspects, and predicting polarity and rating of reviews. We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions.", "cited_paper_abstract": "The technology of opinion extraction allows users to retrieve and analyze people?s opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues. Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks.", "citation": "#REFR extract aspect-clue pairs from weblog posts using a supervised model with parts of dependency trees as features.", "context": "Aspect-based opinion mining typically consists of three subtasks: sentiment lexicon acquisition, aspect-clue pair identification, and overall review opinion prediction. Most approaches to domainspecific sentiment lexicon acquisition start from a manually compiled set of aspects and opinion clues and then expand it with words satisfying certain co-occurrence or syntactic criteria in a domainspecific corpus #OTHEREFR.[Citation]Kelly et al#OTHEREFR use a semi-supervised SVM model with syntactic features to classify the relations between entityproperty pairs. Opinion classification of reviews has been approached using supervised text categorization techniques #OTHEREFR. Sentiment analysis and opinion mining approaches have been proposed for several Slavic languages #OTHEREFR."}
{"citing_paper_id": "P12-1026", "cited_paper_id": "D07-1117", "citing_paper_abstract": "From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.", "cited_paper_abstract": "We present in this paper methods to improve HMM-based part-of-speech (POS) tagging of Mandarin. We model the emission probability of an unknown word using all the characters in the word, and enrich the standard left-to-right trigram estimation of word emission probabilities with a right-to-left prediction of the word by making use of the current and next tags. In addition, we utilize the RankBoost-based reranking algorithm to rerank the N-best outputs of the HMM- based tagger using various n-gram, morphological, and dependency features. Two methods are proposed to improve the generalization performance of the reranking algorithm. Our reranking model achieves an accuracy of 94.68% using n-gram and morphological features on the Penn Chinese Treebank 5.2, and is able to further improve the accuracy to 95.11% with the addition of dependency features.", "citation": "While state-of-theart tagging systems have achieved accuracies above 97% on English, Chinese POS tagging has proven to be more challenging and obtained accuracies about 93-94% #OTHEREFRb; #REFR.", "context": "In some cases, the methods work well without large modifications, such as for German. But a number of augmentations and changes become necessary when dealing with highly inflected or agglutinative languages, as well as analytic languages, of which Chinese is the focus ?This work is mainly finished when this author (corresponding author) was in Saarland University and DFKI. of this paper. The Chinese language is characterized by the lack of formal devices such as morphological tense and number that often provide important clues for syntactic processing tasks.[Citation]It is generally accepted that Chinese POS tagging often requires more sophisticated language processing techniques that are capable of drawing inferences from more subtle linguistic knowledge. From a linguistic point of view, meaning arises from the differences between linguistic units, including words, phrases and so on, and these differences are of two kinds: paradigmatic (concerning substitution) and syntagmatic (concerning positioning). The distinction is a key one in structuralist semiotic analysis."}
{"citing_paper_id": "D07-1096", "cited_paper_id": "D07-1123", "citing_paper_abstract": "The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.", "cited_paper_abstract": "We describe an incremental parser that was trained to minimize cost over sentences rather than over individual parsing actions. This is an attempt to use the advantages of the two top-scoring systems in the CoNLL-X shared task. In the evaluation, we present the performance of the parser in the Multilingual task, as well as an evaluation of the contribution of bidirectional parsing and beam search to the parsing performance.", "citation": "Two more outliers can be observed in the system of #REFRb), which improves from its average rank 12 to rank 4 for Basque and Turkish.", "context": "Their only outlier is for Chinese, where the system occupies rank 14, with a LAS approximately 9 percentage points below the top scoring system for Chinese #OTHEREFR. However, Hall et al #OTHEREFRa) point out that the official results for Chinese contained a bug, and the true performance of their system was actually much higher. The greatest improvement of a system with respect to its average rank occurs for English, for which the system by Nguyen et al #OTHEREFR improved from the average rank 15 to rank 6.[Citation]The authors attribute this high performance to their parser?s good performance on small training sets. However, this hypothesis is contradicted by their results for Greek and Italian, the other two languages with small training sets. For these two languages, the system?s rank is very close to its average rank."}
{"citing_paper_id": "W11-0906", "cited_paper_id": "D08-1008", "citing_paper_abstract": "This paper suggests two ways of improving semantic role labeling (SRL). First, we introduce a novel transition-based SRL algorithm that gives a quite different approach to SRL. Our algorithm is inspired by shift-reduce parsing and brings the advantages of the transitionbased approach to SRL. Second, we present a self-learning clustering technique that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL?09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.", "cited_paper_abstract": "We present a PropBank semantic role labeling system for English that is integrated with a dependency parser. To tackle the problem of joint syntactic?semantic analysis, the system relies on a syntactic and a semantic subcomponent. The syntactic model is a projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. The complete syntactic?semantic output is selected from a candidate pool generated by the subsystems. We evaluate the system on the CoNLL- 2005 test sets using segment-based and dependency-based metrics. Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+Brown test set, or 78.84 if punctuation is treated consistently. Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008. Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance.", "citation": "However, #REFR, who showed state-of-the-art performance in CoNLL?08, evaluated their system with settings very similar to ours.", "context": "Table 5: Labeling accuracies evaluated on the Brown. We also compare our results against another stateof-the-art system. Unfortunately, no other system has been evaluated with our exact environmental settings.[Citation]Their task was exactly the same as ours; given predicate identification, they evaluated their dependency-based semantic role labeler for argument identification and classification on the WSJ and Brown corpora, distributed by the CoNLL?05 shared task #OTHEREFR. Since the CoNLL?05 data was not dependency-based, they applied heuristics to build dependency-based predicate argument structures. Their converted data may appear to be a bit different from the CoNLL?09 data we use (e.g., hyphenated words are tokenized by the hyphens in CoNLL?09 data whereas they are not in CoNLL?05 data), but semantic role annotations on headwords should look very similar."}
{"citing_paper_id": "D11-1049", "cited_paper_id": "D08-1009", "citing_paper_abstract": "We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al, 2010). This new system improves significantly over NELL?s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks.", "cited_paper_abstract": "Most Web-based Q/A systems work by finding pages that contain an explicit answer to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve this problem, we introduce the HOLMES system, which utilizes textual inference (TI) over tuples extracted from text. Whereas previous work on TI (e.g., the literature on textual entailment) has been applied to paragraph-sized texts, HOLMES utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, HOLMES doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, HOLMES?s runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus?they are ?approximately? functional in a well-defined sense.", "citation": "The HOLMES system #REFR derives new assertions using a few manually written inference rules.", "context": "This approach is similar to the random walk with restart approach which is used as a baseline in our experiment. The FactRank system #OTHEREFR compares different ways of constructing random walks, and combining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type information, which is important for achieving high accuracy predictions.[Citation]A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules automatically from training data. Similarly, the Markov Logic Networks #OTHEREFR are Markov networks constructed corresponding to the grounding of rules to knowledge bases."}
{"citing_paper_id": "P10-1112", "cited_paper_id": "D08-1012", "citing_paper_abstract": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.", "cited_paper_abstract": "The intersection of tree transducer-based translation models with n-gram language models results in huge dynamic programs for machine translation decoding. We propose a multipass, coarse-to-fine approach in which the language model complexity is incrementally introduced. In contrast to previous orderbased bigram-to-trigram approaches, we focus on encoding-based methods, which use a clustered encoding of the target language. Across various encoding schemes, and for multiple language pairs, we show speed-ups of up to 50 times over single-pass decoding while improving BLEU score. Moreover, our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram-to-trigram decoder.", "citation": "Here, we observe an effect seen in previous work #OTHEREFR, #REFR), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection).", "context": "Figure 5: Effect of coarse-pass pruning on parsing accuracy (WSJ, training ? 20 words, tested on dev-set ? 20 words). This graph shows that the fortuitous improvement due to pruning is very small and that the peak accuracy is almost equal to the accuracy without pruning (the dotted line). from no pruning to pruning with a ?6.2 log posterior threshold.10 Figure 4 depicts the variation in parsing accuracies in response to the amount of pruning done by the coarse-pass. Higher posterior pruning thresholds induce more aggressive pruning.[Citation]However, these ?fortuitous? search errors give only a small improvement and the peak accuracy is almost equal to the parsing accuracy without any pruning (as seen in Figure 5).11 This outcome suggests that the coarsepass pruning is critical for tractability but not for performance. 10Unpruned experiments could not be run for 40-word test sentences even with 50GB of memory, therefore we calculated the improvement factors using a smaller experiment with full training and sixty 30-word test sentences. 11To run experiments without pruning, we used training and dev sentences of length ? 20 for the graph in Figure 5. tree-to-graph encoding"}
{"citing_paper_id": "P11-2075", "cited_paper_id": "D08-1014", "citing_paper_abstract": "Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios. To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text. This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch. Various prior work have achieved positive results using this approach. In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems. First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT. Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered. This paper will describe a series of carefullydesigned experiments that led us to these conclusions.", "cited_paper_abstract": "Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language. In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages. Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages. Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.", "citation": "Certainly there are several successful studies #OTHEREFR; #REFR, but we think it is important to consider the possibility that cross-lingual adaptation has some fundamental differences.", "context": "This is a puzzling result considering that both use the same unlabeled data. Why does TSVM exhibit such a large variance on cross-lingual problems, but not on cross-market problems. Is unlabeled target data interacting with source data in some unexpected way.[Citation]We conjecture that adapting from artificially-generated text (e.g. MT output) is a different story than adapting from naturallyoccurring text (e.g. cross-market). In short, MT is ripe for cross-lingual adaptation; what is not ripe is probably our understanding of the special characteristics of the adaptation problem."}
{"citing_paper_id": "W13-1612", "cited_paper_id": "D08-1014", "citing_paper_abstract": "Up until now most of the methods published for polarity classification are applied to English texts. However, other languages on the Internet are becoming increasingly important. This paper presents a set of experiments on English and Spanish product reviews. Using a comparable corpus, a supervised method and two unsupervised methods have been assessed. Furthermore, a list of Spanish opinion words is presented as a valuable resource.", "cited_paper_abstract": "Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language. In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages. Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages. Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.", "citation": "#REFR showed that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.", "context": "Subsequently, they generated the parallel EVOCA corpus (English version of OCA) by translating the OCA corpus automatically into English. The results showed that they are comparable to other English experiments, since the loss of precision due to the translation process is very slight, as can be seen in #OTHEREFRb). Regarding Spanish, there are also some interesting studies.[Citation]In #OTHEREFR several experiments are presented dealing with Spanish and English resources. They conclude that although the ML techniques can provide a good baseline performance, it is necessary to integrate language-specific knowledge and resources in order to achieve an improvement. Cruz et al#OTHEREFR manually recollected the MuchoCine (MC) corpus to develop a sentiment polarity classifier based on the semantic orientation of the phrases and words."}
{"citing_paper_id": "D10-1004", "cited_paper_id": "D08-1016", "citing_paper_abstract": "We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages.", "cited_paper_abstract": "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively.", "citation": "Recall that (i) #REFR proposed a factor graph #OTHEREFR approximate parsing as the solution of a linear program.", "context": "We next present our main contribution: a formal connection between two recent approximate dependency parsers, which at first sight appear unrelated.[Citation]Here, we fill the blanks in the two approaches: we derive explicitly the variational problem addressed in (i) and we provide the underlying factor graph in (ii). This puts the two approaches side-by-side as approximate methods for marginal and MAP inference. Since both rely on ?local? approximations (in the sense of Eq."}
{"citing_paper_id": "N10-1117", "cited_paper_id": "D08-1016", "citing_paper_abstract": "Recently, relaxation approaches have been successfully used for MAP inference on NLP problems. In this work we show how to extend the relaxation approach to marginal inference used in conditional likelihood training, posterior decoding, confidence estimation, and other tasks. We evaluate our approach for the case of second-order dependency parsing and observe a tenfold increase in parsing speed, with no loss in accuracy, by performing inference over a small subset of the full factor graph. We also contribute a bound on the error of the marginal probabilities by a sub-graph with respect to the full graph. Finally, while only evaluated with BP in this paper, our approach is general enough to be applied with any marginal inference method in the inner loop.", "cited_paper_abstract": "We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively.", "citation": "This requires us to find the best tree in terms of marginal probabilities on the link variables #REFR.", "context": "In our experiments we seek to answer the following questions. First, how fast is our relaxation approach compared to full marginal inference at comparable dependency accuracy.[Citation]Second, how good is the final relaxed graph as an approximation of the full graph. Finally, how does incremental relaxation scale with sentence length?"}
{"citing_paper_id": "W11-1606", "cited_paper_id": "D08-1019", "citing_paper_abstract": "We examine the task of strict sentence intersection: a variant of sentence fusion in which the output must only contain the information present in all input sentences and nothing more. Our proposed approach involves alignment and generalization over the input sentences to produce a generation lattice; we then compare a standard search-based approach for decoding an intersection from this lattice to an integer linear program that preserves aligned content while minimizing the disfluency in interleaving text segments. In addition, we introduce novel evaluation strategies for intersection problems that employ entailmentstyle judgments for determining the validity of system-generated intersections. Our experiments show that the proposed models produce valid intersections a majority of the time and that the segmented decoder yields advantages over the search-based approach.", "cited_paper_abstract": "We present a novel unsupervised sentence fusion method which we apply to a corpus of biographies in German. Given a group of related sentences, we align their dependency trees and build a dependency graph. Using integer linear programming we compress this graph to a new tree, which we then linearize. We use GermaNet and Wikipedia for checking semantic compatibility of co-arguments. In an evaluation with human judges our method outperforms the fusion approach of Barzilay & McKeown (2005) with respect to readability.", "citation": "Due to the absence of adequate training data for intersection, our approach to the task is unsupervised, similar to previous work in fusion #OTHEREFR; #REFRb) and sentence compression #OTHEREFRa).", "context": "Table 2: Example sentence pairs from the McKeown et al #OTHEREFR corpus. Table 3 contains the corresponding systemgenerated intersections for these sentence pairs. hypothesized that the task is more confusing for untrained annotators. A similar phenomenon was noted by Krahmer et al #OTHEREFR: while demonstrating that query-based human fusions exhibited less variation than generic fusions, it was also observed that intersections varied more than unions.[Citation]Additionally, we focus on the case of pairwise sentence intersection and assume that the common information between the input sentence pair can be represented within a single output sentence. As a result, although the McK- eown et al #OTHEREFR corpus cannot be used for training an intersection model, we can make use of the sentence pairs it contains for evaluation."}
{"citing_paper_id": "W11-1417", "cited_paper_id": "D08-1020", "citing_paper_abstract": "We apply a previously reported measure of dialog cohesion to a corpus of spoken tutoring dialogs in which motivation was measured. We find that cohesion significantly predicts changes in student motivation, as measured with a modified MSLQ instrument. This suggests that non-intrusive dialog measures can be used to measure motivation during tutoring.", "cited_paper_abstract": "We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers? judgments of text readability. This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text. We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.", "citation": "Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. #REFR), measuring stylistic differences in text #OTHEREFR.", "context": "We hypothesize that this measure of lexical similarity may be related to motivation in part because other measures of dialog similarity have been shown to be related to task success. For example, there is evidence that perceived similarity between a student?s own speech rate and that of a recorded task request increases the student?s feelings of immediacy, which are in turn linked to greater compliance with the request to perform a task #OTHEREFR. 1 In addition, Ward and Litman #OTHEREFR investigated a measure of lexical similarity between 1In this experiment, the task was to watch a series of videos. the tutor and student partners in a tutoring dialog which was shown to be correlated with task success in several corpora of tutorial dialogs.[Citation]Given the previously mentioned results relating motivation to educational task success, these links between task success and cohesion lead us to hypothesize a direct correlation between motivation and cohesion when using the Itspoke tutor. We will first briefly describe the Itspoke tutor, and the corpus of tutoring dialogs used in this study. We will then describe the instrument we used to measure motivation both before and immediately after tutoring, then we will describe the algorithm used to measure cohesion in the tutoring dialogs."}
{"citing_paper_id": "W11-2504", "cited_paper_id": "D08-1021", "citing_paper_abstract": "This paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. Raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods. Our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection.", "cited_paper_abstract": "We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.", "citation": "They assigned two values to each sentence using the 5-point scales defined in #REFR.", "context": "Judges evaluated the paraphrase quality through a substitution test: For each sampled sentence, the test phrase is substituted with automatically-generated paraphrases. The sentences and the phrases are drawn from the English side of the Europarl corpus. Judges indicated the amount of the original meaning preserved by the paraphrases and the grammaticality of the resulting sentences.[Citation]The 100 test phrases consisted of 25 unigrams, 25 bigrams, 25 trigrams and 25 4-grams. These 25 phrases were randomly sampled from the paraphrase table generated by the bilingual pivoting method, with the following restrictions: . The phrase must have occurred at least 5 times in the parallel corpus and must have appeared in the web-scale n-grams. ."}
{"citing_paper_id": "N13-1003", "cited_paper_id": "D08-1024", "citing_paper_abstract": "There have been many recent investigations into methods to tune SMT systems using large numbers of sparse features. However, there have not been nearly so many examples of helpful sparse features, especially for phrasebased systems. We use sparse features to address reordering, which is often considered a weak point of phrase-based translation. Using a hierarchical reordering model as our baseline, we show that simple features coupling phrase orientation to frequent words or wordclusters can improve translation quality, with boosts of up to 1.2 BLEU points in Chinese- English and 1.8 in Arabic-English. We compare this solution to a more traditional maximum entropy approach, where a probability model with similar features is trained on wordaligned bitext. We show that sparse decoder features outperform maximum entropy handily, indicating that there are major advantages to optimizing reordering features directly for BLEU with the decoder in the loop.", "cited_paper_abstract": "Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik?s soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 B??? on a subset of the NIST 2006 Arabic-English evaluation data.", "citation": "For all systems, parameters are tuned with a batch-lattice variant of hope-fear MIRA #REFR.", "context": "HRM orientations are determined using an unrestricted shiftreduce parser #OTHEREFR. We also employ a standard distortion penalty incorporating the minimum completion cost described by Moore and Quirk #OTHEREFR. Our multi-stack phrase-based decoder is quite similar to Moses #OTHEREFR.[Citation]Preliminary experiments suggested that the sparse reordering features have a larger impact when tuned with lattices as opposed to n-best lists."}
{"citing_paper_id": "P10-2042", "cited_paper_id": "D08-1033", "citing_paper_abstract": "Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al, 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method?s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently converges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.", "cited_paper_abstract": "We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment. We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data. Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrasebased translation systems.", "citation": "Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level #REFR.", "context": "The same technique was also shown to improve the parsing algorithm. These improvements are in no way limited to our particular choice of a TSG parsing model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al #OTHEREFR for which it would be trivial to adapt our technique to develop a blocked sampler.[Citation]We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is difficult to extend the local Gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints."}
{"citing_paper_id": "N10-1143", "cited_paper_id": "D08-1035", "citing_paper_abstract": "Hierarchical discourse segmentation is a useful technology, but it is difficult to evaluate. I propose an error measure based on the word error rate of Beeferman et al (1999). I then show that this new measure not only reliably distinguishes baseline segmentations from lexically-informed hierarchical segmentations and more informed segmentations from less informed segmentations, but it also offers an improvement over previous linear error measures.", "cited_paper_abstract": "This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework.1", "citation": "The Bayesian framework explored by #REFR is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook.", "context": "Another strand of research, including Galley et al #OTHEREFR, make use of a wide variety of linguistic and orthographic cues. And the discourse parsing systems take advantage of even more linguistic cues. The ideal segmentation algorithm needs to combine the advantages of each of these approaches, but the frameworks are not straightforwardly compatible.[Citation]The HBT and GBEM algorithms, which were based on that work, do not attempt to go beyond lexical cohesion, but it does provide a framework for hierarchical segmentation algorithms that take advantage of other cues."}
{"citing_paper_id": "N13-1019", "cited_paper_id": "D08-1035", "citing_paper_abstract": "We present a new hierarchical Bayesian model for unsupervised topic segmentation. This new model integrates a point-wise boundary sampling algorithm used in Bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents. We develop an MCMC inference algorithm to split/merge segment(s). Experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi?s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets.", "cited_paper_abstract": "This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework.1", "citation": "Instead of assuming documents in a dataset share the same set of topics, Bayesseg #REFR treats words in a segment generated from a segment specific multinomial language model, i.e., it assumes each segment is generated from one topic, and a later hierarchical extension #OTHEREFR assumes each segment is generated from one topic or its parents.", "context": "Using a similar Markov structure, SITS #OTHEREFR. Unlike PLDA, SITS assumes each text passage is associated with a speaker identity that is attached to the topic shift variable as supervising information. SITS further assumes speakers have different topic change probabilities that work as priors on topic shift variables.[Citation]Other methods using as input the output of topic models include #OTHEREFR. In this paper we take a generative approach lying between PLDA and SITS. In contrast to PLDA, which uses a flat topic model (i.e., LDA), we assume each text has a latent topic structure that can reflect the topic coherence pattern, and the model adapts its parameters to the segments to further improve performance."}
{"citing_paper_id": "P11-1034", "cited_paper_id": "D08-1049", "citing_paper_abstract": "This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker?s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.", "cited_paper_abstract": "We investigate the combination of several sources of information for the purpose of subjectivity recognition and polarity classification in meetings. We focus on features from two modalities, transcribed words and acoustics, and we compare the performance of three different textual representations: words, characters, and phonemes. Our experiments show that character-level features outperform wordlevel features for these tasks, and that a careful fusion of all features yields the best performance. 1", "citation": "Only a handful studies have used conversational speech for opinion recognition #OTHEREFR; #REFR, in which some domain-specific features are utilized such as structural features and prosodic features.", "context": "Most previous work in sentiment analysis has focused on reviews #OTHEREFR. Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection.[Citation]Our work is also related to question answering #OTHEREFR answers some specific opinion questions like ?Why do people criticize Richard Branson?? by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker?s opinion towards a given topic. There exists some work on opinion summarization."}
{"citing_paper_id": "D10-1068", "cited_paper_id": "D08-1050", "citing_paper_abstract": "Parser disambiguation with precision grammars generally takes place via statistical ranking of the parse yield of the grammar using a supervised parse selection model. In the standard process, the parse selection model is trained over a hand-disambiguated treebank, meaning that without a significant investment of effort to produce the treebank, parse selection is not possible. Furthermore, as treebanking is generally streamlined with parse selection models, creating the initial treebank without a model requires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these HPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources.", "cited_paper_abstract": "Most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains, making parser adaptation a pressing issue. In this paper we demonstrate that a CCG parser can be adapted to two new domains, biomedical text and questions for a QA system, by using manually-annotated training data at the POS and lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation.", "citation": "There has also been some work on using lexical probabilities for domain adaptation of a model #OTHEREFR; #REFR.", "context": "Dalrymple #OTHEREFR both look at how discriminatory a tag sequence is in filtering a parse forest. This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh #OTHEREFR, with p < 0.05. work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al #OTHEREFR present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features.[Citation]In Dridan #OTHEREFR, tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high. We use a similar method, one level removed, in that we use the tag sequences to select the ?gold? parse(s) that are then used to train a model, as in the previous sections."}
{"citing_paper_id": "P09-2022", "cited_paper_id": "D08-1055", "citing_paper_abstract": "This paper presents a predicate-argument structure analysis that simultaneously conducts zero-anaphora resolution. By adding noun phrases as candidate arguments that are not only in the sentence of the target predicate but also outside of the sentence, our analyzer identifies arguments regardless of whether they appear in the sentence or not. Because we adopt discriminative models based on maximum entropy for argument identification, we can easily add new features. We add language model scores as well as contextual features. We also use contextual information to restrict candidate arguments.", "cited_paper_abstract": "This paper describes a new automatic method for Japanese predicate argument structure analysis. The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types, words, semantic categories, parts of speech, functional words and predicate voices. We constructed decision lists in which these features were sorted by their learned weights. Using our method, we integrated the tasks of semantic role labeling and zero-pronoun identification, and achieved a 17% improvement compared with a baseline method in a sentence level performance analysis.", "citation": "Compared with #REFR, they were higher in the nominative and accusative cases but were lower in the dative case.", "context": "This table shows accuracies of the argument identification according to each case and each dependency relation between predicates and arguments. The predicate-argument accuracy on the test set was 59.4% (15,140/25,500). First, focusing on the F scores of the Dep. relations, which denote a predicate and an argument in the same sentence and directly depend upon each other, scores of over 80% were obtained for all cases.[Citation]Overall, we obtained F scores between 73.2% and 89.2%. Next, focusing on the intra-sentential (Zero- Intra) and inter-sentential (Zero-Intra) zeroanaphora, the analyzer identified arguments at some level from the viewpoint of precision. However, the recall rates and F scores were very low."}
{"citing_paper_id": "W11-1605", "cited_paper_id": "D08-1057", "citing_paper_abstract": "In our work we use an existing classifier to quantify and analyze the level of specific and general content in news documents and their human and automatic summaries. We discover that while human abstracts contain a more balanced mix of general and specific content, automatic summaries are overwhelmingly specific. We also provide an analysis of summary specificity and the summary quality scores assigned by people. We find that too much specificity could adversely affect the quality of content in the summary. Our findings give strong evidence for the need for a new task in abstractive summarization: identification and generation of general sentences.", "cited_paper_abstract": "We examine the problem of content selection in statistical novel sentence generation. Our approach models the processes performed by professional editors when incorporating material from additional sentences to support some initially chosen key summary sentence, a process we refer to as Sentence Augmentation. We propose and evaluate a method called ?Seed and Grow? for selecting such auxiliary information. Additionally, we argue that this can be performed using schemata, as represented by word-pair co-occurrences, and demonstrate its use in statistical summary sentence generation. Evaluation results are supportive, indicating that a schemata model significantly improves over the baseline.", "citation": "However, #REFR introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source.", "context": "The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende #OTHEREFR use some heuristic proportions. Many systems that deal with sentence compression #OTHEREFR, do not take into account the specificity of the original or desired sentence.[Citation]More recently, Marsi et al #OTHEREFR manually annotated the transformations between source and compressed phrases and observe that generalization is a frequent transformation. But it is not known what distribution of general and specific content is natural for summaries. In addition, an analysis of whether this aspect is related to quality of the summary has also not been done so far."}
{"citing_paper_id": "P13-2111", "cited_paper_id": "D08-1059", "citing_paper_abstract": "Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ?2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences.", "cited_paper_abstract": "Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beamsearch. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.", "citation": "Beam search incremental parsers #OTHEREFR; #REFR provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars).", "context": "[Citation]In terms of purning strategies, they can be broadly divided into two categories: the first group #OTHEREFR which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ?Supported in part by DARPA FA8750-13-2-0041 (DEFT). belief, in most standard implementations their actual runtime is in fact O(kn2) rather than linear. Although this argument in general also applies to dynamic programming #OTHEREFR) and it benefits more from our improved algorithms."}
{"citing_paper_id": "P14-1021", "cited_paper_id": "D08-1059", "citing_paper_abstract": "This paper presents the first dependency model for a shift-reduce CCG parser. Modelling dependencies is desirable for a number of reasons, including handling the ?spurious? ambiguity of CCG; fitting well with the theory of CCG; and optimizing for structures which are evaluated at test time. We develop a novel training technique using a dependency oracle, in which all derivations are hidden. A challenge arises from the fact that the oracle needs to keep track of exponentially many goldstandard derivations, which is solved by integrating a packed parse forest with the beam-search decoder. Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing, competitive CCG parsing models.", "cited_paper_abstract": "Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beamsearch. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively.", "citation": "The decoder is based on beam-search #REFR with the advantage of linear-time decoding #OTHEREFR.", "context": "In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG #OTHEREFR, and the left-to-right, incremental nature of the decoding fits with CCG?s cognitive claims. The discriminative model is global and trained with the structured perceptron.[Citation]A main contribution of the paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time."}
{"citing_paper_id": "W10-1756", "cited_paper_id": "D08-1065", "citing_paper_abstract": "We present a unified approach to performing minimum risk training and minimum Bayes risk (MBR) decoding with BLEU in a phrase-based model. Key to our approach is the use of a Gibbs sampler that allows us to explore the entire probability distribution and maintain a strict probabilistic formulation across the pipeline. We also describe a new sampling algorithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereof. Our approach is theoretically sound and gives better (up to +0.6%BLEU) and more stable results than the standard MERT optimization algorithm. By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods.", "cited_paper_abstract": "We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al, 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.", "citation": "Recently, #REFR have shown that using a search lattice to improve the estimation of the true probability distribution can lead to improved MBR performance.", "context": "The second step is to find the correct scale factor for the scores using a hyper-parameter search over held-out data. This is needed because the model parameters for the first-pass decoder are normally learnt using MERT #OTHEREFR, which is invariant under scaling of the scores. Both these steps are theoretically unsatisfactory methods of estimating the posterior probability distribution since the approximation to Z is an unbounded term and the scaling factor is an artificial way of inducing a probability distribution.[Citation]However, these approaches still rely on MERT for training the base model, and in fact introduce several extra parameters which must also be estimated using either grid search or a second MERT run. The lattice pruning required to make these techniques tractable is quite drastic, and is in addition to the pruning already performed during the search. Such extensive pruning is liable to render any probability estimates heavily biased #OTHEREFR."}
{"citing_paper_id": "D10-1057", "cited_paper_id": "D08-1070", "citing_paper_abstract": "Domain adaptation, the problem of adapting a natural language processing system trained in one domain to perform well in a different domain, has received significant attention. This paper addresses an important problem for deployed systems that has received little attention ? detecting when such adaptation is needed by a system operating in the wild, i.e., performing classification over a stream of unlabeled examples. Our method uses A- distance, a metric for detecting shifts in data streams, combined with classification margins to detect domain shifts. We empirically show effective domain shift detection on a variety of data sets and shift conditions.", "cited_paper_abstract": "We present a novel learning framework for pipeline models aimed at improving the communication between consecutive stages in a pipeline. Our method exploits the confidence scores associated with outputs at any given stage in a pipeline in order to compute probabilistic features used at other stages downstream. We describe a simple method of integrating probabilistic features into the linear scoring functions used by state of the art machine learning algorithms. Experimental evaluation on dependency parsing and named entity recognition demonstrate the superiority of our approach over the baseline pipeline models, especially when upstream stages in the pipeline exhibit low accuracy.", "citation": "Pipeline models using confidence estimates at one stage as weights for further downstream stages improve over baseline dependency parsing and named entity recognition pipeline models #REFR.", "context": "EM-based confidence estimation has been used to estimate the confidence of patterns derived from partially supervised relation extraction #OTHEREFR. Confidence estimation has also been used to improve the overall effectiveness of NLP systems. Confidence estimates obtained via neural networks have shown gains for speech recognition, spoken language understanding, and machine translation #OTHEREFR.[Citation]An alternative formulation of domain adaptation trains on different corpora from many different domains, then uses linear combinations of models trained on the different corpora#OTHEREFR. Work in novelty detection is relevant to the task of detecting domain shifts #OTHEREFRCWPM0 Dom ain C lassif ier 0 50 100 150 200 250 300CWPM0 Dom ain C lassif ier Figure 6: A-distance over a stream of 1s and 0s produced by a supervised classifier trained to differentiate between the source and target domain."}
{"citing_paper_id": "C10-1124", "cited_paper_id": "D08-1076", "citing_paper_abstract": "A distributed system is described that reliably mines parallel text from large corpora. The approach can be regarded as cross-language near-duplicate detection, enabled by an initial, low-quality batch translation. In contrast to other approaches which require specialized metadata, the system uses only the textual content of the documents. Results are presented for a corpus of over two billion web pages and for a large collection of digitized public-domain books.", "cited_paper_abstract": "Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N - best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared to N -best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N -best MERT.", "citation": "Minimum error rate training #REFR under the BLEU criterion is used to optimize the feature function weights on development data consisting of the nv-dev2007 and news-dev2009 data sets provided by the organizers of the 2007 and 2009 WMT shared translation tasks1.", "context": "To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem #OTHEREFR. We train the systems on the Europarl Corpus #OTHEREFR.[Citation]We use a 4-gram language model trained on a variety of large monolingual corpora. The BLEU scores of our baseline translation system 1available at http://statmt.org on the test sets from various WMT shared translation tasks are listed in Table 5. An empirical analysis of the impact of the baseline translation system quality on the data mining system is given in Section 6.3."}
{"citing_paper_id": "D10-1059", "cited_paper_id": "D08-1076", "citing_paper_abstract": "Minimum Error Rate Training is the algorithm for log-linear model parameter training most used in state-of-the-art Statistical Machine Translation systems. In its original formulation, the algorithm uses N-best lists output by the decoder to grow the Translation Pool that shapes the surface on which the actual optimization is performed. Recent work has been done to extend the algorithm to use the entire translation lattice built by the decoder, instead of N-best lists. We propose here a third, intermediate way, consisting in growing the translation pool using samples randomly drawn from the translation lattice. We empirically measure a systematic improvement in the BLEU scores compared to training using N-best lists, without suffering the increase in computational complexity associated with operating with the whole lattice.", "cited_paper_abstract": "Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N - best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared to N -best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N -best MERT.", "citation": "Recognizing this shortcoming, #REFR extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it.", "context": "The MERT algorithm suffers from the following problem: it assumes at each iteration that the set of candidates with a chance to make it to the top (for some value of the parameter vector) is well represented in the translation pool. If the translation pool is formed in the standard way by merging N-best lists, this assumption is easily violated in practice. Indeed, the N-best list often contains only candidates displaying minor differences, and represents only a very small sample of alternative possible translations, strongly biased by the current parameter setting.[Citation]This is achieved via an elegant but relatively heavy dynamic programming algorithm that propagates sufficient statistics (called envelopes) throughout the whole search graph. The reported theoretical worst-case complexity of this algorithm is O(|V ||E| log |E|), where V and E are the vertex set and the edge set of the lattice respectively. We propose here an alternative method consisting in sampling a list of candidate translations from the probability distribution induced by the translation lattice."}
{"citing_paper_id": "W09-0434", "cited_paper_id": "D08-1078", "citing_paper_abstract": "Reordering is a serious challenge in statistical machine translation. We propose a method for analysing syntactic reordering in parallel corpora and apply it to understanding the differences in the performance of SMT systems. Results at recent large-scale evaluation campaigns show that synchronous grammar-based statistical machine translation models produce superior results for language pairs such as Chinese to English. However, for language pairs such as Arabic to English, phrasebased approaches continue to be competitive. Until now, our understanding of these results has been limited to differences in BLEU scores. Our analysis shows that current state-of-the-art systems fail to capture the majority of reorderings found in real data.", "cited_paper_abstract": "The performance of machine translation systems varies greatly depending on the source and target languages involved. Determining the contribution of different characteristics of language pairs on system performance is key to knowing what aspects of machine translation to improve and which are irrelevant. This paper investigates the effect of different explanatory variables on the performance of a phrase-based system for 110 European language pairs. We show that three factors are strong predictors of performance in isolation: the amount of reordering, the morphological complexity of the target language and the historical relatedness of the two languages. Together, these factors contribute 75% to the variability of the performance of the system.", "citation": "#REFR proposed a method for extracting reorderings from aligned parallel sentences.We extend this method in order to constrain the reorderings to a derivation over the source sentence where possible.", "context": "They show that the hierarchical models do slightly better for Chinese-English systems, but worse for Arabic-English. However, there was no analysis of the reorderings existing in their parallel corpora, or on what kinds of reorderings were produced in their output. We perform a focused evaluation of these issues.[Citation]"}
{"citing_paper_id": "P10-1154", "cited_paper_id": "D08-1080", "citing_paper_abstract": "One of the main obstacles to highperformance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervisedWSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.", "cited_paper_abstract": "Information of interest to users is often distributed over a set of documents. Users can specify their request for information as a query/topic ? a set of one or more sentences or questions. Producing a good summary of the relevant information relies on understanding the query and linking it with the associated set of documents. To ?understand? the query we expand it using encyclopedic knowledge in Wikipedia. The expanded query is linked with its associated documents through spreading activation in a graph that represents words and their grammatical connections in these documents. The topic expanded words and activated nodes in the graph are used to produce an extractive summary. The method proposed is tested on the DUC summarization data. The system implemented ranks high compared to the participating systems in the DUC competitions, confirming our hypothesis that encyclopedic knowledge is a useful addition to a summarization system.", "citation": "Applications using the knowledge contained in Wikipedia include, among others, text categorization #OTHEREFRb), multi-document summarization #REFR, and text generation #OTHEREFR.", "context": "However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction #OTHEREFR.[Citation]In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas?ca #OTHEREFR that pages in Wikipedia can be taken as word senses. Mihalcea #OTHEREFR manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD."}
{"citing_paper_id": "D13-1056", "cited_paper_id": "D08-1084", "citing_paper_abstract": "We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model?s alignment score approaches the state of the art.", "cited_paper_abstract": "The alignment problem?establishing links between corresponding phrases in two related sentences?is as important in natural language inference (NLI) as it is in machine translation (MT). But the tools and techniques of MT alignment do not readily transfer to NLI, where one cannot assume semantic equivalence, and for which large volumes of bitext are lacking. We present a new NLI aligner, the MANLI system, designed to address these challenges. It uses a phrase-based alignment representation, exploits external lexical resources, and capitalizes on a new set of supervised training data. We compare the performance of MANLI to existing NLI and MT aligners on an NLI alignment task over the well-known Recognizing Textual Entailment data. We show that MANLI significantly outperforms existing aligners, achieving gains of 6.2% in F1 over a representative NLI aligner and 10.5% over GIZA++.", "citation": "The MANLI aligner #REFR and its derivations #OTHEREFR are the first known phrasebased aligners specifically designed for aligning English sentence pairs.", "context": "Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance #OTHEREFR. These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step.[Citation]It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown #OTHEREFR through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding."}
{"citing_paper_id": "P11-2005", "cited_paper_id": "D08-1087", "citing_paper_abstract": "We investigate the empirical behavior of ngram discounts within and across domains. When a language model is trained and evaluated on two corpora from exactly the same domain, discounts are roughly constant, matching the assumptions of modified Kneser-Ney LMs. However, when training and test corpora diverge, the empirical discount grows essentially as a linear function of the n-gram count. We adapt a Kneser-Ney language model to incorporate such growing discounts, resulting in perplexity improvements over modified Kneser-Ney and Jelinek-Mercer baselines.", "cited_paper_abstract": "In domains with insufficient matched training data, language models are often constructed by interpolating component models trained from partially matched corpora. Since the ngrams from such corpora may not be of equal relevance to the target domain, we propose an n-gram weighting technique to adjust the component n-gram probabilities based on features derived from readily available segmentation and metadata information for each corpus. Using a log-linear combination of such features, the resulting model achieves up to a 1.2% absolute word error rate reduction over a linearly interpolated baseline language model on a lecture transcription task.", "citation": "#REFR employ a log-linear model for multiplicatively discounting n-grams in Kneser-Ney; when they include the logcount of an n-gram as the only feature, they achieve 75% of their overall word error rate reduction, suggesting that predicting discounts based on n-gram count can substantially improve the model.", "context": "In the held-out experiments of section 2.1, growing discounts only emerge when one evaluates against a dissimilar held-out corpus, whereas his model would predict discount growth even in NYT95/NYT95?, where we do not observe it. Adaptation across corpora has also been addressed before. Bellegarda #OTHEREFR to using explicit models of syntax or semantics.[Citation]Their work also improves on the second assumption of Kneser-Ney, that of the inadequacy of the average empirical discount as a discount constant, by employing various other features in order to provide other criteria on which to discount n-grams. Taking a different approach, both Klakow #OTHEREFR use subsampling to select the domain-relevant portion of a large, general corpus given a small in-domain corpus. This can be interpreted as a form of hard discounting, and implicitly models both growing discounts, since frequent n-grams will appear in more of the rejected sentences, and nonuniform discounting over n-grams of each count, since the sentences are chosen according to a likelihood criterion."}
{"citing_paper_id": "D12-1109", "cited_paper_id": "D08-1089", "citing_paper_abstract": "Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.", "cited_paper_abstract": "While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).", "citation": "#REFR use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering.", "context": "For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al#OTHEREFR proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree.[Citation]This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly."}
{"citing_paper_id": "P13-1156", "cited_paper_id": "D08-1089", "citing_paper_abstract": "Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called Phrasal- Hiero to address Hiero?s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding discontinuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for Arabic- English, Chinese-English and German- English translation.", "cited_paper_abstract": "While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).", "citation": "In all experiments we use phrase-orientation lexicalized reordering #REFR2 which models monotone, swap, discontinuous orientations from both reordering with previous phrase pair and with the next phrase pair.", "context": "[Citation]There are total six features in lexicalized reordering model. We will report the impact of integrating phrasebased features into Hiero systems for three language pairs: Arabic-English, Chinese-English and German-English."}
{"citing_paper_id": "W12-3125", "cited_paper_id": "D08-1089", "citing_paper_abstract": "The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al, 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output.", "cited_paper_abstract": "While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).", "citation": "Thus far, they have been used to enable a hierarchical re-ordering model, or HRM #REFR, as well as an ITG constraint #OTHEREFR.", "context": "Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities #OTHEREFR. Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders. These efficient parsers analyze the sequence of phrases used to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions.[Citation]We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering. We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning?s solution from O(n4) to O(n2)."}
{"citing_paper_id": "W12-3126", "cited_paper_id": "D08-1089", "citing_paper_abstract": "Statistical phrase-based machine translation requires no linguistic information beyond word-aligned parallel corpora (Zens et al, 2002; Koehn et al, 2003). Unfortunately, this linguistic agnosticism often produces ungrammatical translations. Syntax, or sentence structure, could provide guidance to phrasebased systems, but the ?non-constituent? word strings that phrase-based decoders manipulate complicate the use of most recursive syntactic tools. We address these issues by using Combinatory Categorial Grammar, or CCG, (Steedman, 2000), which has a much more flexible notion of constituency, thereby providing more labels for putative nonconstituent multiword translation phrases. Using CCG parse charts, we train a syntactic analogue of a lexicalized reordering model by labelling phrase table entries with multiword labels and demonstrate significant improvements in translating between Urdu and English, two language pairs with divergent sentence structure.", "cited_paper_abstract": "While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).", "citation": "In future work we would like explore whether further improvements can be gained by using more sophisticated reordering models, such as reordering graphs #OTHEREFR and hierarchical reordering models #REFR both for our word-based and syntactic reordering models.", "context": "The result is a significant improvement in Urdu-English (SOV . SVO) translation scores over two baselines: a traditional phrase-based baseline with a lexicalized reordering model and a phrase-based baseline with an additional supertag reordering model. Moreover, we have provided qualitative examples that confirm the improvements in automatic metrics.[Citation]Further, as in prior work #OTHEREFR.tar.gz. 13We exclude combination entries that are combinations of multiple systems with different algorithmic approaches. et al, 2010; Almaghout et al, 2010), our categorial labels could also be used to derive CCG-augmented SCFG rules, both lexicalized and unlexicalized, cf. #OTHEREFR ? the latter being the SCFG analogue of our current model. Acknowledgments The authors would like to thank Chong Min Lee, Aoife Cahill and Nitin Madnani at ETS for taking the time to read earlier drafts of this (and closely related) work."}
{"citing_paper_id": "W14-3325", "cited_paper_id": "D08-1089", "citing_paper_abstract": "This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi- English translation task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), Multi- Alignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task.", "cited_paper_abstract": "While phrase-based statistical machine translation systems currently deliver state-of-theart performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05).", "citation": "Starting from this baseline system, we exploit various methods including Context- Informed PB-SMT #OTHEREFR; #REFR, various Multiple Alignment Combination #OTHEREFR and Language Model Interpolation(LMI).", "context": "This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model #OTHEREFR.[Citation]In the next section, the preprocessing steps are explained. In Section 3 a detailed explanation of the technique we exploit is provided. Then in Section 4, we provide our experimental results and resultant discussion."}
{"citing_paper_id": "N09-2024", "cited_paper_id": "D08-1090", "citing_paper_abstract": "The paper presents a novel sentence pair extraction algorithm for comparable data, where a large set of candidate sentence pairs is scored directly at the sentence-level. The sentencelevel extraction relies on a very efficient implementation of a simple symmetric scoring function: a computation speed-up by a factor of 30 is reported. On Spanish-English data, the extraction algorithm finds the highest scoring sentence pairs from close to 1 trillion candidate pairs without search errors. Significant improvements in BLEU are reported by including the extracted sentence pairs into the training of a phrase-based SMT (Statistical Machine Translation) system.", "cited_paper_abstract": "Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model. While bi-lingual parallel data are expensive to generate, monolingual data are relatively common. Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language. This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. The method exploits the existence of comparable text?multiple texts in the target language that discuss the same or similar stories as found in the source language document. For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to the source documents. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system.", "citation": "It differs from similar algorithms that select translation correspondences explicitly at the document level #OTHEREFR; #REFR.", "context": "The paper presents a simple sentence-level translation pair extraction algorithm from comparable monolingual news data.[Citation]In these publications, the authors use Information-Retrieval (IR) techniques to match document pairs that are likely translations of each other. More complex sentence-level models are then used to extract parallel sentence pairs (or fragments). From a computational perspective, the document-level filtering steps are needed to reduce the number of candidate sentence pairs."}
{"citing_paper_id": "W09-1120", "cited_paper_id": "D08-1093", "citing_paper_abstract": "The average results obtained by unsupervised statistical parsers have greatly improved in the last few years, but on many specific sentences they are of rather low quality. The output of such parsers is becoming valuable for various applications, and it is radically less expensive to create than manually annotated training data. Hence, automatic selection of high quality parses created by unsupervised parsers is an important problem. In this paper we present PUPA, a POS-based Unsupervised Parse Assessment algorithm. The algorithm assesses the quality of a parse tree using POS sequence statistics collected from a batch of parsed sentences. We evaluate the algorithm by using an unsupervised POS tagger and an unsupervised parser, selecting high quality parsed sentences from English (WSJ) and German (NEGRA) corpora. We show that PUPA outperforms the leading previous parse assessment algorithm for supervised parsers, as well as a strong unsupervised baseline. Consequently, PUPA allows obtaining high quality parses without any human involvement.", "cited_paper_abstract": "Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications. However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain.", "citation": "Assessing the quality of a learning algorithm?s output and selecting high quality instances has been addressed for supervised algorithms #OTHEREFR; #REFR.", "context": "When requirements are relaxed, only asking for an F-score higher than 85%, percentage is still low, 42% for WSJ10 and 15% for NEGRA10. In this paper we address the task of a fully unsupervised assessment of high quality parses created by an unsupervised parser. The assessment should be unsupervised in order to avoid the problems mentioned above with manually trained supervised parsers.[Citation]Moreover, it has been shown to be valuable for supervised parser adaptation between domains #OTHEREFR. However, as far as we know the present paper is the first to address the task of unsupervised assessment of the quality of parses created by unsupervised parsers. Our POS-based Unsupervised Parse Assessment (PUPA) algorithm uses statistics about POS tag sequences in a batch of parsed sentences1."}
{"citing_paper_id": "C14-1165", "cited_paper_id": "D08-1094", "citing_paper_abstract": "We present a novel approach to the problem of multilingual conceptual metaphor recognition. Our approach extends recent work in conceptual metaphor discovery by combining a complex methodology for facet-based concept induction with a distributional vector space model of linguistic and conceptual metaphor. In the evaluation of our system in English, Spanish, Russian, and Farsi, we experiment with several state-of-the-art vector space models and demonstrate a clear benefit to the fine-grained concept representation that forms the basis of our methodology for conceptual metaphor recognition.", "cited_paper_abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words? argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.", "citation": "In this section, we describe our methodology for inducing conceptual domains for a linguistic metaphor by adapting techniques for unsupervised word-sense induction #REFR.", "context": "In Section 4, we described our DepVec representation of terms as vectors in a high-dimensional distributional space. These vector representations encode both the dominant grammatical contexts of a term as well as the selectional preference information associated with it in the form of G-test scores.[Citation]In particular, we induce conceptual domains in an unconstrained manner by extracting the grammatical co-occurrents of an LM source term (i.e., the ?concept candidates?) and clustering them into semantically-related concept clusters. Both the clusters and our http://www.cis.uni-muenchen.de/ ? schmid/tools/TreeTagger/ http://code.google.com/p/hunpos/ given source domains are then mapped into a distributional vector space, allowing us to compute clusterto-domain scores. Finally, each source domain is assigned a score based on its affinity to each individual cluster with these affinity scores weighted according to cluster quality."}
{"citing_paper_id": "D10-1115", "cited_paper_id": "D08-1094", "citing_paper_abstract": "We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.", "cited_paper_abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words? argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.", "citation": "#REFR adopt the same formalism but focus on the nature of input vectors, suggesting that when a verb is composed with a noun, the noun component is given by an average of verbs that the noun is typically object of #OTHEREFR also focused on composite input vectors, within an additive framework).", "context": "They also evaluate a weighted combination of the simplified additive and multiplicative functions. The best results on the task of paraphrasing noun-verb combinations with ambiguous verbs (sales slump is more like declining than slouching) are obtained using the multiplicative approach, and by weighted combination of addition and multiplication (we do not test model combinations in our current experiments). The multiplicative approach also performs best #OTHEREFR.[Citation]Again, the multiplicative model works best in Erk and Pado??s experiments. The above-mentioned researchers do not exploit corpus evidence about the p vectors that result from composition, despite the fact that it is straightforward (at least for short constructions) to extract direct distributional evidence about the composite items from the corpus (just collect co-occurrence information for the composite item from windows around the contexts in which it occurs). The main innovation of Guevara #OTHEREFR, who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara?s approach after we had developed our own model, that also exploits observed ANs for training)."}
{"citing_paper_id": "E12-1005", "cited_paper_id": "D08-1094", "citing_paper_abstract": "A major focus of current work in distributional models of semantics is to construct phrase representations compositionally from word representations. However, the syntactic contexts which are modelled are usually severely limited, a fact which is reflected in the lexical-level WSD-like evaluation methods used. In this paper, we broaden the scope of these models to build sentence-level representations, and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks. We find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference.", "cited_paper_abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words? argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.", "citation": "E&P #REFR introduce a structured vector space model which uses syntactic dependencies to model the selectional preferences of words.", "context": "M&L Mitchell and Lapata #OTHEREFR propose a framework for compositional distributional semantics using a standard term-context vector space word representation. A phrase is represented as a vector of context-word counts (actually, pmi-scaled values), which is derived compositionally by a function over constituent vectors, such as component-wise addition or multiplication. This model ignores syntactic relations and is insensitive to word-order.[Citation]The vector representation of a word in context depends on the inverse selectional preferences of its dependents, and the selectional preferences of its head. For example, suppose catch occurs with a dependent ball in a direct object relation. The vector for catch would then be influenced by the inverse direct object preferences of ball (e.g. throw, organize), and the vector for ball would be influenced by the selectional preferences of catch (e.g. cold, drift)."}
{"citing_paper_id": "P10-2017", "cited_paper_id": "D08-1094", "citing_paper_abstract": "This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.", "cited_paper_abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words? argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.", "citation": "This problem has typically been approached by modifying the type vector for a target to better match a given context #OTHEREFR; #REFR.", "context": "Typically, distributional models compute a single ?type? vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the top 20 features for coach, we get match and team (for the ?trainer? sense) as well as driver and car (for the ?bus? sense).[Citation]In the terms of research on human concept representation, which often employs feature vector representations, the use of type vectors can be understood as a prototype-based approach, which uses a single vector per category. From this angle, computing prototypes throws away much interesting distributional information. A rival class of models is that of exemplar models, which memorize each seen instance of a category and perform categorization by comparing a new stimulus to each remembered exemplar vector."}
{"citing_paper_id": "W09-0201", "cited_paper_id": "D08-1094", "citing_paper_abstract": "We propose an approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-specific semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks.", "cited_paper_abstract": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. We present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words? argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases.", "citation": "We plan to explore how contextual effects can be modeled in our framework, focusing in particular on how composition affects word meaning #REFR.", "context": "Other tasks should also be explored. Here, we viewed our distributional memory in line with how cognitive scientists look at the semantic memory of healthy adults, i.e., as an essentially stable long term knowledge repository. However, much interesting semantic action takes place when underlying knowledge is adapted to context.[Citation]Similarity could be measured directly on the underlying graph, by relying on graph-based similarity algorithms ? an elegant approach that would lead us to an even more unitary view of what distributional semantic memory is and what it does. Alternatively, DM could be represented as a three-mode tensor in the framework of Turney #OTHEREFR, enabling smoothing operations analogous to singular value decomposition. Acknowledgments We thank Ken McRae and Peter Turney for providing data-sets, Amac."}
{"citing_paper_id": "W12-4104", "cited_paper_id": "D08-1095", "citing_paper_abstract": "We learn graph-based similarity measures for the task of extracting word synonyms from a corpus of parsed text. A constrained graph walk variant that has been successfully applied in the past in similar settings is shown to outperform a state-of-the-art syntactic vectorbased approach on this task. Further, we show that learning specialized similarity measures for different word types is advantageous.", "cited_paper_abstract": "We consider a parsed text corpus as an instance of a labelled directed graph, where nodes represent words and weighted directed edges represent the syntactic relations between them. We show that graph walks, combined with existing techniques of supervised learning, can be used to derive a task-specific word similarity measure in this graph. We also propose a new path-constrained graph walk method, in which the graph walk process is guided by high-level knowledge about meaningful edge sequences (paths). Empirical evaluation on the task of named entity coordinate term extraction shows that this framework is preferable to vector-based models for smallsized corpora. It is also shown that the pathconstrained graph walk algorithm yields both performance and scalability gains.", "citation": "In conducting the constrained walk, we applied a threshold of 0.5 to truncate paths associated with lower probability of reaching a relevant response, following on previous work #REFR.", "context": "The path trees were constructed using the paths leading to the node known to be a correct answer, as well as to the otherwise irrelevant top-ranked 10 terms. We required the paths considered by PCW to include exactly 6 segments (edges). Such paths represent distributional similarity phenomena, allowing a direct comparison against the DV method.[Citation]We implemented DV using code made available by its authors,3 where we converted the syntactic patterns specified to Stanford dependency parser conventions. The parameters of the DV method were set to medium context and oblique edge weighting scheme, which were found to perform best #OTHEREFR. In applying a vector-space based method, a similarity score needs to be computed between every candidate from the corpus and the query term to construct a ranked list."}
{"citing_paper_id": "N09-1010", "cited_paper_id": "D08-1109", "citing_paper_abstract": "We investigate the problem of unsupervised part-of-speech tagging when raw parallel data is available in a large number of languages. Patterns of ambiguity vary greatly across languages and therefore even unannotated multilingual data can serve as a learning signal. We propose a non-parametric Bayesian model that connects related tagging decisions across languages through the use of multilingual latent variables. Our experiments show that performance improves steadily as the number of languages increases.", "cited_paper_abstract": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The key hypothesis of multilingual learning is that by combining cues from multiple languages, the structure of each becomes more apparent. We formulate a hierarchical Bayesian model for jointly predicting bilingual streams of part-of-speech tags. The model learns language-specific features while capturing cross-lingual patterns in tag distribution for aligned words. Once the parameters of our model have been learned on bilingual parallel data, we evaluate its performance on a held-out monolingual test set. Our evaluation on six pairs of languages shows consistent and significant performance gains over a state-of-the-art monolingual baseline. For one language pair, we observe a relative reduction in error of 53%.", "citation": "In recent work, #REFR presented a model for unsupervised part-of-speech tagging trained from a bilingual parallel corpus.", "context": "Bilingual Part-of-Speech Tagging Early work on multilingual tagging focused on projecting annotations from an annotated source language to a target language #OTHEREFR. In contrast, we assume no labeled data at all; our unsupervised model instead symmetrically improves performance for all languages by learning cross-lingual patterns in raw parallel data. An additional distinction is that projection-based work utilizes pairs of languages, while our approach allows for continuous improvement as languages are added to the mix.[Citation]This bilingual model and the model presented here share a number of similarities: both are Bayesian graphical models building upon hidden Markov models. However, the bilingual model explicitly joins each aligned word-pair into a single coupled state. Thus, the state-space of these joined nodes grows exponentially in the number of languages."}
{"citing_paper_id": "D14-1097", "cited_paper_id": "D08-1112", "citing_paper_abstract": "Active learning (AL) consists of asking human annotators to annotate automatically selected data that are assumed to bring the most benefit in the creation of a classifier. AL allows to learn accurate systems with much less annotated data than what is required by pure supervised learning algorithms, hence limiting the tedious effort of annotating a large collection of data. We experimentally investigate the behavior of several AL strategies for sequence labeling tasks (in a partially-labeled scenario) tailored on Partially-Labeled Conditional Random Fields, on four sequence labeling tasks: phrase chunking, part-of-speech tagging, named-entity recognition, and bioentity recognition.", "cited_paper_abstract": "Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.", "citation": "In particular, AL proved its usefulness in sequence labeling tasks #REFR.", "context": "AL has been demonstrated to work well and to produce accurate classifiers while saving much human annotation effort. One critical issue is to define a measure of the informativeness which should reflect how much new information a new example would give in the learning of a new classifier once annotated. A lot of work has been done on the AL field in the past years #OTHEREFR for an exhaustive overview).[Citation]Yet, researchers have always adopted as annotation unit an entire sequence (i.e., the annotator is asked to annotate the whole sequence) while it looks like it could be much more relevant to ask for labeling only small parts of it (e.g., the ones with highest ambiguity). A few works have investigated this idea. For instance, Wanvarie et al. #OTHEREFR able to deal with partially-labeled sequences, thus enabling to adopt as annotation unit single tokens and still learning from full sequences."}
{"citing_paper_id": "W11-0313", "cited_paper_id": "D09-1009", "citing_paper_abstract": "Feature feedback is an alternative to instance labeling when seeking supervision from human experts. Combination of instance and feature feedback has been shown to reduce the total annotation cost for supervised learning. However, learning problems may not benefit equally from feature feedback. It is well understood that the benefit from feature feedback reduces as the amount of training data increases. We show that other characteristics such as domain, instance granularity, feature space, instance selection strategy and proportion of relevant text, have a significant effect on benefit from feature feedback. We estimate the maximum benefit feature feedback may provide; our estimate does not depend on how the feedback is solicited and incorporated into the model. We extend the complexity measures proposed in the literature and propose some new ones to categorize learning problems, and find that they are strong indicators of the benefit from feature feedback.", "cited_paper_abstract": "Methods that learn from prior information about input features such as generalized expectation (GE) have been used to train accurate models with very little effort. In this paper, we propose an active learning approach in which the machine solicits ?labels? on features rather than instances. In both simulated and real user experiments on two sequence labeling tasks we show that our active learning method outperforms passive learning with features as well as traditional active learning with instances. Preliminary experiments suggest that novel interfaces which intelligently solicit labels on multiple features facilitate more efficient annotation.", "citation": "Prior work #OTHEREFR; #REFR has shown that a combination of instance and feature labeling can be used to reduce the total annotation cost required to learn the target concept.", "context": "An alternative approach is to seek indirect feedback on structured features #OTHEREFR. For example, when classifying the sentiment of a movie review, rationales are spans of text in the review that support the sentiment label for the review. Assuming a fixed cost per unit of work, it might be cheaper to ask the user to label a few features, i.e. identify relevant features and their class association, than to label several instances.[Citation]However, the benefit from feature feedback may vary across learning problems. If we can estimate the benefit from feature feedback for a given problem, we can minimize the total annotation cost for achieving the desired performance by selecting the optimal annotation strategy (feature feedback or not) at every stage in learning. In this paper, we present the ground work for this research problem by analyzing how benefit from feature feedback varies across different learning problems and what characteristics of a learning problem have a significant effect on benefit from feature feedback."}
{"citing_paper_id": "P11-1114", "cited_paper_id": "D09-1016", "citing_paper_abstract": "The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords. We propose a multilayered event extraction architecture that progressively ?zooms in? on relevant information. Our extraction model includes a document genre classifier to recognize event narratives, two types of sentence classifiers, and noun phrase classifiers to extract role fillers. These modules are organized as a pipeline to gradually zero in on event-related information. We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.", "cited_paper_abstract": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.", "citation": "GLACIER #REFR jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework.", "context": "Figure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. #OTHEREFR use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman #OTHEREFR use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. #OTHEREFR developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers.[Citation]Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event extraction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role , and (2) we exploit text genre to incorporate a third level of analysis that enables the system to aggressively hunt for role fillers in documents that are event narratives. In Section 5, we compare the performance of our model with both the GLACIER system and Patwardhan & Riloff?s semantic affinity model."}
{"citing_paper_id": "D11-1015", "cited_paper_id": "D09-1018", "citing_paper_abstract": "Polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis. This paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities. Firstly, a discourse scheme with discourse constraints on polarity was defined empirically based on Rhetorical Structure Theory (RST). Then, a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations (SSRs). Finally, an unsupervised method was adopted to generate, weigh and filter new SSRs without cue phrases for recognizing discourse relations. Experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification.", "cited_paper_abstract": "This work investigates design choices in modeling a discourse scheme for improving opinion polarity classification. For this, two diverse global inference paradigms are used: a supervised collective classification framework and an unsupervised optimization framework. Both approaches perform substantially better than baseline approaches, establishing the efficacy of the methods and the underlying discourse scheme. We also present quantitative and qualitative analyses showing how the improvements are achieved.", "citation": "The most closely related works were #OTHEREFR and #REFR, which proposed opinion frames as a representation of discourse-level associations on dialogue andmodeled the scheme to improve opinion polarity classification.", "context": "Nonetheless, contrastive relations were only one type of discourse relations which may help polarity classification. Sadamitsu et al #OTHEREFR modeled polarity reversal using HCRFs integrated with inter-sentence discourse structures. However, our work is on intrasentence level and our purpose is not to find polarity reversals but trying to adapt general discourse schemes (e.g., RST) to help determine the overall polarity of ambiguous sentences.[Citation]However, opinion frames was difficult to be implemented because the recognition of opinion target was very challenging in general text. Our work differs from their approaches in two key aspects: (1) we distinguished nucleus and satellite in discourse but opinion frames did not; (2) our method for discourse discovery was unsupervised while their method needed annotated data. Most research works about discourse classification were not related to sentiment analysis."}
{"citing_paper_id": "W11-2139", "cited_paper_id": "D09-1023", "citing_paper_abstract": "This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11). We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training.", "cited_paper_abstract": "We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle ?non-local? features. Similar approximate inference techniques support efficient parameter estimation with hidden variables. We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism.", "citation": "On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance #OTHEREFR; #REFR.", "context": "Improving phrase-based translation systems is challenging in part because our intuitions about what makes a ?good? phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance #OTHEREFR, although our intuitions might suggest this is a reasonable thing to do.[Citation]Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a ?correct? parse of the source sentence is under the translation grammar. Like the ?soft syntactic features? used in pre- 2Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work #OTHEREFR, we propose features to assess the tree structure induced during translation. However, unlike that work, we do not rely on linguistic source parses, but instead only make use of features that are directly computable from the source sentence and the parse structure being considered in the decoder."}
{"citing_paper_id": "C10-1095", "cited_paper_id": "D09-1025", "citing_paper_abstract": "This paper proposes a co-training style algorithm called Co-STAR that acquires hyponymy relations simultaneously from structured and unstructured text. In Co- STAR, two independent processes for hyponymy relation acquisition ? one handling structured text and the other handling unstructured text ? collaborate by repeatedly exchanging the knowledge they acquired about hyponymy relations. Unlike conventional co-training, the two processes in Co-STAR are applied to different source texts and training data. We show the effectiveness of this algorithm through experiments on largescale hyponymy-relation acquisition from Japanese Wikipedia and Web texts. We also show that Co-STAR is robust against noisy training data.", "cited_paper_abstract": "Combining information extraction systems yields significantly higher quality resources than each system in isolation. In this paper, we generalize such a mixing of sources and features in a framework called Ensemble Semantics. We show very large gains in entity extraction by combining state-of-the-art distributional and patternbased systems with a large set of features from a webcrawl, query logs, and Wikipedia. Experimental results on a webscale extraction of actors, athletes and musicians show significantly higher mean average precision scores (29% gain) compared with the current state of the art.", "citation": "They either tried to improve semantic relation acquisition by putting the different evidence together into a single classifier #REFR or to improve the coverage of semantic relations by combining and ranking the semantic relations obtained from two source texts #OTHEREFR.", "context": "Many algorithms have been developed to automatically acquire semantic relations from structured and unstructured text. Because term pairs are encoded in structured and unstructured text in different styles, different kinds of evidence have been used for semantic relation acquisition: Evidence from unstructured text: lexicosyntactic patterns and distributional similarity #OTHEREFR. Recently, researchers have used both structured and unstructured text for semantic-relation acquisition, with the aim of exploiting such different kinds of evidence at the same time.[Citation]In this paper we propose an algorithm called Co-STAR. The main contributions of this work can be summarized as follows. . Co-STAR is a semi-supervised learning method composed of two parallel and iterative processes over structured and unstructured text."}
{"citing_paper_id": "D10-1013", "cited_paper_id": "D09-1030", "citing_paper_abstract": "Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation that makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e. paraphrases) with only monolingual knowledge of the source language. Evaluations demonstrate that this approach can yield substantial improvements in translation quality.", "cited_paper_abstract": "Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon?s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.", "citation": "The value of this upper bound is quite consistent with the bound computed similarly by #REFR.", "context": "The TP oracle results establish that by taking advantage of monolingual human speakers, it is possible to obtain quite substantial gains in translation quality. The TP one-best results demonstrate that the majority of that oracle gain is obtained in automatic hypothesis selection, simply by selecting the paraphrase-based alternative translation with the highest translation score. The last line in Table 1 shows a human upper bound computed using the reference translations via cross validation; that is, for each of the four reference translations, we evaluate it as a hypothesized translation using the other three references as ground truth; these four scores were then averaged.[Citation]"}
{"citing_paper_id": "C10-2078", "cited_paper_id": "D09-1033", "citing_paper_abstract": "We investigate the effectiveness of different linguistic cues for distinguishing literal and non-literal usages of potentially idiomatic expressions. We focus specifically on features that generalize across different target expressions. While idioms on the whole are frequent, instances of each particular expression can be relatively infrequent and it will often not be feasible to extract and annotate a sufficient number of examples for each expression one might want to disambiguate. We experimented with a number of different features and found that features encoding lexical cohesion as well as some syntactic features can generalize well across idioms.", "cited_paper_abstract": "We propose a novel unsupervised approach for distinguishing literal and non-literal use of idiomatic expressions. Our model combines an unsupervised and a supervised classifier. The former bases its decision on the cohesive structure of the context and labels training data for the latter, which can then take a larger feature space into account. We show that a combination of both classifiers leads to significant improvements over using the unsupervised classifier alone.", "citation": "#REFR propose another unsupervised method which detects the presence or absence of cohesive links between the component words of the idiom and the surrounding discourse.", "context": "Cook et al exploit this behaviour and propose an unsupervised method in which an expression is classified as idiomatic if it occurs in canonical form and literal otherwise. Canonical forms are determined automatically using a statistical, frequency-based measure. Birke and Sarkar #OTHEREFR model literal vs. nonliteral classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set.[Citation]If such links can be found the expression is classified as literal otherwise as non-literal. Li and Sporleder #OTHEREFR later extended this work by combining the unsupervised classifier with a secondstage supervised classifier. Hashimoto and Kawahara #OTHEREFR."}
{"citing_paper_id": "N10-1039", "cited_paper_id": "D09-1033", "citing_paper_abstract": "We present a Gaussian Mixture model for detecting different types of figurative language in context. We show that this model performs well when the parameters are estimated in an unsupervised fashion using EM. Performance can be improved further by estimating the parameters from a small annotated data set.", "cited_paper_abstract": "We propose a novel unsupervised approach for distinguishing literal and non-literal use of idiomatic expressions. Our model combines an unsupervised and a supervised classifier. The former bases its decision on the cohesive structure of the context and labels training data for the latter, which can then take a larger feature space into account. We show that a combination of both classifiers leads to significant improvements over using the unsupervised classifier alone.", "citation": "The first set (idiom set) is taken from #REFR and consists of 3964 idiom occurrences (17 idiom types) which were manually labeled as ?literal? or ?figurative?.", "context": "We evaluate our method on two data sets.[Citation]The second data set (V+NP set), consists of a randomly selected sample of"}
{"citing_paper_id": "P10-1021", "cited_paper_id": "D09-1045", "citing_paper_abstract": "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.", "cited_paper_abstract": "In this paper we propose a novel statistical language model to capture long-range semantic dependencies. Specifically, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words. We also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones. The composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone. We also obtain perplexity reductions when integrating our models with a structured language model.", "citation": "Following #REFR, we constructed a simple semantic space based on cooccurrence statistics from the BLLIP training set.", "context": "We used a development corpus of 50,006 words and a test corpus of similar size. All words were converted to lowercase and numbers were replaced with the symbol ?num?. A vocabulary of 20,000 words was chosen and the remaining tokens were replaced with ?unk?.[Citation]We used the 2,000 most frequent word types as contexts and a symmetric five word window. Vector components were defined as in Equation (6). We also trained the LDA model on BLLIP, using the Gibb?s sampling procedure discussed in Griffiths et al #OTHEREFR."}
{"citing_paper_id": "W11-0404", "cited_paper_id": "D09-1046", "citing_paper_abstract": "There has been a great deal of excitement recently about using the ?wisdom of the crowd? to collect data of all kinds, quickly and cheaply (Howe, 2008; von Ahn and Dabbish, 2008). Snow et al (Snow et al, 2008) were the first to give a convincing demonstration that at least some kinds of linguistic data can be gathered from workers on the web more cheaply than and as accurately as from local experts, and there has been a steady stream of papers and workshops since then with similar results. e.g. (Callison-Burch and Dredze, 2010). Many of the tasks which have been successfully crowdsourced involve judgments which are similar to those performed in everyday life, such as recognizing unclear writing (von Ahn et al, 2008), or, for those tasks that require considerable judgment, the responses are usually binary or from a small set of responses, such as sentiment analysis (Mellebeek et al, 2010) or ratings (Heilman and Smith, 2010). Since the FrameNet process is known to be relatively expensive, we were interested in whether the FrameNet process of fine word sense discrimination and marking of dependents with semantic roles could be performed more cheaply and equally accurately using Amazon?s Mechanical Turk (AMT) or similar resources. We report on a partial success in this respect and how it was achieved.", "cited_paper_abstract": "Word sense disambiguation is typically phrased as the task of labeling a word in context with the best-fitting sense from a sense inventory such as WordNet. While questions have often been raised over the choice of sense inventory, computational linguists have readily accepted the bestfitting sense methodology despite the fact that the case for discrete sense boundaries is widely disputed by lexical semantics researchers. This paper studies graded word sense assignment, based on a recent dataset of graded word sense annotation.", "citation": "Such cases are pushing us toward trying to incorporate blending of senses into our paradigm, along the lines of #REFR.", "context": "I RIP up an old T-shirt of mine and offer it. The shirt is certainly damaged and almost certainly fragmented as a result of the same action. . . . the Oklahoma was RIPPED apart when seven torpedoes hit her. strictly speaking, the ship is caused to fragment, but the military purpose is to damage her beyond repair, if possible. And there are fairly often examples where the sentence in isolation is ambiguous: Rain RIPPED another piece of croissant, The sky RIPPED and hung in tatters , revealing plasterboard and lath behind.[Citation]"}
{"citing_paper_id": "P10-3010", "cited_paper_id": "D09-1052", "citing_paper_abstract": "We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification.", "cited_paper_abstract": "The recently introduced online confidence-weighted (CW) learning algorithm for binary classification performs well on many binary NLP tasks. However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case. We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus. Our best algorithm outperforms state-of-the-art online and batch methods on eight of the nine tasks. We also show that the confidence information maintained during learning yields useful probabilistic information at test time.", "citation": "#REFR extend the approach to multiclass classification and show that also in this setting the classifiers often outperform SVMs.", "context": "The classifiers also use Passive-Aggressive updates #OTHEREFR to try to maximize the margin between positive and negative training instances. CW classifiers are online-algorithms and are therefore fast to train, and it is not necessary to keep all training examples in memory. Despite this they perform as well or better than SVMs #OTHEREFR.[Citation]They show that updating only the weights of the best of the wrongly classified classes yields the best results. We also use this approach, called top-1, here. Crammer et al #OTHEREFR present different updaterules for CW classification and show that the ones based on standard deviation rather than variance yield the best results."}
{"citing_paper_id": "W10-2918", "cited_paper_id": "D09-1061", "citing_paper_abstract": "This paper presents a comparative study of three closely related Bayesian models for unsupervised document level sentiment classification, namely, the latent sentiment model (LSM), the joint sentimenttopic (JST) model, and the Reverse-JST model. Extensive experiments have been conducted on two corpora, the movie review dataset and the multi-domain sentiment dataset. It has been found that while all the three models achieve either better or comparable performance on these two corpora when compared to the existing unsupervised sentiment classification approaches, both JST and Reverse-JST are able to extract sentiment-oriented topics. In addition, Reverse-JST always performs worse than JST suggesting that the JST model is more appropriate for joint sentiment topic detection.", "cited_paper_abstract": "While traditional work on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the author?s mood, gender, age, or sentiment. Without knowing the user?s intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. To address this problem, we propose a novel way of incorporating user feedback into a clustering algorithm, which allows a user to easily specify the dimension along which she wants the data points to be clustered via inspecting only a small number of words. This distinguishes our method from existing ones, which typically require a large amount of effort on the part of humans in the form of document annotation or interactive construction of the feature space. We demonstrate the viability of our method on several challenging sentiment datasets.", "citation": "More recently, #REFR proposed an unsupervised sentiment classification algorithm by integrating user feedbacks into a spectral clustering algorithm.", "context": "Intuitively, sentiment polarities are dependent on contextual information, such as topics or domains. In this regard, some recent work #OTHEREFRa) has tried to model both sentiment and topics. However, these two models either require postprocessing to calculate the positive/negative coverage in a document for polarity identification #OTHEREFRa).[Citation]Features induced for each dimension of spectral clustering can be considered as sentimentoriented topics. Nevertheless, human judgement of identifying the most important dimensions during spectral clustering is required. Lin and He #OTHEREFR proposed a joint sentimenttopic (JST) model for unsupervised joint sentiment topic detection."}
{"citing_paper_id": "P14-1051", "cited_paper_id": "D09-1062", "citing_paper_abstract": "The sentiment captured in opinionated text provides interesting and valuable information for social media services. However, due to the complexity and diversity of linguistic representations, it is challenging to build a framework that accurately extracts such sentiment. We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level. Our framework can greatly reduce the human effort for building a domainspecific sentiment lexicon with high quality. Specifically, in our evaluation, working with just 20 manually labeled reviews, it generates a domain-specific sentiment lexicon that yields weighted average F- Measure gains of 3%. Our sentiment classification model achieves approximately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units.", "cited_paper_abstract": "Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. There are a number of such lexical resources available, but it is often suboptimal to use them as is, because general purpose lexical resources do not reflect domain-specific lexical usage. In this paper, we propose a novel method based on integer linear programming that can adapt an existing lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification.", "citation": "High-quality sentiment lexicons can improve the performance of sentiment analysis models over general-purpose lexicons #REFR.", "context": "Automatically extracting sentiments from usergenerated opinionated text is important in building social media services. However, the complexity and diversity of the linguistic representations of sentiments make this problem challenging.[Citation]More advanced methods such as #OTHEREFR adopt domain knowledge by extracting sentiment words from the domain-specific corpus. However, depending on the context, the same word can have different polarities even in the same domain #OTHEREFR. In respect to sentiment classification, Pang et al. #OTHEREFR infer the sentiments using basic features, such as bag-of-words."}
{"citing_paper_id": "W10-0214", "cited_paper_id": "D09-1062", "citing_paper_abstract": "This work explores the utility of sentiment and arguing opinions for classifying stances in ideological debates. In order to capture arguing opinions in ideological stance taking, we construct an arguing lexicon automatically from a manually annotated corpus. We build supervised systems employing sentiment and arguing opinions and their targets as features. Our systems perform substantially better than a distribution-based baseline. Additionally, by employing both types of opinion features, we are able to perform better than a unigrambased system.", "cited_paper_abstract": "Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. There are a number of such lexical resources available, but it is often suboptimal to use them as is, because general purpose lexical resources do not reflect domain-specific lexical usage. In this paper, we propose a novel method based on integer linear programming that can adapt an existing lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification.", "citation": "In order to detect the sentence polarity, we use the Vote and Flip algorithm from #REFR.", "context": "In addition to positive (+) and negative (?) words, this lexicon also contains subjective words that are themselves neutral (=) with respect to polarity. Examples of neutral entries are ?absolutely?, ?amplify?, ?believe?, and ?think?. We find the sentiment polarity of the entire sentence and assign this polarity to each content word in the sentence (denoted, for example, as target+).[Citation]This algorithm essentially counts the number of positive, negative and neutral lexicon hits in a given expression and accounts for negator words. The algorithm is used as is, except for the default polarity assignment (as we do not know the most prominent polarity in the corpus). Note that the Vote and Flip algorithm has been developed for expressions but we employ it on sentences."}
{"citing_paper_id": "W14-3010", "cited_paper_id": "D09-1063", "citing_paper_abstract": "Sentiment Analysis, an important area of Natural Language Understanding, often relies on the assumption that lexemes carry inherent sentiment values, as reflected in specialized resources. We examine and measure the contribution that eight intensifying adverbs make to the sentiment value of sentences, as judged by human annotators. Our results show, first, that the intensifying adverbs are not themselves sentiment-laden but strengthen the sentiment conveyed by words in their contexts to different degrees. We consider the consequences for appropriate modifications of the representation of the adverbs in sentiment lexicons.", "cited_paper_abstract": "Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has properties that support the Polyanna Hypothesis. Using the General Inquirer as gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above.", "citation": "Dictionary-based acquisition relies on a set of seed words that is expanded by using external resources, such as Word- Net: e.g., #OTHEREFR; #REFR.", "context": "We give only an overview of the related work here. SLs are acquired by one of three methods. Manual tagging is performed by human annotators: e.g., OF, and AL.[Citation]In corpus-based acquisition a set of seed words is expanded by using a large corpus of documents #OTHEREFR. To our knowledge, none of these works include the polarity intensifiers that we introduce in this paper."}
{"citing_paper_id": "W11-2205", "cited_paper_id": "D10-1056", "citing_paper_abstract": "The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research. The primary advantage of these methods is that they do not require annotated data to learn a model. However, this advantage makes them difficult to evaluate against a manually labeled gold standard. Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods. Instead, we argue that the rarely used in-context evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied. Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.", "cited_paper_abstract": "Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difficult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abilities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on non- English corpora.", "citation": "In the same spirit, #REFR used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein #OTHEREFR.", "context": "However, they observed that the clustering evaluation scores did not correlate with the results of this extrinsic evaluation. In other words, better clustering evaluation scores did not always result in better features for shallow parsing. Van Gael et al noted that homogeneity correlated better with shallow parsing performance, hypothesizing it is probably worse to assign the same state identifier to tokens that belong to different PoS tags, e.g. verb and adverbs, rather than to generate more than one state identifier for the same PoS.[Citation]Like Van Gael et al., they also found that better clustering evaluation scores did not result in better seeds. Given these results, as well as remembering that unsupervised learning methods do not use any label information in model learning, one is entitled to question whether it is reasonable to expect their output to match a particular labeled gold standard. Why not assume that the state identifiers obtained correlate with named entity recognition tags or categorial grammar tags instead of PoS tags, tasks for which sequential models are very common."}
{"citing_paper_id": "P13-2002", "cited_paper_id": "D10-1058", "citing_paper_abstract": "The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.", "cited_paper_abstract": "A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.", "citation": "The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two #REFR.", "context": "The first line is a baseline HMM using exact posterior computation and inference with the standard dynamic programming algorithms. The next line shows the fertility HMM with approximate posterior computation from Gibbs sampling but with final alignment selected by the Viterbi algorithm. Clearly fertility modeling is improving alignment quality.[Citation]Here, however, the difference between a dual decomposition and Viterbi is significant: their results were likely due to search error."}
{"citing_paper_id": "D13-1116", "cited_paper_id": "D10-1069", "citing_paper_abstract": "It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 ? this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to other grammar extraction strategies.", "cited_paper_abstract": "It is well known that parsing accuracies drop significantly on out-of-domain data. What is less known is that some parsers suffer more from domain shifts than others. We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers, which are of highest interest for practical applications because of their linear running time, drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance.", "citation": "Examples are parser co-training #OTHEREFR, product model PCFG-LA parsing #REFR, using dual decomposition to combine dependency and phrase structure models #OTHEREFR.", "context": "Parser Model Combination It is well known that improved parsing performance can be achieved by leveraging the alternative perspectives provided by several parsing models rather than relying on just one.[Citation]In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. Function Label Parsing Although function labels have been available in the Penn Treebank #OTHEREFR, they have been to a large extent overlooked in English parsing research ? most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed."}
{"citing_paper_id": "P13-1076", "cited_paper_id": "D10-1082", "citing_paper_abstract": "This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.", "cited_paper_abstract": "We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. Such decoding is enabled by: (1) separating full word features from partial word features so that feature templates can be instantiated incrementally, according to whether the current character is separated or appended; (2) deciding the POS-tag of a potential word when its first character is processed. Early-update is used with perceptron training so that the linear model gives a high score to a correct partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature.", "citation": "In the past years, several proposed supervised joint models #OTHEREFR; #REFR achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags.", "context": "The pipeline approach is very simple to implement, but frequently causes error propagation, given that wrong segmentations in the earlier stage harm the subsequent POS tagging #OTHEREFR. The joint approaches of word segmentation and POS tagging (joint S&T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence #OTHEREFR.[Citation]However, the production of such labeled data is extremely timeconsuming and expensive #OTHEREFR. Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model."}
{"citing_paper_id": "W13-2806", "cited_paper_id": "D10-1092", "citing_paper_abstract": "Chinese and Japanese have a different sentence structure. Reordering methods are effective, but need reliable parsers to extract the syntactic structure of the source sentences. However, Chinese has a loose word order, and Chinese parsers that extract the phrase structure do not perform well. We propose a framework where only POS tags and unlabeled dependency parse trees are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods.", "cited_paper_abstract": "Automatic evaluation of Machine Translation (MT) quality is essential to developing highquality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate ?A because B? as ?B because A.. Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics.", "citation": "For comparison purposes with the work in #REFRb), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method.", "context": "We used MeCab 4 #OTHEREFR to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 #OTHEREFR. Following the work in #OTHEREFR, we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees.[Citation]DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system #OTHEREFR, using a default distance reordering model and a lexicalized reordering model ?msd-bidirectionalfe?. A 5-gram language model was built using SRILM #OTHEREFR on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ #OTHEREFR."}
{"citing_paper_id": "P13-2147", "cited_paper_id": "D10-1101", "citing_paper_abstract": "Opinion mining is often regarded as a classification or segmentation task, involving the prediction of i) subjective expressions, ii) their target and iii) their polarity. Intuitively, these three variables are bidirectionally interdependent, but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that cannot model the bidirectional interaction between these variables. Towards better understanding the interaction between these variables, we propose a model that allows for analyzing the relation of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. We report results on two public datasets (cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts.", "cited_paper_abstract": "In this paper, we focus on the opinion target extraction as part of the opinion mining task. We model the problem as an information extraction task, which we address based on Conditional Random Fields (CRF). As a baseline we employ the supervised algorithm by Zhuang et al (2006), which represents the state-of-the-art on the employed data. We evaluate the algorithms comprehensively on datasets from four different domains annotated with individual opinion target instances on a sentence level. Furthermore, we investigate the performance of our CRF-based approach and the baseline in a singleand cross-domain opinion target extraction setting. Our CRF-based approach improves the performance by 0.077, 0.126, 0.071 and 0.178 regarding F-Measure in the single-domain extraction in the four domains. In the crossdomain setting our approach improves the performance by 0.409, 0.242, 0.294 and 0.343 regarding F-Measure over the baseline.", "citation": "Results on these data sets are compared to #REFR.", "context": "Power and Associates Sentiment Corpora2, an annotated data set of blog posts in the car and in the camera domain #OTHEREFR. From the rich annotation set, we use subjective terms and entity mentions which are in relation to them as targets. We do not consider comitter, negator, neutralizer, comparison, opo, or descriptor annotations to be subjective expressions.[Citation]In addition, we report results on a Twitter data set3 for the first time #OTHEREFR. Here, we use a Twitter-specific tokenizer and POS tagger4 #OTHEREFR instead of the Stanford parser. Hence, the single-edge-based feature described in Section 2.2 is not used for this dataset."}
{"citing_paper_id": "D14-1088", "cited_paper_id": "D10-1108", "citing_paper_abstract": "Taxonomies are the backbone of many structured, semantic knowledge resources. Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text. These approaches, however, often show low coverage due to the lack of contextual analysis across sentences. To address this issue, we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term, a subsumption relation between the two terms is inferred. We apply this method to the task of taxonomy construction from scratch, where we introduce another novel graph-based algorithm for taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure.", "cited_paper_abstract": "Although many algorithms have been developed to harvest lexical resources, few organize the mined terms into taxonomies. We propose (1) a semi-supervised algorithm that uses a root concept, a basic level concept, and recursive surface patterns to learn automatically from the Web hyponym-hypernym pairs subordinated to the root; (2) a Web based concept positioning procedure to validate the learned pairs? is-a relations; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested.", "citation": "Previous methods for the pruning task #REFR treat the identified taxonomic relations equally, and the pruning task is thus reduced to finding the best trade-off between path length and the connectivity of traversed nodes.", "context": "The SCS shows itself (Section 3.1) to be complementary to linguistic pattern matching. After the relation identification, the identified taxonomic relations should be integrated into a graph for the task of taxonomy construction from scratch or associated with existing concepts of a given taxonomy via is-a relations #OTHEREFR. In this step of taxonomic structure construction, there is a need for pruning incorrect and redundant relations.[Citation]This assumption, however, is not always true due to the fact that the identified taxonomic relations may have different confidence values, and the relations with high confidence values can be incorrectly eliminated during the pruning process. We thus propose a novel method for the taxonomy induction by utilizing the evidence scores from the relation identification method and the topological properties of the graph. We show that it can effectively prune redundant edges and remove loops while preserving the correct edges of taxonomy."}
{"citing_paper_id": "W14-1505", "cited_paper_id": "D10-1115", "citing_paper_abstract": "This paper presents a series of experiments in applying compositional distributional semantic models to dialogue act classification. In contrast to the widely used bag-ofwords approach, we build the meaning of an utterance from its parts by composing the distributional word vectors using vector addition and multiplication. We investigate the contribution of word sequence, dialogue act sequence, and distributional information to the performance, and compare with the current state of the art approaches. Our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence; more advanced approaches (e.g. sequenceor grammar-driven, such as categorical, word vector composition) are required.", "cited_paper_abstract": "We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.", "citation": "These improvements suggest that distributional information does improve performance, but that more sophisticated compositional operations such as matrix multiplication #REFR should provide further benefits.", "context": "However, since utterances generally consist of more than one word, one has to be able to extend such similarity-based models from single words to sentences and/or complete utterances. Hence, we consider here the application of compositional distributional semantics for this task. Here, we extend bag-of-word models common in previous approaches #OTHEREFR and examine the improvements gained.[Citation]The state of the art is a supervised method based on Recurrent Convolutional Neural Networks #OTHEREFR. This method learns both the sentence model and the discourse model from the same training corpus, making it hard to understand how much of the contribution comes from the inclusion of distributional word meaning, and how much from learning patterns specific to the corpus at hand. Here, in contrast, we use an external unlabeled resource to obtain a model of word meaning, composing words to obtain representations for utterances, and rely on training data only for discourse learning for the tagging task itself."}
{"citing_paper_id": "N12-1087", "cited_paper_id": "D10-1125", "citing_paper_abstract": "We present a general framework containing a graded spectrum of Expectation Maximization (EM) algorithms called Unified Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as Constraint- Driven Learning (Chang et al, 2007) and Posterior Regularization (Ganchev et al, 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al, 2008; Koo et al, 2010; Rush and Collins, 2011). UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn?t available earlier, exhibiting the benefits of the UEM framework.", "cited_paper_abstract": "This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.", "citation": "For the constrained inference step in UEM we present an efficient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature #OTHEREFR; #REFR.", "context": "We present a general framework containing a graded spectrum of Expectation Maximization #OTHEREFR, along with a range of new EM algorithms.[Citation]UEM is as efficient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn?t available earlier, exhibiting the benefits of the UEM framework."}
{"citing_paper_id": "D12-1001", "cited_paper_id": "D11-1006", "citing_paper_abstract": "We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).", "cited_paper_abstract": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.", "citation": "Following the procedure from #REFR, for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank.", "context": "We first evaluate our model under the BANKS data condition.[Citation]Again, despite the values of our PROJ queries being sensitive to which language we are currently parsing, the signatures are language independent, so discriminative training still makes sense over such a combined treebank. Training our PROJ features on the non-English treebanks in this concatenation can be understood as trying to learn which lexico-syntactic properties transfer ?universally,? or at least transfer broadly within the families of languages we are considering. Table 1 shows the performance of the DELEX feature set and the DELEX+PROJ feature set using both AUTOMATIC and MANUAL bilingual lexicons."}
{"citing_paper_id": "D12-1125", "cited_paper_id": "D11-1006", "citing_paper_abstract": "We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing.1", "cited_paper_abstract": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.", "citation": "However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data #OTHEREFR; #REFR.", "context": "Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data #OTHEREFR.[Citation]These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages #OTHEREFR), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually."}
{"citing_paper_id": "P13-1029", "cited_paper_id": "D11-1006", "citing_paper_abstract": "In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing transfer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars . CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1", "cited_paper_abstract": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.", "citation": "Transfer learning in parsing has been applied in different contexts, such as multilingual learning #OTHEREFR; #REFR, domain adaptation #OTHEREFR.", "context": "Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations.[Citation]There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may specify how to convert traces and functional tags in Penn Treebank to the f-structure in LFG #OTHEREFR."}
{"citing_paper_id": "P13-2056", "cited_paper_id": "D11-1006", "citing_paper_abstract": "Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages. However, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly. To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. Experiments showed that our methods improve the performances greatly on projections between English and Chinese.", "cited_paper_abstract": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.", "citation": "One intuitive and effective method is to build a common feature space for all languages, so that the model trained on one language could be directly used on other languages #REFR.", "context": "Unfortunately, it is impossible to build sufficient labeled data for all tasks in all languages. To address NLP tasks in resource-scarce languages, cross-lingual projection methods were proposed, which make use of existing resources in resource-rich language (also called source language) to help NLP tasks in resource-scarce language (also named as target language). There are several types of projection methods.[Citation]We call it direct projection, which becomes very popular recently. The main limitation of these methods is that target language has to be similar to source language. Otherwise the performance will degrade especially when the orders of phrases between source and target languages differ a lot."}
{"citing_paper_id": "P14-1126", "cited_paper_id": "D11-1006", "citing_paper_abstract": "We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resourcerich language. We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization. Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages. We perform experiments on three Data sets . Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems.", "cited_paper_abstract": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.", "citation": "Unfortunately, the unsupervised grammar induction systems? parsing accuracies often significantly fall behind those of supervised systems #REFR.", "context": "Several supervised dependency parsing algorithms #OTHEREFR. However, the manually annotated treebanks that these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages. This led to a vast amount of research on unsupervised grammar induction #OTHEREFR, which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers.[Citation]Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. In this paper, we consider a practically motivated scenario, in which we want to build statistical parsers for resource-poor target languages, using existing resources from a resource-rich source language (like English).1 We assume that there are absolutely no labeled training data for the target language, but we have access to parallel data with a resource-rich language and a sufficient amount of labeled training data to build an accurate parser for the resource-rich language. This scenario appears similar to the setting in bilingual text parsing."}
{"citing_paper_id": "W13-5007", "cited_paper_id": "D11-1011", "citing_paper_abstract": "Bootstrapping has recently become the focus of much attention in natural language processing to reduce labeling cost. In bootstrapping, unlabeled instances can be harvested from the initial labeled ?seed? set. The selected seed set affects accuracy, but how to select a good seed set is not yet clear. Thus, an ?iterative seeding? framework is proposed for bootstrapping to reduce its labeling cost. Our framework iteratively selects the unlabeled instance that has the best ?goodness of seed? and labels the unlabeled instance in the seed set. Our framework deepens understanding of this seeding process in bootstrapping by deriving the dual problem. We propose a method called expected model rotation (EMR) that works well on not well-separated data which frequently occur as realistic data. Experimental results show that EMR can select seed sets that provide significantly higher mean reciprocal rank on realistic data than existing naive selection methods or random seed sets.", "cited_paper_abstract": "Class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. Yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. We propose two graph-theoretic methods (centrality and regularization), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. We carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. We conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy.", "citation": "This definition follows the definitions of bootstrapping in existing NLP papers #OTHEREFR; #REFR.", "context": "Bootstrapping has recently drawn a great deal of attention in natural language processing (NLP) research. We define bootstrapping as a method for harvesting ?instances? similar to given ?seeds? by recursively harvesting ?instances? and ?patterns? by turns over corpora using the distributional hypothesis #OTHEREFR.[Citation]Bootstrapping can greatly reduce the cost of labeling instances, which is especially needed for tasks with high labeling costs. The performance of bootstrapping algorithms, however, depends on the selection of seeds. Although various bootstrapping algorithms have been proposed, randomly chosen seeds are usually used instead."}
{"citing_paper_id": "D12-1041", "cited_paper_id": "D11-1018", "citing_paper_abstract": "A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the first step in this line of research, we verify the effectiveness of our approach in a BTG- based phrasal system, and propose four FDT- based component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality.", "cited_paper_abstract": "When translating among languages that differ substantially in word order, machine translation (MT) systems benefit from syntactic preordering?an approach that uses features from a syntactic parse to permute source words into a target-language-like order. This paper presents a method for inducing parse trees automatically from a parallel corpus, instead of using a supervised parser trained on a treebank. These induced parses are used to preorder source sentences. We demonstrate that our induced parser is effective: it not only improves a state-of-the-art phrase-based system with integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction.", "citation": "Recently, #REFR proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks.", "context": "Pre-reordering #OTHEREFR used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods.[Citation]First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT model training."}
{"citing_paper_id": "D14-1149", "cited_paper_id": "D11-1024", "citing_paper_abstract": "Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language. This paper develops a corpus-based method of extracting coherent clusters of satellite terminology ? terms on the edge of the lexicon ? using co-occurrence networks of unstructured text. Term clusters are identified by extracting communities in the cooccurrence graph, after which the largest is discarded and the remaining words are ranked by centrality within a community. The method is tractable on large corpora, requires no document structure and minimal normalization. The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size, content and structure. The findings also confirm that language consists of a densely connected core (observed in dictionaries) and systematic, semantically coherent groups of terms at the edges of the lexicon.", "cited_paper_abstract": "Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet al location, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).", "citation": "Another crucial difference is that topic size from a single sampling iteration tends to correlate with coherence #REFR, but in the current method, there is no correlation between cluster size and coherence (p = 0.98).", "context": "Topic models produce a probability distribution over words to define a topic, which can be summarized by the top 10 to 20 most likely words. Instead of probabilities, the withincommunity hub-scores were used to rank words in each cluster. This means that the actual structure of the community (to which topics have no analogue) is responsible for producing the scores that rate words? internal relevance.[Citation]The other important difference is that whereas topic models produce a topic-document mixture that can be used for posterior inference, to perform such inference with our method, the output would have to be used indirectly. One understated strength of the community detection method is the minimal required preprocessing. Whereas many solutions in NLP (including topic models) require document segmentation, lexical normalization and statistical normalizations on the co-occurrence matrix itself, the only variable in our method is the cooccurrence window size."}
{"citing_paper_id": "C14-1005", "cited_paper_id": "D11-1026", "citing_paper_abstract": "We present a hierarchical topical segmenter for free text. Hierarchical Affinity Propagation for Segmentation (HAPS) is derived from a clustering algorithm Affinity Propagation. Given a document, HAPS builds a topical tree. The nodes at the top level correspond to the most prominent shifts of topic in the document. Nodes at lower levels correspond to finer topical fluctuations. For each segment in the tree, HAPS identifies a segment centre ? a sentence or a paragraph which best describes its contents. We evaluate the segmenter on a subset of a novel manually segmented by several annotators, and on a dataset of Wikipedia articles. The results suggest that hierarchical segmentations produced by HAPS are better than those obtained by iteratively running several one-level segmenters. An additional advantage of HAPS is that it does not require the ?gold standard? number of segments in advance.", "cited_paper_abstract": "This paper presents a new algorithm for linear text segmentation. It is an adaptation of Affinity Propagation, a state-of-the-art clustering algorithm in the framework of factor graphs. Affinity Propagation for Segmentation, or APS, receives a set of pairwise similarities between data points and produces segment boundaries and segment centres ? data points which best describe all other data points within the segment. APS iteratively passes messages in a cyclic factor graph, until convergence. Each iteration works with information on all available similarities, resulting in highquality results. APS scales linearly for realistic segmentation tasks. We derive the algorithm from the original Affinity Propagation formulation, and evaluate its performance on topical text segmentation in comparison with two state-of-the art segmenters. The results suggest that APS performs on par with or outperforms these two very competitive baselines.", "citation": "Most of the topical segmenters #OTHEREFR; #REFR can only produce single-level segmentation, a worthy endeavour in and of itself.", "context": "Topical segmentation is a lightweight form of such structural analysis: given a sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the author, such as speech transcripts, meeting notes or literature. The past decade has witnessed significant progress in the area of text segmentation.[Citation]Yet, to view the structure of a document linearly, as a sequence of segments, is in certain discord with most theories of discourse structure, where it is more customary to consider documents as trees #OTHEREFR. Regardless of the theory, we hypothesize that it may be useful to have an idea about fluctuations of topic in documents beyond the coarsest level. It is the contribution of this work that we develop such a hierarchical segmenter, implement it and do our best to evaluate it."}
{"citing_paper_id": "W12-1002", "cited_paper_id": "D11-1037", "citing_paper_abstract": "We present a new transcription mode for the annotation tool ELAN. This mode is designed to speed up the process of creating transcriptions of primary linguistic data (video and/or audio recordings of linguistic behaviour). We survey the basic transcription workflow of some commonly used tools (Transcriber, BlitzScribe, and ELAN) and describe how the new transcription interface improves on these existing implementations. We describe the design of the transcription interface and explore some further possibilities for improvement in the areas of segmentation and computational enrichment of annotations.", "cited_paper_abstract": "In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.", "citation": "At the same time, a working set of annotyzers will promote more standardised glossing, which can then be used for further automated research, cf. automatic treebank production or similar #REFR.", "context": "Even though the ELAN modules will offer support for such rules, our focus is on the automation of machinelearning systems in order to scale the annotation process. Our main aim for the future is to incorporate learning systems that support the linguists by improving the suggested new annotations on the bases of choices the linguist made earlier. The goal there is, again, to reduce annotation time, so that the linguist can work more on linguistic analysis and less on annotating.[Citation]"}
{"citing_paper_id": "P13-1151", "cited_paper_id": "D11-1038", "citing_paper_abstract": "In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams.", "cited_paper_abstract": "Text simplification aims to rewrite text into simpler versions, and thus make information accessible to a broader audience. Most previous work simplifies sentences using handcrafted rules aimed at splitting long sentences, or substitutes difficult words using a predefined dictionary. This paper presents a datadriven model based on quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite operations. We describe how such a grammar can be induced from Wikipedia and propose an integer linear programming model for selecting the most appropriate simplification from the space of possible rewrites generated by the grammar. We show experimentally that our method creates simplifications that significantly reduce the reading difficulty of the input, while maintaining grammaticality and preserving its meaning.", "citation": "Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English #OTHEREFR; #REFR and in other languages #OTHEREFR.", "context": "Our goal is more general: to examine the relationship between simple and normal data and determine whether normal data is helpful. Previous domain adaptation research is complementary to our experiments and could be explored in the future for additional performance improvements. Simple language models play a role in a variety of text simplification applications.[Citation]Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification #OTHEREFR. Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences #OTHEREFR."}
{"citing_paper_id": "S13-1045", "cited_paper_id": "D11-1039", "citing_paper_abstract": "Existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding databases. This paper introduces FreeParser, a system that trains on one domain and one set of predicate and constant symbols, and then can parse sentences for any new domain, including sentences that refer to symbols never seen during training. FreeParser uses a domain-independent architecture to automatically identify sentences relevant to each new database symbol, which it uses to supplement its manually-annotated training data from the training domain. In cross-domain experiments involving 23 domains, FreeParser can parse sentences for which it has seen comparable unannotated sentences with an F1 of 0.71.", "cited_paper_abstract": "Conversations provide rich opportunities for interactive, continuous learning. When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals. In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers. We demonstrate learning without any explicit annotation of the meanings of user utterances. Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation. This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations.", "citation": "Another set of approaches has investigated the case where no logical forms are provided, but instead some form of feedback or response from the world is used as evidence for what the correct logical form must have been #OTHEREFR; #REFR.", "context": "These approaches have yielded steady improvements on standard test sets like Geo- Query, but are difficult to apply to Freebase because of their built-in assumption that relation symbols will be observed during training. There has been a recent push towards developing techniques which reduce the annotation cost or the data complexity of the models. Models have been developed which can handle some ambiguity in terms of which logical form is the correct label for each training sentence #OTHEREFR.[Citation]While such techniques are important, they can only reduce the annotation cost per domain, and annotation efforts would still be required for each new domain that contains new database symbols. The goal of the Freebase semantic parser, in contrast, is to program actor role Party Down Ryan Hansen Kyle Bradway structure owner CN Tower Canada Lands Co. particle sub-particle number Proton Up Quark 2 TV domain cast member table Architecture domain ownership table Physics domain particle composition Figure 2: Example Freebase relations (tables) and instances for three domains. port to all domains automatically, without any new manually-labeled data per domain."}
{"citing_paper_id": "E12-1066", "cited_paper_id": "D11-1057", "citing_paper_abstract": "Morphological lexica are often implemented on top of morphological paradigms, corresponding to different ways of building the full inflection table of a word. Computationally precise lexica may use hundreds of paradigms, and it can be hard for a lexicographer to choose among them. To automate this task, this paper introduces the notion of a smart paradigm. It is a metaparadigm, which inspects the base form and tries to infer which low-level paradigm applies. If the result is uncertain, more forms are given for discrimination. The number of forms needed in average is a measure of predictability of an inflection system. The overall complexity of the system also has to take into account the code size of the paradigms definition itself. This paper evaluates the smart paradigms implemented in the open-source GF Resource Grammar Library. Predictability and complexity are estimated for four different languages: English, French, Swedish, and Finnish. The main result is that predictability does not decrease when the complexity of morphology grows, which means that smart paradigms provide an efficient tool for the manual construction and/or automatically bootstrapping of lexica.", "cited_paper_abstract": "We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types). It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words. Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits. It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples). Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finitestate transducers with language-specific parameters to be learned. These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming. Given 50?100 seed paradigms, adding a 10- million-word corpus reduces prediction error for morphological inflections by up to 10%.", "citation": "Of particular interest are #OTHEREFR and #REFR, dealing with the automatic extraction of paradigms from text and investigate how good these can become.", "context": "Examples of guessers include #OTHEREFR for German. Another related domain is the unsupervised learning of morphology where machine learning is used to automatically build a language morphology from corpora #OTHEREFR. The main difference is that with the smart paradigms, the paradigms and the guess heuristics are implemented manually and with a high certainty; in unsupervised learning of morphology the paradigms are induced from the input forms with much lower certainty.[Citation]The main contrast is, again, that our work deals with handwritten paradigms that are correct by design, and we try to see how much information we can drop before losing correctness. Once given, a set of paradigms can be used in automated lexicon extraction from raw data, as in #OTHEREFR, by a method that tries to collect a sufficient number of forms to determine that a word belongs to a certain paradigm. Smart paradigms can then give the method to actually construct the full inflection tables from the characteristic forms."}
{"citing_paper_id": "N12-1045", "cited_paper_id": "D11-1059", "citing_paper_abstract": "In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations. The model is essentially an infinite HMM that infers the number of states from data. Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step. We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction.", "cited_paper_abstract": "In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.", "citation": "Models presented in #REFR and #OTHEREFR are also built on Dirichlet-multinomials and, rather than defining a sequence model, present a clustering model based on features.", "context": "Inference is done using a collapsed Gibbs sampler and concentration parameter values are learned during inference. The model is token-based, allowing different words of the same type in different locations to have a different tag. This model can actually be classified as semi-supervised as it assumes the presence of a tagging dictionary that contains the list of possible POS tags for each word typean assumption that is clearly not realistic in an unsupervised setting.[Citation]Both report good results on type basis and use #OTHEREFR using the suffixes obtained from an unsupervised morphology induction system. Nonparametric Bayesian POS induction has been studied in #OTHEREFR. The model in #OTHEREFR uses Pitman-Yor Process (PYP) prior but the model itself is finite in the sense that the size of the tagset is fixed."}
{"citing_paper_id": "W12-1914", "cited_paper_id": "D11-1059", "citing_paper_abstract": "We propose an unsupervised approach to POS tagging where first we associate each word type with a probability distribution over word classes using Latent Dirichlet Allocation. Then we create a hierarchical clustering of the word types: we use an agglomerative clustering algorithm where the distance between clusters is defined as the Jensen- Shannon divergence between the probability distributions over classes associated with each word-type. When assigning POS tags, we find the tree leaf most similar to the current word and use the prefix of the path leading to this leaf as the tag. This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets.", "cited_paper_abstract": "In this paper we present a fully unsupervised syntactic class induction system formulated as a Bayesian multinomial mixture model, where each word type is constrained to belong to a single class. By using a mixture model rather than a sequence model (e.g., HMM), we are able to easily add multiple kinds of features, including those at both the type level (morphology features) and token level (context and alignment features, the latter from parallel corpora). Using only context features, our system yields results comparable to state-of-the art, far better than a similar model without the one-class-per-type constraint. Using the additional features provides added benefit, and our final system outperforms the best published results on most of the 25 corpora tested.", "citation": "Second, learning categories has been cast as unsupervised part-of-speech tagging task #OTHEREFR, #REFR), and primarily motivated as useful for tagging under-resourced languages.", "context": "Unsupervised induction of word categories has been approached from three broad perspectives. First, it is of interest to cognitive scientists who model syntactic category acquisition by children #OTHEREFR, where the primary concern is matching human performance patterns and satisfying cognitively motivated constraints such as incremental learning.[Citation]Finally, learning categories has also been researched from the point of view of feature learning, where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application #OTHEREFR. The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation."}
{"citing_paper_id": "P12-1022", "cited_paper_id": "D11-1067", "citing_paper_abstract": "The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. Secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.", "cited_paper_abstract": "Multiword expressions (MWE), a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics. Previous work onMWE identification has relied primarily on surface statistics, which perform poorly for longer MWEs and cannot model discontinuous expressions. To address these problems, we show that even the simplest parsing models can effectively identify MWEs of arbitrary length, and that Tree Substitution Grammars achieve the best results. Our experiments show a 36.4% F1 absolute improvement for French over an n-gram surface statistics baseline, currently the predominant method for MWE identification. Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy.", "citation": "We used the latest edition of the corpus #OTHEREFR that we preprocessed with the Stanford Parser preprocessing tools #REFR.", "context": "The French Treebank2 [FTB] #OTHEREFR is a syntactically annotated corpus made up of journalistic articles from Le Monde newspaper.[Citation]It contains 473,904 tokens and 15,917 sentences. One benefit of this corpus is that its compounds are marked. Their annotation was driven by linguistic criteria such as the ones in #OTHEREFR."}
{"citing_paper_id": "C14-1087", "cited_paper_id": "D11-1072", "citing_paper_abstract": "Document enrichment is the task of retrieving additional knowledge from external resource over what is available through source document. This task is essential because of the phenomenon that text is generally replete with gaps and ellipses since authors assume a certain amount of background knowledge. The recovery of these gaps is intuitively useful for better understanding of document. Conventional document enrichment techniques usually rely on Wikipedia which has great coverage but less accuracy, or Ontology which has great accuracy but less coverage. In this study, we propose a document enrichment framework which automatically extracts ?argument , predicate, argument ? triple from any text corpus as background knowledge, so that to ensure the compatibility with any resource (e.g. news text, ontology, and on-line encyclopedia) and improve the enriching accuracy. We first incorporate source document and background knowledge together into a triple based document-level graph and then propose a global iterative ranking model to propagate relevance score and select the most relevant knowledge triple. We evaluate our model as a ranking problem and compute the MAP and P&N score to validate the ranking result. Our final result, a MAP score of 0.676 and P&20 score of 0.417 outperform a strong baseline based on search engine by 0.182 in MAP and 0.04 in P&20.", "cited_paper_abstract": "Disambiguating named entities in naturallanguage text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs.", "citation": "In early stage, most researches rely on the similarity between the context of the mention and the definition of candidate entities by proposing different measuring criteria such as dot product, cosine similarity, KL divergence, Jaccard distance and more complicated ones #OTHEREFR; #REFR.", "context": "Document enrichment focuses on introducing external knowledge into source document. There are mainly two kinds of works in this topic according to the resource they relying on. The first line of works make use of WikiPedia and enrich source document by linking the entity to its corresponding Wiki page #OTHEREFR.[Citation]However, these methods mainly rely on text similarity but neglect the internal structure between mentions. So another kind of works explore the structure information with collective disambiguation #OTHEREFR. These methods make use of structure information within context and resolve different mentions based on the coherence among decisions."}
{"citing_paper_id": "D13-1041", "cited_paper_id": "D11-1072", "citing_paper_abstract": "Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure between entity categories and context document, performance is further improved.", "cited_paper_abstract": "Disambiguating named entities in naturallanguage text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs.", "citation": "Collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously #OTHEREFR; #REFR.", "context": "L2R approaches are very flexible and expressive. Features like name matching, context similarity #OTHEREFR can be incorporated with ease. Nevertheless, decisions are made independently and inconsistent results are found from time to time.[Citation]Collective approaches can improve performance when local evidence is not confident enough. They often utilize semantic relations across different mentions, and is why they are called global approaches, while L2R methods fall into local approaches #OTHEREFR. However, collective inference processes are often expensive and involve an exponential search space."}
{"citing_paper_id": "W13-0807", "cited_paper_id": "D11-1082", "citing_paper_abstract": "Deciding whether a synchronous grammar formalism generates a given word alignment (the alignment coverage problem) depends on finding an adequate instance grammar and then using it to parse the word alignment. But what does it mean to parse a word alignment by a synchronous grammar. This is formally undefined until we define an unambiguous mapping between grammatical derivations and word-level alignments. This paper proposes an initial, formal characterization of alignment coverage as intersecting two partially ordered sets (graphs) of translation equivalence units, one derived by a grammar instance and another defined by the word alignment. As a first sanity check, we report extensive coverage results for ITG on automatic and manual alignments. Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work.", "cited_paper_abstract": "Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian.", "citation": "This is useful for literature on learning from word aligned parallel corpora #OTHEREFR; #REFR).", "context": "And finally, intersect the sets of nodes in the two sets of synchronous trees to check whether the grammar can generate (parts of) the word alignment. The formal detail of each of these three steps is provided in sections 3 to 5. We think that alignment parsing is relevant for current research because it highlights the difference between alignments in training data and alignments accepted by a synchronous grammar (learned from data).[Citation]A theoretical, formalized characterization of the alignment parsing problem is likely to improve the choices made in empirical work as well. We exemplify our claims by providing yet another empirical study of the stability of the ITG hypothesis. Our study highlights some of the technical choices left implicit in preceding work as explained in the next section."}
{"citing_paper_id": "D13-1031", "cited_paper_id": "D11-1090", "citing_paper_abstract": "Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.", "cited_paper_abstract": "This paper investigates improving supervised word segmentation accuracy with unlabeled data. Both large-scale in-domain data and small-scale document text are considered. We present a unified solution to include features derived from unlabeled data to a discriminative learning model. For the large-scale data, we derive string statistics from Gigaword to assist a character-based segmenter. In addition, we introduce the idea about transductive, document-level segmentation, which is designed to improve the system recall for out-ofvocabulary (OOV) words which appear more than once inside a document. Novel features1 result in relative error reductions of 13.8% and 15.4% in terms of F-score and the recall of OOV words respectively.", "citation": "In-domain data is mainly used to solve the problem of data sparseness #REFR.", "context": "To accurately calculate the precise label distribution, we use a framework similar to the cotraining algorithm to adjust the feature values iteratively. Generally speaking, unlabeled data can be classified as in-domain data and out-ofdomain data. In previous works these two kinds of unlabeled data are used separately for different purposes.[Citation]On the other hand, out-of domain data is used for domain adaptation #OTHEREFR. In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus. We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoff."}
{"citing_paper_id": "W14-2910", "cited_paper_id": "D11-1096", "citing_paper_abstract": "This paper proposes an evaluation scheme to measure the performance of a system that detects hierarchical event structure for event coreference resolution. We show that each system output is represented as a forest of unordered trees, and introduce the notion of conceptual event hierarchy to simplify the evaluation process. We enumerate the desiderata for a similarity metric to measure the system performance. We examine three metrics along with the desiderata, and show that metrics extended from MUC and BLANC are more adequate than a metric based on Simple Tree Matching.", "cited_paper_abstract": "A central topic in natural language processing is the design of lexical and syntactic features suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels.", "citation": "Tree kernels have been also widely studied and applied to NLP tasks, more specifically, to capture the similarity between parse trees #OTHEREFR or between dependency trees #REFR.", "context": "Another tree similarity metric is Simple Tree Matching #OTHEREFR. STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees.[Citation]This method is based on a supervised learning model with training data; hence we need a number of pairs of trees and associated numeric similarity values between these trees as input. Thus, it is not appropriate for an evaluation setting."}
{"citing_paper_id": "N13-2009", "cited_paper_id": "D11-1108", "citing_paper_abstract": "We examine the application of data-driven paraphrasing to natural language understanding. We leverage bilingual parallel corpora to extract a large collection of syntactic paraphrase pairs, and introduce an adaptation scheme that allows us to tackle a variety of text transformation tasks via paraphrasing. We evaluate our system on the sentence compression task. Further, we use distributional similarity measures based on context vectors derived from large monolingual corpora to annotate our paraphrases with an orthogonal source of information. This yields significant improvements in our compression system?s output quality, achieving state-of-the-art performance. Finally, we propose a refinement of our paraphrases by classifying them into natural logic entailment relations. By extending the synchronous parsing paradigm towards these entailment relations, we will enable our system to perform recognition of textual entailment.", "cited_paper_abstract": "Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.", "citation": "We extend this method to extract syntactic paraphrases #REFR.", "context": "Since ?thrown into jail? is aligned to multiple German phrases, and since each of those German phrases align back to a variety of English phrases, the method extracts a wide range of possible paraphrases including good paraphrase like: imprisoned and thrown into prison. It also produces less good paraphrases like: in jail and put in prison for, and bad paraphrases, such as maltreated and protection, because of noisy/inaccurate word alignments and other problems. To rank these, Bannard and Callison-Burch #OTHEREFR.[Citation]Table 1 shows example paraphrases produced by our system. While phrasal systems memorize phrase pairs without any further generalization, a syntactic paraphrasing system can learn more generic patterns. These can be better applied to unseen data."}
{"citing_paper_id": "N12-1052", "cited_paper_id": "D11-1116", "citing_paper_abstract": "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.", "cited_paper_abstract": "Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy.", "citation": "Word cluster features have been shown to be useful in various tasks in natural language processing, including syntactic dependency parsing #OTHEREFR; #REFR, syntactic chunking #OTHEREFR.", "context": "[Citation]Intuitively, the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues. In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model #OTHEREFR. However, rather than the more commonly used model of Brown et al #OTHEREFR."}
{"citing_paper_id": "E14-1035", "cited_paper_id": "D11-1125", "citing_paper_abstract": "Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features.", "cited_paper_abstract": "We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al, 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al, 2007; Chiang et al, 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO?s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.", "citation": "Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO #REFR.", "context": "Word alignments are trained on the concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based French- English system trained on the concatenation of all parallel data. It was built with the Moses toolkit #OTHEREFR using the 14 standard core features including a 5gram language model.[Citation]We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method #OTHEREFRb) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables."}
{"citing_paper_id": "W12-3160", "cited_paper_id": "D11-1125", "citing_paper_abstract": "The introduction of large-margin based discriminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process. By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features. However, these methods have not yet met with wide-spread adoption. This may be partly due to the perceived complexity of implementation, and partly due to the lack of standard methodology for applying these methods to MT. This papers aims to shed light on large-margin learning for MT, explicitly presenting the simple passive-aggressive algorithm which underlies many previous approaches, with direct application to MT, and empirically comparing several widespread optimization strategies.", "cited_paper_abstract": "We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al, 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al, 2007; Chiang et al, 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO?s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.", "citation": "The major motivation for this has been that while MERT is able to efficiently optimize a small number of parameters directly toward an external evaluation metric, such as BLEU #OTHEREFR; #REFR.", "context": "Statistical machine translation (SMT) systems represent knowledge sources in the form of features, and rely on parameters, or weights, on each feature, to score alternative translations. As in all statistical models, these parameters need to be learned from the data. In recent years, there has been a growing trend of moving away from discriminative training using batch log-linear optimization, with Minimum- Error Rate Training #OTHEREFR.[Citation]Furthermore, it is designed for batch learning, which may be prohibitive or undesirable in certain scenarios, for instance if we have a large tuning set. One or both of these limitations have led to recent introduction of alternative optimization strategies, such as minimum-risk #OTHEREFR. A popular method of large-margin optimization is the margin-infused relaxed algorithm #OTHEREFR."}
{"citing_paper_id": "W11-2507", "cited_paper_id": "D11-1129", "citing_paper_abstract": "Formal and distributional semantic models offer complementary benefits in modeling meaning. The categorical compositional distributional model of meaning of Coecke et al (2010) (abbreviated to DisCoCat in the title) combines aspects of both to provide a general framework in which meanings of words, obtained distributionally, are composed using methods from the logical setting to form sentence meaning. Concrete consequences of this general abstract setting and applications to empirical data are under active study (Grefenstette et al, 2011; Grefenstette and Sadrzadeh, 2011). In this paper, we extend this study by examining transitive verbs, represented as matrices in a DisCoCat. We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011).", "cited_paper_abstract": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.", "citation": "The experiment is on the dataset developed in #REFR.", "context": "In this section, we describe the experiment used to evaluate and compare these three methods.[Citation]Parameters We used the parameters described by Mitchell and Lapata #OTHEREFR for the noun and verb vectors. All vectors were built from a lemmatised version of the BNC. The noun basis was the 2000 most common context words, basis weights were the probability of context words given the target word divided by the overall probability of the context word."}
{"citing_paper_id": "P12-1075", "cited_paper_id": "D11-1135", "citing_paper_abstract": "To discover relation types from text, most methods cluster shallow or syntactic patterns of relation mentions, but consider only one possible sense per pattern. In practice this assumption is often violated. In this paper we overcome this issue by inducing clusters of pattern senses from feature representations of patterns. In particular, we employ a topic model to partition entity pairs associated with patterns into sense clusters using local and global features. We merge these sense clusters into semantic relations using hierarchical agglomerative clustering. We compare against several baselines: a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial.", "cited_paper_abstract": "We explore unsupervised approaches to relation extraction between two named entities; for instance, the semantic bornIn relation between a person and location entity. Concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. The output of each model is a clustering of observed relation tuples and their associated textual expressions to underlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline.", "citation": "For example, varieties of topic models are employed for both open domain #REFR and in-domain relation discovery #OTHEREFR.", "context": "Our approach falls into the same category. Moreover, we explore path senses and global features for relation discovery. Many generative probabilistic models have been applied to relation extraction.[Citation]Our approach employs generative models for path sense disambiguation, which achieves better performance than directly applying generative models to unsupervised relation discovery."}
{"citing_paper_id": "D14-1134", "cited_paper_id": "D11-1140", "citing_paper_abstract": "We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser. Existing methods incrementally expand the lexicon by greedily adding entries, considering a single training datapoint at a time. We propose using corpus-level statistics for lexicon learning decisions. We introduce voting to globally consider adding entries to the lexicon, and pruning to remove entries no longer required to explain the training data. Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions, achieving up to 25% error reduction, with lexicons that are up to 70% smaller and are qualitatively less noisy.", "cited_paper_abstract": "We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.", "citation": "We adopt a factored representation for CCG lexicons #REFR, where entries are dynamically generated by combining lexemes and templates.", "context": "Each intermediate parse node is constructed by applying one of a small set of binary CCG combinators or unary operators. For example, in Figure 2 the category of the span walk forward is combined with the category of twice using backward application (<). Parsing concludes with a logical form that captures the meaning of the complete sentence.[Citation]A lexeme is a pair that consists of a natural language string and a set of logical constants, while the template contains the syntactic and semantic components of a CCG category, abstracting over logical constants. For example, consider the lexical entry walk ` S/NP : ?x.?a.move(a) ? direction(a, x). Under the factored representation, this entry can be constructed by combining the lexeme ?walk, {move,direction}? and the template ?v .?v .[S/NP : ?x.?a.v (a) ? v (a, x)]."}
{"citing_paper_id": "P14-1112", "cited_paper_id": "D11-1140", "citing_paper_abstract": "We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision. The trained parser produces a full syntactic parse of any sentence, while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser?s predicate vocabulary. We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology. A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach. A syntactic evaluation on CCGbank demonstrates that the parser?s dependency F- score is within 2.5% of state-of-the-art.", "cited_paper_abstract": "We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring.", "citation": "Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions #OTHEREFR; #REFR.", "context": "The parsing model in this paper is loosely based on C&C #OTHEREFRa), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses #OTHEREFR. However, these approaches to semantics do not ground the text to beliefs in a knowledge base.[Citation]This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions #OTHEREFR, but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO #OTHEREFR."}
{"citing_paper_id": "D13-1183", "cited_paper_id": "D11-1142", "citing_paper_abstract": "The distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings, has inspired several Web mining algorithms for paraphrasing semantically equivalent phrases. Unfortunately, these methods have several drawbacks, such as confusing synonyms with antonyms and causes with effects. This paper introduces three Temporal Correspondence Heuristics, that characterize regularities in parallel news streams, and shows how they may be used to generate high precision paraphrases for event relations. We encode the heuristics in a probabilistic graphical model to create the NEWSSPIKE algorithm for mining news streams. We present experiments demonstrating that NEWSSPIKE significantly outperforms several competitive baselines. In order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE.", "cited_paper_abstract": "Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpos. More than 30% of REVERB?s extractions are at precision 0.8 or higher? compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB?s errors, suggesting directions for future work.1", "citation": "Extract shallow relation tuples using the OpenIE system #REFR.", "context": "Collect RSS news seeds, which contain the title, time-stamp, and abstract of the news items. . Use these titles to query the Bing news search engine API and collect additional time-stamped news articles. . Strip HTML tags from the news articles using Boilerpipe #OTHEREFR; keep only the title and first paragraph of each article. .[Citation]We performed these steps every day from January 1 to February 22, 2013. In total, we collected 546,713 news articles, for which 2.6 million extractions had 529 thousand unique relations. We used several types of features for paraphrasing: 1) spike features obtained from time series; 2) tense features, such as whether two relation phrases are both in the present tense; 3) cause-effect features, such as whether two relation phrases often appear successively in the news articles; 4) text features, such as whether sentences are similar; 5) syntactic features, such as whether a relation phrase appears in a clausal complement; and 6) semantic features, such as whether a relation phrase contains negative words."}
{"citing_paper_id": "W13-1716", "cited_paper_id": "D11-1148", "citing_paper_abstract": "Our submission for this NLI shared task used for the most part standard features found in recent work. Our focus was instead on two other aspects of our system: at a high level, on possible ways of constructing ensembles of multiple classifiers; and at a low level, on the granularity of part-of-speech tags used as features. We found that the choice of ensemble combination method did not lead to much difference in results, although exploiting the varying behaviours of linear versus logistic regression SVM classifiers could be promising in future work; but part-of-speech tagsets showed noticeable differences. We also note that the overall architecture, with its feature set and ensemble approach, had an accuracy of 83.1% on the test set when trained on both the training data and development data supplied, close to the best result of the task. This suggests that basically throwing together all the features of previous work will achieve roughly the state of the art.", "cited_paper_abstract": "Attempts to profile authors according to their characteristics extracted from textual data, including native language, have drawn attention in recent years, via various machine learning approaches utilising mostly lexical features. Drawing on the idea of contrastive analysis, which postulates that syntactic errors in a text are to some extent influenced by the native language of an author, this paper explores the usefulness of syntactic features for native language identification. We take two types of parse substructure as features? horizontal slices of trees, and the more general feature schemas from discriminative parse reranking?and show that using this kind of syntactic feature results in an accuracy score in classification of seven native languages of around 80%, an error reduction of more than 30%.", "citation": "The first is the proposal and use of new features that might have relevance to NLI: for example, #REFR, motivated by the Contrastive Analysis Hypothesis #OTHEREFR.", "context": "Among the efflorescence of work on Native Language Identification (NLI) noted by the shared task organisers, there are two trends in recent work in particular that we considered in building our submission.[Citation]Starting from the features introduced in these papers and others, then, other recent papers have compiled a comprehensive collection of features based on the earlier work . Tetreault et al. #OTHEREFR is an example, combining and analysing most of the features used in previous work. Given the timeframe of the shared task, there seemed to be not much mileage in trying new features that were likely to be more peripheral to the task."}
{"citing_paper_id": "P13-1117", "cited_paper_id": "D12-1001", "citing_paper_abstract": "Semantic Role Labeling (SRL) has become one of the standard tasks of natural language processing and proven useful as a source of information for a number of other applications. We address the problem of transferring an SRL model from one language to another using a shared feature representation. This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method.", "cited_paper_abstract": "We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).", "citation": "Cross-lingual model transfer methods #OTHEREFR; #REFR have also been receiving much attention recently.", "context": "Although the models thus obtained are generally imperfect, they can be further refined for a particular language and domain using techniques such as active learning #OTHEREFR. Cross-lingual annotation projection #OTHEREFR. Interestingly, it has also been used to propagate morphosyntactic information between old and modern versions of the same language #OTHEREFR.[Citation]The basic idea behind model transfer is similar to that of cross-lingual annotation projection, as we can see from the way parallel data is used in, for example, McDonald et al #OTHEREFR. A crucial component of direct transfer approaches is the unified feature representation. There are at least two such representations of lexical information #OTHEREFR, but both work on word level."}
{"citing_paper_id": "E14-1066", "cited_paper_id": "D12-1007", "citing_paper_abstract": "We propose the use of a generative model to simulate user behaviour in a novel taskoriented dialog domain, where user goals are spatial routes across artificial landscapes. We show how to derive an efficient feature-based representation of spatial goals, admitting exact inference and generalising to new routes. The use of a generative model allows us to capture a range of plausible behaviour given the same underlying goal. We evaluate intrinsically using held-out probability and perplexity, and find a substantial reduction in uncertainty brought by our spatial representation. We evaluate extrinsically in a human judgement task and find that our model?s behaviour does not differ significantly from the behaviour of real users.", "cited_paper_abstract": "User simulation is frequently used to train statistical dialog managers for task-oriented domains. At present, goal-driven simulators (those that have a persistent notion of what they wish to achieve in the dialog) require some task-specific engineering, making them impossible to evaluate intrinsically. Instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. In this paper, we propose the first fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. Our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. We evaluate on two standard dialog resources, the Communicator and Let?s Go datasets, and demonstrate that our model has substantially better fit to held out data than competing approaches. We also show that features derived from our model allow significantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs.", "citation": "An alternative approach to dealing with the lack of consistency is to extend N-grams to explicitly model user goals and condition utterances on them #OTHEREFR; #REFR.", "context": "However, they do not enforce user consistency throughout the dialog. Deterministic simulators with trainable parameters mitigate the lack of consistency using rules in conjunction with explicit goals or agendas #OTHEREFR. However, they require large amounts of hand crafting and restrict the variability in user responses, which by extension restricts the access of the dialog manager to potentially interesting states.[Citation]"}
{"citing_paper_id": "P13-2127", "cited_paper_id": "D12-1017", "citing_paper_abstract": "We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.", "cited_paper_abstract": "Computational approaches to metonymy resolution have focused almost exclusively on the local context, especially the constraints placed on a potentially metonymic word by its grammatical collocates. We expand such approaches by taking into account the larger context. Our algorithm is tested on the data from the metonymy resolution task (Task 8) at SemEval 2007. The results show that incorporation of the global context can improve over the use of the local context alone, depending on the types of metonymies addressed. As a second contribution, we move towards unsupervised resolution of metonymies, made feasible by considering ontological relations as possible readings. We show that such an unsupervised approach delivers promising results: it beats the supervised most frequent sense baseline and performs close to a supervised approach using only standard lexico-syntactic features.", "citation": "After collecting annotated data, the natural next step is to attempt class-based word-sense disambiguation (WSD) to predict the senses in Tables 3 and 4 using a state-of-the-art system like #REFR.", "context": "[Citation]We will consider a sense-assignment method (voting or MACE) as more appropriate if it provides the sense tags that are easiest to learn by our WSD system. However, learnability is only one possible parameter for quality, and we also want to develop an expert-annotated gold standard to compare our data against. We also consider the possibility of developing a sense-assignment method that relies both on the theoretical assumption behind the voting scheme and the latent-variable approach used by MACE."}
{"citing_paper_id": "P13-1042", "cited_paper_id": "D12-1035", "citing_paper_abstract": "Supervised training procedures for semantic parsers produce high-quality semantic parsers, but they have difficulty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data. We present a technique for developing semantic parsers for large databases based on a reduction to standard supervised training algorithms, schema matching, and pattern learning. Leveraging techniques from each of these areas, we develop a semantic parser for Freebase that is capable of parsing questions with an F1 that improves by 0.42 over a purely-supervised learning algorithm.", "cited_paper_abstract": "The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the question translation and the resulting query answering.", "citation": "There has been recent interest in producing such semantic parsers for large, heterogeneous databases like Freebase #OTHEREFR and Yago2 #REFR, which has driven the development of semi-supervised and distantlysupervised training methods for semantic parsing.", "context": "Semantic parsing is the task of translating natural language utterances to a formal meaning representation language #OTHEREFR.[Citation]Previous purely-supervised approaches have been limited to smaller domains and databases, such as the GeoQuery database, in part because of the cost of labeling enough samples to cover all of the logical constants involved in a domain. This paper investigates a reduction of the problem of building a semantic parser to three standard problems in semantics and machine learning: supervised training of a semantic parser, schema matching, and pattern learning. Figure 1 provides a visualization of our system architecture."}
{"citing_paper_id": "D14-1165", "cited_paper_id": "D12-1042", "citing_paper_abstract": "While relation extraction has traditionally been viewed as a task relying solely on textual data, recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data, the performance of relation extraction can be improved significantly. Following this new paradigm, we propose a tensor decomposition approach for knowledge base embedding that is highly scalable, and is especially suitable for relation extraction. By leveraging relational domain knowledge about entity type information, our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database. In addition, when applied to a relation extraction task, our approach alone is comparable to several existing systems, and improves the weighted mean average precision of a state-of-theart method by 10 points when used as a subcomponent.", "cited_paper_abstract": "Distant supervision for relation extraction (RE) ? gathering training data by aligning a database of facts with text ? is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.", "citation": "We compare the proposed TRESCAL model to RI13 #OTHEREFR and SU12 #REFR .", "context": "In addition, special types, PER, LOC, ORG and MISC, are assigned to the remaining 26,862 entities based on the predicted NER tags provided by the corpus. A type is considered incompatible to a relation or a surface pattern if in the training data, none of the argument entities of the relation belongs to the type. We use r = 400 and ? = 0.1 in TRESCAL to factorize the tensor.[Citation]We follow the protocol used in #OTHEREFR to evaluate the results. Given a relation as query, the top 1,000 entity pairs output by each system are collected and the top 100 ones are judged manually. Besides comparing individual models, we also report the results of combined models."}
{"citing_paper_id": "W13-1732", "cited_paper_id": "D12-1064", "citing_paper_abstract": "We report on the performance of two different feature sets in the Native Language Identification Shared Task (Tetreault et al 2013). Our feature sets were inspired by existing literature on native language identification and word networks. Experiments show that word networks have competitive performance against the baseline feature set, which is a promising result. We also present a discussion of feature analysis based on information gain, and an overview on the performance of different word network features in the Native Language Identification task.", "cited_paper_abstract": "The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem, typically using as features a mix of n-grams over characters and part of speech tags (for small and fixed n) and unigram function words. To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise. In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words, using both maxent and induced syntactic language model approaches to classification. After presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model.", "citation": "Researchers have used different types of features for the NLI problem, including but not limited to function words #OTHEREFRb); Adaptor Grammar features #REFR; L1-influence #OTHEREFR.", "context": "The NLI task is closely related to traditional NLP problems of authorship attribution #OTHEREFR, and shares many of the same features. Like authorship attribution, NLI is greatly benefitted by having function words and character n-grams as features #OTHEREFRb). Native languages form a part of an author?s socio-cultural and psychological profiles, thereby being related to author profiling #OTHEREFR.[Citation]State-of-the-art results are typically in the 80%-90% range, with results above 90% reported in some cases #OTHEREFRb). Note, however, that results vary greatly across different datasets, depending on the number of languages being considered, size and difficulty of data, etc."}
{"citing_paper_id": "C14-1220", "cited_paper_id": "D12-1110", "citing_paper_abstract": "The state-of-the-art methods used for relation classification are primarily based on statistical machine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language processing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings . Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the features are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.", "cited_paper_abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "citation": "Table 3: Classifier, their feature sets and the F1-score for relation classification. proposed by #REFR.", "context": "These systems design a series of features and take advantage of a variety of resources (WordNet, ProBank, and FrameNet, for example). RNN represents recursive neural networks for relation classification, as The corpus contains a Perl-based automatic evaluation tool. Classifier Feature Sets F1 SVM POS, stemming, syntactic patterns 60.1 SVM word pair, words in between 72.5 SVM POS, stemming, syntactic patterns, WordNet 74.8 MaxEnt POS, morphological, noun compound, thesauri, Google n-grams, WordNet 77.6 SVM POS, prefixes, morphological, WordNet, dependency parse, Levin classed, ProBank, FrameNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner 82.2 RNN - 74.8 POS, NER, WordNet 77.6 MVRNN - 79.1 POS, NER, WordNet 82.4 Proposed word pair, words around word pair, WordNet 82.7[Citation]This method learns vectors in the syntactic tree path that connect two nominals to determine their semantic relationship. The MVRNN model builds a single compositional semantics for the minimal constituent, including both nominals as RNN #OTHEREFR. It is almost certainly too much to expect a single fixed transformation to be able to capture the meaning combination effects of all natural language operators."}
{"citing_paper_id": "P13-1088", "cited_paper_id": "D12-1110", "citing_paper_abstract": "Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.", "cited_paper_abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "citation": "Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings #OTHEREFR; #REFRb).", "context": "This approach represents single words as distributional vectors, implying that a word?s meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words #OTHEREFR. While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition.[Citation]While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level. In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree."}
{"citing_paper_id": "P13-1104", "cited_paper_id": "D12-1133", "citing_paper_abstract": "We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.", "cited_paper_abstract": "Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.", "citation": "Greedy transition-based dependency parsing has been widely deployed because of its speed #OTHEREFR; #REFR.", "context": "Transition-based parsing gives complexities as low as O#OTHEREFR.1 The complexity is lower for projective parsing because a parser can deterministically skip tokens violating projectivity, while this property is not assumed for non-projective parsing. Nonetheless, it is possible to perform non-projective parsing in expected linear time because the amount of nonprojective dependencies is notably smaller #OTHEREFR. 1We refer parsing approaches that produce only projective dependency trees as projective parsing and both projective and non-projective dependency trees as non-projective parsing.[Citation]These approaches generate multiple transition sequences given a sentence, and pick one with the highest confidence. Coupled with dynamic programming, transition-based dependency parsing with beam search can be done very efficiently and gives significant improvement to parsing accuracy. One downside of beam search is that it always uses a fixed size of beam even when a smaller size of beam is sufficient for good results."}
{"citing_paper_id": "C14-1153", "cited_paper_id": "D13-1006", "citing_paper_abstract": "Classifying nouns into semantic categories (e.g., animals, food) is an important line of research in both cognitive science and natural language processing. We present a minimally supervised model for noun classification, which uses symmetric patterns (e.g., ?X and Y?) and an iterative variant of the k-Nearest Neighbors algorithm. Unlike most previous works, we do not use a predefined set of symmetric patterns, but extract them automatically from plain text, in an unsupervised manner. We experiment with four semantic categories and show that symmetric patterns constitute much better classification features compared to leading word embedding methods. We further demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-ofthe-art label propagation alternatives for this task. In experiments, our model obtains 82%-94% accuracy using as few as four labeled examples per category, emphasizing the effectiveness of simple search and representation techniques for this task.", "cited_paper_abstract": "Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error.", "citation": "Most works on noun classification to semantic categories require large amounts of human annotation to build training corpora for supervised algorithms #OTHEREFR; #REFR or rely on language-specific resources such as WordNet #OTHEREFR.", "context": "There are several types of semantic categories expressed by languages, e.g., objects, actions, and properties. We follow human development, acquiring coarse-grained categories and distinctions before detailed ones #OTHEREFR. Specifically, we focus on the major class of concrete ?things? #OTHEREFR.[Citation]Such heavy supervision is labor intensive and makes these models domain and language dependent. Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can compensate for the lack of input from the senses in text corpora. Our model therefore performs semantic category classification using only a small number of labeled seed words per category."}
{"citing_paper_id": "C14-1136", "cited_paper_id": "D13-1089", "citing_paper_abstract": "In this paper, we address the role of syntactic parsing for distributional similarity. On the one hand, we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers. On the other hand, we explore whether single unsupervised parsers, or their combination, can contribute to better distributional similarities, or even replace supervised parsing as a preprocessing step for word similarity. We evaluate distributional thesauri against manually created taxonomies both for English and German for five unsupervised parsers. While for English, a supervised parser is the best single parser in this evaluation, we find an unsupervised parser to work best for German. For both languages, we show significant improvements in word similarity when combining features from supervised and unsupervised parsers. To our knowledge, this is the first work where unsupervised parsers are systematically evaluated extrinsically in a semantic task, and the first work to show that unsupervised parsing can complement and even replace supervised parsing, when used as a pre-processing feature.", "cited_paper_abstract": "We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.", "citation": "#REFR also show that the results, using the WordNet based approach, are highly correlated to the results observed with Curran?s approach using a manually created thesaurus.", "context": "The score is 0.5 for terms that stand in a direct hypernym relation, and so on. While the absolute scores are hard to interpret due to inhomogeneity in the granularity of WordNet, they are well-suited for relative comparison when operating on the same set of target terms. The evaluation in this work is performed by comparing the average score of the top ten entries in the DT for each of the target terms and report separately on frequent and rare words.[Citation]This justifies the usage of manually created taxonomies for this evaluation."}
{"citing_paper_id": "W14-1619", "cited_paper_id": "D13-1089", "citing_paper_abstract": "Most traditional distributional similarity models fail to capture syntagmatic patterns that group together multiple word features within the same joint context. In this work we introduce a novel generic distributional similarity scheme under which the power of probabilistic models can be leveraged to effectively model joint contexts. Based on this scheme, we implement a concrete model which utilizes probabilistic n-gram language models. Our evaluations suggest that this model is particularly wellsuited for measuring similarity for verbs, which are known to exhibit richer syntagmatic patterns, while maintaining comparable or better performance with respect to competitive baselines for nouns. Following this, we propose our scheme as a framework for future semantic similarity models leveraging the substantial body of work that exists in probabilistic language modeling.", "cited_paper_abstract": "We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora.", "citation": "To automatically construct largerscale test-sets for semantic similarity, we sampled large target word subsets from our corpus and used Wordnet as a gold standard for their semantically similar words, following related previous evaluations #OTHEREFR; #REFR.", "context": "There is a shortage of large scale test-sets for semantic similarity. Popular test-sets such as Word- Sim353 and the TOEFL synonyms test contain only 353 and 80 test items respectively, and therefore make it difficult to obtain statistically significant results.[Citation]We constructed two test-sets for our primary evaluation, one for verb similarity and another for noun similarity. To perform the verb similarity evaluation, we randomly sampled 1,000 verbs from the target verb set, where the probability of each verb to be sampled is set to be proportional to its frequency in the learning corpus. Next, for each sampled verb a we constructed a Wordnet-based gold standard set of semantically similar words."}
{"citing_paper_id": "C14-1008", "cited_paper_id": "D13-1170", "citing_paper_abstract": "Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that exploits from characterto sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: the Stanford Sentiment Treebank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7% accuracy, and fine-grained classification, with 48.3% accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4%.", "cited_paper_abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "citation": "In #REFRb) the authors propose the Recursive Neural Tensor Network (RNTN) architecture, which represents a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor-based composition function.", "context": "The method learns vector space representation for multi-word phrases and exploits the recursive nature of sentences. In #OTHEREFR, it is proposed a matrix-vector recursive neural network model for semantic compositionality, which has the ability to learn compositional vector representations for phrases and sentences of arbitrary length. The vector captures the inherent meaning of the constituent, while the matrix captures how the meaning of neighboring words and phrases are changed.[Citation]Our approach differ from these previous works because it uses a feed-forward neural network instead of a recursive one. Moreover, it does not need any input about the syntactic structure of the sentence. Regarding convolutional networks for NLP tasks, in #OTHEREFR, the authors use a convolutional network for the semantic role labeling task with the goal avoiding excessive task-specific feature engineering."}
{"citing_paper_id": "C14-1041", "cited_paper_id": "D13-1174", "citing_paper_abstract": "We investigate the use of generalized representations (POS, morphological analysis and word clusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM). Our integration enables these models to learn richer lexical and reordering patterns, consider wider contextual information and generalize better in sparse data conditions. When interpolating generalized OSM models on the standard IWSLT and WMT tasks we observed improvements of up to +1.35 on the English-to-German task and +0.63 for the German-to-English task. Using automatically generated word classes in standard phrase-based models and the OSM models yields an average improvement of +0.80 across 8 language pairs on the IWSLT shared task.", "cited_paper_abstract": "Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific wordand phrase-level translations that are added to a standard translation model as ?synthetic? phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.", "citation": "#REFR found mapping data to 600 Och clusters useful, so we used this as well.", "context": "The overall goal was to study whether using unsupervised word classes can serve the same purpose as POS tags and to compare the two methods of annotating the data. We obtained Och clusters using the mkcls utility #OTHEREFR. This is generally run during the alignment process where data is divided into 50 classes to estimate IBM Model-4.[Citation]We additionally experimented with using 200 and 1000 classes. We integrated Och clusters as additional factors when training the phrase-translation models and used a monolingual n-gram model over cluster-ids built on the target-side of the in-domain corpus. Then we added a 5-gram OSM model over cluster-ids."}
{"citing_paper_id": "D14-1101", "cited_paper_id": "D14-1162", "citing_paper_abstract": "We propose a neural network approach to benefit from the non-linearity of corpuswide statistics for part-of-speech (POS) tagging. We investigated several types of corpus-wide information for the words, such as word embeddings and POS tag distributions. Since these statistics are encoded as dense continuous features, it is not trivial to combine these features comparing with sparse discrete features. Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features. By using several recent advances in the activation functions for neural networks, the proposed method marks new state-of-the-art accuracies for English POS tagging tasks.", "cited_paper_abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.", "citation": "In particular, in our experiments, we used two recently proposed algorithms, word2vec #OTHEREFR and glove #REFR, which are simple and scalable, although our method could use other word embeddings.", "context": "Word embeddings, or distributed word representations, embed the words into a low-dimensional continuous space. Most of the neural network applications for NLP use word embeddings #OTHEREFR highlights the benefit of word embeddings on sequential labeling tasks.[Citation]Word2vec trains the word embeddings to predict the words surrounding each word, and glove trains the word embeddings to predict the logarithmic count of the surrounding words of each word. Thus, these embeddings can be seen as the distributed versions of the distributional features since the word vectors compactly represent the distribution of the context in which a word appears. We normalized the word embeddings to unit length and used the average vector of training vocabulary for the unknown tokens."}
{"citing_paper_id": "W08-0329", "cited_paper_id": "E06-1005", "citing_paper_abstract": "Confusion network decoding has been the most successful approach in combining outputs from multiple machine translation (MT) systems in the recent DARPA GALE and NIST Open MT evaluations. Due to the varying word order between outputs from different MT systems, the hypothesis alignment presents the biggest challenge in confusion network decoding. This paper describes an incremental alignment method to build confusion networks based on the translation edit rate (TER) algorithm. This new algorithm yields significant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBN?s submission to the WMT08 shared translation task.", "cited_paper_abstract": "This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.", "citation": "The second, syscomb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in #REFR.", "context": "The following three rows show the scores of three combination outputs where the only difference was the hypothesis alignment method. The first, syscomb pw, corresponds BLEU System de-en fr-en worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the German-English #OTHEREFR.[Citation]The third, syscomb inc, corresponds to the incremental TER alignment presented in this paper. Finally, oracle corresponds to an estimate of the lower bound on the translation quality obtained by extracting the TER oracle output from the confusion networks generated by the incremental TER alignment. It is unlikely that there exists a set of weights that would yield the oracle output after decoding, though."}
{"citing_paper_id": "W12-3124", "cited_paper_id": "E06-1005", "citing_paper_abstract": "Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combination via confusion network decoding. Controlled experiments using identical pre-processing, decoding, and weight tuning methods on standard system combination evaluation sets are presented. Translation quality is assessed using case insensitive BLEU scores and bootstrapping is used to establish statistical significance of the score differences. All aligners yield significant BLEU score gains over the best individual system included in the combination. Incremental indirect hidden Markov model and a novel incremental inversion transduction grammar with flexible matching consistently yield the best translation quality, though keeping all things equal, the differences between aligners are relatively small. ?The work reported in this paper was carried out while the authors were at Raytheon BBN Technologies and ?RWTH Aachen University.", "cited_paper_abstract": "This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.", "citation": "#REFR proposed an alignment based on GIZA++ Toolkit which introduced word reordering not present in MSA, and Sim et al #OTHEREFR scoring.", "context": "Confusion network decoding has proven to be the most popular as it does not require deep N - best lists1 and operates on the surface strings. It has 1N -best lists of around N = 10 have been used in confusion network decoding yielding small gains over using 1-best also been shown to be very successful in combining speech recognition outputs #OTHEREFR. The first application of confusion network decoding in MT system combination appeared in #OTHEREFR where a multiple string alignment (MSA), made popular in biological sequence analysis, was applied to the MT system outputs.[Citation]Extensions of the last two are included in this study together with alignments based on hidden Markov model #OTHEREFR. System combinations produced via confusion network decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation #OTHEREFR. However, there has not been a comparison of the most popular hypothesis alignment algorithms using the same sets of MT system outputs and otherwise identical combination pipelines."}
{"citing_paper_id": "I08-1030", "cited_paper_id": "E06-1006", "citing_paper_abstract": "We propose a general approach for translating Chinese unknown words (UNK) for SMT. This approach takes advantage of the properties of Chinese word composition rules, i.e., all Chinese words are formed by sequential characters. According to the proposed approach, the unknown word is re-split into a subword sequence followed by subword translation with a subwordbased translation model. ?Subword? is a unit between character and long word. We found the proposed approach significantly improved translation quality on the test data of NIST MT04 and MT05. We also found that the translation quality was further improved if we applied named entity translation to translate parts of unknown words before using the subword-based translation.", "cited_paper_abstract": "We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level. The model is evaluated on the Europarl corpus for German-English and Finnish- English translation and shows improvements over state-of-the-art phrase-based models.", "citation": "Using unknown word modeling such as backoff models was proposed by #REFR.", "context": "We also used similar methods for translating named entities in this work. Some used stem and morphological analysis for UNKs such as #OTHEREFR. Morphological analysis is effective for inflective languages but not for Chinese.[Citation]Other proposed methods include paraphrasing #OTHEREFR that uses the feature of phonetic similarity. However, This approach does not work if no phonetic relationship is found. Splitting compound words into translatable subwords as we did in this work have been used by #OTHEREFR for languages other than Chinese where detailed splitting methods are proposed."}
{"citing_paper_id": "P13-2065", "cited_paper_id": "E06-1006", "citing_paper_abstract": "Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated. Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese.", "cited_paper_abstract": "We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level. The model is evaluated on the Europarl corpus for German-English and Finnish- English translation and shows improvements over state-of-the-art phrase-based models.", "citation": "Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as #OTHEREFR; #REFR.", "context": "Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness.[Citation]These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. In agglutinative languages, stem is the base part of word not including inflectional affixes. Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation."}
{"citing_paper_id": "D07-1101", "cited_paper_id": "E06-1011", "citing_paper_abstract": "We present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.", "cited_paper_abstract": "In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.", "citation": "Specifically, these approaches considered sibling relations of the modifier token #OTHEREFR; #REFR.", "context": "However, richer representations translate into higher complexity of the inference algorithms associated with the model. In dependency parsing, the basic first-order model is defined by a decomposition of a tree into headmodifier dependencies. Previous work extended this basic model to include second-order relations?i.e. dependencies that are adjacent to the main dependency of the factor.[Citation]In this paper we extend the parsing model with other types of second-order relations. In particular, we incorporate relations between the head and modifier tokens and the children of the modifier. One paradigmatic case where the relations we consider are relevant is PP-attachment."}
{"citing_paper_id": "D08-1017", "cited_paper_id": "E06-1011", "citing_paper_abstract": "We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al, 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers.", "cited_paper_abstract": "In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.", "citation": "We adapted this system to first perform unlabeled parsing, then label the arcs using a log-linear classifier with access to the full unlabeled parse #OTHEREFRb; #REFR.", "context": "The publicly available version of MSTParser performs parsing and labeling jointly.[Citation]In stacking experiments, the arc labels from the level 0 parser are also used as a feature.4 In the following subsections, we refer to our modification of the MSTParser as MST 1O (the arcfactored version) and MST 2O (the second-order arc-pair-factored version). All our experiments use the non-projective version of this parser. We refer to the MaltParser as Malt ."}
{"citing_paper_id": "P10-1003", "cited_paper_id": "E06-1011", "citing_paper_abstract": "This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a targetside tree fragment that corresponds to a source-side tree fragment is identified via word alignment and mapping rules that are automatically learned. Then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English.", "cited_paper_abstract": "In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.", "citation": "Due to the limitations of the parsing algorithm #REFR, we only use bigramand trigram-subtrees in our approach.", "context": "There are two main problems: MtoN (words) mapping and reordering, which often occur in translation. MtoN (words) mapping means that a source subtree with M words is mapped onto a target subtree with N words. For example, 2to3 means that a source bigram-subtree is mapped onto a target trigram-subtree.[Citation]We generate the mapping rules for the 2to2, 2to3, 3to3, and 3to2 cases. For trigram-subtrees, we only consider the parentchild-grandchild type. As for the use of other types of trigram-subtrees, we leave it for future work."}
{"citing_paper_id": "W11-0906", "cited_paper_id": "E06-1011", "citing_paper_abstract": "This paper suggests two ways of improving semantic role labeling (SRL). First, we introduce a novel transition-based SRL algorithm that gives a quite different approach to SRL. Our algorithm is inspired by shift-reduce parsing and brings the advantages of the transitionbased approach to SRL. Second, we present a self-learning clustering technique that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL?09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.", "cited_paper_abstract": "In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.", "citation": "Thus, not all dependency parsing algorithms, such as a maximum spanning tree algorithm #REFR, can be naively applied to semantic role labeling.", "context": "However, they are distinguished in two major ways. First, unlike dependency parsing that tries to find some kind of relation between any word pair, semantic role labeling restricts its search only to top-down relations between predicate and argument pairs. Second, dependency parsing requires one head for each word, so the final output is a tree, whereas semantic role labeling allows multiple predicates for each argument.[Citation]Some transition-based dependency parsing algorithms have been adapted to semantic role labeling and shown good results #OTHEREFR. However, these algorithms are originally designed for dependency parsing, so are not necessarily customized for semantic role labeling. Here, we present a novel transition-based algorithm dedicated to semantic role labeling."}
{"citing_paper_id": "P07-1054", "cited_paper_id": "E06-1025", "citing_paper_abstract": "This paper presents an application of PageR- ank, a random-walk model originally devised for ranking Web search results, to ranking WordNet synsets in terms of how strongly they possess a given semantic property. The semantic properties we use for exemplifying the approach are positivity and negativity, two properties of central importance in sentiment analysis. The idea derives from the observation that WordNet may be seen as a graph in which synsets are connected through the binary relation ?a term belonging to synset sk occurs in the gloss of synset si?, and on the hypothesis that this relation may be viewed as a transmitter of such semantic properties. The data for this relation can be obtained from eX- tended WordNet, a publicly available sensedisambiguated version of WordNet. We argue that this relation is structurally akin to the relation between hyperlinked Web pages, and thus lends itself to PageRank analysis. We report experimental results supporting our intuitions.", "cited_paper_abstract": "Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of ?subjective? terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation. This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter. We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as ?subjective? or ?objective? is available, which is usually not the case. In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation. We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection. Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.", "citation": "Some authors go a step further and attach these properties not to terms but to term senses #OTHEREFRa; #REFR.", "context": "Recent years have witnessed an explosion of work on opinion mining #OTHEREFR ?Fondo Unico per la Ricerca? funding scheme. cipline that deals with the quantitative and qualitative analysis of text for the purpose of determining its opinion-related properties (ORPs). An important part of this research has been the work on the automatic determination of the ORPs of terms, as e.g., in determining whether an adjective tends to give a positive, a negative, or a neutral nature to the noun phrase it appears in. While many works #OTHEREFR view them as graded (i.e., a term may be positive to a certain degree), with the underlying interpretation varying from fuzzy to probabilistic.[Citation]In this paper we contribute to this latter literature with a novel method for ranking the entire set of WordNet synsets, irrespectively of POS, according to their ORPs. Two rankings are produced, one according to positivity and one according to negativity. The two rankings are independent, i.e., it is not the case that one is the inverse of the other, since e.g., the least positive synsets may be negative or neutral synsets alike."}
{"citing_paper_id": "W14-4003", "cited_paper_id": "E06-1031", "citing_paper_abstract": "We introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT?s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference and machine translations, but subject to obeying the semantic frames in both. Resulting improvements support the presumption that ITGs, which constrain the allowable permutations between compositional segments across the reference and MT output, score the phrasal similarity of the semantic role fillers more accurately than the simple word alignment heuristics (bag-of-word alignment or maximum alignment) used in previous version of MEANT. The approach successfully integrates (1) the previously demonstrated extremely high coverage of cross-lingual semantic frame alternations by ITGs, with (2) the high accuracy of evaluating MT via weighted f-scores on the degree of semantic frame preservation.", "cited_paper_abstract": "Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences. In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation. Our measure can be exactly calculated in quadratic time. Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs. The correlation of the new measure with human judgment has been investigated systematically on two different language pairs. The experimental results will show that it significantly outperforms state-of-the-art approaches in sentence-level correlation. Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment.", "citation": "Across a variety of language pairs and genres, it has been shown thatMEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU #OTHEREFR, as well as editdistance based metrics such as CDER #REFR, WER #OTHEREFR.", "context": "We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics #OTHEREFR?emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers.[Citation]Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages #OTHEREFRb). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation."}
{"citing_paper_id": "D11-1042", "cited_paper_id": "E06-1032", "citing_paper_abstract": "Automatically produced texts (e.g. translations or summaries) are usually evaluated with n-gram based measures such as BLEU or ROUGE, while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes. In this paper we first present an indepth analysis of the state of the art in order to clarify this issue. After this, we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies. These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process. In addition, the greater the heterogeneity of the measures (which is measurable) the higher their combined reliability. These results support the use of heterogeneous measures in order to consolidate text evaluation results.", "cited_paper_abstract": "We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu?s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.", "citation": "Some drawbacks of BLEU are the lack of interpretability #OTHEREFRa), the fact that it is not necessary to increase BLEU to improve systems (Callison-burch and #REFR, the overscoring of statistical MT systems #OTHEREFR.", "context": "Some are based on edit distance, e.g., WER #OTHEREFR. Other metrics are based on computing lexical precision, e.g., BLEU #OTHEREFR. The lexical measure BLEU has been criticized in many ways.[Citation]The reaction to these criticisms has been focused on the development of more sophisticated measures in which candidate and reference translations are automatically annotated and compared at different linguistic levels. Some of the features employed include parts of speech #OTHEREFR. In general, when a higher linguistic level is incorporated, linguistic features at lower levels are preserved."}
{"citing_paper_id": "D10-1038", "cited_paper_id": "E06-1035", "citing_paper_abstract": "This work concerns automatic topic segmentation of email conversations. We present a corpus of email threads manually annotated with topics, and evaluate annotator reliability. To our knowledge, this is the first such email corpus. We show how the existing topic segmentation models (i.e., Lexical Chain Segmenter (LCSeg) and Latent Dirichlet Allocation (LDA)) which are solely based on lexical information, can be applied to emails. By pointing out where these methods fail and what any desired model should consider, we propose two novel extensions of the models that not only use lexical information but also exploit finer level conversation structure in a principled way. Empirical evaluation shows that LCSeg is a better model than LDA for segmenting an email thread into topical clusters and incorporating conversation structure into these models improves the performance significantly.", "cited_paper_abstract": "In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue. We extend prior work in two ways. We first apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries. We then explore the impact on performance of using ASR output as opposed to human transcription. Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries, the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries, the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues, such as cue phrases and overlapping speech, are better indicators for the toplevel prediction task. We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features, but do not change the general preference of approach for the two tasks.", "citation": "Their supervised approach performs significantly better than LCSeg. #REFR follow the same approaches as #OTHEREFR on both manual transcripts and ASR output of meetings.", "context": "Therefore, we approximate the optimal solution by spectral clustering #OTHEREFR. Moving to the task of segmenting dialogs, #OTHEREFR first proposed the lexical chain based unsupervised segmenter (LCSeg) and a supervised segmenter for segmenting meeting transcripts. Their supervised approach uses C4.5 and C4.5 rules binary classifiers with lexical and conversational features (e.g., cue phrase, overlap, speaker, silence, and lexical cohesion function).[Citation]They perform segmentation at both coarse (topic) and fine (subtopic) levels. For the topic level, they achieve similar results as #OTHEREFR, with the supervised approach outperforming LCSeg. However, for the subtopic level, LCSeg performs significantly better than the supervised one."}
{"citing_paper_id": "W06-1422", "cited_paper_id": "E06-1040", "citing_paper_abstract": "We propose to organise a series of sharedtask NLG events, where participants are asked to build systems with similar input/output functionalities, and these systems are evaluated with a range of different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events.", "cited_paper_abstract": "We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NIST, BLEU, and ROUGE. We find that NIST scores correlate best (> 0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.", "citation": "As we showed previously #REFR that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results.", "context": "Corpus-based evaluations: We will develop new, linguistically grounded evaluation metrics, and compare these to existing metrics including BLEU, NIST, and string-edit distance. We will also investigate how sensitive different metrics are to size and make-up of the reference corpus. Human-based preference judgements: We will investigate different experimental designs and methods for overcoming respondent bias (e.g. what is known as ?central tendency bias?, where some respondents avoid judgements at either end of a scale).[Citation]Task performance. This depends on the domain, but e.g. in the nurse-report domain we could use the methodology of #OTHEREFR, who showed medical professionals the texts, asked them to make a treatment decision, and then rated the correctness of the suggested treatments. As well as recommendations about the appropriateness of existing evaluation techniques, we hope the above experiments will allow us to suggest new evaluation techniques for NLG."}
{"citing_paper_id": "D13-1038", "cited_paper_id": "E06-1041", "citing_paper_abstract": "In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.", "cited_paper_abstract": "This paper discusses two problems that arise in the Generation of Referring Expressions: (a) numeric-valued attributes, such as size or location; (b) perspective-taking in reference. Both problems, it is argued, can be resolved if some structure is imposed on the available knowledge prior to content determination. We describe a clustering algorithm which is sufficiently general to be applied to these diverse problems, discuss its application, and evaluate its performance.", "citation": "Based on these two principles, previous works have developed different algorithms for perceptual grouping #OTHEREFR; #REFR.", "context": "Instead of keeping all possible n-ary relations (i.e., hyperarcs), we only retain those relations that are likely used by humans to produce referring expressions, based on two heuristics. The first heuristic is based on perceptual principles, also called the Gestalt Laws of perception #OTHEREFR, which describe how people group visually similar objects into entities or groups. Two well known principles of perceptual grouping are proximity and similarity #OTHEREFR: objects that lie close together are often perceived as groups; objects of similar shape, size or color are more likely to form groups than objects differing along these dimensions.[Citation]In our investigation, we adopted Gatt?s algorithm #OTHEREFR, which has shown to be more accurate for spatial grouping. Given the results from spatial grouping, we only retain hyperarcs that represent spatial relations between two objects, between two perceived groups, between one object and a perceived group, or between one object and the group it belongs to. The second heuristic is based on the observation that, given a certain orientation, people tend to use a relatum that is closer to the referent than more distant relata."}
{"citing_paper_id": "D11-1063", "cited_paper_id": "E06-1042", "citing_paper_abstract": "Metaphor is ubiquitous in text, even in highly technical text. Correct inference about textual entailment requires computers to distinguish the literal and metaphorical senses of a word. Past work has treated this problem as a classical word sense disambiguation task. In this paper, we take a new approach, based on research in cognitive linguistics that views metaphor as a method for transferring knowledge from a familiar, well-understood, or concrete domain to an unfamiliar, less understood, or more abstract domain. This view leads to the hypothesis that metaphorical word usage is correlated with the degree of abstractness of the word?s context. We introduce an algorithm that uses this hypothesis to classify a word sense in a given context as either literal (denotative) or metaphorical (connotative). We evaluate this algorithm with a set of adjectivenoun phrases (e.g., in dark comedy, the adjective dark is used metaphorically; in dark hair, it is used literally) and with the TroFi (Trope Finder) Example Base of literal and nonliteral usage for fifty verbs. We achieve state-of-theart performance on both datasets.", "cited_paper_abstract": "In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extrasentential context. Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%. Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community.", "citation": "Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task #REFR.", "context": "Metaphor is a natural consequence of our ability to reason by analogy #OTHEREFR. It is so common in our daily language that we rarely notice it #OTHEREFR. Identifying metaphorical word usage is important for reasoning about the implications of text.[Citation]Here, we take a different approach to the problem. Lakoff and Johnson #OTHEREFR argue that metaphor is a method for transferring knowledge from a concrete domain to an abstract domain. Therefore we hypothesize that the degree of abstractness in a word?s context is correlated with the likelihood that the word is used metaphorically."}
{"citing_paper_id": "W06-2004", "cited_paper_id": "E06-2007", "citing_paper_abstract": "This paper describes a method of discriminating ambiguous names that relies upon features found in corpora of a more abundant language. In particular, we discriminate ambiguous names in Bulgarian, Romanian, and Spanish corpora using information derived from much larger quantities of English data. We also mix together occurrences of the ambiguous name found in English with the occurrences of the name in the language in which we are trying to discriminate. We refer to this as a language salad, and find that it often results in even better performance than when only using English or the language itself as the source of information for discrimination.", "cited_paper_abstract": "This paper describes an unsupervised knowledge?lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus. It is based on the use of global criterion functions that assess the quality of a clustering solution.", "citation": "Note that we are currently developing methods to automatically select the number of clusters in the data (e.g., #REFR), although we have not yet applied them to this particular work.", "context": "If a word does not have a corresponding entry in the co?occurrence matrix, then it is simply removed from the context. After all the words in the context are checked, then all of the vectors that are selected are averaged together to create a vector representation of the context. Then these contexts are clustered into a pre? specified number of clusters using the k?means algorithm.[Citation]"}
{"citing_paper_id": "W13-2320", "cited_paper_id": "E06-3008", "citing_paper_abstract": "In this paper, we discuss our efforts to annotate nominals in the Hindi Treebank with the semantic property of animacy. Although the treebank already encodes lexical information at a number of levels such as morph and part of speech, the addition of animacy information seems promising given its relevance to varied linguistic phenomena. The suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution, syntactic parsing, verb classification and argument differentiation.", "cited_paper_abstract": "This paper presents results from experiments in automatic classification of animacy for Norwegian nouns using decision-tree classifiers. The method makes use of relative frequency measures for linguistically motivated morphosyntactic features extracted from an automatically annotated corpus of Norwegian. The classifiers are evaluated using leave-oneout training and testing and the initial results are promising (approaching 90% accuracy) for high frequency nouns, however deteriorate gradually as lower frequency nouns are classified. Experiments attempting to empirically locate a frequency threshold for the classification method indicate that a subset of the chosen morphosyntactic features exhibit a notable resilience to data sparseness. Results will be presented which show that the classification accuracy obtained for high frequency nouns (with absolute frequencies >1000) can be maintained for nouns with considerably lower frequencies (?50) by backing off to a smaller set of features at classification.", "citation": "We also plan to utilize the annotated data to build a data driven automatic animacy classifier (?#REFR.", "context": "The annotation was followed by the inter-annotator agreement study for evaluating the confusion over the categories chosen for annotation. The annotators have a significant understanding of the property of animacy as shown by the higher values of Kappa (?). In future, we plan to continue the animacy annotation for the whole Hindi Treebank.[Citation]From a linguistic perspective, an annotation of the type, as discussed in this paper, will also be of great interest for studying information dynamics and see how semantics interacts with syntax in Hindi."}
{"citing_paper_id": "D13-1178", "cited_paper_id": "E09-1005", "citing_paper_abstract": "Chambers and Jurafsky (2009) demonstrated that event schemas can be automatically induced from text corpora. However, our analysis of their schemas identifies several weaknesses, e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor. It is due in part to their pair-wise representation that treats subjectverb independently from verb-object. This often leads to subject-verb-object triples that are not meaningful in the real-world. We present a novel approach to inducing open-domain event schemas that overcomes these limitations. Our approach uses cooccurrence statistics of semantically typed relational triples, which we call Rel-grams (relational n-grams). In a human evaluation, our schemas outperform Chambers?s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community.", "cited_paper_abstract": "In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.", "citation": "This is a standard practice to handle large graphs #REFR.", "context": "It is a sum of all its neighbors? page ranks, weighted by the edge weights; d is the damping probability, which we set to be 0.85 in our implementation. The solution is computed iteratively by initializing the page rank of Q to 1 and all others to 0, then recomputing page rank values until they converge to within a small . This computation remains scalable, since we restrict it to subgraphs a small number of hops away from the query node.[Citation]"}
{"citing_paper_id": "E14-2003", "cited_paper_id": "E09-1005", "citing_paper_abstract": "This paper presents the linguistic analysis infrastructure developed within the XLike project. The main goal of the implemented tools is to provide a set of functionalities supporting the XLike main objectives: Enabling cross-lingual services for publishers, media monitoring or developing new business intelligence applications. The services cover seven major and minor languages: English, German, Spanish, Chinese, Catalan, Slovenian, and Croatian. These analyzers are provided as web services following a lightweigth SOA architecture approach, and they are publically accessible and shared through META-SHARE.", "cited_paper_abstract": "In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.", "citation": "The used WSD engine is the UKB #REFR implementation provided by FreeLing #OTHEREFR.", "context": "The goal of WSD is to map specific languages to a common semantic space, in this case, WN synsets. Thanks to existing connections between WN and other resources, SUMO and Open- CYC sense codes are also output when available. Thanks to PredicateMatrix, the obtained concepts can be projected to FrameNet, achieving a normalization of the semantic roles produced by the SRL (which are treebank-dependent, and thus, not the same for all languages).[Citation]"}
{"citing_paper_id": "P13-1087", "cited_paper_id": "E09-1005", "citing_paper_abstract": "This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval- 2 unsupervised systems reached.", "cited_paper_abstract": "In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.", "citation": "They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text. #OTHEREFR; #REFR.", "context": "As the target words are typically taken from the same text string, they are naturally expected to be interrelated. Disambiguation of a word should affect other words as an important clue. From such characteristics of the task, knowledge-based unsupervised approaches have been extensively studied.[Citation]On the graph structure of lexical knowledge base (LKB), random-walk or other well-known graph-based techniques have been applied to find mutually related senses among target words. Unlike earlier studies disambiguating word-by-word, the graph-based methods obtain sense-interdependent solution for target words. However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context."}
{"citing_paper_id": "P10-4011", "cited_paper_id": "E09-1028", "citing_paper_abstract": "The Deep Web is the collection of information repositories that are not indexed by search engines. These repositories are typically accessible through web forms and contain dynamically changing information. In this paper, we present a system that allows users to access such rich repositories of information on mobile devices using spoken language.", "cited_paper_abstract": "Mobile voice-enabled search is emerging as one of the most popular applications abetted by the exponential growth in the number of mobile devices. The automatic speech recognition (ASR) output of the voice query is parsed into several fields. Search is then performed on a text corpus or a database. In order to improve the robustness of the query parser to noise in the ASR output, in this paper, we investigate two different methods to query parsing. Both methods exploit multiple hypotheses from ASR, in the form of word confusion networks, in order to achieve tighter coupling between ASR and query parsing and improved accuracy of the query parser. We also investigate the results of this improvement on search accuracy. Word confusionnetwork based query parsing outperforms ASR 1-best based query-parsing by 2.7% absolute and the search performance improves by 1.8% absolute on one of our data sets.", "citation": "Details of this extraction are presented in #REFR.", "context": "For a yellow-pages type of query, Where is the Saigon Kitchen in Austin, Texas?, the pertinent search parameters that are parsed out are business-name: Saigon Kitchen, city: Austin, and state: Texas, which are used to construct a search string to search the Yellowpages website. These are just two examples of the kinds of dynamic user queries that we encounter. Within each broad category, there is a wide variety of the sub-types of user queries, and for each sub-type, we have to parse out different search parameters and use different web-forms.[Citation]It is quite likely that many of the dynamic queries may not have all the pertinent search parameters explicitly outlined. For example, a mass transit query may be When is the next train to Princeton?. The bare minimum search parameters needed to answer this query are a from-location, and a to-location."}
{"citing_paper_id": "N13-1023", "cited_paper_id": "E09-1040", "citing_paper_abstract": "The study presented in this work is a first effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics. We address the goal of achieving a system that balances translation accuracy and latency. In order to improve ASR performance for our diverse data set, adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful. In order to improve machine translation (MT) performance, techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use. We also experiment with inserting text segmenters of various types between ASR and MT in a series of real-time translation experiments. Among other results, our experiments demonstrate that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation. It was also found to be important to synchronize various pipeline components in order to minimize latency.", "cited_paper_abstract": "This paper presents the end-to-end evaluation of an automatic simultaneous translation system, built with state-of-the-art components. It shows whether, and for which situations, such a system might be advantageous when compared to a human interpreter. Using speeches in English translated into Spanish, we present the evaluation procedure and we discuss the results both for the recognition and translation components as well as for the overall system. Even if the translation process remains the Achilles? heel of the system, the results show that the system can keep at least half of the information, becoming potentially useful for final users.", "citation": "Nevertheless, the models as well as the pipeline have been optimized in several ways to achieve tasks such as high quality offline speech translation #OTHEREFR; #REFR, etc.", "context": "The quality of automatic speech-to-text and speechto-speech (S2S) translation has improved so significantly over the last several decades that such systems are now widely deployed and used by an increasing number of consumers. Under the hood, the individual components such as automatic speech recognition (ASR), machine translation (MT) and text-tospeech synthesis (TTS) that constitute a S2S system are still loosely coupled and typically trained on disparate data and domains.[Citation]The design of a S2S translation system is highly dependent on the nature of the audio stimuli. For example, talks, lectures and audio broadcasts are typically long and require appropriate segmentation strategies to chunk the input signal to ensure high quality translation. In contrast, single utterance translation in several consumer applications (apps) are typically short and can be processed without the need for additional chunking."}
{"citing_paper_id": "P11-1008", "cited_paper_id": "E09-1044", "citing_paper_abstract": "We describe an exact decoding algorithm for syntax-based statistical translation. The approach uses Lagrangian relaxation to decompose the decoding problem into tractable subproblems, thereby avoiding exhaustive dynamic programming. The method recovers exact solutions, with certificates of optimality, on over 97% of test examples; it has comparable speed to state-of-the-art decoders.", "cited_paper_abstract": "We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-to- English evaluation task.", "citation": "#REFR show that exact FST decoding is feasible for a phrase-based system with limited reordering #OTHEREFR show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars).", "context": "A variety of approximate decoding algorithms have been explored for syntax-based translation systems, including cube-pruning #OTHEREFR. Recent work has developed decoding algorithms based on finite state transducers (FSTs).[Citation]Approximate search methods are used for more complex reordering models or grammars. The FST algorithms are shown to produce higher scoring solutions than cube-pruning on a large proportion of examples. Lagrangian relaxation is a classical technique in combinatorial optimization #OTHEREFR."}
{"citing_paper_id": "D14-1060", "cited_paper_id": "E09-1057", "citing_paper_abstract": "Term translation is of great importance for statistical machine translation (SMT), especially document-informed SMT. In this paper, we investigate three issues of term translation in the context of documentinformed SMT and propose three corresponding models: (a) a term translation disambiguation model which selects desirable translations for terms in the source language with domain information, (b) a term translation consistency model that encourages consistent translations for terms with a high strength of translation consistency throughout a document, and (c) a term bracketing model that rewards translation hypotheses where bracketable source terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT and evaluate their effectiveness on NIST Chinese- English translation tasks with large-scale training data. Experiment results show that all three models can achieve significant improvements over the baseline. Additionally, we can obtain a further improvement when combining the three models.", "cited_paper_abstract": "We present a language-pair independent terminology extraction module that is based on a sub-sentential alignment system that links linguistically motivated phrases in parallel texts. Statistical filters are applied on the bilingual list of candidate terms that is extracted from the alignment output. We compare the performance of both the alignment and terminology extraction module for three different language pairs (French-English, French-Italian and French-Dutch) and highlight languagepair specific problems (e.g. different compounding strategy in French and Dutch). Comparisons with standard terminology extraction programs show an improvement of up to 20% for bilingual terminology extraction and competitive results (85% to 90% accuracy) for monolingual terminology extraction, and reveal that the linguistically based alignment module is particularly well suited for the extraction of complex multiword terms.", "citation": "The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms #OTHEREFR; #REFR.", "context": "Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NC- value #OTHEREFR. The extracted monolingual terms are then paired together #OTHEREFR.[Citation]In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair."}
{"citing_paper_id": "E14-1041", "cited_paper_id": "E09-1076", "citing_paper_abstract": "Previous methods for extracting attributes (e.g., capital, population) of classes (Empires) from Web documents or search queries assume that relevant attributes occur verbatim in the source text. The extracted attributes are short phrases that correspond to quantifiable properties of various instances (ottoman empire, roman empire, mughal empire) of the class. This paper explores the extraction of noncontiguous class attributes (manner (it) claimed legitimacy of rule), from factseeking and explanation-seeking queries. The attributes cover properties that are not always likely to be extracted as short phrases from inherently-noisy queries.", "cited_paper_abstract": "An important part of question answering is ensuring a candidate answer is plausible as a response. We present a flexible approach based on discriminative preference ranking to determine which of a set of candidate answers are appropriate. Discriminative methods provide superior performance while at the same time allow the flexibility of adding new and diverse features. Experimental results on a set of focused What ...? and Which ...? questions show that our learned preference ranking methods perform better than alternative solutions to the task of answer typing. A gain of almost 0.2 in MRR for both the first appropriate and first correct answers is observed along with an increase in precision over the entire range of recall.", "citation": "Sophisticated techniques are sometimes employed to identify the type of the expected answers of open-domain questions #REFR.", "context": "To increase diversity within a ranked list of attributes, the extraction method in this paper employs a synonym vocabulary to approximately identify groups of near-duplicate attributes. As reported for previous methods, the resulting lists may still contain lexically different but semantically equivalent attributes. Scenarios where detecting all equivalent attributes is important may benefit from other techniques for paraphrase acquisition #OTHEREFR.[Citation]In comparison, the loose typing of the values of our noncontiguous attributes is mostly coarse-grained. It relies on wh-prefixes (when, how long, where, how) and possibly subsequent words (what nutritional value) from the queries, to determine whether the values are expected to be a date/time, length/duration, location, manner, nutritional value etc. Relations extracted from document sentences #OTHEREFR)."}
{"citing_paper_id": "W11-2109", "cited_paper_id": "E09-1087", "citing_paper_abstract": "Current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations. We propose a truly automatic evaluation metric based on IBM1 lexicon probabilities which does not need any reference translations. Several variants of IBM1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM1 scores are competitive with the classic evaluation metrics, the most promising being IBM1 scores calculated on morphemes and POS-4grams.", "cited_paper_abstract": "This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, first introduced for this task by (Collins, 2002). Experiments with an iterative training on standard-sized supervised (manually annotated) dataset (106 tokens) combined with a relatively modest (in the order of 108 tokens) unsupervised (plain) data in a bagging-like fashion showed significant improvement of the POS classification task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %).", "citation": "Spanish, French, German and English POS tags were produced using the TreeTagger1, and the Czech texts are tagged using the COMPOST tagger #REFR.", "context": "The IBM1 probabilities necessary for the IBM1 scores are learnt using the WMT 2010 News Commentary bilingual corpora consisting of the Spanish-English, French-English, German-English and Czech-English parallel texts.[Citation]The morphemes for all languages are obtained using the Morfessor tool #OTHEREFR. The tool is corpus-based and language-independent: it takes a text as input and produces a segmentation of the word forms observed in the text. The obtained results are not strictly linguistic, however they often resemble a linguistic morpheme segmentation."}
{"citing_paper_id": "D11-1106", "cited_paper_id": "E09-1097", "citing_paper_abstract": "Machine-produced text often lacks grammaticality and fluency. This paper studies grammaticality improvement using a syntax-based algorithm based on CCG. The goal of the search problem is to find an optimal parse tree among all that can be constructed through selection and ordering of the input words. The search problem, which is significantly harder than parsing, is solved by guided learning for best-first search. In a standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system.", "cited_paper_abstract": "Abstract-like text summarisation requires a means of producing novel summary sentences. In order to improve the grammaticality of the generated sentence, we model a global (sentence) level syntactic structure. We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree. We treat the allocation of modifiers to heads as a weighted bipartite graph matching (or assignment) problem, a well studied problem in graph theory. Using BLEU to measure performance on a string regeneration task, we found an improvement, illustrating the benefit of the spanning tree approach armed with an argument satisfaction model.", "citation": "Our system gave higher BLEU scores than the dependency-based system of #REFR.", "context": "We proposed a grammaticality improvement system using CCG, and evaluated it using a standard input word ordering task.[Citation]We showed that the complex search problem can be solved effectively using guided learning for best-first search. Potential improvements to our system can be made in several areas. First, a large scale language model can be incorporated into our model in the search algorithm, or through reranking."}
{"citing_paper_id": "E14-1060", "cited_paper_id": "E09-2008", "citing_paper_abstract": "We present a semi-supervised approach to the problem of paradigm induction from inflection tables. Our system extracts generalizations from inflection tables, representing the resulting paradigms in an abstract form. The process is intended to be language-independent, and to provide human-readable generalizations of paradigms. The tools we provide can be used by linguists for the rapid creation of lexical resources. We evaluate the system through an inflection table reconstruction task using Wiktionary data for German, Spanish, and Finnish. With no additional corpus information available, the evaluation yields per word form accuracy scores on inflecting unseen base forms in different languages ranging from 87.81% (German nouns) to 99.52% (Spanish verbs); with additional unlabeled text corpora available for training the scores range from 91.81% (German nouns) to 99.58% (Spanish verbs). We separately evaluate the system in a simulated task of Swedish lexicon creation, and show that on the basis of a small number of inflection tables, the system can accurately collect from a list of noun forms a lexicon with inflection information ranging from 100.0% correct (collect 100 words), to 96.4% correct (collect 1000 words).", "cited_paper_abstract": "Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the ?Mathematical Operators. Unicode block.", "citation": "We calculate the LCS by calculating intersections of finite-state machines that encode all subsequences of all words, using the foma finite-state toolkit #REFR.", "context": "The first step, extracting the LCS from a collection of strings, is the well-known multiple longest common subsequence problem (MLCS). It is known to be NP-hard #OTHEREFR. Although the number of strings to find the LCS from may be rather large in real-world data, we find that a few sensible heuristic techniques allow us to solve this problem efficiently for practical linguistic material, i.e., inflection tables.[Citation]While for most tables there is only one way to segment the LCS in the various forms, some ambiguous corner cases need to be resolved by imposing additional criteria for the segmentation, given in steps 2(a) and 2(b). As an example, consider a snippet of a small conjugation table for the Spanish verb comprar (to buy), comprar#compra#compro. Obviously the LCS is compr?however, this can be distributed in two different ways across the strings, as seen below."}
{"citing_paper_id": "W13-1903", "cited_paper_id": "E12-1019", "citing_paper_abstract": "The clinical narrative contains a great deal of valuable information that is only understandable in a temporal context. Events, time expressions, and temporal relations convey information about the time course of a patient?s clinical record that must be understood for many applications of interest. In this paper, we focus on extracting information about how time expressions and events are related by narrative containers. We use support vector machines with composite kernels, which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees. Our experiments show that using tree kernels in addition to standard feature kernels improves F1 classification for this task.", "cited_paper_abstract": "We present work on linking events and fluents (i.e., relations that hold for certain periods of time) to temporal information in text, which is an important enabler for many applications such as timelines and reasoning. Previous research has mainly focused on temporal links for events, and we extend that work to include fluents as well, presenting a common methodology for linking both events and relations to timestamps within the same sentence. Our approach combines tree kernels with classical feature-based learning to exploit context and achieves competitive F1-scores on event-time linking, and comparable F1- scores for fluents. Our best systems achieve F1-scores of 0.76 on events and 0.72 on fluents.", "citation": "This particular configuration was tested because it is most comparable to the bag of word and bag of POS kernels from #REFR, and should help show whether the tree kernel is providing anything over an equivalent set of basic features.", "context": "CLOSEST-R hypothesizes a CON- TAINS link between every EVENT and the closest TIMEX3, which will make many more links. The next configuration, Flat Features, uses the token and part of speech features along with argument semantics features, as described in Section 3. While this feature set may not seem exhaustive, in preliminary work many traditional relation extraction features were tried and found to not have much effect.[Citation]We then examine several composite kernels, all using the same feature kernel, but using different tree kernel-based representations. First, we use a composite kernel which uses the bag of word and bag of POS tree views, as in Hovy et al#OTHEREFR. Next, we add in two additional tree views to the tree kernel, Path-Enclosed Tree and Path Tree, which are intended to examine the effect of using traditional syntax, and the long distance features that they enable."}
{"citing_paper_id": "W13-1204", "cited_paper_id": "E12-1029", "citing_paper_abstract": "This paper describes an approach for investigating the representation of events and their distribution in a corpus. We collect and analyze statistics about subject-verb-object triplets and their content, which helps us compare corpora belonging to the same domain but to different genre/text type. We argue that event structure is strongly related to the genre of the corpus, and propose statistical properties that are able to capture these genre differences. The results obtained can be used for the improvement of Information Extraction.", "cited_paper_abstract": "Most event extraction systems are trained with supervised learning and rely on a collection of annotated documents. Due to the domain-specificity of this task, event extraction systems must be retrained with new annotated data for each domain. In this paper, we propose a bootstrapping solution for event role filler extraction that requires minimal human supervision. We aim to rapidly train a state-of-the-art event extraction system using a small set of ?seed nouns? for each event role, a collection of relevant (in-domain) and irrelevant (outof-domain) texts, and a semantic dictionary. The experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the MUC-4 data set, and achieves performance levels comparable to supervised training with 700 manually annotated documents.", "citation": "We also plan to develop a hybrid methodology, to combine the presented corpus-driven analysis with opendomain techniques for pattern acquisition, #OTHEREFR; #REFR.", "context": "This helps us quickly find all realizations of a particular pattern?for example, all semantic classes that appear in the corpus as objects of verbs that have semantic class LAUNCH-PRODUCT. The subsequent analysis of the frequency lists can help improve the performance of the IE system by suggesting refinements to the ontology and the lexicon, as well as patterns and inference rules appropriate for the particular genre of the corpus. Our current work includes the adaptation of the IE system developed for the analyst reports to the general news corpus devoted to the same topics.[Citation]The approach outlined here for analyzing the distributions of features in documents is useful for studying events within the context of a corpus. It demonstrates that event structure depends on the text genre, and that genre differences can be easily captured and measured. By analyzing document statistics and the output of the pattern-mining, we can demonstrate significant differences between the genres of analyst reports and general news, such as: sentence length, distribution of the domain vocabulary in the text, selectional preference in domain-specific verbs, word co-occurrences, usage of pronouns and proper names."}
{"citing_paper_id": "S13-2039", "cited_paper_id": "E12-1060", "citing_paper_abstract": "This paper describes our system for Task 11 of SemEval-2013. In the task, participants are provided with a set of ambiguous search queries and the snippets returned by a search engine, and are asked to associate senses with the snippets. The snippets are then clustered using the sense assignments and systems are evaluated based on the quality of the snippet clusters. Our system adopts a preexisting Word Sense Induction (WSI) methodology based on Hierarchical Dirichlet Process (HDP), a non-parametric topic model. Our system is trained over extracts from the full text of English Wikipedia, and is shown to perform well in the shared task.", "cited_paper_abstract": "We apply topic modelling to automatically induce word senses of a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses. We start by exploring the utility of standard topic models for word sense induction (WSI), with a pre-determined number of topics (=senses). We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the WSI task. We go on to establish state-of-the-art results over two WSI datasets, and apply the proposed model to a novel sense detection task.", "citation": "Our system is based on the WSI methodology proposed by #REFR for the task of novel word sense detection.", "context": "[Citation]The core machinery of our system is driven by a Latent Dirichlet Allocation #OTHEREFR. In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and the association of topics with documents is represented by a multinomial distribution of topics, with one distribution per document."}
{"citing_paper_id": "P13-2085", "cited_paper_id": "E12-1065", "citing_paper_abstract": "We present an automatic method for analyzing sentiment dynamics between characters in plays. This literary format?s structured dialogue allows us to make assumptions about who is participating in a conversation. Once we have an idea of who a character is speaking to, the sentiment in his or her speech can be attributed accordingly, allowing us to generate lists of a character?s enemies and allies as well as pinpoint scenes critical to a character?s emotional development. Results of experiments on Shakespeare?s plays are presented along with discussion of how this work can be extended to unstructured texts (i.e. novels).", "cited_paper_abstract": "Better representations of plot structure could greatly improve computational methods for summarizing and generating stories. Current representations lack abstraction, focusing too closely on events. We present a kernel for comparing novelistic plots at a higher level, in terms of the cast of characters they depict and the social relationships between them. Our kernel compares the characters of different novels to one another by measuring their frequency of occurrence over time and the descriptive and emotional language associated with them. Given a corpus of 19thcentury novels as training data, our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure.", "citation": "However, we believe that most previous work (except possibly #REFR) has failed to directly address the root cause of Ramsay?s skepticism: can computers extract the emotions encoded in a narrative.", "context": "In fact, some humanists believe literary analysis is so closely tied to the human condition that it is impossible for computers to perform. In his book Reading Machines: Toward an Algorithmic Criticism, Stephen Ramsay #OTHEREFR states: Tools that can adjudicate the hermeneutical parameters of human reading experiences...stretch considerably beyond the most ambitious fantasies of artificial intelligence. Antonio Roque #OTHEREFR has challenged Ramsay?s claim, and certainly there has been successful work done in the computational analysis and modeling of narratives, as we will review in the next section.[Citation]For example, can the love that Shakespeare?s Juliet feels for Romeo be computationally tracked. Empathizing with characters along their journeys to emotional highs and lows is often what makes a narrative compelling for a reader, and therefore we believe mapping these journeys is the first step in capturing the human reading experience. Unfortunately but unsurprisingly, computational modeling of the emotional relationships described in natural language text remains a daunting technical challenge."}
{"citing_paper_id": "D14-1203", "cited_paper_id": "E12-2021", "citing_paper_abstract": "Distant supervision has become the leading method for training large-scale relation extractors, with nearly universal adoption in recent TAC knowledge-base population competitions. However, there are still many questions about the best way to learn such extractors. In this paper we investigate four orthogonal improvements: integrating named entity linking (NEL) and coreference resolution into argument identification for training and extraction, enforcing type constraints of linked arguments, and partitioning the model by relation type signature. We evaluate sentential extraction performance on two datasets: the popular set of NY Times articles partially annotated by Hoffmann et al. (2011) and a new dataset, called GORECO, that is comprehensively annotated for 48 common relations. We find that using NEL for argument identification boosts performance over the traditional approach (named entity recognition with string match), and there is further improvement from using argument types. Our best system boosts precision by 44% and recall by 70%.", "cited_paper_abstract": "We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org", "citation": "To get relation annotations over coreference clusters, we define two human annotation tasks and use the BRAT #REFR tool for visualization and relation and coreference annotations.", "context": "So as long as a relation extraction system extracts a relation annotation s?R(m ,m ) where m and m are allowed options (based on text spans), it will get credit for extracting the relation annotation. We introduce the GORECO (gold relations and coreference) evaluation to satisfy these constraints. We start with an existing gold coreference dataset, ACE 2004 #OTHEREFR newswire, consisting of 128 documents.[Citation]Relation Annotation The annotator is presented with a document with gold mentions indicated and asked to determine for each sentence, what facts involving target relations are expressed by the sentence. They draw an arrow for each fact and label it with the relation. They also have the ability to add mentions not present (ReAnn mentions)."}
{"citing_paper_id": "P14-5016", "cited_paper_id": "E12-2021", "citing_paper_abstract": "In this paper, we present a flexible approach to the efficient and exhaustive manual annotation of text documents. For this purpose, we extend WebAnno (Yimam et al., 2013) an open-source web-based annotation tool. While it was previously limited to specific annotation layers, our extension allows adding and configuring an arbitrary number of layers through a web-based UI. These layers can be annotated separately or simultaneously, and support most types of linguistic annotations such as spans, semantic classes, dependency relations, lexical chains, and morphology. Further, we tightly integrate a generic machine learning component for automatic annotation suggestions of span annotations. In two case studies, we show that automatic annotation suggestions, combined with our split-pane UI concept, significantly reduces annotation time.", "cited_paper_abstract": "We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org", "citation": "However, most of existing web-based annotation tools, such as GATE #OTHEREFR or brat #REFR, depend on external preprocessing and postprocessing plugins or on web services.", "context": "These support all of the original layers and allow the user to define arbitrary custom annotation layers based on either of these structures. Additionally, our approach allows maintaining multiple properties on annotations, e.g. to support morphological annotations, while previously only one property per annotation was supported. Automatic suggestion of annotations is based on machine learning, which is common practice in annotation tools.[Citation]These tools have limitations regarding adaptability (difficulty to adapt to other annotation tasks), reconfigurability (generating a classifier when new features and training documents are available is complicated), and reusability (requires manual intervention to add newly annotated documents into the iteration). For our approach, we assume that an annotator actually does manually verify all annotations to produce a completely labeled dataset. This task can be sped up by automatically suggesting annotations that the annotator may then either accept or correct."}
{"citing_paper_id": "W12-4304", "cited_paper_id": "E12-2021", "citing_paper_abstract": "Anatomical entities such as kidney, muscle and blood are central to much of biomedical scientific discourse, and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts. Although a number of resources and methods addressing aspects of the task have been introduced, there have so far been no annotated corpora for training and evaluating systems for broad-coverage, open-domain anatomical entity mention detection. We introduce the AnEM corpus, a domainand species-independent resource manually annotated for anatomical entity mentions using a fine-grained classification system. The corpus texts are selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientific literature. We demonstrate the use of the corpus through an evaluation of the broad-coverage MetaMap tagger and a CRF-based system trained on the corpus data, considering also a combination of these two methods. The combined system demonstrates a promising level of performance, approaching 80% F-score for mention detection for a relaxed matching criterion. The corpus and other introduced resources are available under open licences from http:// www.nactem.ac.uk/anatomy/.", "cited_paper_abstract": "We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org", "citation": "To avoid overlap with established tasks and to focus on the novel aspects of anatomical entity mention detection, we exclude biological macromolecules and mentions of organism names from the scope of our annotation, as argued in #REFRb).", "context": "We diverge from the scope of anatomy ontologies in two important aspects in our annotation. First, ontologies of anatomy commonly incorporate everything from molecules to whole organisms within their scope. However, in entity mention detection, many molecular level anatomical entities fall within the scope of the established gene/protein mention detection tasks #OTHEREFR.[Citation]Second, these ontologies typically represent canonical anatomy, an idealized state that is rarely #OTHEREFR. As our annotation is intended to cover references to real-world anatomy, we explicitly include in the scope of our annotation also healthy as well as pathological variants of canonical anatomy. We include also entities derived from these anatomical entities through (planned) processing such as surgical or laboratory procedures, even when these processed entities are no longer properly part of the original organism."}
{"citing_paper_id": "W13-2310", "cited_paper_id": "E12-2021", "citing_paper_abstract": "There exist various different discourse annotation schemes that vary both in the perspectives of discourse structure considered and the granularity of textual units that are annotated. Comparison and integration of multiple schemes have the potential to provide enhanced information. However, the differing formats of corpora and tools that contain or produce such schemes can be a barrier to their integration. U-Compare is a graphical, UIMA-based workflow construction platform for combining interoperable natural language processing (NLP) resources, without the need for programming skills. In this paper, we present an extension of U-Compare that allows the easy comparison, integration and visualisation of resources that contain or output annotations based on multiple discourse annotation schemes. The extension works by allowing the construction of parallel subworkflows for each scheme within a single U-Compare workflow. The different types of discourse annotations produced by each sub-workflow can be either merged or visualised side-by-side for comparison. We demonstrate this new functionality by using it to compare annotations belonging to two different approaches to discourse analysis, namely discourse relations and functional discourse annotations. Integrating these different annotation types within an interoperable environment allows us to study the correlations between different types of discourse and report on the new insights that this allows us to discover. ?The authors have contributed equally to the development of this work and production of the manuscript.", "cited_paper_abstract": "We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org", "citation": "Firstly, unlike annotation platforms such as brat #REFR, U-Compare allows for analysis components to be integrated into workflows in a straightforward and user-interactive manner.", "context": "Conversely, the eventbased annotation in the GENIA-MK corpus can help to determine exactly which part of the sentence represents the hypothesis. Furthermore, the cue phrases annotated in the GENIA-MK corpus could be used as additional features in a system trained to assign CoreSC categories. Although in this paper we illustrate only the visualisation of different types of functional discourse annotations, it is worth noting that U-Compare provides support for further processing.[Citation]If, for example, it is of interest to determine the tokens (and the corresponding parts-of-speech) which frequently act as cues in Analysis events, syntactic analysis components (e.g., tokenisers and POS taggers) can be incorporated via a drag-and-drop mechanism. Also, U- Compare allows the annotations to be saved in a computable format using the provided Xmi Writer CAS Consumer component. This facilitates further automatic comparison of annotations."}
{"citing_paper_id": "W14-1704", "cited_paper_id": "E14-1038", "citing_paper_abstract": "The CoNLL-2014 shared task is an extension of last year?s shared task and focuses on correcting grammatical errors in essays written by non-native learners of English. In this paper, we describe the Illinois-Columbia system that participated in the shared task. Our system ranked second on the original annotations and first on the revised annotations. The core of the system is based on the University of Illinois model that placed first in the CoNLL-2013 shared task. This baseline model has been improved and expanded for this year?s competition in several respects. We describe our underlying approach, which relates to our previous work, and describe the novel aspects of the system in more detail.", "cited_paper_abstract": "Verb errors are some of the most common mistakes made by non-native writers of English but some of the least studied. The reason is that dealing with verb errors requires a new paradigm; essentially all research done on correcting grammatical errors assumes a closed set of triggers ? e.g., correcting the use of prepositions or articles ? but identifying mistakes in verbs necessitates identifying potentially ambiguous triggers first, and then determining the type of mistake made and correcting it. Moreover, once the verb is identified, modeling verb errors is challenging because verbs fulfill many grammatical functions, resulting in a variety of mistakes. Consequently, the little earlier work done on verb errors assumed that the error type is known in advance. We propose a linguistically-motivated approach to verb error correction that makes use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach.", "citation": "For more detail on the features used in the agreement module, we refer the reader to #REFR.", "context": "Model combination is another key extension of the Illinois system. In the Illinois-Columbia system, article, preposition, noun, and verb agreement errors are each addressed via a model that combines error predictions made by a classifier trained on the learner data with the AP algorithm and those made by the NB model trained on the Google corpus. The AP classifiers all make use of richer sets of features than the native-trained classifiers: the article, noun number, and preposition classifiers employ features that use POS information, while the verb agreement classifier also makes use of dependency features extracted using a parser #OTHEREFR.[Citation]Finally, all of the AP models use the source word of the author as a feature and, similar to the article AP classifier (Section 3), implement the error inflation method. The combined model generates a union of corrections produced by the components. We found that for every error type, the combined model is superior to each of the single classifiers, as it combines the advantages of both of the classifiers so that they complement one another."}
{"citing_paper_id": "C14-1216", "cited_paper_id": "E14-1071", "citing_paper_abstract": "We examine the task of separating types from brands in the food domain. Framing the problem as a ranking task, we convert simple textual features extracted from a domain-specific corpus into a ranker without the need of labeled training data. Such method should rank brands (e.g. sprite) higher than types (e.g. lemonade). Apart from that, we also exploit knowledge induced by semisupervised graph-based clustering for two different purposes. On the one hand, we produce an auxiliary categorization of food items according to the Food Guide Pyramid, and assume that a food item is a type when it belongs to a category unlikely to contain brands. On the other hand, we directly model the task of brand detection using seeds provided by the output of the textual ranking features. We also harness Wikipedia articles as an additional knowledge source.", "cited_paper_abstract": "We present a weakly-supervised induction method to assign semantic information to food items. We consider two tasks of categorizations being food-type classification and the distinction of whether a food item is composite or not. The categorizations are induced by a graph-based algorithm applied on a large unlabeled domain-specific corpus. We show that the usage of a domain-specific corpus is vital. We do not only outperform a manually designed open-domain ontology but also prove the usefulness of these categorizations in relation extraction, outperforming state-of-the-art features that include syntactic information and Brown clustering.", "citation": "Different types of classification have been explored including ontology mapping #OTHEREFR, dish detection and the categorization of food types according to the Food Guide Pyramid #REFR.", "context": "Min and Park #OTHEREFR examine the aspect of product instance distinction on the use case of product reviews on jeans from Amazon. Their work focuses on temporal features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention.[Citation]Relation extraction tasks have also been examined. While a strong focus is on food-health relations #OTHEREFR. Beyond that, Chahuneau et al. #OTHEREFR relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews."}
{"citing_paper_id": "E85-1020", "cited_paper_id": "E83-1005", "citing_paper_abstract": "A recognition grammar to supply information to a text-to-speech system for the synthesis of Italian must rely heavily upon lexical information, in order to instantiate the appropriate grammatical relations. Italian is an almost free word order language which nonetheless adopts fair ly analysable strategies to move major constituents: some of these can strongly affect the functioning of the phonotogical component. Two basic claims wil l be made: i. di f f icult ies in associating grammatical functions to constituent structure can be overcome only i f Lexical Theory is adopted as a general theoretical framework, and translated into adequate computational formalisms like ATN or CHART; i i . decisions made at previous point affect focus structure construal rules, which are higher level phonologicaI rules which individuate intonation centre, build up adequate Intonational Groups and assign pauses to adequate sites, all being very sensitive to syntactic and semantic i nf ormat i on. We wil l concentrate on Subject/Object function association to c-structure in Italian, and its relation to ATN formalism, in particular HOLD mechanism and F~Gging. Then we wil l show how syntactic decisions interact with an intonation grammar. We shall also introduce two functional notions: STRUCTURE REVERSIBILITY vs. FUNCTIONAL REVERSIBILITY in Italian.", "cited_paper_abstract": "A computer program for the automatic translation of any text of Italian into naturally fluent synthetic speech is presented. The program, or Phonological Processor (hence FP) maps into prosodic structures the phonological rules of Italian. Structural information is provided by such hierarchical prosodic constituents as Syllable (S), Metrical Foot (HF), Phonological Word (PW), Intonational Group (IG). Onto these structures, phonological rules are applied such as the \" letter-tosound\" rules, automatic word stress rules,internal stress hierarchy rules indicating secondary stress,external sandhi rules, phonological focus assignment rules, logical focus assignment rules. The FP constitutes also a model to simulate the reading process aloud, and the psycholinguistics and cognitive aspects related wil l be discussed in the computational model of the FP. At present, Logical Focus assignment rules and the computational model are work in progress s t i l l to be implemented in the FP. Recorded samples of automatically produced synthetic speech will be presented at the conference to i11ustrate the functioning of the rules. O. Introduction The FP which we shall describe in detail in the following pages, is the terminal section of a system of speech synthesis by rule without vocabulary restrictions, implemented at the Centre of Computational Sonology of the University of Padua. From the linguistic point of view the FP is a model to simulate the operations carried out by an Italian speaker when reading aloud any text. To this end, the speaker shall use the rules of his internal grammar to translate graphic signs into natural speech. These rules wi11 have to be implemented in the FP, together with a computational mechanism simulating the psychological end cognitive functions of the reading process. I. The Phonologlcal Rules At the phonological level the FP has to account for low level or segmental phenomena, and high level or suprasegmental ones. The former are represented by three levels of structure, that is S, MF, PW and are governed by phonological rules which are meant to render the movements of the vocal tract and the coarticulatory effects which occur regularly at word level and at word boundaries. The latter are represented by one level of structure, the IG, and are governed by rules which account for long range phenomena like pitch contour formation, intonation centre assignment, pauses. In brief, the rules that the FP shall have to apply are the following: i . transcription from grapheme to nphoneme\", including the most regular coarticulatory and allophonic phenomena of the It~dian language; ii. automatic word stress assignment, including all the most frequent exceptions to the rules as well as individuation of homographs, which are very common in Italian; i i i . internal word stress hierarchy, with secondary stres/es assignment, individuation of unstressed dipththongs, triphthongs, hiatuses; iv. external sandhi rules, operating at word boundaries and resulting in stress retraction, destressing, stress hierarchy modification, elision by assimilation and other phenomena; v. destressing of functional words listed in a table lookup; vi. pauses marked off by punctuation; pauses deriving from a count of PWs; pauses deriving from syntactic structural phenomena; comma intonation marking of parentheticals and similar structures; v i i . rules to restructure the IG when too longmore than . PWs, or too shortless than 5 PWs; v i i i . Focus Assignment Rules or FAR, which at f i rs t mark Phonological Focus, or intonation centre dependent on lexical and phonologically determined phenomena; ix. FAR which mark Logical Focus or intonation centre dependent on structurally determined phenomena. From a general computational point of view,the FP operates bottom-up to apply low level rules, analysing each word at a time until the PW structure is reached; i t operates top-down to apply high level rules and to build the higher structure, the IG.", "citation": "In a recent paper we presented #REFR a phonological processor for Italian which has been implemented at the University of Venice and is used in a text-to-speech system #OTHEREFR for the synthesis of Italian at the Centre of Computational Sonology of the University of Padua.", "context": "[Citation]Recently the system has been equipped with a lexicon and a morphological analyser #OTHEREFR functional and general syntactic parsers. At present we are working at the context-free grammar and the semantic information to be associated with each lexical entry. As it appears, Italian is a much more comptex language to be analysed when compared with English, German and French."}
{"citing_paper_id": "C90-3051", "cited_paper_id": "E85-1019", "citing_paper_abstract": "The purpose of this paper is to compare different ways of adopting reason-maintenance techniques in incremental parsing (and interpretation). A reasonmaintenance system supports incremental tbrmation and revision of beliefs. By viewing the construction of partial analyses of a text as analogous to forming beliefs about the meanings of its parts, a relation between parsing and reason maintenance can be conceived. In line with this, reason maintenance can bc used for realizing a strong notion of incremental parsing, allowing for revisions of previous analyses. Moreover, an assumption-based reason-maintenance system (ATMS) can be used to support eftieicnt comparisons of (competing) interpretations. The paper argues for an approach which is an extension of chart parsing, but which also can be seen as a system consisting of an inference ngine (the parser proper) coupled with a simplified ATMS. Background and Introduction This paper focuses on the problem of incremental parsing (and to some extent interpretation); in particular, how reason-maintenance techniques can be used to achieve a strong notion of incrementality allowing for piecemeal construction, revision, and comp;~rison of partial analyses. Human language understanding is apparently incremental in the sense of proceeding in a piecemeal fashion, (ideally) carried out in small, grad= ual steps as each word is encountered (Tyler and Marslen-Wilson 1977, Marslen-Wilson and Tyler 1980). Work on incremental parsing and interpretation is typically motivated by a desire to model, or mimic, (aspects of) this behaviour, for example, Bobrow and Webber (1980), Ades and Steedman (1982), Mellish (1985), Pulman (1985), and Haddock (1987~ 1988~ 1990). tlowever, there are also clear-cut computational reasons for trying to attain incrernentality. Sparked This research as been supported by the National Swedish Board for Technical Development. off by the rapid development of increasingly powerful, distributed computer hardware, a paradigm of \"immediate computation\" is gaining popularity in interactive applications like WYSIWYG word processing, spreadsheet programs, and programminglanguage ditors (Reps and Teitelbaum 1984, 1987). It is interesting to consider similar systems applied to interactive natural-language processing. The point is that incrementality is a prerequisite of the reactiw~, real~time-based behaviour of such systems. Furthermore, systems that mix for example deictic and natural-language input require that linguistic status be given to sentence fragments, thus demanding incremental analysis (Kobsa et al 1986). One body of work which appears to be usefifl in incremental parsing and interpretation is re ,on (or truth) maintenanceJ A reason-maintenance system (RMS) supports incremental formation and revision of beliefs. By viewing the construction of partial analyses of a text as analogous to forming beliefs about the meanings of its parts, a relation between parsing and reason maintenance can be conceived. An RMS is coupled with an inference ngine (for example, a parser) which makes inferences within the problem domain, and the overall, combined system can be seen as an inferential problem solver. The RMS makes use of two data structures, nodes and justifications. A node represents a datum provided by the inference ngine, such as an assumption or an inferred proposition. Whenever a datum is inferred from a conjunction of other data, the RMS records this dependency ms a justification which relates the respective nodes. The RMS thus keeps track of what data are believed and disbelieved, and why, given the inferences made so far. The traditional approach, justification-based reason maintenance, JTMS (Doyle 1979), is to maintain a (global) belief by associating with each node a status of in (indicating belief in the corresponding datum) or out (indicating lack of belief in the datum) such that every justification is satisfied. 2 The entire set of (consistent) in data make up the cur-", "cited_paper_abstract": "This paper describes an implemented parser-interpreter which is intended as an abstract formal model of part of the process of sentence comprehension. It is illustrated here for Phrase Structure Grammars with a translation into a familiar type of logical form, although the general principles are intended to apply to any grammatical theory sharing certain basic assumptions, which are discussed in the paper. The procedure allows for incremental semantic interpretation as a sentence is parsed, and provides a principled explanation for some familiar observations concerning properties of deeply recursive constructions. Background The starting point for the present work is a set of familiar and, for the most part, uncontroversial c|~Lm~ s about the nature of grammatical description and of human parsing of natural language. These claims and assumptions can be briefly summarised asfollows: A Hierarchical Structure Linguists assign constituent structures to sentences on the basis ~f distributional tests of various kinds. On the basis of these tests, the 'correct' structures are always hierarchical and often deeply nested. The tree representing a sentence may impose a great deal of structure on it, with string-adjacent items often appearing at very different levels in the tree. In general, shallow, 'flat' structures are not generated by grammars, nor warranted on distributional grounds. However, as we shall see, it is likely that these deeply nested structures may be somewhat remote from any that are actually computed during parsing. B Semantics is (1) composit ional and (ll) syntax-drlven. Both of these claims can be made in a variety of versions of different strengths, from the trivially true to the fairly clearly false. What is intended here is the assumption sometimes called the 'rule to rule' hypothesis, hared by almost all current grammatical frameworks, that to each syntactic rule of a grammar (or for each subrree induced by such a rule) there is an associated semantic rule, either producing an interpretation directly, or translating into some formal anguage. Interpretations for whole sentences are built up from the constituent parts in ways specified by these rules, in a fashion which mimics and uses the syntactic structure of the sentence. C Incremental interpretation As a sentence is parsed, its interpretation is built up word by word: there is little or no delay in interpreting it. In particular, we do not wait until all syntactic constituents have been completed before beginning to integrate then into some non-syntactic representation. Ample intuitive and experimental evidence supports this uncontroversial observation. D Limited recurslon. One of the most firmly established facts about human syntactic processing is that constructions which are ineliminably deeply recursive (such as central self-embeddings) are difficult or impossible to parse. A sentence like: I The boy who the girl that the dog bit liked ran away is clumsy at best, and one like:", "citation": "Work on incremental parsing and interpretation is typically motivated by a desire to model, or mimic, #OTHEREFR, #REFR, and Haddock #OTHEREFR. tlowever, there are also clear-cut computational reasons for trying to attain incrernentality.", "context": "The paper argues for an approach which is an extension of chart parsing, but which also can be seen as a system consisting of an inference ngine (the parser proper) coupled with a simplified ATMS. Background and Introduction This paper focuses on the problem of incremental parsing (and to some extent interpretation); in particular, how reason-maintenance techniques can be used to achieve a strong notion of incrementality allowing for piecemeal construction, revision, and comp;~rison of partial analyses. Human language understanding is apparently incremental in the sense of proceeding in a piecemeal fashion, #OTHEREFR.[Citation]Sparked This research as been supported by the National Swedish Board for Technical Development. off by the rapid development of increasingly powerful, distributed computer hardware, a paradigm of \"immediate computation\" is gaining popularity in interactive applications like WYSIWYG word processing, spreadsheet programs, and programminglanguage ditors #OTHEREFR. It is interesting to consider similar systems applied to interactive natural-language processing. The point is that incrementality is a prerequisite of the reactiw~, real~time-based behaviour of such systems."}
{"citing_paper_id": "P87-1027", "cited_paper_id": "E87-1011", "citing_paper_abstract": "We describe a methodology and associated software system for the construction of a large lexicon from an existing machine-readable (published) dictionary. The lexicon serves as a component of an English morphological and syntactic analyesr and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser. We describe a software system with two integrated components. One of these is capable of extracting syntactically rich, theory-neutral lexical templates from a suitable machine-readabh source. The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable, accurate and consistent lexicon from the source dictionary which contains partial (and occasionally inaccurate) information. Finally, we evaluate the utility of the Longman Dictionary of Contemporary EnglgsA as a suitable source dictionary for the target lexicon.", "cited_paper_abstract": "We argue that there are two qualitatively different modes of using a machine-readable dictionary in the context of research in computational linguistics: batch processing of the source with the purpose of collating information for subsequent use by a natural anguage application, and placing the dictionary on-line in an environment which supports fast interactive access to data selected on the basis of a number of linguistic constraints. While it is the former mode of dictionary use which is characteristic of most computational linguistics work to date, it is the latter which has the potential of making maximal use of the information typically found in a machine-readable dictionary. We describe the mounting of the machine-readable source of the Longman Dictionary of Contemporary English on a single user workstation to make it available as a development tool for a number of research projects.", "citation": "#REFR describes the sentence grammar formalism and current coverage of the English grammar in detail.", "context": "The work is being carried out within the theoretical framework of Generalized Phrase Structure Grammar #OTHEREFR, but many of the mechanisms would be usable without a theoretical commitment to GPSG. It is envisaged that the complete integrated toolkit will be used by a number of research and development groups, as a base component for a range of applications. The potential requirements of a diverse user community motivate, in particular, the need for a morphological and syntactic anaiyser with wide coverage of English grammar and vocabulary.[Citation]Russell et al #OTHEREFR describes the morphological analyser and dictionary system. Further relevant details of both projects are provided in section 2. As part of the grammar project, in tandem with the development of the grammar proper, work is underway to develop a sizeable word list which will be integrated with an existing lexicon of about 4000 words, hand crafted by the morphology project."}
{"citing_paper_id": "P93-1019", "cited_paper_id": "E91-1023", "citing_paper_abstract": "Morphotactics and allomorphy are usually modeled in different components, leading to interface problems. To describe both uniformly, we define finite automata (FA) for allomorphy in the same feature description language used for morphotactics. Nonphonologically conditioned allomorphy is problematic in FA models but submits readily to treatment in a uniform formalism.", "cited_paper_abstract": "Prosodic Inheritance (PI) morphology provides uniform treatment of both concatenative and non-concatenative morphological and phonological generalisations using default inheritance. Models of an extensive range of German Umlaut and Arabic intercalation facts, implemented in DATR, show that the PI approach also covers 'hard cases' more homogeneously and more extensively than previous computational treatments. 1, INTRODUCTION Computational models of sentence syntax are increasingly based on well-defined linguistic theories and implemented using general formalisms; by contrast, morphology and phonology in the lexicon tend to be handled with tailor-made hybrid formalisms selected for properties such as finite state compilability, object orientation, default inheritance, or procedural efficiency. The linguistically motivated Prosodic Inheritance (PI) model with defaults captures morphotactic and morphophonological generalisations in a unified declarative formalism, and has broad linguistic coverage of both concatenative morphology and the notorious 'hard cases' of non-concatenative morphology. This paper integrates the PI concepts underlying previous descriptions of German Umlaut (Reinhard 1990a, 1990b), Bantu tone morphology and Arabic C-V intercalation (Gibbon 1990); Umlaut and intercalation are treated here. PI descriptions are currently Implemented in a DATR dialect (Gibbon 1989; for DATR cf. Evans & Gazdar 1989, 1990, 1989a, 1989b); DATR was chosen for its syntactic simplicity and its explicit formal semantics.", "citation": "#REFR and phonology #OTHEREFR have been taken.", "context": "There are several reasons for developing this approach to morphology. First, we prefer the GENERALITY of a system in which linguistic knowledge of all sorts may be expressed--at least as long as we do not sacrifice processing efficiency. This is an overarching goal of HPSG #OTHEREFR, lICf.[Citation]This work is the first to show how allomorphy may be described here. The proposal here would allow one to describe segments using features, as well, but we have not explored this opportunity for reasons of space. Second, the uniform formalism allows the exact and more transparent specification of dependencies which span modules of otherwise different formalisms."}
{"citing_paper_id": "W08-1108", "cited_paper_id": "E91-1028", "citing_paper_abstract": "Referring expression generation has recently been the subject of the first Shared Task Challenge in NLG. In this paper, we analyse the systems that participated in the Challenge in terms of their algorithmic properties, comparing new techniques to classic ones, based on results from a new human task-performance experiment and from the intrinsic measures that were used in the Challenge. We also consider the relationship between different evaluation methods, showing that extrinsic taskperformance experiments and intrinsic evaluation methods yield results that are not significantly correlated. We argue that this highlights the importance of including extrinsic evaluation methods in comparative NLG evaluations.", "cited_paper_abstract": "In this paper, we review Dale is \\[1989\\] algorithm for determining the content of a referring expression. The algorithm, which only permits the use of one-place predicates, is revised and extended to deal with n-ary predicates. We investigate the problem of blocking arecursion' in complex noun phrases and propose a solution in the context of our algorithm.", "citation": "Following Dale #OTHEREFR, several contributions have extended the remit of ASGRE algorithms to handle relations #REFR and gradable attributes #OTHEREFR.", "context": "Since early work on ASGRE, which focused on pragmatic motivations behind different types of reference #OTHEREFR, the focus has increasingly been on definite descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ?distractors?). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms.[Citation]Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim #OTHEREFR, this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribute set that uniquely refers to the intended referent #OTHEREFR."}
{"citing_paper_id": "P06-1052", "cited_paper_id": "E91-1044", "citing_paper_abstract": "We present an efficient algorithm for the redundancy elimination problem: Given an underspecified semantic representation (USR) of a scope ambiguity, compute an USR with fewer mutually equivalent readings. The algorithm operates on underspecified chart representations which are derived from dominance graphs; it can be applied to the USRs computed by large-scale grammars. We evaluate the algorithm on a corpus, and show that it reduces the degree of ambiguity significantly while taking negligible runtime.", "cited_paper_abstract": "This paper describes an algorithm for generating quantifier scopings. The algorithm is designed to generate only logically non-redundant scopings and to partially order the scopings with a given :default scoping first. Removing logical redundancy is not only interesting per se, but also drastically reduces the processing time. The input and output formats are described through a few access and construction functions. Thus, the algorithm is interesting for a modular linguistic theory, which is flexible with respect o syntactic and semantic framework.", "citation": "There has been previous research on enumerating only some representatives of each equivalence class #REFR, but these approaches don?t maintain underspecification: After running their algorithms, they are left with a set of readings rather than an underspecified representation, i.e. we could no longer run other algorithms on an USR.", "context": "The particular USRs we work with are underspecified chart representations, which can be computed from dominance graphs #OTHEREFRb). We evaluate the performance of the algorithm on the Rondane treebank and show that it reduces the median number of readings from 56 to 4, by up to a factor of 666.240 for individual USRs, while running in negligible time. To our knowledge, our algorithm and its less powerful predecessor #OTHEREFR are the first redundancy elimination algorithms in the literature that operate on the level of USRs.[Citation]The paper is structured as follows. We will first define dominance graphs and review the necessary background theory in Section 2. We will then introduce our notion of equivalence in Section 3, and present the redundancy elimination algorithm in Section 4."}
{"citing_paper_id": "P98-1056", "cited_paper_id": "E91-1051", "citing_paper_abstract": "We present wo approaches for syntactic and semantic transfer based on LFG f-structures and compare the results with existing co-description and restriction operator based approaches, focusing on aspects of ambiguity preserving transfer, complex cases of syntactic structural mismatches as well as on modularity and reusability. The two transfer approaches are interfaced with an existing, implemented transfer component (Verbmobi1), by translating f-structures into a term language, and by interfacing fstructure representations with an existing semantic based transfer approach, respectively.", "cited_paper_abstract": "Kaplan et al(1989) present an approach to machine translation based on co-description. In this paper we show that the notation is not as natural and expressive as it appears. We first show that the most natural analysis proposed in Kaplan et al(1989) cannot in fact cover the range of data for the important ranslational phenomenon in question. This contribution extends the work reported on in Sadler et al (1989) and Sadler et al(1990). We then go on to discuss alternatives which depart from or extend the formalism proposed in Kaplan et al (1989) in various respects, pointing out some directions for further research. The strategies discussed have been implemented.", "citation": "The original co-description based approach in #OTHEREFR faced problems when it came to examples involving embedded head-switching and multiple adjuncts #REFR, which led to the introduction of a restriction operator, to enable transfer on partial f-structures or semantic structures #OTHEREFR.", "context": "F-structures and UDRSs are underspecified syntactic and semantic representations, respectively. Both support ambiguity preserving transfer to differing degrees (NP scope, operators, adjuncts). F-structure based syntactic representations may come up against structural mismatches in transfer.[Citation]One might suppose that the need to refer to partial structures i an artifact of the correspondencebased approach, which does not allow the mapping from a single node of the source f-structure to distinct nodes in the target f-structure without violation of the functional property of the correspondence. On closer inspection, though, the rewriting approach to syntactic f-structureterm translations presented above suffers from the very same problems that were met by the correspondence-based approach in #OTHEREFR. By contrast, transfer on the semantic UDRS representations does not suffer from such problems."}
{"citing_paper_id": "E95-1032", "cited_paper_id": "E93-1025", "citing_paper_abstract": "The paper describes a substitutional approach to ellipsis resolution giving comparable results to (Dalrymple et al, 1991), but without he need for order-sensitive interleaving of quantifier scoping and ellipsis resolution. It is argued that the orderindependence r sults from viewing semantic interpretation as building a description of a semantic omposition, instead of the more common view of interpretation as actually performing the composition.", "cited_paper_abstract": "We give an analysis of ellipsis resolution in terms of a straightforward iscourse copying algorithm that correctly predicts a wide range of phenomena. The treatment does not suffer from problems inherent in identity-of-relations analyses. Furthermore, in contrast o the approach of Dalrymple t al. \\[1991\\], the treatment directly encodes the intuitive distinction between full NPs and the referential elements that corefer with them through what we term role linking. The correct predictions for several problematic examples of ellipsis naturally result. Finally, the analysis extends directly to other discourse copying phenomena.", "citation": "Selecting ellipsis antecedents and parallel elements within them is an open problem #OTHEREFR; #REFR.", "context": "[Citation]Our approach to parallelism is perhaps heavy-handed, but in the absence of a clear solutions, possibly more flexible. The QLFs shown above omitted category information present in terms and ~orms. Categories are sets of feature value equations containing syntactic information relevant o determining how uninstantiated meta-variables can be resolved."}
{"citing_paper_id": "C94-1104", "cited_paper_id": "E93-1046", "citing_paper_abstract": "We are concerned with the syntactic annotation of unrestricted text. We combine a rule-based analysis with subsequent exploitation of empirical data. The rule~based surface syntactic analyser leaves ome amount of ambiguity in the output that is resolved using empirical patterns. We have implemented a system for generating and applying corpus-based patterns. Somc patterns describe the main constituents in the sentence and some the local context of the each syntactic function. There are several (partly) redmltant patterns, and the \"pattern\" parser selects analysis of the sentence ttmt matches the strictest possible pattern(s). The system is applied to an experimeutal corpus. We present he results and discuss possible refinements of the method from a linguistic point of view.", "cited_paper_abstract": "We are concerned with dependencyoriented morphosyntactic parsing of running text. While a parsing grammar should avoid introducing structurally unresolvable distinctions in order to optimise on the accuracy of the parser, it also is beneficial for the grammarian to have as expressive a structural representation available as possible. In a reductionistic parsing system this policy may result in considerable ambiguity in the input; however, even massive ambiguity can be tackled efficiently with an accurate parsing description and effective parsing technology.", "citation": "A better tag set for surface-syntactic parsing is presented in \\[#REFR\\].", "context": "Some improvements might be achieved by modifying the syntactic tag set of ENGCG. As discussed above, the (syntactic) tag set of the ENGCG is not probably optimal. Some ambiguity is not resolvable (like prepositional ttachment) and some distinctions arc not made (like subjects of the finite and the nonfinite clauses).[Citation]But we have not modified the present ag set because it is not clear whether small changes would improve the result significantly when compared to the effort needed. Although it is not possible to fully disambiguate the syntax in ENGCG, the rate of disambiguation can be improved using a more powerful linguistic rule tbrmalism #OTHEREFR\\]). The results reported in this sudy can most likely be improved by writing a syntactic grammar in the finite-state framework."}
{"citing_paper_id": "W00-0506", "cited_paper_id": "E95-1004", "citing_paper_abstract": "We describe an approach to Machine Translation of transcribed speech, as found in closed captions. We discuss how the colloquial nature and input format peculiarities of closed captions are dealt with in a pre-processing pipeline that prepares the input for effective processing by a core MT system. In particular, we describe components for proper name recognition and input segmentation. We evaluate the contribution of such modules to the system performance. The described methods have been implemented on an MT system for translating English closed captions to Spanish and Portuguese.", "cited_paper_abstract": "This paper shows first the problems raised by proper names in natural anguage processing. Second, it introduces the knowledge representation structure we use based on conceptual graphs. Then it explains the techniques which are used to process known and unknown proper names. At last, it gives the performance of the system and the further works we intend to deal with. or unknown. Some of these techniques are taken out of existing systems but they have been unified and completed in constructing this single operational module. Besides some innovative techniques for desambiguating known proper names using the context have been implemented.", "citation": "Name recognition is made harder in the closed caption domain by the fact that no capitalization information is given, thus making unusable all methods that rely on capitalization as the main way to identify candidates #REFR #OTHEREFR.", "context": "Proper names are ubiquitous in closed captions (see Table 1). Their recognition is important for effective comprehension of closed captions, particularly in consideration of two facts: (i) users have little time to mentally rectify a mistranslation; (ii) a name can occur repeatedly in a program (e.g. a movie), with an annoying effect if it is systematically mistranslated (e.g. a golf tournament where the golfer named Tiger Woods is systematically referred to as los bosques del tigre, lit. 'the woods of the tiger').[Citation]For instance, an expression like 'mark sh ie lds ' , as occurs in Table (1), is problematic in the absence of capitalization, as both 'mark' and ' sh ie lds ' are threeway ambiguous (proper name, common noun and verb). Note that this identical problem may be encountered if an MT system is embedded in a speech-to-speech translation as well. This situation forced us to explore different ways of identifying proper names."}
{"citing_paper_id": "W00-0722", "cited_paper_id": "E95-1021", "citing_paper_abstract": "In this paper we describe the construction of a part-of-speech tagger both for medical document retrieval purposes and XP extraction. Therefore we have designed a double system: for retrieval purposes, we rely on a rule-based architecture, called minimal commitment, which is likely to be completed by a data-driven tool (HMM) when full disambiguation is necessary.", "cited_paper_abstract": "In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language. We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model. We describe the two systems and compare the results. The accuracy of the statistical method is reasonably good, comparable to taggers for English. But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development.", "citation": "Concerning the ambiguities, we found that 5 tokens were responsible for half of the ambiguities, while in unrestricted corpora this number seems around 16 #REFR.", "context": "However, some frequent words were missing, so that together with the MS guesser, we would improve the guessing score by adding some lexemes. Thus, adding 232 entries in the lexicon and linking it with the Swiss compendium (for drugs and chemicals) provides an unknown word rate of less than 3%. This result includes also the pre-processing of patients and physicians names #OTHEREFR.[Citation]3.2.1 Local rules We separated the set A in 8 subsets of about 1000 tokens, in order to write the rules. We wrote around 50 rules (which generated more than 150 operative rules) for the first subset, while for the 8th, only 12 rules were necessary to reach a score close to 100% on set A. These rules are using intermediate symbols (such as the Kleene star) in order to ease and improve the rule-writing process, these symbols are replaced when the operative rules are generated."}
{"citing_paper_id": "W00-0903", "cited_paper_id": "E95-1021", "citing_paper_abstract": "In this paper we compare two types of corpus, focusing on the lexical mnbiguity of each of them. The first corpns consists mainly of newspaper articles and Hterature excerpts, while the second belc)ngs to the medical domain. To conduct he study, we have used two different disambiguation tools. However, first of all, we must verify the performance of each system in its respective application domain. We then use these systems in order to assess and compare both the general ambiguity rate and the particularities of each domain. (mantitative results show that medical documents are lexically less ambiguous than tmrestrieted documents. Our conclusions show the importance of the application area in the design of NLP tools. Introduction and background Although some large-scale valuations carried out on unrestricted texts (Hersh 1998a, Spark- Jones 1999), and even on medical documents (Hersh 1998b), conclude in a quite critical way about using NLP tools for information retrieval, we believe that such tools are likely to solve some lexical ambiguity issues. We also believe that some special settings -particular to the application areamust be taken into account while developing such NLP tools. Let us recall two major problems while retrieving documents with NLP engines (Salton, 1988): 1-Expansion: the user is generally as interested in retrieving documents with exactly the same words, as in retrieving documents with semantically related words (synonyms, generics, specifics...). Thus, a query based on the word liver, should be able to retrieve documents containing words such as hepatic. This expansion process is usually thesaurus-based. The thesaurus can be built manually or automatically (as, for ex~ple, in Nazarenko, 1997). 2-Disambiguation: a search based on tokens may retrieve irrelevant documents since tokens are often lexically ambiguous. Thus, face can refer to a body part, as a noun, or an action, as a verb. Finally, this latter problem may be split into two sub problems. The disambiguafion task can be based on parts-of-speech (POS) or word-sense (WS) information, but the chronological relation is still a discussion within the community. Although, the target of our work (Ruch and al., 1999, Bouillon and al., 2000) is a free-grained semantic disambiguation f medical texts for IR purposes, we believe that the POS disambiguation is an important preliminary step. Therefore this paper focuses on POS tagging, and compares morpho-syntacfic lexical ambiguities (MSLA) in medical texts to MSLA in unrestricted corpora. Although the results of the study conform to preliminary naive expectations, the method is quite original I. Most of the comparative studies, dedicated to corpora, have addressed the problem by applying metrics on words entities or word pieces (as in studies working with n- I We do not claim to be pioneer in the domain, as others authors (Biber 1998, Folch and al., 2000) axe exploring similar metrics. However, it is interesting to notice that for these authors the adaptation fthe NLP tools has rarely been questioned in a technical point-of view, and in order to feed back the design of NLP systems. gram strings), or on special sets of words (the indexing terms, see Salton, 1988) as in the space-vector model (see Kilgariff, 1996, for a survey of these methods), whereas the present paper attempts to compare corpora t a morphosyntactic (MS) level", "cited_paper_abstract": "In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language. We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model. We describe the two systems and compare the results. The accuracy of the statistical method is reasonably good, comparable to taggers for English. But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development.", "citation": "When the surface form is not found in the lexicon, it follows a two.step guessing process: the first level (oraclel) is a more complex morphological nalyzer, based on the morphosemantemes, while the second level guesser (orcale2) attempts to provides a set of MS features looking at the longest ending (as described in #REFR.", "context": "morphology The morphological analysis associates every surface form with a list of morpho-syntactic features.[Citation]The importance of these two levels is not clear for POS tagging, but becomes manifest when dealing with sense tagging. Let us consider three examples of tokens absent fxom the lexicon: allomorphiques, allomorphiquement (equivalent to allomorphic and allomorphically in Eng. remained ambiguous aRer disambiguafion, the residual ambiguity is therefore about 5.5%. In this sample, and before disambiguation, the number of ambiguous tokens was 150, which means an ambiguity rate of 20%."}
{"citing_paper_id": "W97-0910", "cited_paper_id": "E95-1028", "citing_paper_abstract": "We describe two methods relevant to multilingual machine translation systems, which can be used to port linguistic data (grammars, lexicons and transfer rules) between systems used for processing related languages. The methods are fully implemented within the Spoken Language Translator system, and were used to create versions of the system for two new language pairs using only a month of expert effort.", "cited_paper_abstract": "I describe a compiler and development environment for feature-augmented wolevel morphology rules integrated into a full NLP system. The compiler is optimized for a class of languages including many or most European ones, and for rapid development and debugging of descriptions of new languages. The key design decision is to compose morphophonological and morphosyntactic information, but not the lexicon, when compiling the description. This results in typical compilation times of about a minute, and has allowed a reasonably full, feature-based description of French inflectional morphology to be developed in about a month by a linguist new to the system.", "citation": "Morphologies are written in a variant of two-level morphology #REFR, and grammars in a unification-based formalism #OTHEREFR.", "context": "We refer to the backup method as \"word-to-word\" (WW) translation. The two methods are combined, roughly speaking, by using rule-based QLF transfer to translate as much as possible, filling in any gaps with applications of the WW rules. The parts of the system of central interest here are the rule-based components, in particular the morphologies, grammars, lexica and transfer ules.[Citation]The lexicon for each language is divided into three main parts: Domain-independent function (closed class) word entries are written directly in terms of definitions of suitable feature-value assignments, and can from a software-engineering standpoint be regarded as part of the grammar. A collection of language-dependent but domainindependent macros define the feature-value assignments needed for each type of regular content-word, e.g. \"count noun\", \"transitive verb\" and so on. These macros are called paradigm macros."}
{"citing_paper_id": "C04-1028", "cited_paper_id": "E95-1034", "citing_paper_abstract": "We extend Combinatory Categorial Grammar (CCG) with a generalized notion of multidimensional sign, inspired by the types of representations found in constraint-based frameworks like HPSG or LFG. The generalized sign allows multiple levels to share information, but only in a resource-bounded way through a very restricted indexation mechanism. This improves representational perspicuity without increasing parsing complexity, in contrast to full-blown unification used in HPSG and LFG. Well-formedness of a linguistic expressions remains entirely determined by the CCG derivation. We show how the multidimensionality and perspicuity of the generalized signs lead to a simplification of previous CCG accounts of how word order and prosody can realize information structure.", "cited_paper_abstract": "Multiset-CCG is a combinatory categorial formalism that can capture the syntax and interpretation of \"free\" word order in languages uch as Turkish. The formalism compositionally derives the predicate-argument structure and the information structure (e.g. topic, focus) of a sentence, and uniformly handles word order variation among arguments and adjuncts within a clause, as well as in complex clauses and across clause boundaries.", "citation": "The literature presents various proposals for how information structure can be captured in categorial grammar #OTHEREFRa; #REFR.", "context": "To illustrate the approach, we use it to model various aspects of the realization of information structure, an inherent aspect of the (linguistic) meaning of an utterance. Speakers use information structure to present some parts of that meaning as depending on the preceding discourse context and others as affecting the context by adding new content. Languages may realize information structure using different, often interacting means, such as word order, prosody, #OTHEREFR.[Citation]Here, we model the essential aspects of these accounts in a more perspicuous manner by using our generalized signs. The main outcomes of the proposal are threefold: (1) CCG gains a more flexible and general kind of sign; (2) these signs contain multiple levels that interact in a modular fashion and are built via CCG derivations without increasing parsing complexity; and (3) we use these signs to simplify previous CCG?s accounts of the effects of word order and prosody on information structure."}
{"citing_paper_id": "W06-3108", "cited_paper_id": "E99-1010", "citing_paper_abstract": "We present discriminative reordering models for phrase-based statistical machine translation. The models are trained using the maximum entropy principle. We use several types of features: based on words, based on word classes, based on the local context. We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus. Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system.", "cited_paper_abstract": "In statistical natural anguage processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes uitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation.", "citation": "The word classes for the class-based features are trained using the mkcls tool #REFR.", "context": "In our experiments, we use about 10% of the corpus for testing and the remaining part for training the feature weights of the reordering model with the GIS algorithm using YASMET #OTHEREFR. The statistics of the training and test alignment links is shown in Table 2. The number of training events ranges from 119K for Japanese- English to 144K for Arabic-English.[Citation]In the experiments, we use 50 word classes. Alternatively, one could use part-of-speech information for this purpose. Additional experiments were carried out on the large data track of the Chinese-English NIST task."}
{"citing_paper_id": "W04-3254", "cited_paper_id": "E99-1011", "citing_paper_abstract": "We present a new approach to intrinsic summary evaluation, based on initial experiments in van Halteren and Teufel (2003), which combines two novel aspects: comparison of information content (rather than string similarity) in gold standard and system summary, measured in shared atomic information units which we call factoids, and comparison to more than one gold standard summary (in our data: 20 and 50 summaries respectively). In this paper, we show that factoid annotation is highly reproducible, introduce a weighted factoid score, estimate how many summaries are required for stable system rankings, and show that the factoid scores cannot be sufficiently approximated by unigrams and the DUC information overlap measure.", "cited_paper_abstract": "The TIPSTER Text Summarization Evaluation (SUMMAC) has established definitively that automatic text summarization is very effective in relevance assessment tasks. Summaries as short as 17% of full text length sped up decisionmaking by almost a factor of 2 with no statistically significant degradation i F- score accuracy. SUMMAC has also introduced a new intrinsic method for automated evaluation of informative summaries.", "citation": "The summary evaluation performed in SUMMAC #REFR followed that strategy.", "context": "Many researchers in summarisation believe that the best way to evaluate a summary is extrinsic evaluation #OTHEREFR: to measure the quality of the summary on the basis of degree of success in executing a specific task with that summary.[Citation]However, extrinsic evaluations are time-consuming to set up and can thus not be used for the day-to-day evaluation needed during system development. So in practice, a method for intrinsic evaluation is needed, where the properties of the summary itself are examined, independently of its application. Intrinsic evaluation of summary quality is undeniably hard, as there are two subtasks of summarisation which need to be evaluated, information selection and text production ? in fact these two subtasks are often separated in evaluation #OTHEREFR."}
{"citing_paper_id": "C04-1002", "cited_paper_id": "E99-1026", "citing_paper_abstract": "We present a novel algorithm for Japanese dependency analysis. The algorithm allows us to analyze dependency structures of a sentence in linear-time while keeping a state-of-the-art accuracy. In this paper, we show a formal description of the algorithm and discuss it theoretically with respect to time complexity. In addition, we evaluate its efficiency and performance empirically against the Kyoto University Corpus. The proposed algorithm with improved models for dependency yields the best accuracy in the previously published results on the Kyoto University Corpus.", "cited_paper_abstract": "This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models. Our model is created by learning the weights of some features from a training corpus to predict he dependency between bunsetsus or phrasal units. The dependency accuracy of our system is 87.2% using the Kyoto University corpus. We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy.", "citation": "The usage of these articles is the same as in #REFR.", "context": "We used the Kyoto University Corpus Version 2 #OTHEREFR to evaluate the proposed algorithm. Our parser was trained on the articles on January 1st through 8th (7,958 sentences) and tested on the article on January 9th (1,246 sentences). The article on January 10th were used for development.[Citation]"}
{"citing_paper_id": "D07-1041", "cited_paper_id": "H01-1035", "citing_paper_abstract": "We demonstrate an approach for inducing a tagger for historical languages based on existing resources for their modern varieties. Tags from Present Day English source text are projected to Middle English text using alignments on parallel Biblical text. We explore the use of multiple alignment approaches and a bigram tagger to reduce the noise in the projected tags. Finally, we train a maximum entropy tagger on the output of the bigram tagger on the target Biblical text and test it on tagged Middle English text. This leads to tagging accuracy in the low 80?s on Biblical test material and in the 60?s on other Middle English material. Our results suggest that our bootstrapping methods have considerable potential, and could be used to semi-automate an approach based on incremental manual annotation.", "cited_paper_abstract": "This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection. Keywords multilingual, text analysis, part-of-speech tagging, noun phrase bracketing, named entity, morphology, lemmatization, parallel corpora", "citation": "This is very much in line with the methodology of Yarowksy and #REFR, who project a small number of tags out of all those predicted by alignment.", "context": "To test this, we trained a bigram tagger on DICE 1TO1 and the more conservative GIZA 1TO1 projection. This produces further gains for the PPCME Wycliffe, and enormous improvements on the PPCME Test data (see line (d) of Figure 2). This result confirms that conservativity beats wild guessing (at the risk of reduced coverage) for bootstrapping taggers in this way.[Citation]They achieve this restriction by directly adjusting the probabality mass assigned to projected tags; we do it by using two versions of the target text with tags projected in two different 1-to-1 ways."}
{"citing_paper_id": "W09-1104", "cited_paper_id": "H01-1035", "citing_paper_abstract": "We present a simple but very effective approach to identifying high-quality data in noisy data sets for structured problems like parsing, by greedily exploiting partial structures. We analyze our approach in an annotation projection framework for dependency trees, and show how dependency parsers from two different paradigms (graph-based and transition-based) can be trained on the resulting tree fragments. We train parsers for Dutch to evaluate our method and to investigate to which degree graph-based and transitionbased parsers can benefit from incomplete training data. We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006).", "cited_paper_abstract": "This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection. Keywords multilingual, text analysis, part-of-speech tagging, noun phrase bracketing, named entity, morphology, lemmatization, parallel corpora", "citation": "On the word or phrase level, these include morphological analysis, part-of-speech tagging and NP-bracketing #REFR, temporal analysis #OTHEREFR.", "context": "Annotation projection has been applied to many different NLP tasks.[Citation]In these tasks, word labels can technically be introduced in isolation, without reference to the rest of the annotation. This means that an aggressive filter can be used to discard unreliable data points (words in a sentence) without necessarily affecting highprecision data points in the same sentence. By using only the bidirectional word alignment links, one can implement a very robust such filter, as the bidirectional links are generally reliable, even though they have low recall for overall translational correspondences #OTHEREFR."}
{"citing_paper_id": "P10-1072", "cited_paper_id": "H01-1051", "citing_paper_abstract": "We present a game-theoretic model of bargaining over a metaphor in the context of political communication, find its equilibrium, and use it to rationalize observed linguistic behavior. We argue that game theory is well suited for modeling discourse as a dynamic resulting from a number of conflicting pressures, and suggest applications of interest to computational linguists.", "cited_paper_abstract": "In collaboration with colleagues at UW, OGI, IBM, and SRI, we are developing technology to process spoken language from informal meetings. The work includes a substantial data collection and transcription effort, and has required a nontrivial degree of infrastructure development. We are undertaking this because the new task area provides a significant challenge to current HLT capabilities, while offering the promise of a wide range of potential applications. In this paper, we give our vision of the task, the challenges it represents, and the current state of our development, with particular attention to automatic transcription.", "citation": "Generally, computational linguistics research produces algorithms to detect entities of various kinds, be it topics, named entities, metaphors, moves in a multi-party conversations, or syntactic constructions in large corpora; such primary data can be used to trace developments not only in chronological terms #OTHEREFR, research agendas in group meetings #REFR, or social agendas in speed-dates #OTHEREFR.", "context": "Interesting connections have been pointed out between game theory and machine learning: Freund and Schapire #OTHEREFR show similarly that loss minimization in online learning is akin to an equilibrium path in a repeated game. While game theoretic models are not much utilized in computational linguistics, they are quite attractive to tackle some of the problems computational linguists are interested in. For example, generation of referring expressions #OTHEREFR, with corpora annotated for entity mentions informing the design of a model.[Citation]Game theoretical models are well suited for modeling dynamics that emerge under multiple, possibly conflicting constraints, as we exemplify in this article."}
{"citing_paper_id": "W03-1208", "cited_paper_id": "H01-1069", "citing_paper_abstract": "This paper proposes a machine learning based question classification method using a kernel function, Hierarchical Directed Acyclic Graph (HDAG) Kernel. The HDAG Kernel directly accepts structured natural language data, such as several levels of chunks and their relations, and computes the value of the kernel function at a practical cost and time while reflecting all of these structures. We examine the proposed method in a question classification experiment using 5011 Japanese questions that are labeled by 150 question types. The results demonstrate that our proposed method improves the performance of question classification over that by conventional methods such as bag-of-words and their combinations.", "cited_paper_abstract": "We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.", "citation": "For example, #OTHEREFR also defined a large hierarchical question taxonomy, and #REFR defined 141 question types of a hierarchical question taxonomy.", "context": "Numerous question taxonomies have been defined, but unfortunately, no standard exists. In the case of the TREC QA-Track, most systems have their own question taxonomy, and these are reconstructed year by year.[Citation]Within all of these taxonomies, question types are defined from the viewpoint of the target intention of the given questions, and they have hierarchical structures, even though these question taxonomies are defined by different researchers. This because the purpose of question classification is to reduce the large number of answer candidates by restricting the target intention via question types. Moreover, it is very useful to handle question taxonomy constructed in a hierarchical structure in the downstream processes."}
{"citing_paper_id": "D08-1069", "cited_paper_id": "H05-1004", "citing_paper_abstract": "This paper investigates two strategies for improving coreference resolution: (1) training separate models that specialize in particular types of mentions (e.g., pronouns versus proper nouns) and (2) using a ranking loss function rather than a classification function. In addition to being conceptually simple, these modifications of the standard single-model, classification-based approach also deliver significant performance improvements. Specifically, we show that on the ACE corpus both strategies produce f -score gains of more than 3% across the three coreference evaluation metrics (MUC, B3, and CEAF).", "cited_paper_abstract": "The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.", "citation": "Both these strategies lead to improvements for all three standard coreference metrics: MUC #OTHEREFR, and CEAF #REFR.", "context": "Other partially capture the differential preferences between different anaphors via different sample selection strategies during training #OTHEREFR. More recently, Haghighi and Klein #OTHEREFR use the distinction between pronouns, nominals and proper nouns in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy. Here, we show that using specialized models for different types of referential expressions improves performance for supervised models (both classifiers and rankers).[Citation]In particular, our specialized ranker system provides absolute f -score improvements against an otherwise identical standard classifier system by 3.2%, 3.1%, and 3.6% for MUC, B3, and CEAF, respectively."}
{"citing_paper_id": "S10-1020", "cited_paper_id": "H05-1004", "citing_paper_abstract": "Corry is a system for coreference resolution in English. It supports both local (Soon et al. (2001)-style) and global (Integer Linear Programming, Denis and Baldridge (2007)- style) models of coreference. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons. Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al, 2010), optimizing Corry?s performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al, 1995) and CEAF (Luo, 2005). Corry runs have shown the best performance level among all the systems in their track for the corresponding metric.", "cited_paper_abstract": "The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.", "citation": "Three runs have been submitted for the SemEval task 1 on Coreference Resolution #OTHEREFR and CEAF #REFR.", "context": "Corry is a system for coreference resolution in English. It supports both local #OTHEREFR- style) models of coreference. Corry relies on a rich linguistically motivated feature set, which has, however, been manually reduced to 64 features for efficiency reasons.[Citation]Corry runs have shown the best performance level among all the systems in their track for the corresponding metric."}
{"citing_paper_id": "W11-1903", "cited_paper_id": "H05-1004", "citing_paper_abstract": "This paper describes the participation of RELAXCOR in the CoNLL-2011 shared task: ?Modeling Unrestricted Coreference in Ontonotes?. RELAXCOR is a constraint-based graph partitioning approach to coreference resolution solved by relaxation labeling. The approach combines the strengths of groupwise classifiers and chain formation methods in one global method.", "cited_paper_abstract": "The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.", "citation": "Both parameters were empirically adjusted on the development set for the evaluation measure used in this shared task: the unweighted average of MUC #OTHEREFR and entity-based CEAF #REFR. ", "context": "The final weight will be weight = precision? balance. the number of neighbors reduces the computational cost significantly and improves overall performance too. Optimizing this parameter depends on properties like document size and the quality of the information given by the constraints. The development process calculates a grid given the possible values of both parameters: from 0 to 1 for balance with a step of 0.05, and from 2 to 14 for pruning with a step of 2.[Citation]Figure 4: Development process. The figure shows MUC?s precision (red), recall (green), and F1 (blue) for each balance value with pruning adjusted to 6."}
{"citing_paper_id": "N06-1015", "cited_paper_id": "H05-1010", "citing_paper_abstract": "Recently, discriminative word alignment methods have achieved state-of-the-art accuracies by extending the range of information sources that can be easily incorporated into aligners. The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and so on. However, the proposed bipartite matching model of Taskar et al (2005), despite being tractable and effective, has two important limitations. First, it is limited by the restriction that words have fertility of at most one. More importantly, first order correlations between consecutive words cannot be directly captured by the model. In this work, we address these limitations by enriching the model form. We give estimation and inference algorithms for these enhancements. Our best model achieves a relative AER reduction of 25% over the basic matching formulation, outperforming intersected IBM Model 4 without using any overly compute-intensive features. By including predictions of other models as features, we achieve AER of 3.8 on the standard Hansards dataset.", "cited_paper_abstract": "We present a discriminative, largemargin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments.", "citation": "However, the proposed bipartite matching model of #REFR, despite being tractable and effective, has two important limitations.", "context": "Recently, discriminative word alignment methods have achieved state-of-the-art accuracies by extending the range of information sources that can be easily incorporated into aligners. The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and so on.[Citation]First, it is limited by the restriction that words have fertility of at most one. More importantly, first order correlations between consecutive words cannot be directly captured by the model. In this work, we address these limitations by enriching the model form."}
{"citing_paper_id": "P13-2064", "cited_paper_id": "H05-1011", "citing_paper_abstract": "In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.", "cited_paper_abstract": "Bilingual word alignment forms the foundation of most approaches to statistical machine translation. Current word alignment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data.", "citation": "Given that many-to-many links are common in natural languages #REFR, it is necessary to pay attention to the relations among alignment links.", "context": "Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language #OTHEREFR.[Citation]In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (? 2.1). Then we employ a weighted partite hypergraph to encode multiple bigraphs (? 2.2)."}
{"citing_paper_id": "D11-1045", "cited_paper_id": "H05-1012", "citing_paper_abstract": "Preordering of source side sentences has proved to be useful in improving statistical machine translation. Most work has used a parser in the source language along with rules to map the source language word order into the target language word order. The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper. Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with highquality word alignments. Our model learns pairwise costs of a word immediately preceding another word. We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering. We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi. For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse.", "cited_paper_abstract": "This paper presents a maximum entropy word alignment algorithm for Arabic- English based on supervised training data. We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance. The probabilistic model used in the alignment directly models the link decisions. Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests. Performance of the algorithm is contrasted with human annotation performance.", "citation": "For all of our systems we used a combination of HMM #OTHEREFR and MaxEnt alignments #REFR.", "context": "In our experiments, we left the word alignments fixed, i.e we reordered the existing word alignments rather than realigning the sentences after reordering. Redoing the word alignments with the reordered data could potentially give further small improvements. We note that we obtained better baseline performance using DTM systems than the standard Moses/Giza++ pipeline (e.g we obtained a BLEU of 14.9 for English to Hindi with a standard Moses/Giza++ pipeline).[Citation]For our Hindi-English experiments we use a training set of roughly 250k sentences #OTHEREFR and an internal dataset from several domains but dominated by news. Our test set was roughly 1.2K sentences from the news domain with a single reference. To train our reordering model, we used roughly 6K alignments plus 17K snippets selected from MaxEnt alignments as described in Section 5.1 with bigram, ContextPOS and ContextWord features."}
{"citing_paper_id": "D07-1077", "cited_paper_id": "H05-1021", "citing_paper_abstract": "Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al, 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules.", "cited_paper_abstract": "We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system. These models provide properly formulated, non-deficient, probability distributions over reordered phrase sequences. They are implemented by Weighted Finite State Transducers. We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering. Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks. We also show that the procedure scales as the bitext size is increased.", "citation": "More sophisticated models include reordering parameters that are sensitive to lexical information #OTHEREFR; #REFR.", "context": "Our approach is most similar to that of Collins et al #OTHEREFR. Most SMT systems employ some mechanism that allows reordering of the source language during translation (i.e., non-monotonic decoding). The MOSES phrase-based system that we use has a relatively simple reordering model which has a fixed penalty for reordering moves in the decoder.[Citation]The model of Chiang #OTHEREFR employs a synchronous context-free grammar to allow hierarchical approaches to reordering. The syntaxbased models of Yamada and Knight #OTHEREFR build a full parse tree in the target language, again effectively allowing hierarchical reordering based on synchronous grammars. It is worth noting that none of these approaches to reordering make use of explicit syntactic information in the source language?for example, none of the methods make use of an existing source-language parser #OTHEREFR make use of a parser in the target language, i.e., English)."}
{"citing_paper_id": "E09-1044", "cited_paper_id": "H05-1021", "citing_paper_abstract": "We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-to- English evaluation task.", "cited_paper_abstract": "We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system. These models provide properly formulated, non-deficient, probability distributions over reordered phrase sequences. They are implemented by Weighted Finite State Transducers. We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering. Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks. We also show that the procedure scales as the bitext size is increased.", "citation": "The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model #REFR.", "context": "In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM #OTHEREFR.[Citation]Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior #OTHEREFR; MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the large models used in an evaluation task, the TTM system is reported to run largely without pruning #OTHEREFR. The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual glue rule, as shown in left-hand column of Table 1, where T is the set of terminals."}
{"citing_paper_id": "W11-0139", "cited_paper_id": "H05-1042", "citing_paper_abstract": "In this paper, we describe the Baseball Announcers. Language Linked with General Annotation of Meaningful Events (BALLGAME) project ? a text corpus for research in computional semantics. We collected pitch-by-pitch event data for a sample of baseball games and used this data to build an annotated corpus composed of transcripts of radio broadcasts of these games. Our annotation links text from the broadcast to events in a formal representation of the semantics of the baseball game. We describe our corpus model, the annotation tool used to create the corpus, and conclude by discussing applications of this corpus in semantics research and natural language processing.", "cited_paper_abstract": "A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.", "citation": "To overcome this obstacle, several recent studies have turned to the arena of sports, pairing natural language with game statistics in several domains, including RoboCup soccer #OTHEREFR, American football #REFR, and baseball #OTHEREFR.", "context": "The use of large annotated corpora and treebanks has led to many fruitful research programs in computational linguistics. At the time of this writing, Marcus et al #OTHEREFR subsequent papers.2 Such treebanks are invaluable for the training and testing of large-scale syntactic parsers and numerous other applications in the field of Computational Syntax. Unfortunately for the field of Computational Semantics, there are few corresponding annotated corpora or treebanks representing the formalized meaning of natural language sentences, mainly because there is very little agreement on what such a representation of meaning would look like for arbitrary text.[Citation]We have adapted this approach in the creation of a semantics-oriented corpus, using the domain of major-league baseball. The information state of a baseball game can be represented with a small number of variables, such as who is on which base, who is batting, who is playing each position, and the current score and inning. There is even a standard way of representing updates to this information state.3 This makes baseball a logical stepping stone to a fuller representation of the world."}
{"citing_paper_id": "W13-2106", "cited_paper_id": "H05-1042", "citing_paper_abstract": "We present an Integer Linear Programming model of content selection, lexicalization, and aggregation that we developed for a system that generates texts from OWL ontologies. Unlike pipeline architectures, our model jointly considers the available choices in these three text generation stages, to avoid greedy decisions and produce more compact texts. Experiments with two ontologies confirm that it leads to more compact texts, compared to a pipeline with the same components, with no deterioration in the perceived quality of the generated texts. We also present an approximation of our model, which allows longer texts to be generated efficiently.", "cited_paper_abstract": "A content selection component determines which information should be conveyed in the output of a natural language generation system. We present an efficient method for automatically learning content selection rules from a corpus and its related database. Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items. Experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods.", "citation": "The model itself, however, does not produce importance scores, i.e., we assume that the scores are produced by a separate process #REFR, not included in our content selection.", "context": "Given an individual (entity) or class of an OWL ontology and a set of facts (axioms) about the individual or class, we aim to produce a compact text that expresses as many facts in as few words as possible. This is desirable when space is limited or expensive, e.g., when displaying product descriptions on smartphones, or when including advertisements in Web search results. If an importance score is available for each fact, our model can take it into account to prefer expressing important facts, again using as few words as possible.[Citation]In the experiments of this article, we treat all the facts as equally important. Although the search space of our model is very large and ILP problems are in general NP-hard, offthe-shelf ILP solvers can be used, which can be very fast in practice and guarantee finding a global optimum. Experiments with two ontologies show that our ILP model outperforms, in terms of expressed facts per word, an NLG system that uses the same components connected in a pipeline, with no deterioration in perceived text quality; the ILP model may actually lead to texts of higher quality, compared to those of the pipeline, when there are many facts to express."}
{"citing_paper_id": "C10-1103", "cited_paper_id": "H05-1043", "citing_paper_abstract": "The problem addressed in this paper is to predict a user?s numeric rating in a product review from the text of the review. Unigram and n-gram representations of text are common choices in opinion mining. However, unigrams cannot capture important expressions like ?could have been better?, which are essential for prediction models of ratings. N-grams of words, on the other hand, capture such phrases, but typically occur too sparsely in the training set and thus fail to yield robust predictors. This paper overcomes the limitations of these two models, by introducing a novel kind of bag-of-opinions representation, where an opinion, within a review, consists of three components: a root word, a set of modifier words from the same sentence, and one or more negation words. Each opinion is assigned a numeric score which is learned, by ridge regression, from a large, domain-independent corpus of reviews. For the actual test case of a domain-dependent review, the review?s rating is predicted by aggregating the scores of all opinions in the review and combining it with a domaindependent unigram model. The paper presents a constrained ridge regression algorithm for learning opinion scores. Experiments show that the bag-of-opinions method outperforms prior state-of-the-art techniques for review rating prediction.", "cited_paper_abstract": "Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE?s novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.", "citation": "The methods proposed in #OTHEREFR; #REFRb) can also be categorized into the lexicon based framework because their procedure starts with a set of seed words whose polarities are propagated to other opinion bearing words.", "context": "However, this prior work focused mainly on the opinion polarity of opinion words, neglecting the opinion strength. Recently, the lexicon based approaches were extended to learn domaindependent lexicons #OTHEREFR, but these approaches also neglect the aspect of opinion strength. Our method requires only the prior polarity of opinion roots and can thus be used on top of those methods for learning the scores of domain-dependent opinion components.[Citation]"}
{"citing_paper_id": "P06-1034", "cited_paper_id": "H05-1043", "citing_paper_abstract": "Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts. Dictionary creation is a costly process; it is currently done by hand for each dialogue domain. We propose a novel unsupervised method for learning such mappings from user reviews in the target domain, and test it on restaurant reviews. We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision. Experimental analyses show that the mappings learned cover most of the domain ontology, and provide good linguistic variation. A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline.", "cited_paper_abstract": "Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE?s novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.", "citation": "Techniques for automatic feature identification #OTHEREFR; #REFR could possibly help here, although these techniques currently have the limitation that they do not automatically identify different lexicalizations of the same feature.", "context": "For example the contextual filter is based on POS-tags, so that sentences that do not require the prior context for their interpretation are eliminated, such as sentences containing subordinating conjunctions like because, when, if, whose arguments are both given in the same sentence #OTHEREFR. In addition, recall is affected by the domain ontology, and the automatically constructed domain ontology from the review webpages may not cover all of the domain. In some review domains, the attributes that get individual ratings are a limited subset of the domain ontology.[Citation]A different type of limitation is that dialogue systems need to generate utterances for information gathering whereas the mappings we obtained can only be used for information presentation. Thus these would have to be constructed by hand, as in current practice, or perhaps other types of corpora or resources could be utilized. In addition, the utility of syntactic structures in the mappings should be further examined, especially given the failures in DSyntS conversion."}
{"citing_paper_id": "W10-3209", "cited_paper_id": "H05-1043", "citing_paper_abstract": "Opinion mining and sentiment analysis has recently gained increasing attention among the NLP community. Opinion mining is considered a domaindependent task. Constructing lexicons for different domains is labor intensive. In this paper, we propose a framework for constructing Thai language resource for feature-based opinion mining. The feature-based opinion mining essentially relies on the use of two main lexicons, features and polar words. Our approach for extracting features and polar words from opinionated texts is based on syntactic pattern analysis. The evaluation is performed with a case study on hotel reviews. The proposed method has shown to be very effective in most cases. However, in some cases, the extraction is not quite straightforward. The reasons are due to, firstly, the use of conversational language in written opinionated texts and, secondly, the language semantic. We provide discussion with possible solutions on pattern extraction for some of the challenging cases.", "cited_paper_abstract": "Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE?s novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.", "citation": "To obtain such details, a feature-based opinion mining approach has been proposed #OTHEREFR; #REFR.", "context": "Much work in this area focused on evaluating reviews as being positive or negative either at the document level #OTHEREFR. For instance, given some reviews of a product, the system classifies them into positive or negative reviews. No specific details or features are identified about what customers like or dislike.[Citation]This approach typically consists of two following steps."}
{"citing_paper_id": "C10-1103", "cited_paper_id": "H05-1044", "citing_paper_abstract": "The problem addressed in this paper is to predict a user?s numeric rating in a product review from the text of the review. Unigram and n-gram representations of text are common choices in opinion mining. However, unigrams cannot capture important expressions like ?could have been better?, which are essential for prediction models of ratings. N-grams of words, on the other hand, capture such phrases, but typically occur too sparsely in the training set and thus fail to yield robust predictors. This paper overcomes the limitations of these two models, by introducing a novel kind of bag-of-opinions representation, where an opinion, within a review, consists of three components: a root word, a set of modifier words from the same sentence, and one or more negation words. Each opinion is assigned a numeric score which is learned, by ridge regression, from a large, domain-independent corpus of reviews. For the actual test case of a domain-dependent review, the review?s rating is predicted by aggregating the scores of all opinions in the review and combining it with a domaindependent unigram model. The paper presents a constrained ridge regression algorithm for learning opinion scores. Experiments show that the bag-of-opinions method outperforms prior state-of-the-art techniques for review rating prediction.", "cited_paper_abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation": "The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters #REFR, which are used for identifying opinion roots, modifiers and negation words.", "context": "[Citation]Opinion roots are identified as the positive and negative subjectivity clues in the subjectivity lexicon. In the same manner, intensifiers and valence shifters of the type {negation, shiftneg} are mapped to modifiers and negation words. Other modifier candidates are adverbs, conjunctions and modal verbs around opinion roots."}
{"citing_paper_id": "D09-1018", "cited_paper_id": "H05-1044", "citing_paper_abstract": "This work investigates design choices in modeling a discourse scheme for improving opinion polarity classification. For this, two diverse global inference paradigms are used: a supervised collective classification framework and an unsupervised optimization framework. Both approaches perform substantially better than baseline approaches, establishing the efficacy of the methods and the underlying discourse scheme. We also present quantitative and qualitative analyses showing how the improvements are achieved.", "cited_paper_abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation": "Previous work on polarity disambiguation has used contextual clues and reversal words #REFR.", "context": "[Citation]However, these do not capture discourse-level relations. Researchers, such as #OTHEREFR, have developed annotation schemes for interpreting opinions with discourse relations. However, they do not empirically demonstrate how automatic methods can use their ideas to improve polarity classification."}
{"citing_paper_id": "D12-1122", "cited_paper_id": "H05-1044", "citing_paper_abstract": "Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs). CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level. We extend the original semi-CRF model (Sarawagi and Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segmentlevel syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks.", "cited_paper_abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation": "For CRF-style features, we consider the string representation of the current word, its part-ofspeech, and a dictionary-derived feature, which is based on a subjectivity lexicon provided by #REFR.", "context": "Here we described the features used in our model. Very generally, we include CRF-style features that are segment-level extensions of the token-level features. We also include new segment-level features that can be naturally represented in semi-CRFs but not CRFs.[Citation]The lexicon consists of a set of words that can act as strong or weak cues to subjectivity. If the current word appears as an entry in the lexicon, then a feature strong or weak will be fired if the entry is of that strength. These features have been successfully employed in previous work #OTHEREFR."}
{"citing_paper_id": "P07-1123", "cited_paper_id": "H05-1044", "citing_paper_abstract": "This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English. Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language.", "cited_paper_abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation": "The English subjectivity lexicon was evaluated in #OTHEREFR against a corpus of English-language news articles manually annotated for subjectivity (the MPQA corpus #REFR).", "context": "Table 1: Examples of entries in the Romanian subjectivity lexicon also shows the reliability of the expression (weak or strong) and the part of speech ? attributes that are provided in the English subjectivity lexicon. Manual Evaluation. We want to assess the quality of the translated lexicon, and compare it to the quality of the original English lexicon.[Citation]According to this evaluation, 85% of the instances of the clues marked as strong and 71.5% of the clues marked as weak are in subjective sentences in the MPQA corpus. Since there is no comparable Romanian corpus, an alternate way to judge the subjectivity of a Romanian lexicon entry is needed. Two native speakers of Romanian annotated the subjectivity of 150 randomly selected entries."}
{"citing_paper_id": "P12-1105", "cited_paper_id": "H05-1044", "citing_paper_abstract": "Polarity classification of words is important for applications such as Opinion Mining and Sentiment Analysis. A number of sentiment word/sense dictionaries have been manually or (semi)automatically constructed. The dictionaries have substantial inaccuracies. Besides obvious instances, where the same word appears with different polarities in different dictionaries, the dictionaries exhibit complex cases, which cannot be detected by mere manual inspection. We introduce the concept of polarity consistency of words/senses in sentiment dictionaries in this paper. We show that the consistency problem is NP-complete. We reduce the polarity consistency problem to the satisfiability problem and utilize a fast SAT solver to detect inconsistencies in a sentiment dictionary. We perform experiments on four sentiment dictionaries and WordNet.", "cited_paper_abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation": "Several domain independent sentiment dictionaries have been manually or #OTHEREFR, Opinion Finder (OF) #REFR, Appraisal Lexicon #OTHEREFR.", "context": "The general approach is to summarize the semantic polarity #OTHEREFR. Sentiment dictionaries are utilized to facilitate the summarization. There are numerous works that, given a sentiment lexicon, analyze the structure of a sentence/document to infer its orientation, the holder of an opinion, the sentiment of the opinion, etc. #OTHEREFR.[Citation]Q-WordNet and SentiWordNet are lexical resources which classify the synsets(senses) in Word- Net according to their polarities. We call them sentiment sense dictionaries (SSD). OF, GI and AL are called sentiment word dictionaries (SWD)."}
{"citing_paper_id": "S13-2088", "cited_paper_id": "H05-1044", "citing_paper_abstract": "This paper describes an expression-level sentiment detection system that participated in the subtask A of SemEval-2013 Task 2: Sentiment Analysis in Twitter. Our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive, negative or neutral. The proposed approach helps to understand the relevant features that contribute most in this classification task.", "cited_paper_abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation": "Sentiment detection from Twitter data has attracted much attention from the research community in recent times #OTHEREFR; #REFR.", "context": "[Citation]However, most of these approaches classify entire tweets by their overall sentiment (positive, negative or neutral). The task at hand is to classify expressions with their contextual sentiment. Most of these expressions can be found in sentiment lexicons already annotated with their general polarity, but the focus of this task is to detect the polarity of that expression within the context of the tweet it appears in, and therefore, given the context, the polarity of the expression might differ from that found in any lexicon."}
{"citing_paper_id": "W10-2910", "cited_paper_id": "H05-1044", "citing_paper_abstract": "We demonstrate that relational features derived from dependency-syntactic and semantic role structures are useful for the task of detecting opinionated expressions in natural-language text, significantly improving over conventional models based on sequence labeling with local features. These features allow us to model the way opinionated expressions interact in a sentence over arbitrary distances. While the relational features make the prediction task more computationally expensive, we show that it can be tackled effectively by using a reranker. We evaluate a number of machine learning approaches for the reranker, and the best model results in a 10-point absolute improvement in soft recall on the MPQA corpus, while decreasing precision only slightly.", "cited_paper_abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation": "We carried out the experiments on version 2 of the MPQA corpus #REFR, which we split into a test set (150 documents, 3,743 sentences) and a training set (541 documents, 12,010 sentences).", "context": "[Citation]"}
{"citing_paper_id": "P08-1079", "cited_paper_id": "H05-1047", "citing_paper_abstract": "We present a novel framework for the discovery and representation of general semantic relationships that hold between lexical items. We propose that each such relationship can be identified with a cluster of patterns that captures this relationship. We give a fully unsupervised algorithm for pattern cluster discovery, which searches, clusters and merges highfrequency words-based patterns around randomly selected hook words. Pattern clusters can be used to extract instances of the corresponding relationships. To assess the quality of discovered relationships, we use the pattern clusters to automatically generate SAT analogy questions. We also compare to a set of known relationships, achieving very good results in both methods. The evaluation (done in both English and Russian) substantiates the premise that our pattern clusters indeed reflect relationships perceived by humans.", "cited_paper_abstract": "Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system.", "citation": "There are already applications that utilize such knowledge (e.g., #REFR for textual entailment).", "context": "Most established resources (e.g., WordNet) represent only the main and widely accepted relationships such as hypernymy and meronymy. However, there are many other useful relationships between concepts, such as noun-modifier and inter-verb relationships. Identifying and representing these explicitly can greatly assist various tasks and applications.[Citation]One of the leading methods in semantics acquisition is based on patterns #OTHEREFR). The standard process for pattern-based relation extraction is to start with hand-selected patterns or word pairs expressing a particular relationship, and iteratively scan the corpus for co-appearances of word pairs in patterns and for patterns that contain known word pairs. This methodology is semi-supervised, requiring prespecification of the desired relationship or handcoding initial seed words or patterns."}
{"citing_paper_id": "W07-2208", "cited_paper_id": "H05-1059", "citing_paper_abstract": "This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree?s probabilistic model.", "cited_paper_abstract": "This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines.", "citation": "The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger #REFR.", "context": "The performance of each model was analyzed using the sentences in Section 24 of ? 100 words. Table 3 details the numbers and average lengths of the tested sentences of ? 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4.[Citation]LF and UF in the figure are labeled F-score and unlabeled F-score. F-score is the harmonic mean of precision and recall. We evaluated our model in two settings."}
{"citing_paper_id": "W13-3506", "cited_paper_id": "H05-1059", "citing_paper_abstract": "This paper proposes a boosting algorithm that uses a semi-Markov perceptron. The training algorithm repeats the training of a semi-Markov model and the update of the weights of training samples. In the boosting, training samples that are incorrectly segmented or labeled have large weights. Such training samples are aggressively learned in the training of the semi-Markov perceptron because the weights are used as the learning ratios. We evaluate our training method with Noun Phrase Chunking, Text Chunking and Extended Named Entity Recognition. The experimental results show that our method achieves better accuracy than a semi-Markov perceptron and a semi-Markov Conditional Random Fields.", "cited_paper_abstract": "This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines.", "citation": "In the early stages, algorithms for training classifiers, including Maximum Entropy Models #REFR, AdaBoost-based learning algorithms #OTHEREFR were widely used.", "context": "Natural Language Processing (NLP) basic tasks, such as Noun Phrase Chunking, Text Chunking, and Named Entity Recognition, are realized by segmenting words and labeling to the segmented words. To realize these tasks, supervised learning algorithms have been applied successfully.[Citation]Recently, learning algorithms for structured prediction, such as linear-chain structured predictions, and semi-Markov modelbased ones, have been widely used. The examples of linear-chain structured predictions include Conditional Random Fields #OTHEREFR. The examples of semi-Markov model-based ones include semi-Markov model perceptron #OTHEREFR."}
{"citing_paper_id": "D07-1022", "cited_paper_id": "H05-1060", "citing_paper_abstract": "In morphologically rich languages, should morphological and syntactic disambiguation be treated sequentially or as a single problem. We describe several efficient, probabilisticallyinterpretable ways to apply joint inference to morphological and syntactic disambiguation using lattice parsing. Joint inference is shown to compare favorably to pipeline parsing methods across a variety of component models. State-of-the-art performance on Hebrew Treebank parsing is demonstrated using the new method. The benefits of joint inference are modest with the current component models, but appear to increase as components themselves improve.", "cited_paper_abstract": "Finite-state approaches have been highly successful at describing the morphological processes of many languages. Such approaches have largely focused on modeling the phoneor character-level processes that generate candidate lexical types, rather than tokens in context. For the full analysis of words in context, disambiguation is also required (Hakkani-Tu?r et al, 2000; Hajic? et al, 2001). In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inflectional languages. The channel model exploits an existing morphological dictionary, constraining each word?s analysis to be linguistically valid. The source model is a factored, conditionally-estimated random field (Lafferty et al, 2001) that learns to disambiguate the full sentence by modeling local contexts. Compared with baseline state-of-the-art methods, our method achieves statistically significant error rate reductions on Korean, Arabic, and Czech, for various training set sizes and accuracy measures.", "citation": "Lately a few supervised disambiguation methods have come about, including hidden Markov models #OTHEREFR; #REFRb), and local support vector machines #OTHEREFR.", "context": "However, they assumed that the input to the parser was already (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing #OTHEREFR. Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers #OTHEREFR.[Citation]There are also morphological disambiguators designed specifically for Hebrew #OTHEREFR."}
{"citing_paper_id": "D08-1017", "cited_paper_id": "H05-1066", "citing_paper_abstract": "We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al, 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers.", "cited_paper_abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "citation": "A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized #REFRb).", "context": "We explore a stacked framework for learning to predict dependency structures for natural language sentences.[Citation]Recently Nivre and McDonald #OTHEREFR used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction."}
{"citing_paper_id": "D12-1044", "cited_paper_id": "H05-1066", "citing_paper_abstract": "We introduce gap inheritance, a new structural property on trees, which provides a way to quantify the degree to which intervals of descendants can be nested. Based on this property, two new classes of trees are derived that provide a closer approximation to the set of plausible natural language dependency trees than some alternative classes of trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways. The 1-Inherit class of trees has exactly the same empirical coverage of natural language sentences as the class of mildly nonprojective trees, yet the optimal scoring tree can be found in an order of magnitude less time. Gap-minding trees (the second class) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals.", "cited_paper_abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "citation": "The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors #REFRb).", "context": "Finding the optimal tree in the set of projective trees can be done efficiently #OTHEREFR. However, the projectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective (Table 1). At the other end of the spectrum, some parsers search over all spanning trees, a class of structures much larger than the set of plausible linguistic structures.[Citation]However, it is NP-hard to extend MST to include sibling or grandparent factors #OTHEREFR. MST- based non-projective parsers that use higher order factors #OTHEREFR, utilize different techniques than the basic MST algorithm. In addition, learning is done over a relaxation of the problem, so the inference procedures at training and at test time are not identical."}
{"citing_paper_id": "P07-1079", "cited_paper_id": "H05-1066", "citing_paper_abstract": "We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing to increase deep parsing accuracy, specifically by combining dependency and HPSG parsing. We show that by using surface dependencies to constrain the application of wide-coverage HPSG rules, we can benefit from a number of parsing techniques designed for highaccuracy dependency parsing, while actually performing deep syntactic analysis. Our framework results in a 1.4% absolute improvement over a state-of-the-art approach for wide coverage HPSG parsing.", "cited_paper_abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "citation": "The second parser is MSTParser, the large-margin maximum spanning tree parser described in #REFR3.", "context": "As a first step, we train two additional parsers with the dependencies extracted from the HPSG Treebank. The first uses the same shiftreduce framework described in section 2.1, but it process the input from right to left (RL). This has been found to work well in previous work on dependency parser combination #OTHEREFR.[Citation]We examine the use of two combination schemes: one using two parsers, and one using three parsers. The first combination approach is to keep only dependencies for which there is agreement between the two parsers. In other words, dependencies that are proposed by one parser but not the other are simply discarded."}
{"citing_paper_id": "Q13-1004", "cited_paper_id": "H05-1066", "citing_paper_abstract": "Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B&B) framework where nonlocal features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. Experiments are conducted on English PTB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to nonprojective dependency parsing or other graphical models.", "cited_paper_abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "citation": "The first three types of features are firstly introduced by #REFRa) and the last two types of features are used by Carreras #OTHEREFR.", "context": "The parser is trained using the automatic POS tags generated by 10 fold cross validation. For Chinese, we use the gold standard POS tags. We use five types of features: unigram features, bigram features, in-between features, adjacent sibling features and outermost grand-child features.[Citation]All the features are the concatenation of surrounding words, lower cased words (English only), word length (Chinese only), prefixes and suffixes of words (Chinese only), POS tags, coarse POS tags which are derived from POS tags using a simple mapping table, distance between head and modifier, direction of edges. For English, we used 674 feature templates to generate large amounts of features, and finally got 86.7M non-zero weighted features after training. The baseline parser got 92.81% UAS on the testing set."}
{"citing_paper_id": "Q14-1001", "cited_paper_id": "H05-1066", "citing_paper_abstract": "We present heterogeneous networks as a way to unify lexical networks with relational data. We build a unified ACL Anthology network, tying together the citation, author collaboration, and term-cooccurence networks with affiliation and venue relations. This representation proves to be convenient and allows problems such as name disambiguation, topic modeling, and the measurement of scientific impact to be easily solved using only this network and off-the-shelf graph algorithms.", "cited_paper_abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "citation": "Graph-based methods have been used to great effect in NLP, on problems such as word sense disambiguation #OTHEREFR, and dependency parsing #REFR.", "context": "[Citation]Most previous studies of networks consider networks with only a single type of node, and in some cases using a network with a single type of node can be an oversimplified view if it ignores other types of relationships. In this paper we will demonstrate heterogeneous networks, networks with multiple different types of nodes and edges, along with several applications of them. The applications in this paper are not presented so much as robust attempts to out-perform the current state-of-the-art, but rather attempts at being competitive against top methods with little effort beyond the construction of the heterogeneous network."}
{"citing_paper_id": "W07-0407", "cited_paper_id": "H05-1066", "citing_paper_abstract": "Discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources. But, the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure (the set of alignment links) and the syntactic divergence between sentences in the parallel text. This is primarily because of the limitation of their search techniques. In this paper, we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively. These features are particularly useful for language pairs with high structural divergence (like English-Hindi, English- Japanese). We have shown that by using the structural features, we have obtained a decrease of 2.3% in the absolute value of alignment error rate (AER). When we add the cooccurence probabilities obtained from IBM model-4 to our features, we achieved the best AER (50.50) for the English-Hindi parallel corpus.", "cited_paper_abstract": "We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.", "citation": "Figure 1: An example of an alignment between an English and a Hindi sentence To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) #REFR.", "context": "Use of structural information associated with the alignment can be particularly helpful for language pairs for which a large amount of unsupervised data is not available to measure accurately the word cooccurence values but which do have a small set of supervised data to learn the structure and divergence across the language pair. We have tested our model on the English-Hindi language pair. Here is an example of an alignment between English-Hindi which shows the complexity of the alignment task for this language pair.[Citation]This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative #OTHEREFR. In the past couple of years, the discriminative models for doing word alignment have gained popularity because of the flexibility they offer in using a large variety of features and in combining information from various sources. #OTHEREFR cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences."}
{"citing_paper_id": "W10-0202", "cited_paper_id": "H05-1073", "citing_paper_abstract": "Prompt and knowledgeable responses to customers? emails are critical in maximizing customer satisfaction. Such emails often contain complaints about unfair treatment due to negligence, incompetence, rigid protocols, unfriendly systems, and unresponsive personnel. In this paper, we refer to these emails as emotional emails. They provide valuable feedback to improve contact center processes and customer care, as well as, to enhance customer retention. This paper describes a method for extracting salient features and identifying emotional emails in customer care. Salient features reflect customer frustration, dissatisfaction with the business, and threats to either leave, take legal action and/or report to authorities. Compared to a baseline system using word ngrams, our proposed approach with salient features resulted in a 20% absolute F- measure improvement.", "cited_paper_abstract": "In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children?s fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a na??ve baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alernations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions.", "citation": "Research has also been done in predicting basic emotions (also referred to as affects) within text #REFR.", "context": "Extensive work has been done on emotion detection. In the context of human-computer dialogs, although richer features including acoustic and intonation are available, there is a general consensus #OTHEREFR about the use of lexical features to significantly improve the accuracy of emotion detection.[Citation]To render speech with prosodic contour conveying the emotional content of the text, one of 6 types of human emotions (e.g., angry, disgusted, fearful, happy, sad, and surprised) are identified for each sentence in the running text. Deducing such emotions from lexical constructs is a hard problem evidenced by little agreement among humans. A Kappa value of 0.24-0.51 was shown in Alm et al #OTHEREFR."}
{"citing_paper_id": "W14-2508", "cited_paper_id": "H05-1079", "citing_paper_abstract": "In this paper we introduce the task of fact checking, i.e. the assessment of the truthfulness of a claim. The task is commonly performed manually by journalists verifying the claims made by public figures. Furthermore, ordinary citizens need to assess the truthfulness of the increasing volume of statements they consume. Thus, developing fact checking systems is likely to be of use to various members of society. We first define the task and detail the construction of a publicly available dataset using statements fact-checked by journalists available online. Then, we discuss baseline approaches for the task and the challenges that need to be addressed. Finally, we discuss how fact checking relates to mainstream natural language processing tasks and can stimulate further research.", "cited_paper_abstract": "We use logical inference techniques for recognising textual entailment. As the performance of theorem proving turns out to be highly dependent on not readily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment. Finally, we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap; the resulting hybrid model achieves high accuracy on the RTE testset, given the state of the art. Our results also show that the different techniques that we employ perform very differently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature.", "citation": "Finally, the compilation of the answers into a verdict could be considered as a form of logic-based textual entailment #REFR.", "context": "The manual fact checking process suggests an approach that is more likely to give an interpretable analysis and would decompose the task into the following stages: 1. extract statements to be fact-checked 2. construct appropriate questions 3. obtain the answers from relevant sources 4. reach a verdict using these answers The stages of this architecture can be mapped to tasks well-explored in the natural language processing community. Statement extraction could be tackled as a sentence classification problem, following approaches similar to those proposed for speculation detection #OTHEREFR. Furthermore, obtaining answers to questions from databases is a task typically addressed in the context of semantic parsing research, while obtaining such answers from textual sources is usually considered in the context of information extraction.[Citation]However, the fact-checking stages described include a novel task, namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing."}
{"citing_paper_id": "N06-2013", "cited_paper_id": "H05-1085", "citing_paper_abstract": "In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality. Our results show that given large amounts of training data, splitting off only proclitics performs best. However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation. Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.", "cited_paper_abstract": "In statistical machine translation, estimating word-to-word alignment probabilities for the translation model can be difficult due to the problem of sparse data: most words in a given corpus occur at most a handful of times. With a highly inflected language such as Czech, this problem can be particularly severe. In addition, much of the morphological variation seen in Czech words is not reflected in either the morphology or syntax of a language like English. In this work, we show that using morphological analysis to modify the Czech input can improve a Czech-English machine translation system. We investigate several different methods of incorporating morphological information, and show that a system that combines these methods yields the best results. Our final system achieves a BLEU score of .333, as compared to .270 for the baseline word-to-word system.", "citation": "This reduction can be achieved by increasing training data or via morphologically driven preprocessing #REFR.", "context": "The anecdotal intuition in the field is that reduction of word sparsity often improves translation quality.[Citation]Recent publications on the effect of morphology on SMT quality focused on morphologically rich languages such as German #OTHEREFR. They all studied 2We conducted several additional experiments that we do not report on here for lack of space but we reserve for a separate technical report. the effects of various kinds of tokenization, lemmatization and POS tagging and show a positive effect on SMT quality. Specifically considering Arabic, Lee #OTHEREFR investigated the use of automatic alignment of POS tagged English and affix-stem segmented Arabic to determine appropriate tokenizations."}
{"citing_paper_id": "N13-1004", "cited_paper_id": "H05-1085", "citing_paper_abstract": "Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words. We present a two-level alignment model that distinguishes between words and morphemes, in which we embed an IBM Model 1 inside an HMM based word alignment model. The model jointly induces word and morpheme alignments using an EM algorithm. We evaluated our model on Turkish-English parallel data. We obtained significant improvement of BLEU scores over IBM Model 4. Our results indicate that utilizing information from morphology improves the quality of word alignments.", "cited_paper_abstract": "In statistical machine translation, estimating word-to-word alignment probabilities for the translation model can be difficult due to the problem of sparse data: most words in a given corpus occur at most a handful of times. With a highly inflected language such as Czech, this problem can be particularly severe. In addition, much of the morphological variation seen in Czech words is not reflected in either the morphology or syntax of a language like English. In this work, we show that using morphological analysis to modify the Czech input can improve a Czech-English machine translation system. We investigate several different methods of incorporating morphological information, and show that a system that combines these methods yields the best results. Our final system achieves a BLEU score of .333, as compared to .270 for the baseline word-to-word system.", "citation": "#REFR proposed a morpheme aware word alignment model for language pairs in which the source language words correspond to only one morpheme.", "context": "This also causes deficiency in the word-and-morpheme version, as single morpheme words are generated twice, as a word and as a morpheme. Nevertheless, we observed that the deficient version of TAM can perform as good as the nondeficient version of TAM, and sometimes performs better. This is not surprising, as deficient word alignment models such as IBM Model 3 or discriminative word alignment models work well in practice.[Citation]Their word alignment model is: P (e|f) = K? k=0 P (ek|f) where ek is the kth morpheme of the word e. The morpheme-only version of our model is a generalization of this model. However, there are major differences in their and our implementation and experimentation."}
{"citing_paper_id": "W13-2010", "cited_paper_id": "H05-1091", "citing_paper_abstract": "We participated in the BioNLP 2013 shared tasks, addressing the GENIA (GE) and the Cancer Genetics (CG) event extraction tasks. Our event extraction is based on the system we recently proposed for mining relations and events involving genes or proteins in the biomedical literature using a novel, approximate subgraph matching-based approach. In addition to handling the GE task involving 13 event types uniformly related to molecular biology, we generalized our system to address the CG task targeting a challenging set of 40 event types related to cancer biology with various arguments involving 18 kinds of biological entities. Moreover, we attempted to integrate a distributional similarity model into our system to extend the graph matching scheme for more events. In addition, we evaluated the impact of using paths of all possible lengths among event participants as key contextual dependencies to extract potential events as compared to using only the shortest paths within the framework of our system. We achieved a 46.38% F-score in the CG task and a 48.93% F-score in the GE task, ranking 3rd and 4th respectively. The consistent performance confirms that our system generalizes well to various event extraction tasks and scales to handle a large number of event and entity types.", "cited_paper_abstract": "We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.", "citation": "While additional information such as individual words in each sentence #OTHEREFR, the shortest path between two tokens in the dependency graph is particularly likely to carry the most valuable information about their mutual relationship #REFR.", "context": "Event rules are learned automatically using the following method. Starting with the dependency graph of each training sentence, for each annotated event, the shortest dependency path connecting the event trigger to each event argument in the undirected version of the graph is selected.[Citation]In case there exists more than one shortest path, all of them are considered. For multi-token event triggers, the shortest path connecting every trigger token to each event argument is extracted, and the union of the paths is then computed for each trigger. For regulatory events that take a sub-event as an argument, the shortest path is extracted so as to connect the trigger of the main event to that of the sub-event."}
{"citing_paper_id": "W08-0323", "cited_paper_id": "H05-1095", "citing_paper_abstract": "Our participation in the shared translation task at WMT-08 focusses on news translation from English to French. Our main goal is to contrast a baseline version of the phrase-based MATRAX system, with a version that incorporates syntactic ?coupling? features in order to discriminate translations produced by the baseline system. We report results comparing different feature combinations.", "cited_paper_abstract": "This paper presents a phrase-based statistical machine translation method, based on non-contiguous phrases, i.e. phrases with gaps. A method for producing such phrases from a word-aligned corpora is proposed. A statistical translation model is also presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data.", "citation": "These MATRAX features are pretty standard phrasebased features, apart from some features dealing explicitly with gapped phrases, and are described in detail in #REFR. .", "context": "We distinguish two sub-tables, according to whether Giza-based alignments or phrasebased alignments were used. . The Generic keyword corresponds to the coupling features introduced in section 1.2.1, based on rectangle counts, independent of the labels of the edges. . The Matrax keyword corresponds to using MATRAX ?internal? features as reranking features, along with the coupling features.[Citation]The Labels and Frequent Labels keywords corresponds to using label-specific features. In the first case (Labels) we extracted all of the aligned label pairs (label pair associated with a coupling rectangle) found in a training set, while in the second case (Frequent Labels), we only kept the most frequently observed among these label pairs. . When several keywords appear on a line, we used the union of the corresponding features, and in the last line of the table, we show a combination involving at the same time some features computed on the basis of Giza-based alignments and of phrase-based alignments. ."}
{"citing_paper_id": "W11-1006", "cited_paper_id": "H05-1097", "citing_paper_abstract": "Mistranslation of an ambiguous word can have a large impact on the understandability of a given sentence. In this article, we describe a thorough evaluation of the translation quality of ambiguous nouns in three different setups. We compared two statistical Machine Translation systems and one dedicated Word Sense Disambiguation (WSD) system. Our WSD system incorporates multilingual information and is independent from external lexical resources. Word senses are derived automatically from word alignments on a parallel corpus. We show that the two WSD classifiers that were built for these experiments (English. French and English?Dutch) outperform the SMT system that was trained on the same corpus. This opens perspectives for the integration of our multilingual WSD module in a statistical Machine Translation framework, in order to improve the automated translation of ambiguous words, and by consequence make the translation output more understandable.", "cited_paper_abstract": "In word sense disambiguation, a system attempts to determine the sense of a word from contextual features. Major barriers to building a high-performing word sense disambiguation system include the difficulty of labeling data for this task and of predicting fine-grained sense distinctions. These issues stem partly from the fact that the task is being treated in isolation from possible uses of automatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a significant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machinetranslation task and can effectively and accurately prune the set of candidate translations for a word.", "citation": "In #REFR, the problem was defined as a word translation task.", "context": "In addition to this, there is a growing feeling in the community that WSD should be used and evaluated in real application such as Machine Translation #OTHEREFR. An important line of research consists in the development of dedicated WSD modules for MT. Instead of assigning a sense label from a monolingual sense-inventory to the ambiguous words, the WSD system has to predict a correct translation for the ambiguous word in a given context.[Citation]The translation choices of ambiguous words are gathered from a parallel corpus by means of word alignment. The authors reported improvements on two simplified translation tasks: word translation and blank filling. The evaluation was done on an English-French parallel corpus but is confronted with the important limitation of having only one valid translation (the aligned translation in the parallel corpus) as a gold standard translation."}
{"citing_paper_id": "P07-1062", "cited_paper_id": "H05-1099", "citing_paper_abstract": "We investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach, including features derived from either finite-state or contextfree annotations. We achieve the best reported performance on this task, and demonstrate that our SPADE-inspired context-free features are critical to achieving this level of accuracy. This counters recent results suggesting that purely finite-state approaches can perform competitively.", "cited_paper_abstract": "In this paper, we look at comparing highaccuracy context-free parsers with highaccuracy finite-state (shallow) parsers on several shallow parsing tasks. We show that previously reported comparisons greatly under-estimated the performance of context-free parsers for these tasks. We also demonstrate that contextfree parsers can train effectively on relatively little training data, and are more robust to domain shift for shallow parsing tasks than has been previously reported. Finally, we establish that combining the output of context-free and finitestate parsers gives much higher results than the previous-best published results, on several common tasks. While the efficiency benefit of finite-state models is inarguable, the results presented here show that the corresponding cost in accuracy is higher than previously thought.", "citation": "We annotate tag sequences onto the word sequence via a competitive discriminatively trained tagger #REFR, trained for each of two kinds of tag sequences: part-of-speech (POS) tags and shallow parse tags.", "context": "We created 101 indicator features, representing percentages from 0 to 100. For a string of length k, at position i, we round i/k to two decimal places and provide a value of 1 for the corresponding quantized position feature and 0 for the other position features. 2.5.1 Basic finite-state features Our baseline finite-state feature set includes simple tagger derived features, as well as features based on position in the string and n-grams4.[Citation]The shallow parse tags define nonhierarchical base constituents #OTHEREFR. These can either be used as tag or chunk sequences. For example, the tree in Figure 2 represents a shallow (non-hierarchical) parse tree, with four base constituents."}
{"citing_paper_id": "P06-1123", "cited_paper_id": "H05-1101", "citing_paper_abstract": "This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why ?syntactic? constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at http://nlp.cs.nyu.edu/GenPar/ACL06", "cited_paper_abstract": "This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation. These models can be viewed as pairs of probabilistic contextfree grammars working in a ?synchronous? way. Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature.", "citation": "More generally, for any positive integer k, it is possible to construct a word alignment that cannot be generated using binary production rules whose nonterminals all have fewer than k gaps #REFR.", "context": "If each link is represented by its own nonterminal, and production rules must be binary-branching, then some of the nonterminals involved in generating this alignment need discontinuities, or gaps. Figure 1(b) illustrates how to generate the sentence pair and its word alignment in this manner. The nonterminals X and Y have one discontinuity each.[Citation]Our study measured the complexity of a word alignment as the minimum number of gaps needed to generate it under the following constraints:"}
{"citing_paper_id": "P06-1053", "cited_paper_id": "H05-1104", "citing_paper_abstract": "The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures. This paper describes a method for incorporating priming into an incremental probabilistic parser. Three models are compared, which involve priming of rules between sentences, within sentences, and within coordinate structures. These models simulate the reading time advantage for parallel structures found in human data, and also yield a small increase in overall parsing accuracy.", "cited_paper_abstract": "Experimental research in psycholinguistics has demonstrated a parallelism effect in coordination: speakers are faster at processing the second conjunct of a coordinate structure if it has the same internal structure as the first conjunct. We show that this phenomenon can be explained by the prevalence of parallel structures in corpus data. We demonstrate that parallelism is not limited to coordination, but also applies to arbitrary syntactic configurations, and even to documents. This indicates that the parallelism effect is an instance of a general syntactic priming mechanism in human language processing.", "citation": "While the effect is not always strong, we expect positive adaptation to be higher than negative adaptation #REFR.", "context": "Although both the Within and Copy models succeed at the main task of modeling the parallelism effect, the parsing experiments in Section 4 showed mixed results with respect to F-scores: a slight increase in F-score was observed for the Within model, but the Between model performed below the baseline. We therefore turn to an error analysis, focusing on these two models. Recall that the Within and Between models estimate two probabilities for a rule, which we have been calling the positive adaptation (the probability of a rule when the rule is also in the history), and the negative adaptation (the probability of a rule when the rule is not in the history).[Citation]However, this is not always the case. In the Within model, for example, the rule NP . DT JJ NN has a higher negative than positive adaptation (we will refer to such rules as ?negatively adapted?)."}
{"citing_paper_id": "A92-1026", "cited_paper_id": "H91-1037", "citing_paper_abstract": "I1. is often assumed that when natural anguage processing meets the real world, the ideal of aiming for complete and correct interpretations has to be abandoned. However, our experience with TACITUS; especially in the MUC-3 evaluation, has shown that. principled techniques fox' syntactic and pragmatic analysis can be bolstered with methods for achieving robustness. We describe three techniques for making syntactic analysis more robust--an agendabased scheduling parser, a recovery technique for failed parses, and a new technique called terminal substring parsing. For pragmatics processing, we describe how the method of abductive inference is inherently robust, in that an interpretation is always possible, so that in the absence of the required world knowledge, performance degrades gracefully. Each of these techlfiques have been evaluated and the results of the evaluations are presented.", "cited_paper_abstract": "This paper eports a handful of experiments designed to test the feasibility of applying well-known partial parsing techniques to the problem of automatic data base update from an open-ended source of messages, and the feasiblity of automatically learning semantic knowledge from annotated examples. The challenges arise from the incompleteness of any lexicon, sentences that average over 20 words in length, and the lack of a complete semantics.", "citation": "Such strategies involve deemphasizing the role of syntactic analysis #OTHEREFR; #REFR or resorting to weaker syntactic processing methods uch as conceptual or case-frame based parsing #OTHEREFR.", "context": "Any grammar that successfully accounts for the range of syntactic structures encountered in realworld texts will necessarily produce many alnbigt> ous analyses of most sentences. Assuming that the system can find the possible analyses of a sentence in a reasonable period of time, it is still faced with the problem of choosing the correct one from the many competing ones. Designers of application-oriented text processing systems have adopted a number of strategies for (lea.ling with these problems.[Citation]A common feature shared by these weaker methods is that they ignore certain information that is present in the text, which could be extracted by a more comprehensive analysis. The information that is ignored may be irrelevant o a particular application, or relevant in only an insignificant handful of cases, and thus we cannot argue that approaches to text processing based on weak or even nonexistent syntactic and semantic analysis are doomed to failure in all cases and are not worthy of further investigation. However, it is not obvious how such methods can scale up to handle fine distinctions in attachment, scoping, and inference, although some recent attempts have been made in this direction #OTHEREFRb)."}
{"citing_paper_id": "A94-1014", "cited_paper_id": "H91-1066", "citing_paper_abstract": "A computational model for the acquisition of knowledge from encyclopedic texts is described. The model has been implemented in a program, called SNOWY, that reads unedited texts from The World Book Encyclopedia, and acquires new concepts and conceptual relations about topics dealing with the dietary habits of animals, their classifications and habitats. The program is also able to answer an ample set of questions about the knowledge that it has acquired. This paper describes the essential components of this model, namely semantic interpretation, inferences and representation, and ends with an evaluation of the performance of the program, a sample of the questions that it is able to answer, and its relation to other programs of similar nature.", "cited_paper_abstract": "Ordinarily, one thinks of the problem of natural language understanding as one of making a single, left-to-right pass through an input, producing a progressively refined and detailed interpretation. In text interpretation, however, the constraints of strict left-to-right processing are an encumbrance. Multi-pass methods, especially by interpreting words using corpus data and associating units of text with possible interpretations, can be more accurate and faster than single-pass methods of data extraction. Quality improves because corpus-based data and global context help to control false interpretations; speed improves because processing focuses on relevant sections. The most useful forms of pre-processing for text interpretation use fairly superficial analysis that complements he style of ordinary parsing but uses much of the same knowledge base. Lexico-semantic pattern matching, with rules that combine lexlocal analysis with ordering and semantic ategories, is a good method for this form of analysis. This type of pre-processing is efficient, takes advantage of corpus data, prevents many garden paths and fruitless parses, and helps the parser cope with the complexity and flexibility of real text.", "citation": "We are studying mechanisms under which the interpreter will override the parser and will get it out of trouble in processing very complex sentences #OTHEREFR; #REFR.", "context": "Yet, if the parser does not build a parse, albeit a shallow one, the interpreter will not know what to do. Moreover, the interpreter does not question the parser when it says this constituent is an obj, or subj, or a time-np, etc. This is a situation that we are not happy about, because the parser identifies some constituents incorrectly, especially the timenp.[Citation]"}
{"citing_paper_id": "C04-1010", "cited_paper_id": "H92-1026", "citing_paper_abstract": "This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).", "cited_paper_abstract": "We describe a generative probabilistic model of natural anguage, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast o the usual approach of further grammar tailoring via the usual linguistic introspection i the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabflistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.", "citation": "In this way, our approach can be seen as a form of history-based parsing #REFR.", "context": "Otherwise, it is a set of connected components, each of which is a well-formed dependency graph for a substring of the original input. The transition system defined above is nondeterministic in itself, since several transitions can often be applied in a given configuration. To construct deterministic parsers based on this system, we use classifiers trained on treebank data in order to predict the next transition (and dependency type) given the current configuration of the parser.[Citation]In the experiments reported here, we use memory-based learning to train our classifiers."}
{"citing_paper_id": "A97-1024", "cited_paper_id": "H93-1025", "citing_paper_abstract": "We describe the authoring tool, EasyEnglish, which is part of IBM is internal SGML editing environment, Information Development Workbench. EasyEnglish elps writers produce clearer and simpler English by pointing out ambiguity and complexity as well as performing some standard grammar checking. Where appropriate, EasyEnglish makes uggestions for rephrasings that may be substituted directly into the text by using the editor interface. EasyEnglish is based on a full parse by English Slot Grammar; this makes it possible to produce a higher degree of accuracy in error messages as well as handle a large variety of texts.", "cited_paper_abstract": "The Slot Grammar system isinteresting for natural language applications because it can deliver parses with deep grammatical information on a reasonably broad scale. The paper describes anumerical scoring system used in Slot Grammar for ambiguity resolution, which not only ranks parses but also contributes to parsing efficiency through a parse space pruning algorithm. Details of the method of computing parse scores are given, and test results for the English Slot Grammar are presented.", "citation": "ESG often provides more than one parse, ranked according to a specific numerical ranking system #REFR.", "context": "In order for the disambiguation rules to work properly, it is crucial to have a deep analysis of the text. This deep analysis is provided by English Slot Grammar #OTHEREFR in the form of parse trees expressed as a network structure. The disambiguation rules explore the network to spot ambiguous and potentially ambiguous constructions.[Citation]But, unlike some other systems, e.g. the Boeing Simplified English Checker #OTHEREFR, which look at a whole forest of trees, it is only necessary for EasyEnglish to look at the highest-ranked parse. ESG parsing heuristics often arrive at correct attachments in the highest-ranked parse. But even when the attachment is off, EasyEnglish can often point out other attachment possibilities to the writer."}
{"citing_paper_id": "D14-1160", "cited_paper_id": "H93-1061", "citing_paper_abstract": "Connecting words with senses, namely, sight, hearing, taste, smell and touch, to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge. With this in mind, a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language. However, to the best of our knowledge, there is no systematic attempt in the literature to build such a resource. In this paper, we present a sensorial lexicon that associates English words with senses. To obtain this resource, we apply a computational method based on bootstrapping and corpus statistics. The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing. The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task, both at word and sentence level, and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications.", "cited_paper_abstract": "A semantic oncordance is a textual corpus and a lexicon So combined that every substantive word in the text is linked to its appropriate ~nse in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic oncordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports earches of the tagged text. Some practical uses for semantic oncordances are proposed.", "citation": "Since working on the synset level would raise the data sparsity problem in synset tagged corpora such as SemCor #REFR and we need a corpus that provides sufficient statistical information, we migrate from synset level to lexical level.", "context": "Corpus Statistics After generating the seed lists consisting of synsets for each sense category with the help of a set of WordNet relations and a bootstrapping process, we use corpus statistics to create our final sensorial lexicon. More specifically, we exploit a probabilistic approach based on the co-occurrence of the seeds and the candidate lexical entries.[Citation]Accordingly, we treat each POS role of the same lemmas as a distinct seed and extract 4287 lemma-POS pairs from 2572 synsets. In this section, we explain the steps to construct our final sensorial lexicon in detail. 3.3.1 Corpus and Candidate Words As a corpus, we use a subset of English Giga- Word 5th Edition released by Linguistic Data Consortium (LDC)5."}
{"citing_paper_id": "S10-1087", "cited_paper_id": "H93-1061", "citing_paper_abstract": "We describe two systems that participated in SemEval-2010 task 17 (All-words Word Sense Disambiguation on a Specific Domain) and were ranked in the third and fourth positions in the formal evaluation. Domain adaptation techniques using the background documents released in the task were used to assign ranking scores to the words and their senses. The test data was disambiguated using the Personalized PageRank algorithm which was applied to a graph constructed from the whole of WordNet in which nodes are initialized with ranking scores of words and their senses. In the competition, our systems achieved comparable accuracy of 53.4 and 52.2, which outperforms the most frequent sense baseline (50.5).", "cited_paper_abstract": "A semantic oncordance is a textual corpus and a lexicon So combined that every substantive word in the text is linked to its appropriate ~nse in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic oncordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports earches of the tagged text. Some practical uses for semantic oncordances are proposed.", "citation": "The senses in WordNet are ordered according to their frequency in a manually tagged corpus, Sem- Cor #REFR.", "context": "[Citation]Senses that do not occur in SemCor are ordered arbitrarily after those senses of the word that have occurred. It is known from the results of SENSEVAL2 #OTHEREFR). The first sense baseline?s strong performance is due to the skewed frequency distribution of word senses."}
{"citing_paper_id": "S10-1094", "cited_paper_id": "H93-1061", "citing_paper_abstract": "We describe two approaches for All-words Word Sense Disambiguation on a Specific Domain. The first approach is a knowledge based approach which extracts domain-specific largest connected components from the Wordnet graph by exploiting the semantic relations between all candidate synsets appearing in a domainspecific untagged corpus. Given a test word, disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components. The second approach is a weakly supervised approach which relies on the ?One Sense Per Domain? heuristic and uses a few hand labeled examples for the most frequently appearing words in the target domain. Once the most frequent words have been disambiguated they can provide strong clues for disambiguating other words in the sentence using an iterative disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain.", "cited_paper_abstract": "A semantic oncordance is a textual corpus and a lexicon So combined that every substantive word in the text is linked to its appropriate ~nse in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic oncordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports earches of the tagged text. Some practical uses for semantic oncordances are proposed.", "citation": "The second approach is a weakly supervised approach which uses a few hand labeled examples for the most frequent words in the target domain in addition to the publicly available mixed-domain SemCor #REFR corpus.", "context": "Each target word in a given sentence is then disambiguated using an iterative disambiguation process by considering only those candidate synsets which appear in the top-k largest connected components. Our knowledge based approach performed better than current state of the art knowledge based approach #OTHEREFR. Also, the precision was better than the Wordnet first sense baseline even though the F-score was slightly lower than the baseline.[Citation]The underlying assumption is that words exhibit ?One Sense Per Domain? phenomenon and hence even as few as 5 training examples per word would be sufficient to identify the predominant sense of the most frequent words in the target domain. Further, once the most frequent words have been disambiguated using the predominant sense, they can provide strong clues for disambiguating other words in the sentence. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain."}
{"citing_paper_id": "P14-1040", "cited_paper_id": "H94-1010", "citing_paper_abstract": "We present a simple, data-driven approach to generation from knowledge bases (KB). A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG (Tree Adjoining Grammar); and that it takes into account both syntactic and semantic information. The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data. Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar; and is comparable with a handcrafted symbolic approach.", "cited_paper_abstract": "The Air Travel Information System (ATIS) domain serves as the common evaluation task for ARPA\"spoken language system developers. 1 To support this task, the Multi-Site ATIS Data COllection Working group (MADCOW) coordinates data collection activities. This paper describes recent MADCOW activities. In particular, this paper describes the migration of the ATIS task to a richer elational database and development corpus (ATIS-3) and describes the ATIS-3 corpus. The expanded atabase, which includes information on 46 US and Canadian cities and 23,457 flights, was released inthe fall of 1992, and data collection for the ATIS-3 corpus began shortly thereafter. The ATIS-3 corpus now consists of a total of 8297 released training utterances and 3211 utterances reserved for testing, collected at BBN, CMU, MIT, NIST and SRI. 2906 of the training utterances have been annotated with the correct information from the database. This paper describes the ATIS-3 corpus in detail, including breakdowns of data by type (e.g. context-independent, context-dependent, and unevaluable)and variations in the data collected at different sites. This paper also includes a description of the ATIS-3 database. Finally, we discuss future data collection and evaluation plans.", "citation": "They introduce a novel synchronous context free grammar formalism for generating from lambda terms; induce such a synchronous grammar using a generative model; and extract the best output sentence from the generated forest using a log linear model. #OTHEREFR for weather forecast generation and for the air travel domain (ATIS dataset) by #REFR. #OTHEREFR| |instance-of| |Entity|))) ", "context": "In constrast, we propose an approach which can generate complex sentences from KB data; where the grammar is acquired from the data; and where no assumption is made about the mapping between semantics and NL expressions. Recent work has focused on data-driven generation from frames, lambda terms and data base entries. #OTHEREFR describes an approach for generating from the frames produced by a dialog system. They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of #OTHEREFR focuses on generating natural language sentences from logical form (i.e., lambda terms) using a synchronous context-free grammar.[Citation]Figure 1: Example KBGEN Scenario dom Field to generate from the same meaning representations. Finally, more recent papers propose approaches which perform both surface realisation and content selection. #OTHEREFR proposes a log linear model which decomposes into a sequence of discriminative local decisions. The first classifier determines which records to mention; the second, which fields of these records to select; and the third, which words to use to verbalise the selected fields. #OTHEREFR uses a generative model for content selection and verbalises the selected input using WASP ?1 , an existing generator."}
{"citing_paper_id": "C00-1010", "cited_paper_id": "H94-1020", "citing_paper_abstract": "This paper presents an empirical assessment of the LFG- DOP model introduced by Bed & Kaplan (1998). The parser we describe uses fragments l'rom LFG-aunotated sentences to parse new sentences and Monte Carlo techniques to compute the most probable parse. While our main goal is to test Bed & Kaplan is model, we will also test a version of LFG-DOP which treats generalized fragments as previously unseen events. Experiments with the Verbmobil and Itomecentre corpora show that our version of LFG-DOP outperforms Bed & Kaplan is model, and that LFG is functional information improves the parse accuracy of Iree structures.", "cited_paper_abstract": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides aset of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides ome non-context free annotational mechanism to allow the structure of discontinuous constituents o be easily recovered, and allows for a clear, concise tagging system for some semantic roles.", "citation": "This suggests that our model may be successfully used to exploit the functional annotations in the Penn Treebank #REFR, provided that these annotations can be converted into LFG-style l'unctional structures.", "context": "Moreover, Bed & Kaplan is model turned out to be inadequate in dealing with generalized fragments. We also established that the contribution of generalized fragments to the parse accuracy in our model is minimal and statistically insignil'icant. Finally, we showed that LFG is l'unctional structures contribute to significantly higher parse accuracy on tree structures.[Citation]As future research, we want to test LFG-DOP using log-linear models, as such models maximize the likelihood o1' the traiuing corpus."}
{"citing_paper_id": "P04-1041", "cited_paper_id": "H94-1020", "citing_paper_abstract": "This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2002), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees. Currently our best automatically induced grammars achieve 80.97% f-score for fstructures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).", "cited_paper_abstract": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides aset of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides ome non-context free annotational mechanism to allow the structure of discontinuous constituents o be easily recovered, and allows for a clear, concise tagging system for some semantic roles.", "citation": "Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: #OTHEREFR show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank #REFR with discriminative (loglinear) parameter estimation techniques.", "context": "Resolution of such long-distance dependencies #OTHEREFR. Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise.[Citation]The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars #OTHEREFR."}
{"citing_paper_id": "W01-0904", "cited_paper_id": "H94-1020", "citing_paper_abstract": "In this paper we discuss the need for corpora with a variety of annotations to provide suitable resources to evaluate different Natural Language Processing systems and to compare them. A supervised machine learning technique is presented for translating corpora between syntactic formalisms and is applied to the task of translating the Penn Treebank annotation into a Categorial Grammar annotation. It is compared with a current alternative approach and results indicate annotation of broader coverage using a more compact grammar.", "cited_paper_abstract": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides aset of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides ome non-context free annotational mechanism to allow the structure of discontinuous constituents o be easily recovered, and allows for a clear, concise tagging system for some semantic roles.", "citation": "For example, the Penn Treebank #OTHEREFR; #REFR provides a large corpus of syntactically annotated examples mostly from the Wall Street Journal.", "context": "In this case the learned artefact is used to annotate the examples, which can then be compared against the correctly annotated version. Hence, correctly annotated corpora are vital for the evaluation of a very large number of NLP tasks. Unfortunately, there are often no suitably annotated corpora for a given task.[Citation]It is an excellent resource for tasks dealing with the syntax of written English. However, if the annotation formalism (a phrase-structure grammar with some simple features) does not match that of one?s NLP system, it is of very little use. For example, suppose a parser using Categorial Grammar #OTHEREFR is developed and applied to the examples in the corpus."}
{"citing_paper_id": "W03-0401", "cited_paper_id": "H94-1020", "citing_paper_abstract": "This paper presents a new approach to syntactic disambiguation based on lexicalized grammars. While existing disambiguation models decompose the probability of parsing results into that of primitive dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars.", "cited_paper_abstract": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides aset of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides ome non-context free annotational mechanism to allow the structure of discontinuous constituents o be easily recovered, and allows for a clear, concise tagging system for some semantic roles.", "citation": "We evaluated the validity of our model through experiments on a disambiguation task of parsing the Penn Treebank #REFR with an automatically acquired LTAG grammar.", "context": "Since the syntax and semantics probabilities are separate, we can improve them individually. For example, the syntax model can be improved by smoothing using the syntactic classes of words, while the semantics model should be able to be improved by using semantic classes. In addition, the model can be a starting point that allows the theory of syntax and semantics to be evaluated through consulting an extensive corpus.[Citation]To assess the contribution of the syntax and semantics probabilities to the accuracy of parsing and to evaluate the validity of applying maximum entropy estimation for feature forests, we compared three models trained with the same training set and the same set of features. Following the experimental results, we concluded that i) a parser with the syntax probability only achieved high accuracy with the lexicalized grammar, ii) the incorporation of preferences for lexical association through the semantics probability resulted in significant improvements, and iii) our model recorded an accuracy that was quite close to the traditional model, which indicated the validity of applying maximum entropy estimation for feature forests. In what follows, we first describe the existing models for syntactic disambiguation, and discuss problems with them in Section 2."}
{"citing_paper_id": "W96-0212", "cited_paper_id": "H94-1051", "citing_paper_abstract": "Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first. Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser. While several parsers described in the literature have used such techniques, there is no published data on their efficacy, much less attempts to judge their relative merits. We propose and evaluate several figures of merit for best-first parsing.", "cited_paper_abstract": "We describe a series of three experiments in which supervised learning techniques were used to acquire three different ypes of grammars for English news stories. The acquired grammar types were: 1) context-free, 2) context-dependent, and 3) probabilistic context-free. Training data were derived from University of Pennsylvania Treebank parses of 50 Wall Street Journal articles. In each case, the system started with essentially no grammatical knowledge, and learned a set of grammar rules exclusively from the training data. Performance for each gr~rnar type was then evaluated on an independent set of test sentences using Parseval, a standard measure of parsing accuracy. These experimental results yield a direct qtmntitative comparison between each of the three methods.", "citation": "#REFR compare the performance of parsers using three different types of grammars, and show that a probabilistic ontextfree grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar.", "context": "Bobrow #OTHEREFR introduced statistical agenda-based parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser is tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word.[Citation]Kochman and Kupin #OTHEREFR propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. Magerman and Marcus #OTHEREFR use the geometric mean to compute a figure of merit that is independent of constituent length."}
{"citing_paper_id": "C10-2139", "cited_paper_id": "I05-3017", "citing_paper_abstract": "We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation: word-based models and character-based models. We show that, in spite of similar performance overall, the two models produce different distribution of segmentation errors, in a way that can be explained by theoretical properties of the two models. The analysis is further exploited to improve segmentation accuracy by integrating a word-based segmenter and a character-based segmenter. A Bootstrap Aggregating model is proposed. By letting multiple segmenters vote, our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff.", "cited_paper_abstract": "The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation. Twenty three groups submitted 130 result sets over two tracks and four different corpora. We found that the technology has improved over the intervening two years, though the out-of-vocabulary problem is still or paramount importance. 1! Introduction Chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing applications including monoand cross-lingual information retrieval and text-to-speech systems. This word segmentation problem has been active area of research in computational linguistics for almost two decades and is a topic of active research around the world. As the very notion of ?word-hood? in Chinese is hotly debated, so the determination of the correct division of a Chinese sentence into ?words? can be very complex. In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International Chinese Word Segmentation Bakeoff (Sproat and Emerson, 2003). That competition was the first conducted outside of China and has become the benchmark with which researchers evaluate their segmentation systems. During the winter of 2004 it was decided to hold a second evaluation to determine how the latest research has affected segmentation technology. 2! Details of the Contest 2.1! The Corpora Four corpora were used in the evaluation, two each using Simplified and Traditional Chinese characters.1 The Simplified Chinese corpora were provided by Beijing University and Microsoft Research Beijing. The Traditional Chinese corpora were provided by Academia Sinica in Taiwan and the City University of Hong Kong. Each provider supplied separate training and truth data sets. Details on each corpus are provided in Table!1. With one exception, all of the corpora were provided in a single character encoding. We decided to provide all of the data in both Unicode (UTF-8 encoding) and the standard encoding used in each locale. This would allow systems that use one or the other encoding to chose appropriately while ensuring consistent transcoding across all sites. This conversion was problematic in two cases:", "citation": "We used the data provided by the second SIGHAN Bakeoff #REFR to test the two segmentation models.", "context": "[Citation]The data contains four corpora from different sources: Academia Sinica Corpus (AS), City University of Hong Kong (CU), Microsoft Research Asia (MSR), and Peking University (PKU). There is no fixed standard for Chinese word segmentation. The four data sets above are annotated with different standards."}
{"citing_paper_id": "P13-2056", "cited_paper_id": "I05-3027", "citing_paper_abstract": "Cross-lingual projection methods can benefit from resource-rich languages to improve performances of NLP tasks in resources-scarce languages. However, these methods confronted the difficulty of syntactic differences between languages especially when the pair of languages varies greatly. To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. Experiments showed that our methods improve the performances greatly on projections between English and Chinese.", "cited_paper_abstract": "We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a conditional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system is designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).", "citation": "Stanford word segmentor #REFR was used for Chinese word segmentation.", "context": "The universal tag set made the task of POS tagging easier since the fine-grained types are no more cared. The Brown clusters were trained on Chinese Wikipedia. The bodies of all articles are retained to induce 1000 clusters using the algorithm in #OTHEREFR .[Citation]When English Brown clusters were in need, we trained the word clusters on the tokenized English Wikipedia. We chose LDC2003E14 as the parallel corpus, which contains about 200,000 sentences. GIZA++ #OTHEREFR was used to generate word alignments."}
{"citing_paper_id": "W13-2228", "cited_paper_id": "I08-2089", "citing_paper_abstract": "This paper describes QCRI-MES?s submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation. We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA++ and build a phrase-based machine translation system. For tuning, we use a variation of PRO which provides better weights by optimizing BLEU+1 at corpus-level. We transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system. For the Russian to English translation direction, we apply linguistically motivated pre-processing on the Russian side of the data.", "cited_paper_abstract": "This paper presents methods to combine large language models trained from diverse text sources and applies them to a state-ofart French?English and Arabic?English machine translation system. We show gains of over 2 BLEU points over a strong baseline by using continuous space language models in re-ranking.", "citation": "We follow the approach of #REFR by training domain-specific language models separately and then linearly interpolate them using SRILM with weights optimized on the held-out development set.", "context": "The amount of bitext used for the estimation of the translation model is ? 2M parallel sentences. We use newstest2012a for tuning and newstest2012b #OTHEREFR as development set. The language model is estimated using large monolingual corpus of Russian ? 21.7M sentences.[Citation]We divide the tuning set newstest2012a into two halves and use the first half for tuning and second for test in order to obtain stable weights #OTHEREFR."}
{"citing_paper_id": "P99-1039", "cited_paper_id": "J90-1001", "citing_paper_abstract": "The paper shows that movement or equivalent computational structure-changing operations of any kind at the level of logical form can be dispensed with entirely in capturing quantifer scope ambiguity. It offers a new semantics whereby the effects of quantifier scope alternation can be obtained by an entirely monotonic derivation, without ypechanging rules. The paper follows Fodor (1982), Fodor and Sag (1982), and Park (1995, 1996) in viewing many apparent scope ambiguities as arising from referential categories rather than true generalized quantitiers.", "cited_paper_abstract": "ION The analysis of bound anaphora brings up a wide range of issues in syntax, semantics, and pragmatics, most of which I will ignore in this paper. I will assume that possible coreferences are determined elsewhere and that the role of the bound anaphora rules here is simply to derive the appropriate semantic interpretation for phrases involving pronouns. Two approaches are possible here. Working with the functionality rules alone, a noun phrase will be associated to an assumption of the form u : e. The interpretation of a pronoun coindexed with that noun phrase will be another occurrence of the assumption. When the antecedent is a trace or a quantified noun phrase (interpretation of quantified noun phrases is discussed in the next section), the assumption will eventually be discharged. The definition of derivation in Section 2 ensures that all occurrences of u will be bound by the same application of \\[abs\\]. The second approach relies on a pair of derived rules, pronoun licensing and abstraction. These rules of course do not add new semantic consequences, but facilitate the representation of the syntactic licensing of bound anar : t Xx.r : e - - -~ t l But this can be mapped irectly into the schematic derivation x :e I r : t Ax.r : e ---* t 1 y: trace I owns own: e-->e-~t y: e \\[trace-lie\\] John j :e own(y):~--~t \\[appl own(y)O): t \\[appl I that that:(~--~t)--*(e~t)-~(e~t) ~.y.own(y)(j): e--~t \\[trace-ebsl car car:~--~t that(~.y.own(y)(j)): (e---~t)~(e~t) \\[app\\] that(~.y.own~)(j))(car): e--~t \\[appl  Figure 2 Using the Trace Rules. Computational Linguistics Volume 16, Number 1, March 1990 5 Fernando C. N. Pereira Categoriai Semantics and Seoping phora. The two rules are as follows: (x : pron) x :pron s: .a u : f l \\[pron-lic\\] : x: e \\[pron-abs\\] : (~x .s ) (u ) : ot The pronoun resolution rule \\[pron-abs\\] applies only when u : fl is an undischarged assumption ofs : a such that either fl is trace or quant (q) for some quantifier q, or the assumption is licensed by some proper noun. These rules deal only with the construction of the meaning of phrases containing bound anaphora. In a more detailed grammar, the licensing of both rules would be further estricted by linguistic onstraints on coreference-- for instance, those usually associated with c-command (Reinhart 1983), which seem to need access to syntactic information (Williams 1986). In particular, the rules as given do not by themselves enforce any constraints on the possible antecedents of reflexives. The soundness of the rules can be seen by noting that the schematic derivation x : pron ~ x :e S:o/ U:~ (Xx .s ) (u ) :a t corresponds simply to a schematic derivation involving multiple uses of the assumption u : fl u:~. . .u :# six~u\\] : ,~ where s \\[x/u\\] denotes the result of substituting u for every free occurrence ofx in s.  Figure 3 shows a simple derivation involving the pronoun rules. The last derivation ode in the figure is the application of \\[pron-abs\\] to the assumption to be discharged x : pron and the antecedent assumption j : e, with result (kx.bored (x)( j ) ) ( j ) --- bored (j)( j) . A more interesting case, involving interactions between pronoun and quantifier assumptions, occurs in the derivation of Figure 5 for sentence (2).", "citation": "The notion of syntactic derivation embodied in CCG is the most powerful limitation on the number of available readings, and allows all logical-form level constraints on scope orderings to be dispensed with, a result related to, but more powerful than, that of #REFR.", "context": "The above observations imply that only those socalled quantifiers in English which can engender dependency-inducing scope inversion have interpretations corresponding to genuine quantifiers. The others are not quantificationai tall, but are various types of arbitrary individuals translated as Skolem terms. These give the appearance of taking narrow scope when they are bound to truly quantified variables, and of taking wide scope when they are unbound, and therefore \"take scope everywhere.\" Available readings can be computed monotonically from syntactic derivation alone.[Citation]"}
{"citing_paper_id": "P01-1056", "cited_paper_id": "N01-1003", "citing_paper_abstract": "Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rulebased approaches. In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments. In order to perform an exhaustive comparison, we also evaluate a hand-crafted template-based generation component, two rule-based sentence planners, and two baseline sentence planners. We show that the trainable sentence planner performs better than the rule-based systems and the baselines, and as well as the handcrafted system.", "cited_paper_abstract": "Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences. In this paper, we present SPoT, a sentence planner, and a new methodology for automatically training SPoT on the basis of feedback provided by human judges. We reconceptualize the task into two distinct phases. First, a very simple, randomized sentence-plangenerator (SPG) generates a potentially large list of possible sentence plans for a given text-plan input. Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan. The SPR uses ranking rules automatically learned from training data. We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan.", "citation": "#REFR describe SPOT in detail.", "context": "Over 3,000 features were discovered from the generated trees by routines that encode structural and lexical aspects of the sp-trees and the DSyntS. RankBoost identified the features that contribute most to a realization?s ranking. The SPR uses these rules to rank alternative sp-trees, and then selects the top-ranked output as input to the surface realizer.[Citation]"}
{"citing_paper_id": "W02-1004", "cited_paper_id": "N01-1006", "citing_paper_abstract": "This paper demonstrates the substantial empirical success of classifier combination for the word sense disambiguation task. It investigates more than 10 classifier combination methods, including second order classifier stacking, over 6 major structurally different base classifiers (enhanced Na?ve Bayes, cosine, Bayes Ratio, decision lists, transformationbased learning and maximum variance boosted mixture models). The paper also includes in-depth performance analysis sensitive to properties of the feature space and component classifiers. When evaluated on the standard SENSEVAL1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets.", "cited_paper_abstract": "Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacri cing performance. The paper compares and contrasts the training time needed and performance achieved by our modi ed learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a signi cant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.", "citation": "An electronically available transformation-based POS tagger #REFR was trained on standard labeled data for English (Penn Treebank), Swedish (SUC- 1 corpus), and Basque.", "context": "Lemmatization Part-of-speech tagger availability varied across the languages that are studied here.[Citation]For Spanish, an minimally supervised tagger #OTHEREFR was used. Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods #OTHEREFR for all the other languages."}
{"citing_paper_id": "W03-0433", "cited_paper_id": "N01-1006", "citing_paper_abstract": "This paper investigates stacking and voting methods for combining strong classifiers like boosting, SVM, and TBL, on the named-entity recognition task. We demonstrate several effective approaches, culminating in a model that achieves error rate reductions on the development and test sets of 63.6% and 55.0% (English) and 47.0% and 51.7% (German) over the CoNLL-2003 standard baseline respectively, and 19.7% over a strong AdaBoost baseline model from CoNLL-2002.", "cited_paper_abstract": "Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacri cing performance. The paper compares and contrasts the training time needed and performance achieved by our modi ed learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a signi cant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.", "citation": "The experiments presented in this paper were performed using the fnTBL toolkit #REFR, which implements several optimizations in rule learning to drastically speed up the time needed for training.", "context": "Transformation-based learning #OTHEREFR (TBL) is a rule-based machine learning algorithm that was first introduced by Brill and used for part-of-speech tagging. The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set. An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made.[Citation]"}
{"citing_paper_id": "W04-0845", "cited_paper_id": "N01-1006", "citing_paper_abstract": "This paper describes the HKPolyU-HKUST systems which were entered into the Semantic Role Labeling task in Senseval-3. Results show that these systems, which are based upon common machine learning algorithms, all manage to achieve good performances on the non-restricted Semantic Role Labeling task.", "cited_paper_abstract": "Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacri cing performance. The paper compares and contrasts the training time needed and performance achieved by our modi ed learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a signi cant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.", "citation": "The sentence was first part-of-speech tagged and chunked with the fnTBL transformationbased learning tools #REFR.", "context": "The position (e.g. before, after) of the constituent, with respect to the target word. In addition to the above features, we also extracted a set of features which required the use of some statistical NLP tools: . Transitivity and voice of the target word .[Citation]Simple heuristics were then used to deduce the transitivity voice of the target word. . Head word (and its part-of-speech tag) of the constituent . After POS tagging, a syntactic parser #OTHEREFR was then used to obtain the parse tree for the sentence."}
{"citing_paper_id": "C02-1115", "cited_paper_id": "N01-1011", "citing_paper_abstract": "In this paper, a supervised learning system of word sense disambiguation is presented. It is based on conditional maximum entropy models. This system acquires the linguistic knowledge from an annotated corpus and this knowledge is represented in the form of features. Several types of features have been analyzed using the SENSEVAL-2 data for the Spanish lexical sample task. Such analysis shows that instead of training with the same kind of information for all words, each one is more e ectively learned using a di erent set of features. This bestfeature-selection is used to build some systems based on di erent maximum entropy classi ers, and a voting system helped by a knowledgebased method.", "cited_paper_abstract": "This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.", "citation": "The former approach relies on previously acquired linguistic knowledge, and the latter uses techniques from statistics and machine learning to induce models of language usage from large samples of text #REFR.", "context": "The task of WSD consists in assigning the correct sense to words using an electronic dictionary as the source of word de nitions. This is a hard problem that is receiving a great deal of attention from the research community. Currently, there are two main methodological approaches in this research area: knowledgebased methods and corpus-based methods.[Citation]Learning can be supervised or unsupervised. With supervised This paper has been partially supported by the Spanish Government #OTHEREFR. At SENSEVAL-2, researchers showed the latest contributions to WSD."}
{"citing_paper_id": "H05-1030", "cited_paper_id": "N01-1016", "citing_paper_abstract": "We identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques.", "cited_paper_abstract": "We present a simple architecture for parsing transcribed speech in which an edited-word detector rst removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words. The edit detector achieves a misclassi cation rate on edited words of 2.2%. (The NULL-model, which marks everything as not edited, has an error rate of 5.9%.) To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indi erent to the exact tree position of EDITED nodes. By this metric the parser achieves 85.3% precision and 86.5% recall.", "citation": "Extensions of #REFR look at using quantized combinations of prosodic features as additional ?words?, similar to the use of punctuation in parsing written text #OTHEREFR, but do not find that the prosodic features are useful.", "context": "Since then, parsing has advanced considerably, and the use of statistical parsers makes the candidate pruning benefits of prosody less important. This raises the question of whether prosody is useful for improving parsing accuracy for conversational speech, apart from its use in sentence Figure 2: System architecture boundary detection.[Citation]It may be that with the short ?sentences? in spontaneous speech, sentenceinternal prosody is rarely of use in parsing. However, in edit detection using a parsing model #OTHEREFR are found to be useful. The seeming discrepancy between results could be explained if prosodic cues to IPs are useful but not other sub-sentence prosodic constituents."}
{"citing_paper_id": "N04-4032", "cited_paper_id": "N01-1016", "citing_paper_abstract": "The lack of sentence boundaries and presence of disfluencies pose difficulties for parsing conversational speech. This work investigates the effects of automatically detecting these phenomena on a probabilistic parser?s performance. We demonstrate that a state-of-the-art segmenter, relative to a pause-based segmenter, gives more than 45% of the possible error reduction in parser performance, and that presentation of interruption points to the parser improves performance over using sentence boundaries alone.", "cited_paper_abstract": "We present a simple architecture for parsing transcribed speech in which an edited-word detector rst removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words. The edit detector achieves a misclassi cation rate on edited words of 2.2%. (The NULL-model, which marks everything as not edited, has an error rate of 5.9%.) To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indi erent to the exact tree position of EDITED nodes. By this metric the parser achieves 85.3% precision and 86.5% recall.", "citation": "Work in statistically parsing conversational speech #REFR has examined the performance of a parser that removes edit regions in an earlier step.", "context": "This paper explores the usefulness of identifying boundaries of sentence-like units (referred to as SUs) and IPs in parsing conversational speech. Early work in parsing conversational speech was rulebased and limited in domain #OTHEREFR. Results from another rule-based system #OTHEREFR suggests that standard parsers can be used to identify speech repairs in conversational speech.[Citation]In contrast, we train a parser on the complete (human-specified) segmentation, with edit-regions included. We choose to work with all of the words within edit regions anticipating that making the parallel syntactic structures of the edit region available to the parser can improve its performance in identifying that structure. Our work makes use of the Structured Language Model (SLM) as a parser and an existing SU-IP detection algorithm, described next."}
{"citing_paper_id": "P10-1119", "cited_paper_id": "N01-1021", "citing_paper_abstract": "A number of results in the study of realtime sentence comprehension have been explained by computational models as resulting from the rational use of probabilistic linguistic information. Many times, these hypotheses have been tested in reading by linking predictions about relative word difficulty to word-aggregated eye tracking measures such as go-past time. In this paper, we extend these results by asking to what extent reading is well-modeled as rational behavior at a finer level of analysis, predicting not aggregate measures, but the duration and location of each fixation. We present a new rational model of eye movement control in reading, the central assumption of which is that eye movement decisions are made to obtain noisy visual information as the reader performs Bayesian inference on the identities of the words in the sentence. As a case study, we present two simulations demonstrating that the model gives a rational explanation for between-word regressions.", "cited_paper_abstract": "In human sentence processing, cognitive load can be de ned many ways. This report considers a de nition of cognitive load in terms of the total probability of structural options that have been discon rmed at some point in a sentence: the surprisal of word w i given its pre x w 0...i?1 on a phrase-structural language model. These loads can be e ciently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke?s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.", "citation": "Most of this work has taken as its task predicting the difficulty of each word in a sentence, a major result being that a large component of the difficulty of a word appears to be a function of its probability in context #REFR.", "context": "Sophisticated models have been developed that explain many of these effects using the tools of computational linguistics and large-scale corpora to make normative predictions for optimal performance in these tasks #OTHEREFR. To the extent that the behavior of these models looks like human behavior, it suggests that humans are making rational use of all the information available to them in language processing. In the domain of incremental language comprehension, especially, there is a substantial amount of computational work suggesting that humans behave rationally #OTHEREFR.[Citation]Much of the empirical basis for this work comes from studying reading, where word difficulty can be related to the amount of time that a reader spends on a particular word. To relate these predictions about word difficulty to the data obtained in eye tracking experiments, the eye movement record has been summarized through word aggregate measures, such as the average duration of the first fixation on a word, or the amount of time between when a word is first fixated and when the eyes move to its right (?go-past time?). It is important to note that this notion of word difficulty is an abstraction over the actual task of reading, which is made up of more fine-grained decisions about how long to leave the eyes in their current position, and where to move them next, producing the series of relatively stable periods (fixations) and movements (saccades) that characterize the eye tracking record."}
{"citing_paper_id": "C10-2101", "cited_paper_id": "N03-1002", "citing_paper_abstract": "In this paper, we present a two-stage approach to acquire Japanese unknown morphemes from text with full POS tags assigned to them. We first acquire unknown morphemes only making a morphologylevel distinction, and then apply semantic classification to acquired nouns. One advantage of this approach is that, at the second stage, we can exploit syntactic clues in addition to morphological ones because as a result of the first stage acquisition, we can rely on automatic parsing. Japanese semantic classification poses an interesting challenge: proper nouns need to be distinguished from common nouns. It is because Japanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them. We explore lexico-syntactic clues that are extracted from automatically parsed text and investigate their effects.", "cited_paper_abstract": "Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering. A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking. However, there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting. To cope with the unit problem, we propose a character-based chunking method. Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers. Then, each character is annotated with its character types and its possible POS tags of the top n-best answers. Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs. This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result. We apply our method to IREX NE extraction task. The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method.", "citation": "In fact, POS tags given to pre-defined morphemes are useful for applications of morphological analysis, such as dependency parsing #OTHEREFR, named entity recognition #REFR and anaphora resolution #OTHEREFR.", "context": "The Japanese POS tagset derives from traditional grammar. It is a mixture of several linguistic levels: morphology, syntax and semantics. In other words, information encoded in a POS tag is more than how the morpheme behaves in a sequence of morphemes.[Citation]In these applications, POS tags are incorporated as features for models. On the other hand, the mixed nature of the POS tagset poses a challenge to unknown morpheme acquisition. Previous approaches #OTHEREFR directly or indirectly reply on morphology, or our knowledge on how a morpheme behaves in a sequence of morphemes."}
{"citing_paper_id": "D11-1011", "cited_paper_id": "N03-1011", "citing_paper_abstract": "Class-instance label propagation algorithms have been successfully used to fuse information from multiple sources in order to enrich a set of unlabeled instances with class labels. Yet, nobody has explored the relationships between the instances themselves to enhance an initial set of class-instance pairs. We propose two graph-theoretic methods (centrality and regularization), which start with a small set of labeled class-instance pairs and use the instance-instance network to extend the class labels to all instances in the network. We carry out a comparative study with state-of-the-art knowledge harvesting algorithm and show that our approach can learn additional class labels while maintaining high accuracy. We conduct a comparative study between class-instance and instance-instance graphs used to propagate the class labels and show that the latter one achieves higher accuracy.", "cited_paper_abstract": "The discovery of semantic relations from text becomes increasingly important for applications such as Question Answering, Information Extraction, Text Summarization, Text Understanding, and others. The semantic relations are detected by checking selectional constraints. This paper presents a method and its results for learning semantic constraints to detect part-whole relations. Twenty constraints were found. Their validity was tested on a 10,000 sentence corpus, and the targeted partwhole relations were detected with an accuracy of 83%.", "citation": "Many efforts have also focused on the extraction of is-a relations #OTHEREFR, part-of relations (Gi#REFR and general facts #OTHEREFR.", "context": "In the past decade, we have reached a good understanding on the knowledge harvesting technology from structured #OTHEREFR and unstructured text. Researchers have harvested with varying success semantic lexicons #OTHEREFR.[Citation]Various approaches have been proposed following the patterns of #OTHEREFR. A substantial body of work has explored issues such as reranking the harvested knowledge using mutual information #OTHEREFR. Since pattern-based approaches tend to be highprecision and low-recall in nature, recently of great interest to the research community is the development of approaches that can increment the recall of the harvested class-instance pairs. #OTHEREFR conducted a comparative study of graph algorithms and showed that class-instance extraction can be improved using additional information that can be modeled as instance-attribute edges."}
{"citing_paper_id": "N06-2026", "cited_paper_id": "N03-1014", "citing_paper_abstract": "We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output. We show conclusive results on joint learning and inference of syntactic and semantic representations.", "cited_paper_abstract": "We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.", "citation": "Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees #OTHEREFR; #REFR have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.", "context": "[Citation]Moving towards a shallow semantic level of representation has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to New York will need to know that from Geneva indicates the origin of the flight and to New York the destination. #OTHEREFR define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees. They use learning features such as phrase type, position, voice, and parse tree path."}
{"citing_paper_id": "E14-1039", "cited_paper_id": "N03-1016", "citing_paper_abstract": "We present an algorithm for incremental statistical parsing with Parallel Multiple Context-Free Grammars (PMCFG). This is an extension of the algorithm by Angelov (2009) to which we added statistical ranking. We show that the new algorithm is several times faster than other statistical PMCFG parsing algorithms on real-sized grammars. At the same time the algorithm is more general since it supports non-binarized and non-linear grammars. We also show that if we make the search heuristics non-admissible, the parsing speed improves even further, at the risk of returning sub-optimal solutions.", "cited_paper_abstract": "We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.", "citation": "The estimation is both admissible and monotonic #REFR which guarantees that we always find a tree whose probability is the global maximum.", "context": "By lifting these restrictions, we make it possible to experiment with novel grammar induction methods #OTHEREFR. By extending the algorithm with a statistical model, we allow the parser to explore only parts of the search space, when only the most probable parse tree is needed. Our cost estimation is similar to the estimation for the Viterbi probability as in Stolcke #OTHEREFR, except that we have to take into account that our grammar is not contextfree.[Citation]We also describe a variant with a nonadmissible estimation, which further improves the efficiency of the parser at the risk of returning a suboptimal parse tree. We start with a formal definition of a weighted PMCFG in Section 2, and we continue with a presentation of our algorithm by means of a weighted deduction system in Section 3. In Section 4, we prove that our estimations are admissible and monotonic."}
{"citing_paper_id": "P04-3032", "cited_paper_id": "N03-1016", "citing_paper_abstract": "We present the first version of a new declarative programming language. Dyna has many uses but was designed especially for rapid development of new statistical NLP systems. A Dyna program is a small set of equations, resembling Prolog inference rules, that specify the abstract structure of a dynamic programming algorithm. It compiles into efficient, portable, C++ classes that can be easily invoked from a larger application. By default, these classes run a generalization of agendabased parsing, prioritizing the partial parses by some figure of merit. The classes can also perform an exact backward (outside) pass in the service of parameter training. The compiler already knows several implementation tricks, algorithmic transforms, and numerical optimization techniques. It will acquire more over time: we intend for it to generalize and encapsulate best practices, and serve as a testbed for new practices. Dyna is now being used for parsing, machine translation, morphological analysis, grammar induction, and finite-state modeling.", "cited_paper_abstract": "We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.", "citation": "Dyna will soon allow user-defined priority functions #OTHEREFR; #REFR.", "context": "Although Dyna supports stack and queue (LIFO and FIFO) disciplines, its default is to use a priority queue prioritized by the size of the update. When parsing with real values, this quickly accumulates a good approximation of the inside probabilities, which permits heuristic early stopping before the agenda is empty. With viterbi values, it amounts to uniform-cost search for the best parse, and an item?s value is guaranteed not to change once it is nonzero.[Citation]"}
{"citing_paper_id": "C08-1127", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Bracketing Transduction Grammar (BTG) is a natural choice for effective integration of desired linguistic knowledge into statistical machine translation (SMT). In this paper, we propose a Linguistically Annotated BTG (LABTG) for SMT. It conveys linguistic knowledge of source-side syntax structures to BTG hierarchical structures through linguistic annotation. From the linguistically annotated data, we learn annotated BTG rules and train linguistically motivated phrase translation model and reordering model. We also present an annotation algorithm that captures syntactic information for BTG nodes. The experiments show that the LABTG approach significantly outperforms a baseline BTG- based system and a state-of-the-art phrasebased system on the NISTMT-05 Chineseto-English translation task. Moreover, we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "With these linguistic annotations, we expect the LABTG to address two traditional issues of standard phrase-based SMT #REFR in a more effective manner.", "context": "Along this line, we propose a novel approach: Linguistically Annotated BTG (LABTG) for SMT. The LABTG annotates BTG rules with linguistic elements that are learned from syntactic parse trees on the source side through an annotation algorithm, which is capable of labelling both syntactic and non-syntactic phrases. The linguistic elements extracted from parse trees capture both internal lexical content and external context of phrases.[Citation]They are (1) phrase translation: translating phrases according to their contexts; (2) phrase reordering: incorporating richer linguistic features for better reordering. The proposed LABTG displays two unique characteristics when compared with BTG-based SMT #OTHEREFR. The first is that two linguistically-informed sub-models are introduced for better phrase translation and reordering: annotated phrase translation model and annotated reordering model."}
{"citing_paper_id": "C10-1126", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Syntax based reordering has been shown to be an effective way of handling word order differences between source and target languages in Statistical Machine Translation (SMT) systems. We present a simple, automatic method to learn rules that reorder source sentences to more closely match the target language word order using only a source side parse tree and automatically generated alignments. The resulting rules are applied to source language inputs as a pre-processing step and demonstrate significant improvements in SMT systems across a variety of languages pairs including English to Hindi, English to Spanish and English to French as measured on a variety of internal test sets as well as a public test set.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "Early distortion models simply penalized longer jumps more than shorter jumps #REFR independent of the source or target phrases in question.", "context": "But, if our systems are to generalize to phrases not seen in the training data, they must explicitly capture and model these reorderings. However, permutations are difficult to model and impractical to search. Presently, approaches that handle reorderings typically model word and phrase movements via a distortion model and rely on the target language model to produce words in the right order.[Citation]Other models #OTHEREFR generalize this to include lexical dependencies on the source. Another approach is to incorporate features, based on the target syntax, during modeling and decoding, and this is shown to be effective for various language pairs #OTHEREFR. Hierarchical phrase-based decoding #OTHEREFR also allows for long range reordering without explicitly modeling syntax."}
{"citing_paper_id": "D07-1105", "cited_paper_id": "N03-1017", "citing_paper_abstract": "This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination. We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneficial for system combination. Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the bestranked machine translation engines in the", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "For instance, word alignment models are often trained using the GIZA++ toolkit #OTHEREFR are employed in order to learn feature function weights for log-linear models; and translation candidates are produced using phrase-based decoders #REFR in combination with n-gram language models #OTHEREFR.", "context": "In particular, we will address the following questions: . To what extent can translation quality benefit from combining systems developed by multiple research labs. Despite an increasing number of translation engines, most state-of-the-art systems in statistical machine translation are nowadays based on implementations of the same techniques.[Citation]All these methods are established as de facto standards and form an integral part of most statistical machine translation systems. This, however, raises the question as to what extent translation quality can be expected to improve when similarly designed systems are combined. . How can a set of diverse translation systems be built from a single translation engine."}
{"citing_paper_id": "D07-1105", "cited_paper_id": "N03-1017", "citing_paper_abstract": "This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination. We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneficial for system combination. Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the bestranked machine translation engines in the", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "As pointed out in Section 2.1, a useful property of the MBR-like system selection method is that system prior weights can easily be trained using the Minimum Error Rate Training #REFR.", "context": "[Citation]In this section, we investigate the effect of using non-uniform system weights for the combination of multiple research systems. Since for each research system, only the first best translation candidate was provided, we used a five-fold cross validation scheme in order to train and evaluate the system prior weights. For this purpose, all research systems were consistently split into five random partitions of almost equal size."}
{"citing_paper_id": "D08-1021", "cited_paper_id": "N03-1017", "citing_paper_abstract": "We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "Automatic word alignments were created for these using Giza++ #REFR.", "context": "Our paraphrase model was trained using the Europarl corpus #OTHEREFR. We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words.[Citation]The English side of each parallel corpus was parsed using the Bikel parser #OTHEREFR. A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit #OTHEREFR."}
{"citing_paper_id": "D13-1141", "cited_paper_id": "N03-1017", "citing_paper_abstract": "We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "We use Minimum Error Rate Training (MERT) #REFR to tune the decoder.", "context": "Our experiments are performed using the Stanford Phrasal phrase-based machine translation system #OTHEREFR. In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results.[Citation]In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; if no word is found in a phrase, a zero vector is assigned 5This is evaluated on 10,000 randomly selected sentence pairs from the MT training set."}
{"citing_paper_id": "D14-1176", "cited_paper_id": "N03-1017", "citing_paper_abstract": "This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs). Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm. The approach is evaluated in a series of Arabic- English and Chinese-English translation experiments. The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by Niehues et al. (2011). Further improvements of up to 0.45 BLEU for Arabic- English and up to 0.59 BLEU for Chinese- English are obtained by combining our dependency BiLM with a lexicalized BiLM. An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "The simplest reordering model is linear distortion #REFR which scores the distance between phrases translated at steps t and t + 1 of the derivation.", "context": "Determining the correct reordering during decoding is a major challenge for SMT. This problem has received a lot of attention in the literature #OTHEREFR. In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding.[Citation]This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeling #OTHEREFR conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous."}
{"citing_paper_id": "P04-1023", "cited_paper_id": "N03-1017", "citing_paper_abstract": "The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora. We show that significant improvements in the alignment and translation quality of such models can be achieved by additionally including wordaligned data during training. Incorporating wordlevel alignments into the parameter estimation of the IBM models reduces alignment error rate and increases the Bleu score when compared to training the same models only on sentence-aligned data. On the Verbmobil data set, we attain a 38% reduction in the alignment error rate and a higher Bleu score with half as many training examples. We discuss how varying the ratio of word-aligned to sentencealigned data affects the expected performance gain.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "To perform our experiments with word-level alignements we modified GIZA++, an existing and freely available implementation of the IBM models and HMM variants #REFR.", "context": "[Citation]Our modifications involved circumventing the E-step for sentences which had word-level alignments and incorporating these observed alignment statistics in the M-step. The observed and expected statistics were weighted accordingly by ? and (1? ?) respectively as were their contributions to the mixed log likelihood. In order to measure the accuracy of the predictions that the statistical translation models make under our various experimental settings, we choose the alignment error rate #OTHEREFR."}
{"citing_paper_id": "P08-1009", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Phrase-based decoding produces state-of-theart translations with no regard for syntax. We add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. The constraint allows the decoder to employ arbitrary, non-syntactic phrases, but ensures that those phrases are translated in an order that respects the source tree?s structure. In this way, we target the phrasal decoder?s weakness in order modeling, without affecting its strengths. To further increase flexibility, we incorporate cohesion as a decoder feature, creating a soft constraint. The resulting cohesive, phrase-based decoder is shown to produce translations that are preferred over non-cohesive output in both automatic and human evaluations.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "Word alignments are provided by GIZA++ #REFR with grow-diag-final combination, with infrastructure for alignment combination and phrase extraction provided by the shared task.", "context": "We test our cohesion-enhanced Moses decoder trained using 688K sentence pairs of Europarl French-English data, provided by the SMT 2006 Shared Task #OTHEREFR.[Citation]We decode withMoses, using a stack size of 100, a beam threshold of 0.03 and a distortion limit of 4. Weights for the log-linear model are set using MERT, as implemented by Venugopal and Vogel #OTHEREFR. Our tuning set is the first 500 sentences of the SMT06 development data."}
{"citing_paper_id": "P09-2058", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Combining word alignments trained in two translation directions has mostly relied on heuristics that are not directly motivated by intended applications. We propose a novel method that performs combination as an optimization process. Our algorithm explicitly maximizes the effectiveness function with greedy search for phrase table training or synchronized grammar extraction. Experimental results show that the proposed method leads to significantly better translation quality than existing methods. Analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "We tune all feature weights automatically #REFR to maximize the BLEU #OTHEREFR score on the dev set. ", "context": "They are holdout from training data as tuning and testing. The test set 2 is the standard NIST offline evaluation set, where 4 references are available for each sentence. The dev and test set 1 are much closer to the training set than the standard test set 2.[Citation]Table 2 shows BLEU score of different combination methods on all three sets. Union performs much worse on the dev and test1 than intersection, while intersection achieved the same performance on test2 as union but with more than 6 times of phrase table size. Grow-diagonal (GD) has more than 1 bleu point on test2 than intersection but with less than half of phrase table size."}
{"citing_paper_id": "P10-1049", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "A phrase-based SMT system takes a source sentence and produces a translation by segmenting the sentence into phrases and translating those phrases separately #REFR.", "context": "[Citation]The phrase translation table, which contains the bilingual phrase pairs and the corresponding translation probabilities, is one of the main components of an SMT system. The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data #OTHEREFR. In this method, all phrases of the sentence pair that match constraints given by the alignment are extracted."}
{"citing_paper_id": "P10-2067", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, marginand query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "We first automatically align the Cn-En corpus using GIZA++ #REFR.", "context": "[Citation]We then use the learned model in running our link selection algorithm over the entire corpus to determine the most uncertain links according to each active learning strategy. The links are then looked up in the gold-standard human alignment database and corrected. In case a link is not present in the gold-standard data, we introduce a NULL alignment, else we propose the alignment as given in"}
{"citing_paper_id": "P11-1065", "cited_paper_id": "N03-1017", "citing_paper_abstract": "While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "Interestingly, early on #REFR exemplified the difficulties of integrating linguistic information in translation systems.", "context": "Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side #OTHEREFR. Hierarchical translation was combined with target side linguistic annotation in #OTHEREFR.[Citation]Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training #OTHEREFR, obscuring the impact of higher level syntactic processes. While it is assumed that linguistic structure does correlate with some translation phenomena, in this work we do not employ it as the backbone of translation."}
{"citing_paper_id": "P14-1064", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates. In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data. The proposed technique first constructs phrase graphs using both source and target language monolingual corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "With large amounts of data, phrase-based translation systems #REFR achieve state-of-the-art results in many typologically diverse language pairs #OTHEREFR.", "context": "Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities.[Citation]However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial . This work was done while the first author was interning at Microsoft Research for effective translation."}
{"citing_paper_id": "P14-1110", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Topic models, an unsupervised technique for inferring translation domains improve machine translation quality. However, previous work uses only the source language and completely ignores the target language, which can disambiguate domains. We propose new polylingual tree-based topic models to extract domain knowledge that considers both source and target languages and derive three different inference schemes. We evaluate our model on a Chinese to English translation task and obtain up to 1.2 BLEU improvement over strong baselines.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "We replicate the SMT pipeline of Eidelman et al #OTHEREFR, and symmetrize #REFR the data.", "context": "We evaluate our new topic model, ptLDA, and existing topic models?LDA, pLDA, and tLDA?on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens.[Citation]We train a modified Kneser- Ney trigram language model on English #OTHEREFR. We use CDEC #OTHEREFR for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05."}
{"citing_paper_id": "W05-0836", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Decision rules that explicitly account for non-probabilistic evaluation metrics in machine translation typically require special training, often to estimate parameters in exponential models that govern the search space and the selection of candidate translations. While the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise linear function in a greedy search of the parameter space, the Minimum Bayes Risk (MBR) decision rule is not well suited to this technique, a condition that makes past results difficult to compare. We present a novel training approach for non-tractable decision rules, allowing us to compare and evaluate these and other decision rules on a large scale translation task, taking advantage of the high dimensional parameter space available to the phrase based Pharaoh decoder. This comparison is timely, and important, as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "The first system is the Pharaoh decoder provided by #REFR for the shared data task.", "context": "We also use the English language model as provided for the shared task. Since each of these decision rules has its respective training process, we split the workshop test set of 2000 sentences into a development and test set using random splitting. We tried two decoders for translating these sets.[Citation]The Pharaoh decoder has support for multiple translation and language model scores as well as simple phrase distortion and word length models. The pruning and distortion limit parameters remain the same as in the provided initialization scripts, i.e., DistortionLimit = 4, BeamThreshold = 0.1, Stack = 100. For further information on these parameter settings, confer #OTHEREFR."}
{"citing_paper_id": "W06-1606", "cited_paper_id": "N03-1017", "citing_paper_abstract": "We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "The decoder is capable of producing nbest derivations and nbest lists #OTHEREFR, which are used for Maximum Bleu training #REFR.", "context": "We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences. The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure #OTHEREFR. The CKY-style decoder computes the probability of English syntactic constituents in a bottom up fashion, by log-linearly interpolating all the submodel scores described in Section 2.3.[Citation]When decoding the test corpus, the decoder returns the translation that has the most probable derivation; in other words, the sum operator in equation 4 is replaced with an argmax."}
{"citing_paper_id": "W06-1608", "cited_paper_id": "N03-1017", "citing_paper_abstract": "We investigate the impact of parse quality on a syntactically-informed statistical machine translation system applied to technical text. We vary parse quality by varying the amount of data used to train the parser. As the amount of data increases, parse quality improves, leading to improvements in machine translation output and results that significantly outperform a state-of-the-art phrasal baseline.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "It has been shown that phrasal machine translation systems are not affected by the quality of the input word alignments #REFR.", "context": "The current study is a response to a question that proponents of syntactically-informed machine translation frequently encounter: How sensitive is a syntactically-informed machine translation system to the quality of the input syntactic analysis.[Citation]This finding has generally been cast in favorable terms: such systems are robust to poor quality word alignment. A less favorable interpretation of these results might be to conclude that phrasal statistical machine translation (SMT) systems do not stand to benefit from improvements in word alignment. In a similar vein, one might ask whether contemporary syntactically-informed machine translation systems would benefit from improvements in parse accuracy."}
{"citing_paper_id": "W08-0301", "cited_paper_id": "N03-1017", "citing_paper_abstract": "The treatment of ?spurious? words of source language is an important problem but often ignored in the discussion on phrase-based SMT. This paper explains why it is important and why it is not a trivial problem, and proposes three models to handle spurious source words. Experiments show that any source word deletion model can improve a phrase-based system by at least 1.6 BLEU points and the most sophisticated model improves by nearly 2 BLEU points. This paper also explores the impact of training data size and training data domain/genre on source word deletion.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "A series of experiments were run to compare the performance of the three SWD models against the baseline, which is the standard phrase-based approach to SMT as elaborated in #REFR.", "context": "[Citation]The experiments are about Chinese-to- English translation. The bilingual training data is the one for NIST MT-2006. The GIGAWORD corpus is used for training language model."}
{"citing_paper_id": "W08-0303", "cited_paper_id": "N03-1017", "citing_paper_abstract": "In this paper a new discriminative word alignment method is presented. This approach models directly the alignment matrix by a conditional random field (CRF) and so no restrictions to the alignments have to be made. Furthermore, it is easy to add features and so all available information can be used. Since the structure of the CRFs can get complex, the inference can only be done approximately and the standard algorithms had to be adapted. In addition, different methods to train the model have been developed. Using this approach the alignment quality could be improved by up to 23 percent for 3 different language pairs compared to a combination of both IBM4- alignments. Furthermore the word alignment was used to generate new phrase tables. These could improve the translation quality significantly.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "This data consists of 1.1 million sentences, a validation set of 37 sentences and a test set of 447 sentences, which have been hand-aligned #REFR.", "context": "To limit the number of different tags for Spanish we grouped them according to the first 2 characters in the tag names. A second group of experiments was done on an English-French text. The data from the 2003 NAACL shared task #OTHEREFR was used.[Citation]For the English POS-tags again the Brill Tagger was used. For the French side, the TreeTagger #OTHEREFR was used. Finally, to test our alignment approach with languages that differ more in structure a Chinese- English task was selected."}
{"citing_paper_id": "W09-0424", "cited_paper_id": "N03-1017", "citing_paper_abstract": "We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "The toolkit also implements suffix-array grammar extraction #OTHEREFR and minimum error rate training #REFR.", "context": "This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare. In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses #OTHEREFR does for regular phrase-based machine translation. Our toolkit is written in Java and implements all the essential algorithms described in Chiang #OTHEREFR: chart-parsing, n-gram language model integration, beamand cube-pruning, and k-best extraction.[Citation]Additionally, parallel and distributed computing techniques are exploited to make it scalable #OTHEREFRb). We have also made great effort to ensure that our toolkit is easy to use and to extend. The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments #OTHEREFRa)."}
{"citing_paper_id": "W10-2916", "cited_paper_id": "N03-1017", "citing_paper_abstract": "The availability of substantial, in-domain parallel corpora is critical for the development of high-performance statistical machine translation (SMT) systems. Such corpora, however, are expensive to produce due to the labor intensive nature of manual translation. We propose to alleviate this problem with a novel, semisupervised, batch-mode active learning strategy that attempts to maximize indomain coverage by selecting sentences, which represent a balance between domain match, translation difficulty, and batch diversity. Simulation experiments on an English-to-Pashto translation task show that the proposed strategy not only outperforms the random selection baseline, but also traditional active learning techniques based on dissimilarity to existing training data. Our approach achieves a relative improvement of 45.9% in BLEU over the seed baseline, while the closest competitor gained only 24.8% with the same number of selected sentences.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "We also train an initial phrase-based SMT system #REFR with the available seed corpus.", "context": "Figure 1 illustrates the proposed active learning architecture in the form of a high-level flowdiagram. We begin by randomly sampling a small fraction of the large monolingual pool P to create a pool training set PT, which is used to train the learner. The remainder, which we call the pool evaluation set PE, is set aside for active selection.[Citation]The pool training set PT, in conjunction with the seed corpus S, initial SMT system, and heldout development set D, is used to derive a number of input features as well as target labels for training two parallel classifiers."}
{"citing_paper_id": "W10-3812", "cited_paper_id": "N03-1017", "citing_paper_abstract": "A major challenge in statistical machine translation is mitigating the word order differences between source and target strings. While reordering and lexical translation choices are often conducted in tandem, source string permutation prior to translation is attractive for studying reordering using hierarchical and syntactic structure. This work contributes an approach for learning source string permutation via transfer of the source syntax tree. We present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for Englishto-Dutch source string permutation under sequence and parse tree constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "Word alignment was estimated with GIZA++ tool2 #REFR, coupled with mkcls3 #OTHEREFR, which allows for statistical word clustering for better generalization.", "context": "The SMT system used in the experiments was implemented within the open-source MOSES toolkit #OTHEREFR. Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page1. The MSD model was used together with a distance-based reordering model.[Citation]An 5-gram target language model was estimated using the SRI LM toolkit #OTHEREFR and smoothed with modified Kneser-Ney discounting. We use the Stanford parser4 #OTHEREFR as a source-side parsing engine. The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags."}
{"citing_paper_id": "W13-2702", "cited_paper_id": "N03-1017", "citing_paper_abstract": "Language transformation can be defined as translating between diachronically distinct language variants. We investigate the transformation of Middle Dutch into Modern Dutch by means of machine translation. We demonstrate that by using character overlap the performance of the machine translation process can be improved for this task.", "cited_paper_abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation": "The GIZA++ statistical alignment package #REFR is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline to build the language transformation model.", "context": "P (F |E)P (E) In phrase-based machine translation the sentence F is segmented into a sequence of I phrases during decoding. Each source phrase is then translated into a target phrase to form sentence E. Phrases may be reordered.[Citation]GIZA++ implements IBM Models 1 to 5 and an HMM word alignment model to find statistically motivated alignments between words. We first tokenize our data. We then lowercase all data and use all sentences from the Modern Dutch part of the corpus to train an n-gram language model with the SRILM toolkit #OTHEREFR."}
{"citing_paper_id": "W07-0702", "cited_paper_id": "N03-1019", "citing_paper_abstract": "Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.", "cited_paper_abstract": "We present a derivation of the alignment template model for statistical machine translation and an implementation of the model using weighted finite state transducers. The approach we describe allows us to implement each constituent distribution of the model as a weighted finite state transducer or acceptor. We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers. One of the benefits of using this framework is that it obviates the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses. We evaluate the implementation of the model on the Frenchto-English Hansards task and report alignment and translation performance.", "citation": "However, these models, which are equivalent to finite-state machines #REFR, are unable to model long range word order differences.", "context": "In large-scale machine translation evaluations, phrase-based models generally outperform syntaxbased models1. Phrase-based models are effective because they capture the lexical dependencies between languages.[Citation]Phrase-based models also lack the ability to incorporate the generalisations implicit in syntactic knowledge and they do not respect linguistic phrase boundaries. This makes it difficult to improve reordering in phrase-based models. Syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation."}
{"citing_paper_id": "D07-1064", "cited_paper_id": "N03-1028", "citing_paper_abstract": "We propose a sequence-alignment based method for detecting and disambiguating coordinate conjunctions. In this method, averaged perceptron learning is used to adapt the substitution matrix to the training data drawn from the target language and domain. To reduce the cost of training data construction, our method accepts training examples in which complete word-by-word alignment labels are missing, but instead only the boundaries of coordinated conjuncts are marked. We report promising empirical results in detecting and disambiguating coordinated noun phrases in the GENIA corpus, despite a relatively small number of training examples and minimal features are employed.", "cited_paper_abstract": "Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.", "citation": "tional random fields #OTHEREFR; #REFR, seeks for a complete path from the initial vertex to the terminal using the Viterbi algorithm.", "context": "[Citation]In an edit graph, on the other hand, coordinations are represented by partial paths. And we somehow need to complement the partial path to make a complete path."}
{"citing_paper_id": "W14-4921", "cited_paper_id": "N03-1030", "citing_paper_abstract": "This paper presents an annotation scheme for a new semantic annotation task with relevance for analysis and computation at both the clause level and the discourse level. More specifically, we label the finite clauses of texts with the type of situation entity (e.g., eventualities, statements about kinds, or statements of belief) they introduce to the discourse, following and extending work by Smith (2003). We take a feature-driven approach to annotation, with the result that each clause is also annotated with fundamental aspectual class, whether the main NP referent is specific or generic, and whether the situation evoked is episodic or habitual. This annotation is performed (so far) on three sections of the MASC corpus, with each clause labeled by at least two annotators. In this paper we present the annotation scheme, statistics of the corpus in its current version, and analyses of both inter-annotator agreement and intra-annotator consistency.", "cited_paper_abstract": "We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.", "citation": "We segment the texts into finite clauses using the SPADE discourse parser #REFR, applying some heuristic post-processing and allowing annotators to mark segments that do not contain a situation (for instance, headlines or by-lines) or that should be merged with another segment in order to describe a complete situation.", "context": "[Citation]We filter out all segments marked by any annotator as having a segmentation problem. Of the 2823 segments automatically created for the news section, 4% were marked as containing no situation by at least one of the three annotators, and 7% were merged to a different segment by at least one annotator. All three annotators agree on the remaining 2515 segments (89%)."}
{"citing_paper_id": "H05-1058", "cited_paper_id": "N03-1033", "citing_paper_abstract": "We present a part-of-speech tagger which introduces two new concepts: virtual evidence in the form of an ?observed child? node, and negative training data to learn the conditional probabilities for the observed child. Associated with each word is a flexible feature-set which can include binary flags, neighboring words, etc. The conditional probability of Tag given Word + Features is implemented using a factored language-model with back-off to avoid data sparsity problems. This model remains within the framework of Dynamic Bayesian Networks (DBNs) and is conditionally-structured, but resolves the label bias problem inherent in the conditional Markov model (CMM).", "cited_paper_abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.", "citation": "The token-level accuracy result obtained from the OCM-II model on the development set (96.64%) can be directly compared to an accuracy of 96.57% reported in #REFR for a cyclic dependency network using similar word features and the same three tag context.", "context": "The two methods of generating negative training data yield nearly identical results, showing that they are comparable. Comparing rows 2 and 3 in the table we see that the computed-counts method is relatively insensitive to the value of n (for n ? 1). OCM-II, which uses the adjacent words as features for both known and unknown words further improves overall accuracy, and produces state-ofthe-art results.[Citation]"}
{"citing_paper_id": "I08-5013", "cited_paper_id": "N03-1033", "citing_paper_abstract": "Much work has already been done on building named entity recognition systems. However most of this work has been concentrated on English and other European languages. Hence, building a named entity recognition (NER) system for South Asian Languages (SAL) is still an open problem because they exhibit characteristics different from English. This paper builds a named entity recognizer which also identifies nested name entities for the Hindi language using machine learning algorithm, trained on an annotated corpus. However, the algorithm is designed in such a manner that it can easily be ported to other South Asian Languages provided the necessary NLP tools like POS tagger and chunker are available for that language. I compare results of Hindi data with English data of CONLL shared task of 2003.", "cited_paper_abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.", "citation": "To make it more even, I ran Stanford POS tagger #REFR on the same mono case CONLL 2003 data and then train the model using only word and POS context.", "context": "The reason is English already has tools like POS tagger and chunker which achieves an F measure around 95 whereas for Hindi we only have an F-measure of 85 for tagger and 80 for chunker. This is the reason why the accuracy of English system didn?t fall when I removed capitalization and introduced common noun phenomena since POS context and chunk context helps a lot. Since CONLL 2003 data is already POS tagged and chunked, hence POS and chunks correspond to capitalized data.[Citation]The numbers drop on test set by more than 15% as shown in Table 7. For development set the overall F-measure is around 74%. Entity Precision Recall F-measure Person 66.97 53.93 59.75 Location 68.57 56.54 61.98 Organization 71.64 53.55 61.29 Misc."}
{"citing_paper_id": "W05-0708", "cited_paper_id": "N03-1033", "citing_paper_abstract": "Natural language processing technology for the dialects of Arabic is still in its infancy, due to the problem of obtaining large amounts of text data for spoken Arabic. In this paper we describe the development of a part-of-speech (POS) tagger for Egyptian Colloquial Arabic. We adopt a minimally supervised approach that only requires raw text data from several varieties of Arabic and a morphological analyzer for Modern Standard Arabic. No dialect-specific tools are used. We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic.", "cited_paper_abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.", "citation": "Many approaches for POS tagging have been developed in the past, including rule-based tagging #OTHEREFR, cyclic dependency networks #REFR, memory-based learning #OTHEREFR, etc.", "context": "Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking #OTHEREFR.[Citation]All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic #OTHEREFR. Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task."}
{"citing_paper_id": "D09-1118", "cited_paper_id": "N03-2012", "citing_paper_abstract": "We describe a process for automatically detecting decision-making sub-dialogues in multi-party, human-human meetings in real-time. Our basic approach to decision detection involves distinguishing between different utterance types based on the roles that they play in the formulation of a decision. In this paper, we describe how this approach can be implemented in real-time, and show that the resulting system?s performance compares well with other detectors, including an off-line version.", "cited_paper_abstract": "To support summarization of automatically transcribed meetings, we introduce a classifier to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues. We show that hand-labeling efforts can be minimized by using unsupervised training on a large unlabeled data set combined with supervised training on a small amount of data. For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%.", "citation": "This recent research has tackled issues such as the automatic detection of agreement and disagreement #REFR, and of the level of involvement of conversational participants #OTHEREFR.", "context": "Decisions are one of the most important meeting outputs. User studies #OTHEREFR found that the development of an automatic decision detection component is critical to the re-use of meeting archives. As a result, with the new availability of substantial meeting corpora such as the ISL #OTHEREFR Meeting Corpora, recent years have seen an increasing amount of research on decision-making dialogue.[Citation]In addition, Verbree et al #OTHEREFR created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only very recent research has specifically investigated the automatic detection of decisions, namely #OTHEREFR. Hsueh and Moore #OTHEREFR used the AMI Meeting Corpus, and attempted to automatically identify dialogue acts (DAs) in meeting transcripts which are ?decision-related?."}
{"citing_paper_id": "C04-1093", "cited_paper_id": "N03-2037", "citing_paper_abstract": "We are developing an automatic method to compile an encyclopedic corpus from the Web. In our previous work, paragraph-style descriptions for a term are extracted from Web pages and organized based on domains. However, these descriptions are independent and do not comprise a condensed text as in hand-crafted encyclopedias. To resolve this problem, we propose a summarization method, which produces a single text from multiple descriptions. The resultant summary concisely describes a term from different viewpoints. We also show the effectiveness of our method by means of experiments.", "cited_paper_abstract": "This paper describes an initial evaluation of systems that answer questions seeking definitions. The results suggest that humans agree sufficiently as to what the basic concepts that should be included in the definition of a particular subject are to permit the computation of concept recall. Computing concept precision is more problematic, however. Using the length in characters of a definition is a crude approximation to concept precision that is nonetheless sufficient to correlate with humans? subjective assessment of definition quality. The TREC question answering track has sponsored a series of evaluations of systems? abilities to answer closed class questions in many domains (Voorhees, 2001). Closed class questions are fact-based, short answer questions. The evaluation of QA systems for closed class questions is relatively simple because a response to such a question can be meaningfully judged on a binary scale of right/wrong. Increasing the complexity of the question type even slightly significantly increases the difficulty of the evaluation because partial credit for responses must then be accommodated. The ARDA AQUAINT1 program is a research initiative sponsored by the U.S. Department of Defense aimed at increasing the kinds and difficulty of the questions automatic systems can answer. A series of pilot evaluations has been planned as part of the research agenda of the AQUAINT program. The purpose of each pilot is to develop an effective evaluation methodology for systems that answer a certain kind of question. One of the first pilots to be implemented was the Definitions Pilot, a pilot to develop an evaluation methodology for questions such as What is mold? and Who is Colin Powell?. 1See http:///www.ic-arda.org/InfoExploit/ aquaint/index.html. This paper presents the results of the pilot evaluation. The pilot demonstrated that human assessors generally agree on the concepts that should appear in the definition for a particular subject, and can find those concepts in the systems? responses. Such judgments support the computation of concept recall, but do not support concept precision since it is not feasible to enumerate all concepts contained within a system response. Instead, the length of a response is used to approximate concept precision. An F-measure score combining concept recall and length is used as the final metric for a response. Systems ranked by average F score correlate well with assessors? subjective opinions as to definition quality.", "citation": "For example, in TREC QA track, definition questions are intended to provide a user with the definition of a target item or person #REFR.", "context": "We intend to alleviate both problems. To the best of our knowledge, no attempt has been made to intend similar purposes. Our research is related to question answering (QA).[Citation]However, while the expected answer for a TREC question is short definition sentences as in a dictionary, we intend to produce an encyclopedic text describing a target term from multiple viewpoints. The summarization method proposed in this paper is related to multi-document summarization #OTHEREFR. The novelty of our research is that we applied MDS to producing a condensed term description from unorganized Web pages, while existing MDS methods used newspaper articles to produce an outline of an event and a biography of a specific person."}
{"citing_paper_id": "W06-1615", "cited_paper_id": "N04-1001", "citing_paper_abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.", "cited_paper_abstract": "Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages.", "citation": "We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from #REFR.", "context": "SCL is a general technique that can be applied to any feature-based discriminative learner. We showed results using SCL to transfer a PoS tagger from the Wall Street Journal to a corpus of MEDLINE abstracts. SCL consistently outperformed both supervised and semi-supervised learning with no labeled target domain training data.[Citation]Finally, we improved parsing performance in the target domain when using the SCL PoS tagger. One of our next goals is to apply SCL directly to parsing. We are also focusing on other potential applications, including chunking #OTHEREFRb; Daume."}
{"citing_paper_id": "H05-1117", "cited_paper_id": "N04-1007", "citing_paper_abstract": "Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called POURPRE, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system?s response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that POURPRE outperforms direct application of existing metrics.", "cited_paper_abstract": "Definition questions represent a largely unexplored area of question answering?they are different from factoid questions in that the goal is to return as many relevant ?nuggets? of information about a concept as possible. We describe a multi-strategy approach to answering such questions using a database constructed offline with surface patterns, a Webbased dictionary, and an off-the-shelf document retriever. Results are presented from component-level evaluation and from an endto-end evaluation of our implemented system at the TREC 2003 Question Answering Track.", "citation": "Since then, the basic method for scoring translation quality has been improved upon by others, e.g., #OTHEREFR; #REFR.", "context": "The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the BLEU metric for machine translation #OTHEREFR.[Citation]The basic idea has been extended to evaluating document summarization with ROUGE #OTHEREFR. Recently, Soricut and Brill #OTHEREFR employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al #OTHEREFR applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variation of document summarization."}
{"citing_paper_id": "I08-1054", "cited_paper_id": "N04-1007", "citing_paper_abstract": "A lightweight extraction method derives text snippets associated to dates from the Web. The snippets are organized dynamically into answers to definition questions. Experiments on standard test question sets show that temporally-anchored text snippets allow for efficiently answering definition questions at accuracy levels comparable to the best systems, without any need for complex lexical resources, or specialized processing modules dedicated to finding definitions.", "cited_paper_abstract": "Definition questions represent a largely unexplored area of question answering?they are different from factoid questions in that the goal is to return as many relevant ?nuggets? of information about a concept as possible. We describe a multi-strategy approach to answering such questions using a database constructed offline with surface patterns, a Webbased dictionary, and an off-the-shelf document retriever. Results are presented from component-level evaluation and from an endto-end evaluation of our implemented system at the TREC 2003 Question Answering Track.", "citation": "As such questions have a less irregular form than other open-domain questions, recognizing their type is relatively easier #REFR.", "context": "Seeking information about an entity or a concept, questions such as ?Who is Caetano Veloso?? offer little guidance as to what particular techniques could be used in order to return relevant information from a large text collection. In fact, the same user may choose to submit a definition question or a simpler exploratory query (Caetano Veloso), and still look for text snippets capturing relevant properties of the question concept. Various studies #OTHEREFR illustrate the challenges introduced by definition questions.[Citation]Conversely, the identification of relevant documents and the extraction of answers to definition questions are more laborious, and the impact on the architecture of QA systems is quite significant. Indeed, separate, dedicated modules, or even end-to-end systems are specifically built for answering definition questions #OTHEREFR. The importance of definition questions among other question categories is confirmed by their inclusion among the evaluation queries from the QA track of TREC evaluations #OTHEREFR."}
{"citing_paper_id": "C08-1024", "cited_paper_id": "N04-1010", "citing_paper_abstract": "This paper presents a method for mining potential troubles or obstacles related to the use of a given object. Some example instances of this relation are ?medicine, side effect? and ?amusement park, height restriction?. Our acquisition method consists of three steps. First, we use an unsupervised method to collect training samples from Web documents. Second, a set of expressions generally referring to troubles is acquired by a supervised learning method. Finally, the acquired troubles are associated with objects so that each of the resulting pairs consists of an object and a trouble or obstacle in using that object. To show the effectiveness of our method we conducted experiments using a large collection of Japanese Web documents for acquisition. Experimental results show an 85.5% precision for the top 10,000 acquired troubles, and a 74% precision for the top 10% of over 60,000 acquired object-trouble pairs.", "cited_paper_abstract": "This paper describes an automatic method for acquiring hyponymy relations from HTML documents on the WWW. Hyponymy relations can play a crucial role in various natural language processing systems. Most existing acquisition methods for hyponymy relations rely on particular linguistic patterns, such as ?NP such as NP?. Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.", "citation": "Thus, we also use verb-noun dependencies as evidence in learning #OTHEREFR; #REFR.", "context": "The acquisition method proposed in this work addresses both these matters. Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word ?trouble?. Although we used the classical lexico-syntactic patterns for hyponymy acquisition #OTHEREFR to reflect this intuition, our experiments show we were unable to attain satisfactory performance using lexico-syntactic patterns alone.[Citation]We treat the evidences uniformly as elements in a feature vector given to a supervised learning method, which allowed us to extract a considerably larger number of trouble expressions than could be acquired by sparse lexicosyntactic patterns alone, while still keeping decent precision. What kind of hyponymy relations can be acquired by noun-verb dependencies is still an open question in NLP. In this work we show that at least trouble expressions can successfully be acquired based on noun-verb dependency information alone."}
{"citing_paper_id": "W04-3202", "cited_paper_id": "N04-1012", "citing_paper_abstract": "Active learning (AL) promises to reduce the cost of annotating labeled datasets for trainable human language technologies. Contrary to expectations, when creating labeled training material for HPSG parse selection and later reusing it with other models, gains from AL may be negligible or even negative. This has serious implications for using AL, showing that additional cost-saving strategies may need to be adopted. We explore one such strategy: using a model during annotation to automate some of the decisions. Our best results show an 80% reduction in annotation cost compared with labeling randomly selected data with a single model.", "cited_paper_abstract": "Supervised estimation methods are widely seen as being superior to semi and fully unsupervised methods. However, supervised methods crucially rely upon training sets that need to be manually annotated. This can be very expensive, especially when skilled annotators are required. Active learning (AL) promises to help reduce this annotation cost. Within the complex domain of HPSG parse selection, we show that ideas from ensemble learning can help further reduce the cost of annotation. Our main results show that at times, an ensemble model trained with randomly sampled examples can outperform a single model trained using AL. However, converting the single-model AL method into an ensemble-based AL method shows that even this much stronger baseline model can be improved upon. Our best results show a reduction in annotation cost compared with single-model random sampling.", "citation": "We showed in #REFR that discriminant cost does provide a more accurate approximation of annotation cost than assigning a fixed unit cost for each sentence.", "context": "More interestingly, it means that the labeling burden is relative to the number of possible parses (rather than the number of constituents in a parse). Data about how many discriminants were needed to annotate each sentence is recorded in Redwoods. Typically, more ambiguous sentences require more discriminant values to be set, reflecting the extra effort put into identifying the best parse.[Citation]We thus use discriminants as the basis of calculating annotation cost to evaluate the effectiveness of different experiment AL conditions. Specifically, we set the cost of annotating a given sentence as the number of discriminants whose value were set by the human annotator plus one to indicate a final ?eyeball? step where the annotator selects the best parse of the few remaining ones.1 The discriminant cost of the examples we use averages 3.34 and ranges from 1 to 14."}
{"citing_paper_id": "W05-1501", "cited_paper_id": "N04-1013", "citing_paper_abstract": "In this paper, we introduce a new parser, called SXLFG, based on the Lexical- Functional Grammars formalism (LFG). We describe the underlying context-free parser and how functional structures are efficiently computed on top of the CFG shared forest thanks to computation sharing, lazy evaluation, and compact data representation. We then present various error recovery techniques we implemented in order to build a robust parser. Finally, we offer concrete results when SXLFG is used with an existing grammar for French. We show that our parser is both efficient and robust, although the grammar is very ambiguous.", "cited_paper_abstract": "This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a loglinear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed.1", "citation": "Therefore, it seems sensible to allow the grammar designer to point out in his or her grammar a set of non-terminal symbols that have a linguistic property of #OTHEREFR; #REFR. to linguistically saturated phrases may be associated with an ordered list of disambiguation methods, each of these non-terminals having its own list.", "context": "Afterwards, the shared forest is pruned, retaining only c-structures that are compatible with the chosen main f-structure(s). On the other hand, on any internal node of the forest, a possibly huge number of f-structures may be computed. If nothing is done, these numerous structures may lead to a combinatorial explosion that prevents parsing from terminating in a reasonable time.[Citation]This allows for swift filtering out on relevant internal nodes of f-structures that could arguably only lead to inconsistent and/or incomplete main f-structures, or that would be discarded later on by applying the same method on the main f-structures. Concomitantly, this leads to a significant improvement of parsing times. This view is a generalization of the classical disambiguation method described above, since the pruning of f-structures (and incidentally of the forest itself) is not reserved any more to the axiom of the CF backbone."}
{"citing_paper_id": "D13-1108", "cited_paper_id": "N04-1014", "citing_paper_abstract": "We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.", "cited_paper_abstract": "Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.", "citation": "According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models #OTHEREFR; #REFR, and dependency-based models #OTHEREFR.", "context": "In recent years, syntax-based models have become a hot topic in statistical machine translation.[Citation]These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner."}
{"citing_paper_id": "D10-1023", "cited_paper_id": "N04-1015", "citing_paper_abstract": "Automated essay scoring is one of the most important educational applications of natural language processing. Recently, researchers have begun exploring methods of scoring essays with respect to particular dimensions of quality such as coherence, technical errors, and relevance to prompt, but there is relatively little work on modeling organization. We present a new annotated corpus and propose heuristic-based and learning-based approaches to scoring essays along the organization dimension, utilizing techniques that involve sequence alignment, alignment kernels, and string kernels.", "cited_paper_abstract": "We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.", "citation": "While many models of text coherence have been developed in recent years (e.g., #REFR, Barzilay and Lapata #OTHEREFR), the same is not true for text organization.", "context": "Note that organization is a different facet of text structure than coherence, which is concerned with the transition of ideas at both the global (e.g., paragraph) and local (e.g., sentence) levels. While organization is an important dimension of essay quality, state-of-the-art essay scoring software such as e-rater V.2 #OTHEREFR employs rather simple heuristicbased methods for computing the score of an essay along this particular dimension. Our goal in this paper is to develop a computational model for the organization of student essays.[Citation]One reason is the availability of training and test data for coherence modeling. Coherence models are typically evaluated on the sentence ordering task, and hence training and test data can be generated simply by scrambling the order of the sentences in a text. On the other hand, it is not particularly easy to find poorly organized texts for training and evaluating organization models."}
{"citing_paper_id": "D10-1049", "cited_paper_id": "N04-1015", "citing_paper_abstract": "We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework. In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains?Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation.", "cited_paper_abstract": "We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.", "citation": "In content selection, #REFR use an approach based on local classification with edge-wise scores between local decisions.", "context": "There has been a fair amount of work both on content selection and surface realization.[Citation]Our model, on the other hand, can capture higher-order constraints to enforce global coherence. Liang et al #OTHEREFR introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for)."}
{"citing_paper_id": "N12-2010", "cited_paper_id": "N04-1015", "citing_paper_abstract": "To date, researchers have proposed different ways to compute the readability and coherence of a text using a variety of lexical, syntax, entity and discourse properties. But these metrics have not been defined with special relevance to any particular genre but rather proposed as general indicators of writing quality. In this thesis, we propose and evaluate novel text quality metrics that utilize the unique properties of different genres. We focus on three genres: academic publications, news articles about science, and machine generated text, in particular the output from automatic text summarization systems.", "cited_paper_abstract": "We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.", "citation": "For some of our experiments done so far, we have taken an approach that is common with prior studies on coherence #REFR.", "context": "For academic writing, we plan to use a collection of biology journal articles marked with the impact factor of the journal. The intuition is that the popular journals are more competitive and so the writing is on average better than less impactful venues. It is however not a direct measure of text quality.[Citation]We take an original article and create a random permutation of its sentences, the latter we consider as an incoherent article and the original version as coherent. For science news, we expect that Amazon Mechanical Turk will be a suitable platform for obtaining ratings of popular and interesting articles from the target audience. We also plan to use proxies such as lists of most emailed/viewed articles from news websites."}
{"citing_paper_id": "P10-1020", "cited_paper_id": "N04-1015", "citing_paper_abstract": "One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.", "cited_paper_abstract": "We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.", "citation": "Local coherence modelling has been shown to be useful for tasks like natural language generation and summarization, #REFR and genre classification #OTHEREFR.", "context": "One type of coherence modelling that has captured recent research interest is local coherence modelling, which measures the coherence of a document by examining the similarity between neighbouring text spans. The entity-based approach, in particular, considers the occurrences of noun phrase entities in a document #OTHEREFR.[Citation]Previous work on English, a language with relatively fixed word order, has identified factors that contribute to local coherence, such as the grammatical roles associated with the entities. There is good reason to believe that the importance of these factors vary across languages. For instance, freerword-order languages exhibit word order patterns which are dependent on discourse factors relating to information structure, in addition to the grammatical roles of nominal arguments of the main verb."}
{"citing_paper_id": "W07-1527", "cited_paper_id": "N04-1016", "citing_paper_abstract": "This paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system. The system relies on cross-linguistic evidence from a set of five Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the five Romance languages, our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl and CLUVI. The training data was annotated with contextual features based on two stateof-the-art classification tag sets.", "cited_paper_abstract": "Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks. So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams. For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.", "citation": "Currently, the best-performing English NP interpretation methods in computational linguistics focus mostly on two consecutive noun instances #OTHEREFR, #REFR, #OTHEREFR.", "context": "For example, a compound family estate should be interpreted as the estate OWNED BY the family; an NP such as dress of silk should be interpreted as denoting a dress MADE FROM silk. The problem, while simple to state is hard to solve. The reason is that the meaning of these constructions is most of the time ambiguous or implicit.[Citation]Unlike unsupervised models, supervised knowledge-rich approaches rely heavily on large sets of annotated training data. For example, we previously showed #OTHEREFR. the task of automatic detection of part-whole relations, our system?s learning curve reached a plateau at 74% F-measure when trained on approximatively 10,000 positive and negative examples. Interpreting NPs correctly requires various types of information from world knowledge to complex context features."}
{"citing_paper_id": "P09-1077", "cited_paper_id": "N04-1020", "citing_paper_abstract": "We present a series of experiments on automatically identifying the sense of implicit discourse relations, i.e. relations that are not marked with a discourse connective such as ?but? or ?because?. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.", "cited_paper_abstract": "In this paper we propose a data intensive approach for inferring sentence-internal temporal relations, which relies on a simple probabilistic model and assumes no manual coding. We explore various combinations of features, and evaluate performance against a goldstandard corpus and human subjects performing the same task. The best model achieves 70.7% accuracy in inferring the temporal relation between two clauses and 97.4% accuracy in ordering them, assuming that the temporal relation is known.", "citation": "Indeed, word pairs form the basic feature of most previous work on classifying implicit relations #OTHEREFR or the simpler task of predicting which connective should be used to express a relation #REFR.", "context": "The words ?popular? and ?oblivion? are almost antonyms, and one might hypothesize that their occurrence in the two text spans is what triggers the contrast relation between the sentences. Similarly, a pair of words such as (rain, rot) might be indicative of a causal relation. If this hypothesis is correct, pairs of words (w1, w2) such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations.[Citation]Semantic relations vs. function word pairs If the hypothesis for word pair triggers of discourse relations were true, the analysis of unambiguous relations can be used to discover pairs of words with causal or contrastive relations holding between them. Yet, feature analysis has not been performed in prior studies to establish or refute this possibility. At the same time, feature selection is always necessary for word pairs, which are numerous and lead to data sparsity problems."}
{"citing_paper_id": "W05-1207", "cited_paper_id": "N04-1020", "citing_paper_abstract": "In this work we investigate methods to enable the detection of a specific type of textual entailment (strict entailment), starting from the preliminary assumption that these relations are often clearly expressed in texts. Our method is a statistical approach based on what we call textual entailment patterns, prototypical sentences hiding entailment relations among two activities. We experimented the proposed method using the entailment relations of WordNet as test case and the web as corpus where to estimate the probabilities; obtained results will be shown.", "cited_paper_abstract": "In this paper we propose a data intensive approach for inferring sentence-internal temporal relations, which relies on a simple probabilistic model and assumes no manual coding. We explore various combinations of features, and evaluate performance against a goldstandard corpus and human subjects performing the same task. The best model achieves 70.7% accuracy in inferring the temporal relation between two clauses and 97.4% accuracy in ordering them, assuming that the temporal relation is known.", "citation": "Potentials of the method are still high as different kinds of textual entailment patterns may be defined or discovered investigating relations between sentences and subsentences as done in #REFR for temporal relations or between near sentences as done in #OTHEREFR for cause-effect relations between domain events.", "context": "We have defined a method to recognise and extract entailment relations between verb pairs based on what we call textual entailment pattern. In this work we defined a first kernel of textual entailment patterns based on subject-verb relations.[Citation]Some interesting and simple inter-sentential patters are defined in #OTHEREFR. Moreover, with respect to anchorbased approaches, the method we presented here offers a different point of view on the problem of acquiring textual entailment relation prototypes, as textual entailment patterns do not depend on the repetition of ?similar? facts. This practically independent view may open the possibility to experiment co-training algorithms #OTHEREFR also in this area."}
{"citing_paper_id": "W09-2307", "cited_paper_id": "N04-1021", "citing_paper_abstract": "The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difficulty. While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words. Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77). Our Chinese grammatical relations are also likely to be useful for other NLP tasks.", "cited_paper_abstract": "We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation.", "citation": "More recent work #OTHEREFR; #REFR has introduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases.", "context": "Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements #OTHEREFR. The disadvantage of these models is their insensitivity to the content of the words or phrases.[Citation]Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney #OTHEREFR proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when plugged into a phrase-based MT system. Their framework allows us to easily add in extra features."}
{"citing_paper_id": "D13-1111", "cited_paper_id": "N04-1022", "citing_paper_abstract": "This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT.", "cited_paper_abstract": "We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.", "citation": "There is also similarity between our approach and minimum Bayes risk decoding #REFR, variational decoding #OTHEREFR.", "context": "Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al#OTHEREFR used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations.[Citation]These all seek a single translation that is most similar on average to the model?s preferred translations. In this way, they try to capture the model?s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model?s beliefs about plausible translations for the input."}
{"citing_paper_id": "W10-1727", "cited_paper_id": "N04-1022", "citing_paper_abstract": "In this paper we report on experiments with three preprocessing strategies for improving translation output in a statistical MT system. In training, two reordering strategies were studied: (i) reorder on the basis of the alignments from Giza++, and (ii) reorder by moving all verbs to the end of segments. In translation, out-ofvocabulary words were preprocessed in a knowledge-lite fashion to identify a likely equivalent. All three strategies were implemented for our English?German system submitted to the WMT10 shared task. Combining them lead to improvements in both language directions.", "cited_paper_abstract": "We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.", "citation": "To these systems we added minimum Bayes risk (MBR) decoding #REFR.", "context": "For the final Liu shared task submission we used the base+verb+reorder+OOV system for German?English and the base+verb+OOV system for English?German, which had the best overall scores considering all metrics.[Citation]In standard decoding, the top suggestion of the translation system is chosen as the system output. In MBR decoding the risk is spread by choosing the translation that is most similar to the N highest scoring translation suggestions from the system, with N = 100, as suggested in Koehn et al #OTHEREFR. MBR decoding gave hardly any changes in automatic scores, as shown in Tables 6 and 7."}
{"citing_paper_id": "W10-1756", "cited_paper_id": "N04-1022", "citing_paper_abstract": "We present a unified approach to performing minimum risk training and minimum Bayes risk (MBR) decoding with BLEU in a phrase-based model. Key to our approach is the use of a Gibbs sampler that allows us to explore the entire probability distribution and maintain a strict probabilistic formulation across the pipeline. We also describe a new sampling algorithm called corpus sampling which allows us at training time to use BLEU instead of an approximation thereof. Our approach is theoretically sound and gives better (up to +0.6%BLEU) and more stable results than the standard MERT optimization algorithm. By comparing our approach to lattice MBR, we are also able to gain crucial insights about both methods.", "cited_paper_abstract": "We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.", "citation": "This solution is often referred to as the Minimum Bayes Risk (MBR) solution #REFR.", "context": "According to statistical decision theory, the optimal decision rule for any statistical model is the solution that minimizes its risk (expected loss).[Citation]Since machine translation #OTHEREFR, a loss function which rewards partial matches, the MBR solution is to be preferred to the Maximum A Posteriori (MAP) solution. In most statistical MT (SMT) systems, MBR is implemented as a reranker of a list1 of translations generated by a first-pass decoder. This decoder typically assigns unnormalised log probabilities (known as scores) to each translation hypoth- 1We use the term list to denote any enumerable representation of translation hypotheses e.g n-best list, translation lattice or forest. esis, so these scores must be converted to probabilities in order to apply MBR."}
{"citing_paper_id": "P06-1096", "cited_paper_id": "N04-1023", "citing_paper_abstract": "We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We first discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We find that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic.", "cited_paper_abstract": "This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a rankedbest list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.", "citation": "Unlike minimum error rate training #OTHEREFR, our system is able to exploit large numbers of specific features in the same manner as static reranking systems #REFR.", "context": "Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. #OTHEREFR, but uses an online perceptron training scheme to learn model parameters.[Citation]However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter estimates improve, the system produces better nbest lists, which can in turn enable better updates in future training iterations."}
{"citing_paper_id": "P12-1039", "cited_paper_id": "N04-1023", "citing_paper_abstract": "This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our approach is to reduce the tasks of content selection (?what to say?) and surface realization (?how to say?) into a common parsing problem. We define a probabilistic context-free grammar that describes the structure of the input (a corpus of database records and text describing some of them) and represent it compactly as a weighted hypergraph. The hypergraph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.", "cited_paper_abstract": "This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a rankedbest list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.", "citation": "Discriminative reranking has been employed in many NLP tasks such as syntactic parsing #OTHEREFR, machine translation #REFR and semantic parsing #OTHEREFR.", "context": "They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output.[Citation]Our model is closest to Huang #OTHEREFR. We adapt forest reranking to generation and introduce several task-specific features that boost performance. Although conceptually related to Angeli et al #OTHEREFR, our model optimizes content selection and surface realization simultaneously, rather than as a sequence."}
{"citing_paper_id": "W07-0401", "cited_paper_id": "N04-1023", "citing_paper_abstract": "In this paper, we describe a sourceside reordering method based on syntactic chunks for phrase-based statistical machine translation. First, we shallow parse the source language sentences. Then, reordering rules are automatically learned from source-side chunks and word alignments. During translation, the rules are used to generate a reordering lattice for each sentence. Experimental results are reported for a Chinese-to-English task, showing an improvement of 0.5%?1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level.", "cited_paper_abstract": "This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a rankedbest list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.", "citation": "Another kind of approaches is to use syntactic information in rescoring methods. #OTHEREFR and #REFR describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains.", "context": "One approach makes use of bitext grammars to parse both the source and target languages. Another approach makes use of syntactic information only in the target language. Note that these models have radically different structures and parameterizations than phrase-based models for SMT.[Citation]In this paper, we present a strategy to reorder a source sentence using rules based on syntactic chunks. It is possible to integrate reordering rules directly into the search process, but here, we consider a more modular approach: easy to exchange reordering strategy. To avoid hard decisions before SMT, we generate a source-reordering lattice instead of a single reordered source sentence as input to the SMT system."}
{"citing_paper_id": "C14-1090", "cited_paper_id": "N04-1024", "citing_paper_abstract": "This paper presents an investigation of lexical chaining (Morris and Hirst, 1991) for measuring discourse coherence quality in test-taker essays. We hypothesize that attributes of lexical chains, as well as interactions between lexical chains and explicit discourse elements, can be harnessed for representing coherence. Our experiments reveal that performance achieved by our new lexical chain features is better than that of previous discourse features used for this task, and that the best system performance is achieved when combining lexical chaining features with complementary discourse features, such as those provided by a discourse parser based on rhetorical structure theory, and features that reflect errors in grammar, word usage, and mechanics.", "cited_paper_abstract": "CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements). We describe a new system that enhances Criterion?s capability, by evaluating multiple aspects of coherence in essays. This system identifies features of sentences based on semantic similarity measures and discourse structure. A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements. Intra-sentential quality is evaluated with rule-based heuristics. Results indicate that the system yields higher performance than a baseline on all three aspects.", "citation": "#REFR implemented a genre-dependent system to predict discourse coherence quality in essays.", "context": "Our approach also differs from models that measure local coherence between adjacent sentences #OTHEREFR, in that lexical chains can run though the length of the entire text, and hence the features derived from them are able to capture aggregate thematic properties of the entire text such as number, distribution and elaboration of topics. Discourse coherence models have been previously employed for the task of information-ordering in well-formed texts #OTHEREFR). In our tasks, discourse coherence quality is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization.[Citation]Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion."}
{"citing_paper_id": "W11-2146", "cited_paper_id": "N04-1030", "citing_paper_abstract": "This paper describes the statistical machine translation system submitted to the WMT11 Featured Translation Task, which involves translating Haitian Creole SMS messages into English. In our experiments we try to address the issue of noise in the training data, as well as the lack of parallel training data. Spelling normalization is applied to reduce out-of-vocabulary words in the corpus. Using Semantic Role Labeling rules we expand the available training corpus. Additionally we investigate extracting parallel sentences from comparable data to enhance the available parallel data.", "cited_paper_abstract": "In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.", "citation": "First, we parsed and labeled the semantic roles of the English side of the corpus, using the AS- SERT labeler #REFR.", "context": "Labeling To address the problem of limited resources, we tried to expand the training corpus by applying the corpus expansion method described in #OTHEREFR.[Citation]Next, using the word alignment models of the parallel corpus, we extracted Semantic Role Label (SRL) substitution rules. SRL rules consist of source and target phrases that cover whole constituents of semantic roles, the verb frames they belong to, and the role labels of the constituents. The source and target phrases must comply with the restrictions detailed in #OTHEREFR."}
{"citing_paper_id": "D13-1052", "cited_paper_id": "N04-1035", "citing_paper_abstract": "Machine translation benefits from system combination. We propose flexible interaction of hypergraphs as a novel technique combining different translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models?specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification?a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans?comes out as the strongest, yielding improvement up to 0.9 (Ter-Bleu)/2 points. We also provide a detailed experimental and qualitative analysis of the results.", "cited_paper_abstract": "We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.", "citation": "Finally, we apply the forest-based GHKM algorithm #OTHEREFR; #REFR to extract tree-to-string translation rules from forest-string pairs.", "context": "The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). Language models are trained on the English side of the parallel corpus, and on monolingual corpora, such as Gigaword #OTHEREFRT07) and Google News, altogether comprising around 10 billion words. We use a modified version of the Berkeley parser #OTHEREFR to obtain a parse forest for each training sentence, then we prune it with the marginal probability-based inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length.[Citation]In the decoding step, we prune the input hypergraphs to 10n nodes before we use fast patternmatching #OTHEREFR to convert the parse forest into the translation forest. We tune on 1275 sentences, each with 4 references, from the LDC2010E30 corpus, initially released under the DARPA GALE program. All MT experiments are optimized with MIRA #OTHEREFR to maximize (Ter-Bleu)/2."}
{"citing_paper_id": "N12-1061", "cited_paper_id": "N04-1035", "citing_paper_abstract": "We present a novel method to detect parallel fragments within noisy parallel corpora. Isolating these parallel fragments from the noisy data in which they are contained frees us from noisy alignments and stray links that can severely constrain translation-rule extraction. We do this with existing machinery, making use of an existing word alignment model for this task. We evaluate the quality and utility of the extracted data on large-scale Chinese-English and Arabic-English translation tasks and show significant improvements over a state-of-the-art baseline.", "cited_paper_abstract": "We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.", "citation": "In our experiments we use a tree-to-string syntax-based MT system #REFR, and evaluate on a standard test set, NIST08.", "context": "We evaluate our parallel fragment extraction in a large-scale Chinese-English and Arabic-English MT setting.[Citation]We parse the English side of our parallel corpus with the Berkeley parser #OTHEREFR. We decode with an integrated language model trained on about 4 billion words of English. Chinese-English We align a parallel corpus of 8.4M parallel segments, with 210M words of English and 193M words of Chinese."}
{"citing_paper_id": "W08-0411", "cited_paper_id": "N04-1035", "citing_paper_abstract": "We describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages. The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees. We first apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees. Next, we extract all aligned sub-sentential syntactic constituents from the parallel sentences, and create a syntax-based phrase-table. Finally, we treat the node alignments as tree decomposition points and extract from the corpus all possible synchronous parallel tree fragments. These are then converted into synchronous context-free rules. We describe the approach and analyze its application to Chinese-English parallel data.", "cited_paper_abstract": "We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.", "citation": "Accurate identification of sub-sentential translation equivalents, however, is a critical process in all data-driven MT approaches, including a variety of data-driven syntax-based approaches that have been developed in recent years. #OTHEREFR #REFR.", "context": "Phrase-based Statistical MT #OTHEREFR has become the predominant approach to Machine Translation in recent years. PB-SMT requires broad-coverage databases of phrase-to-phrase translation equivalents. These are commonly acquired from large volumes of automatically wordaligned sentence-parallel text corpora.[Citation]In this paper, we describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages. The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees for both languages. Our method consists of three steps."}
{"citing_paper_id": "D09-1101", "cited_paper_id": "N04-1037", "citing_paper_abstract": "Traditional learning-based coreference resolvers operate by training a mentionpair classifier for determining whether two mentions are coreferent or not. Two independent lines of recent research have attempted to improve these mention-pair classifiers, one by learning a mentionranking model to rank preceding mentions for a given anaphor, and the other by training an entity-mention classifier to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches.", "cited_paper_abstract": "State-of-the-art pronoun interpretation systems rely predominantly on morphosyntactic contextual features. While the use of deep knowledge and inference to improve these models would appear technically infeasible, previous work has suggested that predicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge. We test this idea in several system configurations, and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax.", "citation": "Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not #OTHEREFRb), #REFR, Ponzetto and Strube #OTHEREFR).", "context": "Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept.[Citation]Despite their initial successes, these mention-pair models have at least two major weaknesses. First, since each candidate antecedent for a mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates. In other words, they fail to answer the critical question of which candidate antecedent is most probable."}
{"citing_paper_id": "E06-2015", "cited_paper_id": "N04-1037", "citing_paper_abstract": "Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance.", "cited_paper_abstract": "State-of-the-art pronoun interpretation systems rely predominantly on morphosyntactic contextual features. While the use of deep knowledge and inference to improve these models would appear technically infeasible, previous work has suggested that predicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge. We test this idea in several system configurations, and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax.", "citation": "We speculate that this contrasts with the disappointing findings of #REFR since SRL provides a more fine grained level of information when compared to predicate argument statistics.", "context": "Empirical results show that coreference resolution can benefit from SRL. The analysis of the relevance of features, which had not been previously addressed, indicates that incorporating semantic information as shallow event descriptions improves the performance of the classifier. The generated model is able to learn selection preferences in cases where surface morpho-syntactic features do not suffice, i.e. pronoun resolution.[Citation]As it models the semantic relationship that a syntactic constituent has with a predicate, it carries indirectly syntactic preference information. In addition, when used as a feature it allows the classifier to infer semantic role co-occurrence, thus inducing deep representations of the predicate argument relations for learning in coreferential contexts. Acknowledgements: This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany."}
{"citing_paper_id": "P10-1142", "cited_paper_id": "N04-1037", "citing_paper_abstract": "The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.", "cited_paper_abstract": "State-of-the-art pronoun interpretation systems rely predominantly on morphosyntactic contextual features. While the use of deep knowledge and inference to improve these models would appear technically infeasible, previous work has suggested that predicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge. We test this idea in several system configurations, and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax.", "citation": "While we have focused our discussion on supervised approaches, coreference researchers have also attempted to reduce a resolver?s reliance on annotated data by combining a small amount of labeled data and a large amount of unlabeled data using general-purpose semi-supervised learning algorithms such as co-training #OTHEREFR, self-training #REFRa), and EM #OTHEREFR.", "context": "[Citation]Interestingly, recent results indicate that unsupervised approaches to coreference resolution #OTHEREFR) rival their supervised counterparts, casting doubts on whether supervised resolvers are making effective use of the available labeled data. Another issue that we have not focused on but which is becoming increasingly important is multilinguality. While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese #OTHEREFR)."}
{"citing_paper_id": "P08-1090", "cited_paper_id": "N04-1038", "citing_paper_abstract": "Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised induction of similar schemata called narrative event chains from raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. We introduce two evaluations: the narrative cloze to evaluate event relatedness, and an order coherence task to evaluate narrative order. We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence.", "cited_paper_abstract": "We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.", "citation": "#REFR proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.", "context": "In summarization, topic signatures are a set of terms indicative of a topic #OTHEREFR. They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data.[Citation]A caseframe is a verb/event and a semantic role (e.g. <patient> kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (<patient> kidnapped and <patient> abducted) or related events (<patient> kidnapped and <patient> released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution."}
{"citing_paper_id": "W06-2918", "cited_paper_id": "N04-1042", "citing_paper_abstract": "Much work on information extraction has successfully used gazetteers to recognise uncommon entities that cannot be reliably identified from local context alone. Approaches to such tasks often involve the use of maximum entropy-style models, where gazetteers usually appear as highly informative features in the model. Although such features can improve model accuracy, they can also introduce hidden negative effects. In this paper we describe and analyse these effects and suggest ways in which they may be overcome. In particular, we show that by quarantining gazetteer features and training them in a separate model, then decoding using a logarithmic opinion pool (Smith et al, 2005), we may achieve much higher accuracy. Finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified.", "cited_paper_abstract": "With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs.", "citation": "In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing #OTHEREFR and information extraction from research papers #REFR.", "context": "[Citation]Discriminative models offer a significant advantage over their generative counterparts by allowing the specification of powerful, possibly non-independent features which would be difficult to tractably encode in a generative model. In a task such as NER, one sometimes encounters an entity which is difficult to identify using local contextual cues alone because the entity has not be seen before. In these cases, a gazetteer or dictionary of possible entity identifiers is often useful."}
{"citing_paper_id": "W06-1643", "cited_paper_id": "N04-4027", "citing_paper_abstract": "We describe a probabilistic approach to content selection for meeting summarization. We use skipchain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as QUESTION-ANSWER that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task. We also discuss different approaches for ranking all utterances in a sequence using CRFs. Our best performing system achieves 91.3% of human performance when evaluated with the Pyramid evaluation metric, which represents a 3.9% absolute increase compared to our most competitive non-sequential classifier.", "cited_paper_abstract": "Summarizing threads of email is different from summarizing other types of written communication as it has an inherent dialog structure. We present initial research which shows that sentence extraction techniques can work for email threads as well, but profit from email-specific features. In addition, the presentation of the summary should take into account the dialogic structure of email communication.", "citation": "In email summarization #REFR, Shrestha and McKeown #OTHEREFR obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange.", "context": "Following an intuition that the probability of a given sentence may be locally conditioned on the previous one, Conroy #OTHEREFR built a HMM-based summarizer that consistently ranked among the top systems in recent Document Understanding Conference (DUC) evaluations. Inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties. In the case of summarization of conversational speech, Zechner #OTHEREFR found, for instance, that a simple technique consisting of linking together questions and answers in summaries?and thus preventing the selection of orphan questions or answers?significantly improved their readability according to various human summary evaluations.[Citation]In a combined chat and email summarization task, a technique #OTHEREFR consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines. The need to model pragmatic influences, such as between a question and an answer, is also prevalent in meeting summarization. In fact, questionanswer pairs are not the only discourse relations that we need to preserve in order to create coherent summaries, and, as we will see, most instances of APs would need to be preserved together, either inside or outside the summary."}
{"citing_paper_id": "W05-0101", "cited_paper_id": "N04-4030", "citing_paper_abstract": "In Fall 2004 I introduced a new course called Applied Natural Language Processing, in which students acquire an understanding of which text analysis techniques are currently feasible for practical applications. The class was intended for interdisciplinary students with a somewhat technical background. This paper describes the topics covered and the programming exercises, emphasizing which aspects were successful and which problematic, and makes recommendations for future versions of the course.", "cited_paper_abstract": "Currently, information architects create metadata category hierarchies manually. We present a nearly-automated approach for deriving such hierarchies, by converting the lexical hierarchy WordNet into a format that reflects the contents of a target information collection. We use the term ?nearly-automated? because an information architect should have to make only small adjustments to produce an acceptable metadata structure. We contrast the results with an algorithm that uses lexical co-occurrence statistics.", "citation": "Another was to improve on or apply an automatic hierarchy generation tool that we have developed in our research #REFR.", "context": "Those who tried other topics were often too ambitious and had trouble getting meaningful results. However, several of those students were trying ideas that they planned to apply to their capstone projects, and so it was highly valuable for them to get a preview of what worked and what did not. One suggestion I made was to create a back-ofthe-book indexer, specifically for a recipe book, and one team did a good job with this project.[Citation]Students working on a project to collect metadata for camera phone images successfully applied this tool to this problem. Again, social networking analysis topics were popular but not particularly successful; NLP tools are not advanced enough yet to meet the needs of this intriguing topic area. Not surprisingly, when students started with a new (interesting) text collection, they were bogged down in the preprocessing stage before they could get much interesting work done."}
{"citing_paper_id": "C10-1131", "cited_paper_id": "N06-1006", "citing_paper_abstract": "A range of Natural Language Processing tasks involve making judgments about the semantic relatedness of a pair of sentences, such as Recognizing Textual Entailment (RTE) and answer selection for Question Answering (QA). A key challenge that these tasks face in common is the lack of explicit alignment annotation between a sentence pair. We capture the alignment by using a novel probabilistic model that models tree-edit operations on dependency parse trees. Unlike previous tree-edit models which require a separate alignment-finding phase and resort to ad-hoc distance metrics, our method treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. We demonstrate the robustness of our model by conducting experiments for RTE and QA, and show that our model performs competitively on both tasks with the same set of general features.", "cited_paper_abstract": "This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality. Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.", "citation": "Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success #REFR.", "context": "In each of these tasks, inputs to the systems are pairs of sentences that may or may not convey the desired semantic property (e.g., in RTE, whether the hypothesis sentence can be entailed from the premise sentence; in QA, whether the answer candidate sentence correctly answers the question), and the output of the system is a binary classification decision (or a regression score,as in MTE). Earlier studies in these domains have concluded that simple word overlap measures #OTHEREFRa). A common problem identified in these earlier systems is the lack of understanding of the semantic relation between words and phrases.[Citation]Studies have also shown that certain prominent syntactic features are often found beneficial #OTHEREFR. More recent studies gained further leverage from systematic exploration of the syntactic feature space through analysis of parse trees #OTHEREFR. There are two key challenges imposed by these tasks."}
{"citing_paper_id": "I08-1070", "cited_paper_id": "N06-1008", "citing_paper_abstract": "In a broad range of natural language processing tasks, large-scale knowledge-base of paraphrases is anticipated to improve their performance. The key issue in creating such a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability, i.e., paraphrasability, between given pair of expressions. This paper addresses the issues of computing paraphrasability, focusing on syntactic variants of predicate phrases. Our model estimates paraphrasability based on traditional distributional similarity measures, where the Web snippets are used to overcome the data sparseness problem in handling predicate phrases. Several feature sets are evaluated through empirical experiments.", "cited_paper_abstract": "This paper shows that inference rules with temporal constraints can be acquired by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun cooccurrences. For example, our unsupervised acquisition method could obtain the inference rule ?If someone enforces a law, usually someone enacts the law at the same time as or before the enforcing of the law? since the verbs ?enact? and ?enforce? frequently co-occurred in coordinated sentences and the verbs also frequently cooccurred with the noun ?law?. We also show that the accuracy of the acquisition is improved by using the occurrence frequency of a single verb, which we assume indicates how generic the meaning of the verb is.", "citation": "To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized #OTHEREFR; #REFR.", "context": "As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified.[Citation]Although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 1See http://nlp.cs.nyu.edu/WTEP/ paraphrases accurately. The notion of Inferential Selectional Preference #OTHEREFR. ISP can capture more general phenomena than above two; however, it lacks abilities to distinguish antonym relations."}
{"citing_paper_id": "I08-1070", "cited_paper_id": "N06-1008", "citing_paper_abstract": "In a broad range of natural language processing tasks, large-scale knowledge-base of paraphrases is anticipated to improve their performance. The key issue in creating such a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability, i.e., paraphrasability, between given pair of expressions. This paper addresses the issues of computing paraphrasability, focusing on syntactic variants of predicate phrases. Our model estimates paraphrasability based on traditional distributional similarity measures, where the Web snippets are used to overcome the data sparseness problem in handling predicate phrases. Several feature sets are evaluated through empirical experiments.", "cited_paper_abstract": "This paper shows that inference rules with temporal constraints can be acquired by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun cooccurrences. For example, our unsupervised acquisition method could obtain the inference rule ?If someone enforces a law, usually someone enacts the law at the same time as or before the enforcing of the law? since the verbs ?enact? and ?enforce? frequently co-occurred in coordinated sentences and the verbs also frequently cooccurred with the noun ?law?. We also show that the accuracy of the acquisition is improved by using the occurrence frequency of a single verb, which we assume indicates how generic the meaning of the verb is.", "citation": "One possible way to overcome the problem is to take back-off statistics assuming the independence between constituent words #REFR.", "context": "In general, phrases appear less frequently than single words. This raises a crucial problem in computing paraphrasability of phrases, i.e., the sparseness of features for given phrases.[Citation]This approach, however, has a risk of involving noises due to ambiguity of words. We take another approach, which utilizes the Web as a source of examples instead of a limited size of corpus. For each of the source and target phrases, we retrieve snippets via the Yahoo API2."}
{"citing_paper_id": "P09-1104", "cited_paper_id": "N06-1014", "citing_paper_abstract": "This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.", "cited_paper_abstract": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions.", "citation": "We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference #REFR.", "context": "Both discriminative methods require repeated model inference: MIRA depends upon lossaugmented Viterbi parsing, while conditional like- 6Note that alignments that achieve the minimal loss would not introduce any alignments not either sure or possible, so it suffices to keep track only of the number of sure recall errors. lihood uses the inside-outside algorithm for computing cell posteriors. Exhaustive computation of these quantities requires an O(n6) dynamic program that is prohibitively slow even on small supervised training sets. However, most of the search space can safely be pruned using posterior predictions from a simpler alignment models.[Citation]Our first pruning technique is broadly similar to Cherry and Lin #OTHEREFRa). We select high-precision alignment links from the HMM models: those word pairs that have a posterior greater than 0.9 in either model. Then, we prune all bitext cells that would invalidate more than 8 of these high-precision alignments."}
{"citing_paper_id": "C08-1071", "cited_paper_id": "N06-1020", "citing_paper_abstract": "Self-training has been shown capable of improving on state-of-the-art parser performance (McClosky et al, 2006) despite the conventional wisdom on the matter and several studies to the contrary (Charniak, 1997; Steedman et al, 2003). However, it has remained unclear when and why selftraining is helpful. In this paper, we test four hypotheses (namely, presence of a phase transition, impact of search errors, value of non-generative reranker features, and effects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations.", "cited_paper_abstract": "We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f -score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.", "citation": "To test this, first we present some statistics on the nbest lists (n = 50) from the baseline WSJ trained parser and self-trained model3 from #REFR.", "context": "Another possible explanation of self-training?s improvements is that seeing newly labeled data results in fewer search errors (Daniel Marcu, personal communication). A search error would indicate that the parsing model could have produced better (more probable) parses if not for heuristics in the search procedure. The additional parse trees may help produce sharper distributions and reduce data sparsity, making the search process easier.[Citation]We use each model to parse sentences from held-out data (sections 1, 22, and 24) and examine the n-best lists. We compute statistics of the WSJ and selftrained n-best lists with the goal of understanding how much they intersect and whether there are search errors. On average, the n-best lists overlap by 66.0%."}
{"citing_paper_id": "P09-1006", "cited_paper_id": "N06-1020", "citing_paper_abstract": "We address the issue of using heterogeneous treebanks for parsing by breaking it down into two sub-problems, converting grammar formalisms of the treebanks to the same one, and parsing on these homogeneous treebanks. First we propose to employ an iteratively trained target grammar parser to perform grammar formalism conversion, eliminating predefined heuristic rules as required in previous methods. Then we provide two strategies to refine conversion results, and adopt a corpus weighting technique for parsing on homogeneous treebanks. Results on the Penn Treebank show that our conversion method achieves 42% error reduction over the previous best result. Evaluation on the Penn Chinese Treebank indicates that a converted dependency treebank helps constituency parsing and the use of unlabeled data by self-training further increases parsing f-score to 85.2%, resulting in 6% error reduction over the previous best result.", "cited_paper_abstract": "We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f -score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.", "citation": "Recently there have been some works on using multiple treebanks for domain adaptation of parsers, where these treebanks have the same grammar formalism #REFR.", "context": "Here heterogeneous treebanks refer to two or more treebanks with different grammar formalisms, e.g., one treebank annotated with dependency structure (DS) and the other annotated with phrase structure (PS). It is important to acquire additional labeled data for the target grammar parsing through exploitation of existing source treebanks since there is often a shortage of labeled data. However, to our knowledge, there is no previous study on this issue.[Citation]Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank #OTHEREFR. These works were done either on multiple treebanks with the same grammar formalism or on only one converted treebank. We see that their scenarios are different from ours as we work with multiple heterogeneous treebanks."}
{"citing_paper_id": "W09-1104", "cited_paper_id": "N06-1020", "citing_paper_abstract": "We present a simple but very effective approach to identifying high-quality data in noisy data sets for structured problems like parsing, by greedily exploiting partial structures. We analyze our approach in an annotation projection framework for dependency trees, and show how dependency parsers from two different paradigms (graph-based and transition-based) can be trained on the resulting tree fragments. We train parsers for Dutch to evaluate our method and to investigate to which degree graph-based and transitionbased parsers can benefit from incomplete training data. We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006).", "cited_paper_abstract": "We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f -score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.", "citation": "Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training #OTHEREFR over bootstrapping approaches like selftraining #REFR to feature-based enhancements of discriminative reranking models #OTHEREFR.", "context": "Systematic non-parallelisms between source and target language are then addressed by hand-crafted rules in a post-projection step. These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese. But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language.[Citation]The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes. Our approach is similar to that of Clark and Curran #OTHEREFR who use partial training data (CCG lexical categories) for domain adaptation; however, they assume an existing CCG resource for the language in question to provide this data."}
{"citing_paper_id": "W14-6106", "cited_paper_id": "N06-1020", "citing_paper_abstract": "We apply the well-known parsing technique of self-training to a new type of text: languagelearner text. This type of text often contains grammatical and other errors which can cause problems for traditional treebank-based parsers. Evaluation on a small test set of student data shows improvement over the baseline, both by training on native or non-native text. The main contribution of this paper adds additional support for the claim that the new self-trained parser has improved over the baseline by carrying out a qualitative linguistic analysis of the kinds of differences between two parsers on non-native text. We show that for a number of linguistically interesting cases, the self-trained parser is able to provide better analyses, despite the sometimes ungrammatical nature of the text.", "cited_paper_abstract": "We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f -score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.", "citation": "Self-training was first successfully applied in the newspaper parsing domain by #REFR who used the Penn Treebank WSJ as their labeled data and unlabeled data from the North American News Text corpus.", "context": "These kinds of errors are known to cause difficulty for automated analyses #OTHEREFR. We explore a previously documented technique for adapting a state-of-the-art parser to be able to better parse learner text: domain adaptation using self-training. Self-training is a semi-supervised learning technique that relies on some labeled data to train an initial model, and then uses large amounts of unlabeled data to iteratively improve that model.[Citation]Previous attempts #OTHEREFR hypothesize that the gain they saw was due to the two-phase nature of the BLLIP parser used in their experiments. In a follow-up study #OTHEREFR they find that one major factor leading to successful self-training is when the process sees known words in new combinations."}
{"citing_paper_id": "N07-1051", "cited_paper_id": "N06-1022", "citing_paper_abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar?s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.", "cited_paper_abstract": "We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse. Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals. We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG. We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level. We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG. We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results.", "citation": "We then parse with each refinement, in sequence, much along the lines of #REFR, except with much more complex and automatically derived intermediate grammars.", "context": "In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our method considers the splitting history of the final grammar, projecting it onto its increasingly refined prior stages. For any projection of a grammar, we give a new method for efficiently estimating the projection?s parameters from the source PCFG itself #OTHEREFR and iterated fixpoint equations.[Citation]Thresholds are automatically tuned on heldout data, and the final system parses up to 100 times faster than the baseline PCFG parser, with no loss in test set accuracy. In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families #OTHEREFR, split PCFGs have a derivation / parse distinction."}
{"citing_paper_id": "W12-4515", "cited_paper_id": "N06-1025", "citing_paper_abstract": "This paper describes the UniTN/Essex submission to the CoNLL-2012 Shared Task on the Multilingual Coreference Resolution. We have extended our CoNLL-2011 submission, based on BART, to cover two additional languages, Arabic and Chinese. This paper focuses on adapting BART to new languages, discussing the problems we have encountered and the solutions adopted. In particular, we propose a novel entity-mention detection algorithm that might help identify nominal mentions in an unknown language. We also discuss the impact of basic linguistic information on the overall performance level of our coreference resolution system.", "cited_paper_abstract": "In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.", "citation": "In fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example, using WordNet #OTHEREFR or Wikipedia #REFR.", "context": "Another issue to be solved when designing a coreference resolution system for a new language is a possible lack of relevant linguistic information. Most state-of-the-art CR algorithms rely on relatively advanced linguistic representations of mentions. This can be seen as a remarkable shift 1Statistical EMD approaches have been proved useful for ACE-style coreference resolution, where mentions are basic units belonging to a restricted set of semantic types. from knowledge-lean approaches of the late nineties #OTHEREFR.[Citation]For languages other than English, however, even the most basic properties of mentions can be intrinsically difficult to extract. For example, Baran and Xue #OTHEREFR have shown that a complex algorithm is needed to identify the number property of Chinese nouns. Both Arabic and Chinese have long linguistic traditions and therefore most grammar studies rely on terminology that can be very confusing for an outsider."}
{"citing_paper_id": "N12-1044", "cited_paper_id": "N06-1030", "citing_paper_abstract": "Sequential transduction tasks, such as grapheme-to-phoneme conversion and machine transliteration, are usually addressed by inducing models from sets of input-output pairs. Supplemental representations offer valuable additional information, but incorporating that information is not straightforward. We apply a unified reranking approach to both grapheme-to-phoneme conversion and machine transliteration demonstrating substantial accuracy improvements by utilizing heterogeneous transliterations and transcriptions of the input word. We describe several experiments that involve a variety of supplemental data and two state-of-the-art transduction systems, yielding error rate reductions ranging from 12% to 43%. We further apply our approach to system combination, with error rate reductions between 4% and 9%.", "cited_paper_abstract": "The speed with which pronunciation dictionaries can be bootstrapped depends on the efficiency of learning algorithms and on the ordering of words presented to the user. This paper presents an active-learning word selection strategy that is mindful of human limitations. Learning rates approach that of an oracle system that knows the final LTS rule set.", "citation": "Excepting languages with highly transparent orthographies, the number of letter-to-sound rules appears to grow geometrically with the lexicon size, with no asymptotic limit #REFR.", "context": "Because of its crucial role in speech synthesis, grapheme-to-phoneme conversion has been researched extensively. Most out-of-vocabulary words are names, which often exhibit idiosyncratic pronunciation #OTHEREFR.[Citation]A number of machine learning approaches have been proposed for G2P, including neural networks #OTHEREFR. The current state-of-the-art is represented by the latter two approaches, which are available as the SEQUITUR and DIRECTL+ systems, respectively. Machine transliteration has also received much attention #OTHEREFR."}
{"citing_paper_id": "C08-1127", "cited_paper_id": "N06-1031", "citing_paper_abstract": "Bracketing Transduction Grammar (BTG) is a natural choice for effective integration of desired linguistic knowledge into statistical machine translation (SMT). In this paper, we propose a Linguistically Annotated BTG (LABTG) for SMT. It conveys linguistic knowledge of source-side syntax structures to BTG hierarchical structures through linguistic annotation. From the linguistically annotated data, we learn annotated BTG rules and train linguistically motivated phrase translation model and reordering model. We also present an annotation algorithm that captures syntactic information for BTG nodes. The experiments show that the LABTG approach significantly outperforms a baseline BTG- based system and a state-of-the-art phrasebased system on the NISTMT-05 Chineseto-English translation task. Moreover, we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering.", "cited_paper_abstract": "We identify problems with the Penn Treebank that render it imperfect for syntaxbased machine translation and propose methods of relabeling the syntax trees to improve translation quality. We develop a system incorporating a handful of relabeling strategies that yields a statistically significant improvement of 2.3 BLEU points over a baseline syntax-based system.", "citation": "LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. #REFR and #OTHEREFR introduce relabeling and supertagging on the target side, respectively.", "context": "There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side #OTHEREFR, just to name a few.[Citation]The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standard plain phrases using linguistic elements on the source side. Compared with the target side annotation which improves fluency and grammaticality of translation output, linguistic annotation on the source side helps to improve translation adequacy."}
{"citing_paper_id": "W07-0403", "cited_paper_id": "N06-1033", "citing_paper_abstract": "We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models. This syntactic model is similar to its flatstring phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training. We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm. We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score. Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method.", "cited_paper_abstract": "Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large. We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system.", "citation": "We can use a linear-time algorithm #REFR to detect non-ITG movement in our high-confidence links, and remove the offending sentence pairs from our training corpus.", "context": "However, these two potential problems cancel each other out. Sentence pairs containing non-ITG translations will tend to have high-confidence links that are also not ITG-compatible. Our EM learner will simply skip these sentence pairs during training, avoiding pollution of our training data.[Citation]This results in only a minor reduction in training data; in our French-English training set, we lose less than 1%. In the experiments described in Section 5, all systems that do not use ITG will take advantage of the complete training set."}
{"citing_paper_id": "D08-1062", "cited_paper_id": "N06-1039", "citing_paper_abstract": "Relationship discovery is the task of identifying salient relationships between named entities in text. We propose novel approaches for two sub-tasks of the problem: identifying the entities of interest, and partitioning and describing the relations based on their semantics. In particular, we show that term frequency patterns can be used effectively instead of supervised NER, and that the pmedian clustering objective function naturally uncovers relation exemplars appropriate for describing the partitioning. Furthermore, we introduce a novel application of relationship discovery: the unsupervised identification of protein-protein interaction phrases.", "cited_paper_abstract": "We are trying to extend the boundary of Information Extraction (IE) systems. Existing IE systems require a lot of time and human effort to tune for a new scenario. Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention. We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables. We present a preliminary system that obtains reasonably good results.", "citation": "These aggregate feature vectors are clustered using complete-linkage HAC, and cluster exemplars are determined by manual inspection for evaluation purposes. #REFR rely further on supervised methods, defining features over a full syntactic parse, and exploit multiple descriptions of the same event in newswire to identify useful relations. #OTHEREFR consider the use of RD for unsupervised relation extraction, and use ", "context": "Existing methods differ primarily in the amount of supervision required and in how contextual features are defined and used. #OTHEREFR use NER to identify frequently co-occurring entities as likely relation phrases. As in this work, they use the vector model and cosine similarity to define a measure of similarity between relations, but build relation vectors out of all instances of each frequently co-occurring entity pair. Therefore, each mention of the same co-occurring pair is assumed to express the same relationship.[Citation]Table 4: Base relations identified using RP-P parameters Exemplar Size P #OTHEREFR and in this work. They also use HAC, and do not address the description of the relations. Arbitrary noun phrases obtained through shallow parsing are used as entities. #OTHEREFR use a feature ranking scheme using separability-based scores, and compare the performance of different variants of HAC (finding single-linkage to perform best)."}
{"citing_paper_id": "D07-1031", "cited_paper_id": "N06-1041", "citing_paper_abstract": "This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers. We find that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed. This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought.", "cited_paper_abstract": "We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system?s error trends.", "citation": "#REFR propose constraining the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag.", "context": "As Clark #OTHEREFR points out, many-to-1 accuracy has several defects. If a system is permitted to posit an unbounded number of hidden states (which is not the case here) then it can achieve a perfect many-to- 1 accuracy by placing every word token into its own unique state. Cross-validation, i.e., identifying the many-to-1 mapping and evaluating on different subsets of the data, would answer many of these objections.[Citation]This mapping is found by greedily assigning hidden states to POS tags until either the hidden states or POS tags are exhausted (note that if the number of hidden states and POS tags differ, some will be unassigned). We call the accuracy of the POS sequence obtained using this map its 1-to-1 accuracy. Finally, several authors have proposed using information-theoretic measures of the divergence between the hidden state and POS tag sequences."}
{"citing_paper_id": "N09-3010", "cited_paper_id": "N06-1041", "citing_paper_abstract": "We demonstrate that a supervised annotation learning approach using structured features derived from tokens and prior annotations performs better than a bag of words approach. We present a general graph representation for automatically deriving these features from labeled data. Automatic feature selection based on class association scores requires a large amount of labeled data and direct voting can be difficult and error-prone for structured features, even for language specialists. We show that highlighted rationales from the user can be used for indirect feature voting and same performance can be achieved with less labeled data.We present our results on two annotation learning tasks for opinion mining from product and movie reviews.", "cited_paper_abstract": "We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system?s error trends.", "citation": "Supervision for simple features has been explored in the literature #OTHEREFR; #REFR.", "context": "Interactive Annotation Learning is a supervised approach to learning annotations with the goal of minimizing the total annotation cost. In this work, we demonstrate that with additional supervision per example, such as distinguishing discriminant features, same performance can be achieved with less annotated data.[Citation]In this work, we propose an approach that seeks supervision from the user on structured features. Features that capture the linguistic structure in text such as n-grams and syntactic patterns, referred to as structured features in this work, have been found to be useful for supervised learning of annotations. For example, Pradhan et al #OTHEREFR show that using features like syntactic path from constituent to predicate improves performance of a semantic parser."}
{"citing_paper_id": "W11-2205", "cited_paper_id": "N06-1041", "citing_paper_abstract": "The development of unsupervised learning methods for natural language processing tasks has become an important and popular area of research. The primary advantage of these methods is that they do not require annotated data to learn a model. However, this advantage makes them difficult to evaluate against a manually labeled gold standard. Using unsupervised part-of-speech tagging as our case study, we discuss the reasons that render this evaluation paradigm unsuitable for the evaluation of unsupervised learning methods. Instead, we argue that the rarely used in-context evaluation is more appropriate and more informative, as it takes into account the way these methods are likely to be applied. Finally, bearing the issue of evaluation in mind, we propose directions for future work in unsupervised natural language processing.", "cited_paper_abstract": "We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system?s error trends.", "citation": "Such approaches include the use of PoS dictionaries by sequential tagging models #REFR, the use of labeled data from different languages #OTHEREFR.", "context": "Finally, in this paper we considered methods whose output consists of state identifiers which are semantically void. However, obtaining meaningful labels such as those found in a gold standard is a useful and important goal in many NLP tasks. However, this purpose is better served by injecting appropriate supervision to the model, instead of trying to achieve it as an afterthought.[Citation]Research in unsupervised learning methods is likely to benefit these partially supervised ones, as they both seek to take advantage of unlabeled data. As the output of such methods uses the same labels as those found in the gold standard, they can be evaluated against a labeled gold standard."}
{"citing_paper_id": "P07-1097", "cited_paper_id": "N06-1049", "citing_paper_abstract": "The idea of ?nugget pyramids? has recently been introduced as a refinement to the nugget-based methodology used to evaluate answers to complex questions in the TREC QA tracks. This paper examines data from the 2006 evaluation, the first large-scale deployment of the nugget pyramids scheme. We show that this method of combining judgments of nugget importance from multiple assessors increases the stability and discriminative power of the evaluation while introducing only a small additional burden in terms of manual assessment. We also consider an alternative method for combining assessor opinions, which yields a distinction similar to microand macro-averaging in the context of classification tasks. While the two approaches differ in terms of underlying assumptions, their results are nevertheless highly correlated.", "cited_paper_abstract": "The present methodology for evaluating complex questions at TREC analyzes answers in terms of facts called ?nuggets?. The official F-score metric represents the harmonic mean between recall and precision at the nugget level. There is an implicit assumption that some facts are more important than others, which is implemented in a binary split between ?vital? and ?okay? nuggets. This distinction holds important implications for the TREC scoring model?essentially, systems only receive credit for retrieving vital nuggets?and is a source of evaluation instability. The upshot is that for many questions in the TREC testsets, the median score across all submitted runs is zero. In this work, we introduce a scoring model based on judgments from multiple assessors that captures a more refined notion of nugget importance. We demonstrate on TREC 2003, 2004, and 2005 data that our ?nugget pyramids? address many shortcomings of the present methodology, while introducing only minimal additional overhead on the evaluation flow.", "citation": "#REFR present experimental evidence in support of nugget pyramids by applying the proposal to results from previous TREC QA evaluations.", "context": "[Citation]Their simulation studies appear to support the assertion that pyramids address many of the issues raised in Section 2.2. Based on the results, NIST proceeded with a trial deployment of nugget pyramids in the TREC 2006 QA track. Although scores based on the binary vital/okay distinction were retained as the ?official? metric, pyramid scores were simultaneously computed."}
{"citing_paper_id": "W10-4117", "cited_paper_id": "N06-1049", "citing_paper_abstract": "This paper investigates techniques to automatically construct training data from social Q&A collections such as Yahoo! Answer to support a machine learningbased complex QA system1. We extract cue expressions for each type of question from collected training data and build question-type-specific classifiers to improve complex QA system. Experiments on 10 types of complex Chinese questions verify that it is effective to mine knowledge from social Q&A collections for answering complex questions, for instance, the F3 improvement of our system over the baseline and translationbased model reaches 7.9% and 5.1%, respectively.", "cited_paper_abstract": "The present methodology for evaluating complex questions at TREC analyzes answers in terms of facts called ?nuggets?. The official F-score metric represents the harmonic mean between recall and precision at the nugget level. There is an implicit assumption that some facts are more important than others, which is implemented in a binary split between ?vital? and ?okay? nuggets. This distinction holds important implications for the TREC scoring model?essentially, systems only receive credit for retrieving vital nuggets?and is a source of evaluation instability. The upshot is that for many questions in the TREC testsets, the median score across all submitted runs is zero. In this work, we introduce a scoring model based on judgments from multiple assessors that captures a more refined notion of nugget importance. We demonstrate on TREC 2003, 2004, and 2005 data that our ?nugget pyramids? address many shortcomings of the present methodology, while introducing only minimal additional overhead on the evaluation flow.", "citation": "The evaluation is conducted by employing Pourpre v1.0c #REFR, which uses the standard scoring methodology for TREC other questions #OTHEREFR, i.e., answer nugget recall NR, nugget precision NP , and a combination score F3 of NR and NP .", "context": "To form a more complete test set, we create another 65 test questions5 . Therefore, the test data used in this paper includes 95 complex questions. For each test question we also provide a list of weighted nuggets, which are used as the gold standard answers for evaluation.[Citation]For better understanding, we evaluate the systems when outputting the top N sentences as answers. 4Because definitional, biography, and relationship questions in the NTCIR 2008 test set are not discussed here. 5The approach of creating test data is same as that in the NTCIR 2008."}
{"citing_paper_id": "W06-1613", "cited_paper_id": "N06-1050", "citing_paper_abstract": "Citation function is defined as the author?s reason for citing a given paper (e.g. acknowledgement of the use of the cited method). The automatic recognition of the rhetorical function of citations in scientific text has many applications, from improvement of impact factor calculations to text summarisation and more informative citation indexers. We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features. We find, amongst other things, a strong relationship between citation function and sentiment classification.", "cited_paper_abstract": "We present an approach to building a test collection of research papers. The approach is based on the Cranfield 2 tests but uses as its vehicle a current conference; research questions and relevance judgements of all cited papers are elicited from conference authors. The resultant test collection is different from TREC?s in that it comprises scientific articles rather than newspaper text and, thus, allows for IR experiments that include citation information. The test collection currently consists of 170 queries with relevance judgements; the document collection is the ACL Anthology. We describe properties of our queries and relevance judgements, and demonstrate the use of the test collection in an experimental setup. One potentially problematic property of our collection is that queries have a low number of relevant documents; we discuss ways of alleviating this.", "citation": "#REFRa) report high accuracy for this task (94% of citations recognised, provided the reference list was error-free).", "context": "The articles are transformed into XML format; headlines, titles, authors and reference list items are automatically marked up. Reference lists are parsed using regular patterns, and cited authors? names are identified. Our citation parser then finds citations and author names in running text and marks them up.[Citation]On average, our papers contain 26.8 citation instances in running text3. For human annotation, we use our own annotation tool based on XML/XSLT technology, which allows us to use a web browser to interactively assign one of the 12 tags (presented as a pull-down list) to each citation. We measure inter-annotator agreement between three annotators (the three authors), who independently annotated 26 articles with the scheme (containing a total of 120,000 running words and 548 citations), using the written guidelines."}
{"citing_paper_id": "P11-1009", "cited_paper_id": "N06-1052", "citing_paper_abstract": "We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining queryproduct associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision.", "cited_paper_abstract": "Language model information retrieval depends on accurate estimation of document models. In this paper, we propose a document expansion technique to deal with the problem of insufficient sampling of documents. We construct a probabilistic neighborhood for each document, and expand the document with its neighborhood information. The expanded document provides a more accurate estimation of the document model, thus improves retrieval accuracy. Moreover, since document expansion and pseudo feedback exploit different corpus structures, they can be combined to further improve performance. The experiment results on several different data sets demonstrate the effectiveness of the proposed document expansion method.", "citation": "More recently, a number of local smoothing models have been proposed #OTHEREFR; #REFR.", "context": "Other methods leverage lower-order background models for low-frequency events, such as Katz? backoff smoothing #OTHEREFR. In the information retrieval community, Ponte and Croft #OTHEREFR are credited for accelerating the use of language models. Initial proposals were based on learning global smoothing models, where the smoothing of a word would be independent of the document that the word belongs to #OTHEREFR.[Citation]Unlike global models, local models leverage relationships between documents in a corpus. In particular, they rely on a graph structure that represents document similarity. Intuitively, the smoothing of a word in a document is influenced by the smoothing of the word in similar documents."}
{"citing_paper_id": "D09-1004", "cited_paper_id": "N06-1055", "citing_paper_abstract": "We present an integrated dependencybased semantic role labeling system for English from both NomBank and Prop- Bank. By introducing assistant argument labels and considering much more feature templates, two optimal feature template sets are obtained through an effective feature selection procedure and help construct a high performance single SRL system. From the evaluations on the date set of CoNLL-2008 shared task, the performance of our system is quite close to the state of the art. As to our knowledge, this is the first integrated SRL system that achieves a competitive performance against previous pipeline systems.", "cited_paper_abstract": "Recent work on semantic role labeling (SRL) has focused almost exclusively on the analysis of the predicate-argument structure of verbs, largely due to the lack of human-annotated resources for other types of predicates that can serve as training and test data for the semantic role labeling systems. However, it is wellknown that verbs are not the only type of predicates that can take arguments. Most notably, nouns that are nominalized forms of verbs and relational nouns generally are also considered to have their own predicate-argument structure. In this paper we report results of SRL experiments on nominalized predicates in Chinese, using a newly completed corpus, the Chinese Nombank. We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement.", "citation": "Though aiming at Chinese SRL, #REFR reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance.", "context": "Though SRL for nominal predicates offers more challenge, it draws relatively little attention #OTHEREFR discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task #OTHEREFR. However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank.[Citation]From the results of CoNLL-2008 shared task, the top system by #OTHEREFR also used two different subsystems to handle verbal and nominal predicates, respectively. Despite all the above facts, an integrated SRL system still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and semantic dependencies, which show that SRL can be well performed using only dependency syntax input."}
{"citing_paper_id": "D08-1082", "cited_paper_id": "N06-1056", "citing_paper_abstract": "In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.", "cited_paper_abstract": "We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.", "citation": "WASP #REFR is a system motivated by statistical machine translation techniques.", "context": "In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. SILT #OTHEREFR learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures.[Citation]It acquires a set of synchronous lexical entries by running the IBM alignment model #OTHEREFR and learns a log-linear model to weight parses. KRISP #OTHEREFR is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels."}
{"citing_paper_id": "N13-1103", "cited_paper_id": "N06-1056", "citing_paper_abstract": "We consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof-the-art semantic parsing baseline, yielding a 29% absolute improvement in accuracy.1", "cited_paper_abstract": "We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.", "citation": "Language Grounding There is a large body of research mapping natural language to some form of meaning representation #OTHEREFR; #REFR.", "context": "Generating Regular Expressions Past work has looked at generating regular expressions from natural language using rule based techniques #OTHEREFR. To the best of our knowledge, however, our work is the first to use training data to learn to automatically generate regular expressions from natural language.[Citation]In some of the considered domains the issue of semantic equivalence does not arise because of the way the data is generated. The most directly related work in these domains, is that by Kwiatkowski et al#OTHEREFR. Similar to our work, Kwiatkowski et alutilize unification to find possible ways to decompose the logical form."}
{"citing_paper_id": "P07-1121", "cited_paper_id": "N06-1056", "citing_paper_abstract": "This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with ?- operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain.", "cited_paper_abstract": "We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.", "citation": "We use the maximum-entropy model proposed in #REFR, which defines a conditional probability distribution over derivations given an observed NL sentence.", "context": "Since the learned ?-SCFG can be ambiguous, a probabilistic model is needed for parse disambiguation.[Citation]The output MR is the yield of the most probable derivation according to this model. Parameter estimation involves maximizing the conditional log-likelihood of the training set. For each rule, r, there is a feature that returns the number of times r is used in a derivation."}
{"citing_paper_id": "D10-1090", "cited_paper_id": "N06-1058", "citing_paper_abstract": "We present PEM, the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments.", "cited_paper_abstract": "This paper studies the impact of paraphrases on the accuracy of automatic evaluation. Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method in the context of machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation. We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.", "citation": "In recent years, there has been an increasing interest in the task of paraphrase generation #OTHEREFR; #REFR.", "context": "[Citation]At the same time, the task has seen applications such as machine translation #OTHEREFR. Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase."}
{"citing_paper_id": "D10-1090", "cited_paper_id": "N06-2009", "citing_paper_abstract": "We present PEM, the first fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, fluency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language N-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that PEM achieves high correlation with human judgments.", "cited_paper_abstract": "State-of-the-art Question Answering (QA) systems are very sensitive to variations in the phrasing of an information need. Finding the preferred language for such a need is a valuable task. We investigate that claim by adopting a simple MT- based paraphrasing technique and evaluating QA system performance on paraphrased questions. We found a potential increase of 35% in MRR with respect to the original question.", "citation": "At the same time, the task has seen applications such as machine translation #OTHEREFRb), and question answering #REFR.", "context": "In recent years, there has been an increasing interest in the task of paraphrase generation #OTHEREFR.[Citation]Despite the research activities, we see two major problems in the field. First, there is currently no consensus on what attributes characterize a good paraphrase. As a result, works on the application of paraphrases tend to build their own PG system in view of the immediate needs instead of using an existing system."}
{"citing_paper_id": "W11-2035", "cited_paper_id": "N06-2010", "citing_paper_abstract": "Within our ongoing effort to develop a computational model to understand multi-modal human dialogue in the field of elderly care, this paper focuses on pronominal and deictic co-reference resolution. After describing our data collection effort, we discuss our annotation scheme. We developed a co-reference model that employs both a simple notion of markable type, and multiple statistical models. Our results show that knowing the type of the markable, and the presence of simultaneous pointing gestures improve co-reference resolution for personal and deictic pronouns.", "cited_paper_abstract": "Coreference resolution, like many problems in natural language processing, has most often been explored using datasets of written text. While spontaneous spoken language poses well-known challenges, it also offers additional modalities that may help disambiguate some of the inherent disfluency. We explore features of hand gesture that are correlated with coreference. Combining these features with a traditional textual model yields a statistically significant improvement in overall performance.", "citation": "Our results are similar to #REFR, but there are two main differences.", "context": "Comparison to previous work is feasible only at a high level, because of the usage of different corpora and/or measurement metrics. This said, our model with gestures outperforms Strube andMu?ller #OTHEREFR, who did not use gesture information to resolve pronouns in spoken dialogue. Strube and Mu?ller #OTHEREFR used the 20 Switchboard dialogues as their experiment dataset, and used the MUC metrics.[Citation]First, the corpus they used is smaller than what we used in this paper. Their corpus was collected by themselves and consisted of 16 videos, each video was 2-3 minutes in length. Second, they used a difference measurement metrics called CEAF #OTHEREFR."}
{"citing_paper_id": "P08-2043", "cited_paper_id": "N06-2031", "citing_paper_abstract": "Cognitive theories of dialogue hold that entrainment, the automatic alignment between dialogue partners at many levels of linguistic representation, is key to facilitating both production and comprehension in dialogue. In this paper we examine novel types of entrainment in two corpora?Switchboard and the Columbia Games corpus. We examine entrainment in use of high-frequency words (the most common words in the corpus), and its association with dialogue naturalness and flow, as well as with task success. Our results show that such entrainment is predictive of the perceived naturalness of dialogues and is significantly correlated with task success; in overall interaction flow, higher degrees of entrainment are associated with more overlaps and fewer interruptions.", "cited_paper_abstract": "Syntactic priming effects, modelled as increase in repetition probability shortly after a use of a syntactic rule, have the potential to improve language processing components. We model priming of syntactic rules in annotated corpora of spoken dialogue, extending previous work that was confined to selected constructions. We find that speakers are more receptive to priming from their interlocutor in task-oriented dialogue than in sponaneous conversation. Low-frequency rules are more likely to show priming.", "citation": "They also alter their amplitude, if the person they are speaking with speaks louder than they do #OTHEREFR, or reuse syntactic constructions employed earlier in the conversation #REFR.", "context": "When people engage in conversation, they adapt the way they speak to their conversational partner. For example, they often adopt a certain way of describing something based upon the way their conversational partner describes it, negotiating a common description, particularly for items that may be unfamiliar to them #OTHEREFR.[Citation]This phenomenon is known in the literature as entrainment, accommodation, adaptation, or alignment. There is a considerable body of literature which posits that entrainment may be crucial to human perception of dialogue success and overall quality, as well as to participants? evaluation of their conversational partners. Pickering and Garrod #OTHEREFR propose that the automatic alignment at many levels of linguistic representation (lexical, syntactic and semantic) is key for both production and comprehension in dialogue, and facilitates interaction."}
{"citing_paper_id": "W11-2214", "cited_paper_id": "N06-4007", "citing_paper_abstract": "Word Sense Induction (WSI) is an unsupervised learning approach to discovering the different senses of a word from its contextual uses. A core challenge to WSI approaches is distinguishing between related and possibly similar senses of a word. Current WSI evaluation techniques have yet to analyze the specific impact of similarity on accuracy. Therefore, we present a new WSI evaluation that quantifies the relationship between the relatedness of a word?s senses and the ability of a WSI algorithm to distinguish between them. Furthermore, we perform an analysis on sense confusions in SemEval-2 WSI task according to sense similarity. Both analyses for a representative selection of clustering-based WSI approaches reveals that performance is most sensitive to the clustering algorithm and not the lexical features used.", "cited_paper_abstract": "SenseClusters is a freely available system that clusters similar contexts. It can be applied to a wide range of problems, although here we focus on word sense and name discrimination. It supports several different measures for automatically determining the number of clusters in which a collection of contexts should be grouped. These can be used to discover the number of senses in which a word is used in a large corpus of text, or the number of entities that share the same name. There are three measures based on clustering criterion functions, and another on the Gap Statistic.", "citation": "We note that this behavior significantly differs from that seen in #REFR, which clustered second-order co-occurrence vectors rather than the first-order features that we use.", "context": "For each clustering algorithm, we see dramatically different trends. Streaming K-Means performs well with co-occurrence based features and it does poorly when either contexts have too many features, as in the 25 Window Co-Occurrence feature space, or the feature space overall is too sparse, as in the Parts of Speech and Ordering feature spaces. K-Means with the gap statistic converges to the most frequent sense baseline for nearly every confounder pair.[Citation]Our analysis showed that the H2 criterion was responsible for this behavior. A subsequent analysis revealed that K-Means still converged to MFS for the E1, E2, I1, and I2 criterion functions #OTHEREFR as well as when the number of artificial datasets was increased up to 100. However, additional tests using the same features on the SemEval-1 WSI task did not converge to MFS."}
{"citing_paper_id": "C10-1018", "cited_paper_id": "N07-1015", "citing_paper_abstract": "Relation extraction is the task of recognizing semantic relations among entities. Given a particular sentence supervised approaches to Relation Extraction employed feature or kernel functions which usually have a single sentence in their scope. The overall aim of this paper is to propose methods for using knowledge and resources that are external to the target sentence, as a way to improve relation extraction. We demonstrate this by exploiting background knowledge such as relationships among the target relations, as well as by considering how target relations relate to some existing knowledge resources. Our methods are general and we suggest that some of them could be applied to other NLP tasks.", "cited_paper_abstract": "Relation extraction is the task of finding semantic relations between entities from text. The state-of-the-art methods for relation extraction are mostly based on statistical learning, and thus all have to deal with feature selection, which can significantly affect the classification performance. In this paper, we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces. We present a general definition of feature spaces based on a graphic representation of relation instances, and explore three different representations of relation instances and features of different complexities within this framework. Our experiments show that using only basic unit features is generally sufficient to achieve state-of-the-art performance, while overinclusion of complex features may hurt the performance. A combination of features of different levels of complexity and from different sentence representations, coupled with task-oriented feature pruning, gives the best performance.", "citation": "Since the dataset has no split of training, development, and test sets, we followed prior work #REFR and performed 5-fold cross validation to obtain our performance results.", "context": "Likewise, our fine-grained classifier has to disambiguate between 47 relation labels. In the dataset, relations do not cross sentence boundaries. For our experiments, we trained regularized averaged perceptrons #OTHEREFR, one for predicting the coarse-grained relations and another for predicting the fine-grained relations.[Citation]For simplicity, we used 5 rounds of training and a regularization parameter of 1.5 for the perceptrons in all our experiments. Finally, we concentrate on the evaluation of fine-grained relations."}
{"citing_paper_id": "W13-2026", "cited_paper_id": "N07-1016", "citing_paper_abstract": "Published literature in molecular genetics may collectively provide much information on gene regulation networks. Dedicated computational approaches are required to sip through large volumes of text and infer gene interactions. We propose a novel sieve-based relation extraction system that uses linear-chain conditional random fields and rules. Also, we introduce a new skip-mention data representation to enable distant relation extraction using first-order models. To account for a variety of relation types, multiple models are inferred. The system was applied to the BioNLP 2013 Gene Regulation Network Shared Task. Our approach was ranked first of five, with a slot error rate of 0.73.", "cited_paper_abstract": "The task of identifying synonymous relations and objects, or Synonym Resolution (SR), is critical for high-quality information extraction. The bulk of previous SR work assumed strong domain knowledge or hand-tagged training examples. This paper investigates SR in the context of unsupervised information extraction, where neither is available. The paper presents a scalable, fully-implemented system for SR that runs in O(KN log N) time in the number of extractions N and the maximum number of synonyms per word, K. The system, called RESOLVER, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. Given two million assertions extracted from the Web, RESOLVER resolves objects with 78% precision and an estimated 68% recall and resolves relations with 90% precision and 35% recall.", "citation": "They first characterized the type of relation appearance in the text according to lexical and syntactic patterns and then trained a CRF using these data along with synonym detection #REFR.", "context": "Additionally, they represented the whole relation descriptor as a single word to use long-range features with a first-order model. We use a similar model but propose a new way of token sequence transformation which discovers the exact relation and not only the descriptor. Banko and Etzioni #OTHEREFR used linear models for the extraction of open relations (i.e. extraction of general relation descriptors without any knowledge about specific target relation type).[Citation]Their method is useful when a few relations in a massive corpus are unknown. However, if higher levels of recall are desired, traditional relation extraction is a better fit. In this study we therefore propose a completely supervised relation extraction method."}
{"citing_paper_id": "N09-1009", "cited_paper_id": "N07-1018", "citing_paper_abstract": "We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus.", "cited_paper_abstract": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.", "citation": "For example, if we make a ?mean-field assumption,? with respect to hidden structure and weights, the variational algorithm for approximately inferring the distribution over ? and trees y resembles the traditional EM algorithm very closely #REFR.", "context": "As stated above, ? comprises a collection of multinomials that weights the grammar. Taking the Bayesian approach, we wish to place a prior on those multinomials, and the Dirichlet family is a natural candidate for such a prior because of its conjugacy, which makes inference algorithms easier to derive.[Citation]In fact, variational inference in this case takes an action similar to smoothing the counts using the exp-? function during the E-step. Variational inference can be embedded in an empirical Bayes setting, in which we optimize the variational bound with respect to the hyperparameters as well, repeating the process until convergence."}
{"citing_paper_id": "N10-1081", "cited_paper_id": "N07-1018", "citing_paper_abstract": "Adaptor grammars extend probabilistic context-free grammars to define prior distributions over trees with ?rich get richer? dynamics. Inference for adaptor grammars seeks to find parse trees for raw text. This paper describes a variational inference algorithm for adaptor grammars, providing an alternative to Markov chain Monte Carlo methods. To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction.", "cited_paper_abstract": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.", "citation": "Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars #OTHEREFR; Toutanova and #REFR.", "context": "[Citation]Such methods have been made more flexible with nonparametric Bayesian #OTHEREFR. One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars #OTHEREFR, in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar."}
{"citing_paper_id": "P13-1077", "cited_paper_id": "N07-1018", "citing_paper_abstract": "Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.", "cited_paper_abstract": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.", "citation": "The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs #REFRb).", "context": "To do so we encode the first term from (2) separately from the second term (corresponding to draws from P1). Summing together these two alternate paths ? i.e., during inside inference ? we recover P2 as shown in (2). The full grammar transform for inside inference is shown in Table 1.[Citation]For each sentencepair, we first decrement the counts associated with its current tree, and then sample a new derivation. This involves first constructing the inside lattice using the productions in Table 1, and then performing a top-down sampling pass. After sampling each derivation from the approximating grammar, we then convert this into its corresponding ITG tree, which we then score with the full model and accept or reject the sample using the 5To support this computation, we track explicit table assignments for every training tree and their component subtrees."}
{"citing_paper_id": "P13-1102", "cited_paper_id": "N07-1018", "citing_paper_abstract": "Probabilistic context-free grammars have the unusual property of not always defining tight distributions (i.e., the sum of the ?probabilities? of the trees the grammar generates can be less than one). This paper reviews how this non-tightness can arise and discusses its impact on Bayesian estimation of PCFGs. We begin by presenting the notion of ?almost everywhere tight grammars? and show that linear CFGs follow it. We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al (2007) are correct under one of them, and provide MCMC samplers for the other two. We conclude with a discussion of the impact of tightness empirically.", "cited_paper_abstract": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.", "citation": "For example, HMMs and many kinds of ?stacked? finite-state machines are equivalent to linear PCFGs, as are the example PCFGs given in #REFR to motivate the MCMC estimation procedures.", "context": "If for any nonterminal A the intersection is not the empty set (with respect to the language that the intersection generates), then the grammar is not linear. Checking whether the intersection is the empty set or not can be done in polynomial time. We conclude this section by remarking that many of the models used in computational linguistics are in fact equivalent to linear PCFGs, so continuous Bayesian priors are almost everywhere tight.[Citation]"}
{"citing_paper_id": "W13-2601", "cited_paper_id": "N07-1018", "citing_paper_abstract": "Cross-linguistic studies on unsupervised word segmentation have consistently shown that English is easier to segment than other languages. In this paper, we propose an explanation of this finding based on the notion of segmentation ambiguity. We show that English has a very low segmentation ambiguity compared to Japanese and that this difference correlates with the segmentation performance in a unigram model. We suggest that segmentation ambiguity is linked to a trade-off between syllable structure complexity and word length distribution.", "cited_paper_abstract": "This paper presents two Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference of probabilistic context free grammars (PCFGs) from terminal strings, providing an alternative to maximum-likelihood estimation using the Inside-Outside algorithm. We illustrate these methods by estimating a sparse grammar describing the morphology of the Bantu language Sesotho, demonstrating that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the Inside-Outside algorithm only produce a trivial grammar.", "citation": "Level 1 generally tends to over-segment utterances as can be seen by comparing the Boundary Recall and Precision scores #REFR.", "context": "Consequently, we evaluate the segmentation that is the result of taking there to be a boundary between every level1 constituent (Level 1 in Table 5) and between every level2 constituent (Level 2 in Table 5 ). From these results , we see that English data has better scores when the lower level represents the Word unit and when the higher level captures regularities above the word. However, Japanese data is best segmented when the higher level is the Word unit and the lower level captures sub-word regularities.[Citation]In fact when the Recall is much higher than the Precision, we can say that the model has a tendency to over-segment. Conversely, we see that Level 2 tends to under-segment utterances as the Boundary Precision is higher than the Recall. Over-segmentation at Level 1 seems to benefit English since it counteracts the tendency of the Unigram model to cluster high frequency collocations."}
{"citing_paper_id": "D09-1062", "cited_paper_id": "N07-1030", "citing_paper_abstract": "Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. There are a number of such lexical resources available, but it is often suboptimal to use them as is, because general purpose lexical resources do not reflect domain-specific lexical usage. In this paper, we propose a novel method based on integer linear programming that can adapt an existing lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification.", "cited_paper_abstract": "Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.", "citation": "We formulate our lexicon adaptation task using integer linear programming #OTHEREFR, #REFR).", "context": "That is, if we are not sure about the polarity of a certain word, we can still make a guess based on the polarities of other words within the same expression and knowledge of the polarity of the expression. The second type of relations are word-to-expression relations: e.g., some words appear in expressions that take on a variety of polarities, while other words are associated with expressions of one polarity class or another. In relation to previous research, analyzing word-to-word #OTHEREFR).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one.[Citation]And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately, one class of constraint that we would like to encode (see Section 2) will require an exponentially many number of constraints when grounded into an actual ILP problem. We therefore propose an approximation scheme to make the problem more practically solvable."}
{"citing_paper_id": "D12-1068", "cited_paper_id": "N07-1030", "citing_paper_abstract": "We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-specified objective measure; (2) it can make document-specific prediction rather than rely on a fixed base model or a fixed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simplification, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the first to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the benefits of (i) on the six domains of the ACE 2005 data set in domain adaptation setting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting.", "cited_paper_abstract": "Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.", "citation": "In the past decade, several coreference resolution systems have been proposed, e.g., #OTHEREFR, #REFR and #OTHEREFR.", "context": "Coreference resolution is a fundamental component of natural language processing #OTHEREFR. It gathers together noun phrases #OTHEREFR.[Citation]All of these focus on the within-domain case ? to use the labeled documents from a domain to predict on the unlabeled ?The work is done during postdoc in NTU, Singapore. documents in the same domain. However, in practice, there is usually limited labeled data in a specific domain of interest, while there may be plenty of labeled data in other related domains. Effective use of data from the other domains for predicting in the domain of interest is therefore an important strategy in NLP."}
{"citing_paper_id": "P08-2012", "cited_paper_id": "N07-1030", "citing_paper_abstract": "A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b3 scorer, and up to 16.5% using cluster f-measure.", "cited_paper_abstract": "Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides fscore improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.", "citation": "When describing our model, we build upon the notation used by #REFR.", "context": "For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity.[Citation]"}
{"citing_paper_id": "D10-1037", "cited_paper_id": "N07-1038", "citing_paper_abstract": "In this paper, we investigate how modeling content structure can benefit text analysis applications such as extractive summarization and sentiment analysis. This follows the linguistic intuition that rich contextual information should be useful in these tasks. We present a framework which combines a supervised text analysis application with the induction of latent content structure. Both of these elements are learned jointly using the EM algorithm. The induced content structure is learned from a large unannotated corpus and biased by the underlying text analysis task. We demonstrate that exploiting content structure yields significant improvements over approaches that rely only on local context.1", "cited_paper_abstract": "We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model.", "citation": "In a multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates #REFR.", "context": "Finally, we?ve got a deleted scenes reel. Most of the excised scenes are actually pretty interesting. Table 1: An excerpt from a DVD review. instance, when performing single-aspect sentiment analysis, the most relevant aspect of content structure is whether a given sentence is objective or subjective #OTHEREFR.[Citation]As we can see from even these closely related applications, the content structure representation should be intimately tied to a specific text analysis task. In this work, we present an approach in which a content model is learned jointly with a text analysis task. We assume complete annotations for the task itself, but we learn the content model from raw, unannotated text."}
{"citing_paper_id": "D12-1129", "cited_paper_id": "N07-1044", "citing_paper_abstract": "We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and fine-grained level.", "cited_paper_abstract": "In word sense disambiguation, choosing the most frequent sense for an ambiguous word is a powerful heuristic. However, its usefulness is restricted by the availability of sense-annotated data. In this paper, we propose an information retrieval-based method for sense ranking that does not require annotated data. The method queries an information retrieval engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance.We also show that the estimated sense frequencies correlate reliably with native speakers? intuitions.", "citation": "Other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an existing thesaurus #OTHEREFR, and synonym-based word occurrence counts #REFR.", "context": "Domain WSD has been the focus of much interest in the last few years. An important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of a target word by means of a semantic similarity measure #OTHEREFR.[Citation]Domain-informed methods have also been proposed which make use of domain labels as cues for disambiguation purposes #OTHEREFR. Domain-driven approaches have been shown to obtain the best performance among the unsupervised alternatives #OTHEREFR. However, their performance is typically lower than supervised systems."}
{"citing_paper_id": "C10-2168", "cited_paper_id": "N07-1051", "citing_paper_abstract": "Given the increasing need to process massive amounts of textual data, efficiency of NLP tools is becoming a pressing concern. Parsers based on lexicalised grammar formalisms, such as TAG and CCG, can be made more efficient using supertagging, which for CCG is so effective that every derivation consistent with the supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning.", "cited_paper_abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar?s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.", "citation": "Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second #OTHEREFR; #REFR.", "context": "TextRunner, for example, is a system that performs open information extraction on the web #OTHEREFR. However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: finite-state shallow parsing technology that is now decades old. TextRunner uses this technology largely for efficiency reasons.[Citation]There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process #OTHEREFR. In this paper we focus on the Combinatory Categorial Grammar #OTHEREFR. One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g."}
{"citing_paper_id": "W11-1011", "cited_paper_id": "N07-1051", "citing_paper_abstract": "We consider SCFG-basedMT systems that get syntactic category labels from parsing both the source and target sides of parallel training data. The resulting joint nonterminals often lead to needlessly large label sets that are not optimized for an MT scenario. This paper presents a method of iteratively coarsening a label set for a particular language pair and training corpus. We apply this label collapsing on Chinese?English and French?English grammars, obtaining test-set improvements of up to 2.8 BLEU, 5.2 TER, and 0.9 METEOR on Chinese?English translation. An analysis of label collapsing?s effect on the grammar and the decoding process is also given.", "cited_paper_abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar?s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.", "citation": "To obtain parse trees over both sides of each parallel corpus, we used the English and Chinese grammars of the Berkeley parser #REFR.", "context": "Experiments are conducted on Chinese-to-English translation using approximately 300,000 sentence pairs from the FBIS corpus.[Citation]Given a parsed and word-aligned parallel sentence, we extract SCFG rules from it following the procedure of Lavie et al #OTHEREFR. The method first identifies node alignments between the two parse trees according to support from the word alignments. A node in the source parse tree will be aligned to a node in the target parse tree if all the words in the yield of the source node are either all aligned to words within the yield of the target node or have no alignments at all."}
{"citing_paper_id": "W12-1706", "cited_paper_id": "N07-1051", "citing_paper_abstract": "Experimental evidence demonstrates that syntactic structure influences human online sentence processing behavior. Despite this evidence, open questions remain: which type of syntactic structure best explains observed behavior?hierarchical or sequential, and lexicalized or unlexicalized. Recently, Frank and Bod (2011) find that unlexicalized sequential models predict reading times better than unlexicalized hierarchical models, relative to a baseline prediction model that takes wordlevel factors into account. They conclude that the human parser is insensitive to hierarchical syntactic structure. We investigate these claims and find a picture more complicated than the one they present. First, we show that incorporating additional lexical n-gram probabilities estimated from several different corpora into the baseline model of Frank and Bod (2011) eliminates all differences in accuracy between those unlexicalized sequential and hierarchical models. Second, we show that lexicalizing the hierarchical models used in Frank and Bod (2011) significantly improves prediction accuracy relative to the unlexicalized versions. Third, we show that using stateof-the-art lexicalized hierarchical models further improves prediction accuracy. Our results demonstrate that the claim of Frank and Bod (2011) that sequential models predict reading times better than hierarchical models is premature, and also that lexicalization matters for prediction accuracy.", "cited_paper_abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar?s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.", "citation": "We also include the state-of-the-art Berkeley grammar #REFR in our comparison.", "context": "Table 1: Contexts and events used to produce surprisal measures under various probabilistic syntactic models. T refers to trees; t refers to POS tags; and w refers to words. dition rule expansions on the current head node2. In addition to the grammars over POS tag sequences used by Frank and Bod #OTHEREFR, we evaluate PSG?s over word sequences.[Citation]Syntactic categories in the Berkeley grammar are automatically split into fine-grained subcategories to improve the likelihood of the training corpus under the model. This increased expressivity allows the parser to achieve state-of-the-art automatic parsing accuracy, but increases grammar size considerably.3"}
{"citing_paper_id": "W13-3606", "cited_paper_id": "N07-1051", "citing_paper_abstract": "We present an approach to grammatical error correction for the CoNLL 2013 shared task based on a weighted tree-to-string transducer. Rules for the transducer are extracted from the NUCLE training data. An n-gram language model is used to rerank k-best sentence lists generated by the transducer. Our system obtains a precision, recall and F1 score of 0.27, 0.1333 and 0.1785, respectively, on the official test set. On the revised annotations, the F1 score increases to 0.2505. Our system ranked 6th out of the participating teams on both the original and revised test set annotations.", "cited_paper_abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar?s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.", "citation": "In order to train a syntax-based model for grammar correction, the correct version of the sentences are parsed with the Berkeley parser #REFR.", "context": "An alternative would be to apply the corrections of other error types to the correct and incorrect versions of the sentences. However, we decided against that in order to keep the training data realistically close to the test data, which will also contain these other errors. We do, however, correct some of the mechanical errors, especially spelling errors, in the incorrect and correct versions of the training data, to reduce noise that these errors may introduce into the model.[Citation]The Berkeley parser is a stateof-the-art unlexicalized parser. Given that the correct side of our training data will still contain errors, it is unlikely that lexicalized parsing will be more accurate. Parser options are set to obtain leftbinarized parse trees under Viterbi decoding."}
{"citing_paper_id": "W14-3324", "cited_paper_id": "N07-1051", "citing_paper_abstract": "This paper describes the string-to-tree systems built at the University of Edinburgh for the WMT 2014 shared translation task. We developed systems for English-German, Czech-English, French- English, German-English, Hindi-English, and Russian-English. This year we improved our English-German system through target-side compound splitting, morphosyntactic constraints, and refinements to parse tree annotation; we addressed the out-of-vocabulary problem using transliteration for Hindi and Russian and using morphological reduction for Russian; we improved our German- English system through tree binarization; and we reduced system development time by filtering the tuning sets.", "cited_paper_abstract": "We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar?s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning.", "citation": "For the systems that translate into English, we used the Berkeley parser #OTHEREFR; #REFR to parse the target-side of the training corpus.", "context": "The training data was normalized using the WMT normalize-punctuation.perl script then tokenized and truecased. Where the target language was English, we used the Moses tokenizer?s -penn option, which uses a tokenization scheme that more closely matches that of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers.[Citation]As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence."}
{"citing_paper_id": "P07-1092", "cited_paper_id": "N07-1061", "citing_paper_abstract": "Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation estimates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system.", "cited_paper_abstract": "We compare two pivot strategies for phrase-based statistical machine translation (SMT), namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a phrase translation table (phrase-table) of the source and target language pair from two phrase-tables; one constructed from the source language and English and one constructed from English and the target language. We then use that phrase-table in a phrase-based SMT system. The sentence translation strategy means that we first translate a source language sentence into n English sentences and then translate these n sentences into target language sentences separately. Then, we select the highest scoring sentence from these target sentences. We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems.", "citation": "A single translation is then selected by finding the candidate that yields the best overall score #OTHEREFR; #REFR or by cotraining #OTHEREFR.", "context": "The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay #OTHEREFR, who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages.[Citation]This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques #OTHEREFR to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval #OTHEREFR. Callison-Burch et al #OTHEREFR propose the use of paraphrases as a means of dealing with unseen source phrases."}
{"citing_paper_id": "E09-1061", "cited_paper_id": "N07-1063", "citing_paper_abstract": "We present a unified view of many translation algorithms that synthesizes work on deductive parsing, semiring parsing, and efficient approximate search algorithms. This gives rise to clean analyses and compact descriptions that can serve as the basis for modular implementations. We illustrate this with several examples, showing how to build search spaces for several disparate phrase-based search strategies, integrate non-local features, and devise novel models. Although the framework is drawn from parsing and applied to translation, it is applicable to many dynamic programming problems arising in natural language processing and other areas.", "cited_paper_abstract": "We present an efficient, novel two-pass approach to mitigate the computational impact resulting from online intersection of an n-gram language model (LM) and a probabilistic synchronous context-free grammar (PSCFG) for statistical machine translation. In first pass CYK-style decoding, we consider first-best chart item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate specific alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the first pass. Model search errors in our approach are comparable to those made by the state-of-the-art ?Cube Pruning? approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars.", "citation": "Syntax-based translation logics are similar to parsing logics; a few examples already appear in the literature #OTHEREFR; #REFR.", "context": "For example, MDd is more complex than WLd, and we expect implementations of the former to require more pruning and suffer from more search errors, while the latter would suffer from more model errors since its space of possible reorderings is smaller. We emphasize that many other translation models can be described this way. Logics for the IBM Models #OTHEREFR would be similar to our logics for phrase-based models.[Citation]For simplicity, we will use the MONOTONE logic for the remainder of our examples, but all of them generalize to more complex logics."}
{"citing_paper_id": "E09-1060", "cited_paper_id": "N07-2027", "citing_paper_abstract": "The quality of the part-of-speech (PoS) annotation in a corpus is crucial for the development of PoS taggers. In this paper, we experiment with three complementary methods for automatically detecting errors in the PoS annotation for the Icelandic Frequency Dictionary corpus. The first two methods are language independent and we argue that the third method can be adapted to other morphologically complex languages. Once possible errors have been detected, we examine each error candidate and hand-correct the corresponding PoS tag if necessary. Overall, based on the three methods, we handcorrect the PoS tagging of 1,334 tokens (0.23% of the tokens) in the corpus. Furthermore, we re-evaluate existing state-ofthe-art PoS taggers on Icelandic text using the corrected corpus.", "cited_paper_abstract": "We describe our linguistic rule-based tagger IceTagger, and compare its tagging accuracy to the TnT tagger, a state-of-theart statistical tagger, when tagging Icelandic, a morphologically complex language. Evaluation shows that the average tagging accuracy is 91.54% and 90.44%, obtained by IceTagger and TnT, respectively. When tag profile gaps in the lexicon, used by the TnT tagger, are filled with tags produced by our morphological analyser IceMorphy, TnT?s tagging accuracy increases to 91.18%.", "citation": "Indeed, we have previously shown that when five taggers all agree on a tag in the IFD corpus, the corresponding accuracy is 98.9% #REFRb).", "context": "It is well known that a combined tagger usually obtains higher accuracy than individual taggers in the combination pool. For example, by using simple voting #OTHEREFR. Moreover, if all the taggers in the pool agree on a vote, one would expect the tagging accuracy for the respective words to be high.[Citation]For the remaining 1.1% tokens, one would expect that the five taggers are actually correct in some of the cases, but the gold standard incorrectly annotated. In general, both the precision and the recall should be higher when relying on five agreeing taggers as compared to using only a single tagger. Thus, we used the five taggers, MBL #OTHEREFR, but with the following minor changes."}
{"citing_paper_id": "P08-1073", "cited_paper_id": "N07-2038", "citing_paper_abstract": "We address two problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and evaluating the result with real users. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is ?bootstrapped? from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows development of optimal strategies for domains where no working prototype is available. We compare the RL-based strategy against a supervised strategy which mimics the wizards? policies. This comparison allows us to measure relative improvement over the training data. Our results show that RL significantly outperforms Supervised Learning when interacting in simulation as well as for interactions with real users. The RL-based policy gains on average 50-times more reward when tested in simulation, and almost 18-times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher.", "cited_paper_abstract": "This paper investigates the problem of bootstrapping a statistical dialogue manager without access to training data and proposes a new probabilistic agenda-based method for simulating user behaviour. In experiments with a statistical POMDP dialogue system, the simulator was realistic enough to successfully test the prototype system and train a dialogue policy. An extensive study with human subjects showed that the learned policy was highly competitive, with task completion rates above 90%.", "citation": "While both studies show promising first results, their simulated environment still contains many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. #REFR propose to ?bootstrap? with a simulated user which is entirely hand-crafted.", "context": "We therefore need to show that a strategy bootstrapped from WOZ data indeed transfers to real HCI. Furthermore, we also need to introduce methods to learn useful user simulations (for training RL) from such limited data. The use of WOZ data has earlier been proposed in the context of RL. #OTHEREFR use WOZ data to build a simulated user and noise model for simulation-based RL.[Citation]In the following we propose an entirely data-driven approach, where all components of the simulated learning environment are learned from WOZ data. We also show that the resulting policy performs well for real users."}
{"citing_paper_id": "W11-0311", "cited_paper_id": "N09-1002", "citing_paper_abstract": "Subjectivity word sense disambiguation (SWSD) is automatically determining which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD has been shown to improve the performance of contextual opinion analysis, but only on a small scale and using manually developed integration rules. In this paper, we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis.", "cited_paper_abstract": "This paper introduces an integrative approach to automatic word sense subjectivity annotation. We use features that exploit the hierarchical structure and domain information in lexical resources such as WordNet, as well as other types of features that measure the similarity of glosses and the overlap among sets of semantically related words. Integrated in a machine learning framework, the entire set of features is found to give better results than any individual type of feature.", "citation": "As discussed in Section 3, #REFR created a subjecvitivity sensetagged corpus piggybacked on SENSEVAL.", "context": "Thus, we decided to compare the performance of a SWSD system trained on nonexpert annotations and on expert annotations. For this purpose, we need a subjectivity sense-tagged corpus where word instances are tagged both by expert and non-expert annotations. Fortunately, we have such a corpus.[Citation]This gives us a gold-standard corpus tagged by experts. There is also a small subjectivity sense-tagged corpus consisting of eight target words obtained from non-expert annotators in #OTHEREFR. This corpus is a subset of the gold-standard corpus from #OTHEREFR and it consists of 60 tagged Acc p-value SWSDGOLD 79.2 - SWSDMJL 78.4 0.542 SWSDMJC 78.8 0.754"}
{"citing_paper_id": "N13-1120", "cited_paper_id": "N09-1003", "citing_paper_abstract": "In this work, we study the problem of measuring relational similarity between two word pairs (e.g., silverware:fork and clothing:shirt). Due to the large number of possible relations, we argue that it is important to combine multiple models based on heterogeneous information sources. Our overall system consists of two novel general-purpose relational similarity models and three specific word relation models. When evaluated in the setting of a recently proposed SemEval-2012 task, our approach outperforms the previous best system substantially, achieving a 54.1% relative increase in Spearman?s rank correlation.", "cited_paper_abstract": "This paper presents and compares WordNetbased and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.", "citation": "Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work #OTHEREFR; #REFR, measuring relational similarity is a relatively new research direction pioneered by Turney #OTHEREFR, but with many potential applications.", "context": "The problem of measuring relational similarity is to determine the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity.[Citation]For instance, problems of identifying specific relations between words, such as synonyms, ?Work conducted while interning at Microsoft Research. antonyms or associations, can be reduced to measuring relational similarity compared to prototypical word pairs with the desired relation #OTHEREFR. In scenarios like information extraction or question answering, where identifying the existence of certain relations is often the core problem, measuring relational similarity provides a more flexible solution rather than creating relational classifiers for predefined or task-specific categories of relations #OTHEREFR. In order to promote this research direction, Jurgens et al#OTHEREFR recently."}
